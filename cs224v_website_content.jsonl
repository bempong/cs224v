{"title": "About CS 224V", "content": "Generative AI, and in particular Large Language Models (LLMs) such as GPT-4, has already changed how we\n            work and study. But this is just the beginning, as it has the potential of assisting and perhaps eventually\n            automating knowledge workers in all areas, from law, medicine, to teaching and mental health therapists.\n            This course will focus on the general principles and the latest research on methodologies and tools that can\n            be applied to all domains. This is a project-oriented course, where students will gain hands-on experience\n            in either methodology research or applying the concepts to create useful assistants for a domain of their\n            choice.\n\n           Topics include:\n\nGrowing LLMs' knowledge through a combination of manual supervised learning and self-learning.", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "Topics include:\n\nGrowing LLMs' knowledge through a combination of manual supervised learning and self-learning.\n\n Stopping LLMs from hallucination by grounding them with external corpora of knowledge, which is\n              necessary for handling new, live, private as well as long-tail data.\nHandling external data corpora in different domains including structured and unstructured data.", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "Experimentation and evaluation of conversational assistants based on LLMs.\nControlling LLMs to achieve tasks.\nPersuasive LLMs.\nMultilingual assistants.\nCombining voice and graphical interfaces.\nTopics include:\nGrowing LLMs' knowledge through a combination of manual supervised learning and self-learning.\nStopping LLMs from hallucination by grounding them with external corpora of knowledge, which is\n              necessary for handling new, live, private as well as long-tail data.\nHandling external data corpora in different domains including structured and unstructured data.\nExperimentation and evaluation of conversational assistants based on LLMs.\nControlling LLMs to achieve tasks.\nPersuasive LLMs.\nMultilingual assistants.\nCombining voice and graphical interfaces.\nHomeworks:\nThere will be two homeworks at the beginning of the quarter, where students will create hallucination-free", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "Multilingual assistants.\nCombining voice and graphical interfaces.\nHomeworks:\nThere will be two homeworks at the beginning of the quarter, where students will create hallucination-free\n            LLM-based agents using given tools. Students can build upon this work for their project in the rest of the\n            quarter. Homework will be done in groups of 2.\nlam at cs\nharshitj at cs\nparnoud\nembunna\nadamchun\nsyfeng\nshl1027\nshiyuz\nLectures: Monday/Wednesday 3:00-4:20pm in person in Gates B1. Attendance is mandatory.\nRecordings: video recordings of the lecture can be found on Canvas.\nSlides: can be found on the Schedule and in the lecture slides folder on\n          Canvas. Posted lecture slides are missing important details to facilitate student participation. Please make sure you watch the lectures.\nHomework: can be found on the Schedule and submissions will be on Gradescope.", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "Homework: can be found on the Schedule and submissions will be on Gradescope.\nContact: Students should ask all course-related questions on Ed, where you will also find announcements. For external\n          inquiries, personal matters, or in emergencies, you can send an email to our staff email\n          cs224v-aut2425-staff@lists.stanford.edu\nAcademic accommodations:  If you need an academic accommodation based on a disability, you should\n          initiate the request with the Office of Accessible Education (OAE). The OAE will evaluate the request,\n          recommend accommodations, and prepare a letter for the teaching staff. Once you receive the letter, send it\n          to the course staff email at cs224v-aut2425-staff@lists.stanford.edu. Students should contact the OAE\n          as soon as possible since timely notice is needed to coordinate accommodations.\nAudit Requests: To audit the course, please send an email to course staff email at", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "as soon as possible since timely notice is needed to coordinate accommodations.\nAudit Requests: To audit the course, please send an email to course staff email at\n          cs224v-aut2425-staff@lists.stanford.edu, with the subject line \"audit cs224v request.\"\nCourse Participation: We offer the course on SCPD to serve remote students; it is not to allow students take conflicting courses.\n          In-class attendance and participation are an important part of the course. We allocate 15% of the course grade to class participation, which is important to make the most out of the\n          course.\n          \nIf you are a local student, 5% of the course grade is allocated to in-class attendance and participation.\nWe allocate 10% and 15% of the grade to local and remote students, respectively, to (1) your weekend updates, and (2) interaction with your project mentor every week.\nContributions in helping others on Ed will be awarded with bonus points.", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "Contributions in helping others on Ed will be awarded with bonus points.\nIf you are a local student, 5% of the course grade is allocated to in-class attendance and participation.\nWe allocate 10% and 15% of the grade to local and remote students, respectively, to (1) your weekend updates, and (2) interaction with your project mentor every week.\nContributions in helping others on Ed will be awarded with bonus points.\nPrerequisites: one of LINGUIST 180/280, CS 124, CS 224N, CS 224S, 224U.\nUpdate (9/20): CS224V has limited enrollment so as to provide adequate project/research supervision to\n        students. At this point, we have given out the remaining spots to a few more students who need to take this\n        course or have demonstrated that they need little project supervision. We anticipate that only a few spots may\n        open up due to attrition; if you have not heard from us, it is unlikely that we can accommodate you. We", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "About CS 224V", "content": "open up due to attrition; if you have not heard from us, it is unlikely that we can accommodate you. We\n        apologize that we cannot accommodate all the students wishing to take the course this year.\nGrading is according to the following scheme:\n      \nParticipation: 15% (see above in logistics)\nHomeworks: 20%\nFinal Project: 65%\nParticipation: 15% (see above in logistics)\nHomeworks: 20%\nFinal Project: 65%", "url": "https://web.stanford.edu/class/cs224v/", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:06.254994"}
{"title": "Cs224V Readings", "content": "[Page 1]\nCS224V: Conversational Virtual Assistants with Deep Learning\nReading List\nFall 2024\n1 Large Language Models (LLMs)\nIntroduction\n1.1 Pretraining LLMs\n1.Attention (Vaswani et al., 2017) [Mandatory]\n2. GPT-3 (Brown et al., 2020)\n3. Instruct-GPT (Ouyang et al., 2022)\n4. LLAMA (Touvron et al., 2023a)\n5. Alpaca (Taori et al., 2023)\n6.Alpaca with Self-Instruct (Wang et al., 2023e)\n7. LLAMA-2 (Touvron et al., 2023b)\n1.2 Prompting LLMs\n1. Chain-of-thought (Wei et al., 2023)\n2. Self-consistency (Wang et al., 2023d)\n3.The Prompt Report: A Systematic Survey of\nPrompting Techniques (Schulhoff et al., 2024)\n1.3 Tool Use\n1. ToolFormer (Schick et al., 2023)\n2.ART: Multi-step tool use (Paranjape et al.,\n2023)\n3. Gorilla LM (Patil et al., 2023) [Mandatory]\n4. ToolAlpaca (Tang et al., 2023)\n2 Knowledge Curation\n1.STORM: writing Wikipedia-like articles from\nscratch (Shao et al., 2024) [Mandatory]\n2.Co-STORM: collaborative STORM (Jiang\net al., 2024) [Mandatory]\n3.Expository Text Generation: (Balepur et al.,\n2023)3 Question-Answering on Free Text\n3.1 Neural Information Retrieval Models\nNon-neural algorithms: TF-IDF and BM-25.\nPopular neural retrieval systems:\n1.ColBERT (Khattab and Zaharia, 2020)\n[Mandatory]\n2. Condenser (Gao and Callan, 2021)\n3. CoCondenser (Gao and Callan, 2022)\n4. CoCo-DR (Yu et al., 2022)\n5. M3-Embedding (Chen et al., 2024)\n6. RankGPT (Sun et al., 2023)\n3.2 Retrieval + Generation\n1.WikiChat: nearly hallucination-free Q&A\ngrounded on Wikipedia. (Best Research of", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Mandatory]\n2. Condenser (Gao and Callan, 2021)\n3. CoCondenser (Gao and Callan, 2022)\n4. CoCo-DR (Yu et al., 2022)\n5. M3-Embedding (Chen et al., 2024)\n6. RankGPT (Sun et al., 2023)\n3.2 Retrieval + Generation\n1.WikiChat: nearly hallucination-free Q&A\ngrounded on Wikipedia. (Best Research of\nthe Year, Wikimedia, 2024).(Semnani et al.,\n2023) [Mandatory]\n2. Citation generation (Gao et al., 2023)\n3.Active retrieval augmented generation (Jiang\net al., 2023)\n3.3 Evaluation\n1.Evaluating Verifiability in Generative Search\nEngines (Liu et al., 2023a)\n2.Generating Benchmarks for Factuality Evalu-\nation (Muhlgay et al., 2023)\n3.Evaluating Open-Domain Question Answer-\ning in the Era of Large Language Models (Ka-\nmalloo et al., 2023)", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Page 2]\n4 Question Answering on Databases,\nKnowledge Graphs, and Heterogeneous\nsources\n1.SUQL: Structured and Unstructured Query\nLanguage (Liu et al., 2024b) [Mandatory]\n2.Spinach: SPARQL-based Agent using an\nAgentic Approach (Liu et al., 2024a) [Manda-\ntory]\n3.GraphReader: Builds a graph from text and\nanswers questions by navigating the graph (Li\net al., 2024)\n4.SPAGHETTI: SOTA for Compmix: WikiData,\nWikipedia text and tables (Zhang et al., 2024)\n[Mandatory]\n5.Compmix: a heterogeneous data set with\nWikiData and Wikipedia\n6.BIRD: Text-to-SQL benchmark for LLMs (Li\net al., 2023a) (Christmann et al., 2023)\n7.ReFinED: Named Entity Disambiguation\n(NED) (Ayoola et al., 2022)\n5 Task-Oriented Dialogue (ToD) Agents\n5.1 English Agents\n1.Reliable LLM-Based Conversational Agents\n(Joshi et al., 2024) [Mandatory]\n2.MultiWOZ: the classic ToD dataset\n(Budzianowski et al., 2018)\n3.ToD Agent Architecture (Campagna et al.,\n2022)\n5.2 Multi-lingual Agents\n1.Prompt-based LLM agent outperforms fine-\ntuning on X-RiSAWOZ, if evaluated prop-\nerly(Lee et al., 2024)\n2.RiSAWOZ: a well-annotated ToD dataset\n(Chinese) (Quan et al., 2020)\n3.X-RiSAWOZ: a multilingual version of Ri-\nSAWOZ (Moradshahi et al., 2023) [Manda-\ntory]6 Social Agents\n1.Zero-shot Persuasive Chatbots with LLM-\nGenerated Strategies and Information Re-\ntrieval (Furumai et al., 2024) [Mandatory]\n2.Controllable mixed-initiative dialogue genera-\ntion through prompting (Chen et al., 2023)\n3.Social Influence Dialogue Systems: A Survey", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "tory]6 Social Agents\n1.Zero-shot Persuasive Chatbots with LLM-\nGenerated Strategies and Information Re-\ntrieval (Furumai et al., 2024) [Mandatory]\n2.Controllable mixed-initiative dialogue genera-\ntion through prompting (Chen et al., 2023)\n3.Social Influence Dialogue Systems: A Survey\nof Datasets and Models For Social Influence\nTasks (Chawla et al., 2023)\n4.Persuasion for Good: Towards a Personalized\nPersuasive Dialogue System for Social Good\n(Wang et al., 2019)\n5. Cardinal Chirpy (Chi et al., 2021)\n6. Blenderbot (Shuster et al., 2022)\n7 Learning Agents\nComplex tasks require LLMs to refine them-\nselves. Here are three use cases.\n7.1 Qualitative Coding\n1. GoLLIE (Sainz et al., 2023) [Mandatory]\n2.Code4Struct: Code Generation for Few-\nShot Event Structure Prediction (Wang et al.,\n2023c)\n3. Document-Level Event Argument Extraction\nby Conditional Generation (Li et al., 2021)\n4. InstructUIE (Wang et al., 2023b)\n7.2 Web Agents\n1.A web agent for universal tasks by remem-\nbering experiences in memory (Wang et al.,\n2024)\n2.SeeAct: Vision web agents work well if we\nsolve grounding on visual features (Zheng\net al., 2024)\n3.Russ: Grounding Open-Domain Instructions\nto Automate Web Support Tasks (Xu et al.,\n2021)\n4.Mind2Web: Towards a Generalist Agent for\nthe Web (Deng et al., 2024) [Mandatory]\n5.DIY assistant: a multi-modal end-user pro-\ngrammable virtual assistant (Fischer et al.,\n2021)", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Page 3]\n7.3 Multimodal apps\n1.ReactGenie: Adding a few lines to React pro-\ngrams for rich multimodal interactions (Yang\net al., 2024b) [Mandatory]\n2.GenieWizard: Using LLMs to define multi-\nmodal features (Yang et al., 2024a)\n8 Formal Reasoning - Theorem Proving\n1.SATLM: Translate NL into SAT (satisfiability)\nformulae and solve with theorem proving. (Ye\net al., 2024)\n2.Access control using SMT (satisfiability mod-\nulo theory) (Campagna et al., 2018)\n9 Advanced Topics in LLMs\n9.1 Distillation of LLMs\n1.Chain-of-Thought distillation (Li et al.,\n2023b)\n2.SCOTT: Self-consistent Chain-of-Thought\ndistillation (Wang et al., 2023a)\n3.Symbolic Commonsense Knowledge Distilla-\ntion (West et al., 2022)\n4.Knowledge Distillation of Large Language\nModels (Gu et al., 2023)\n5. Self-Refine (Madaan et al., 2023)\n9.2 Evaluation of LLMs\n1. HELM (Liang et al., 2022),\n2.Repairing the Cracked Foundation: A Survey\nof Obstacles in Evaluation Practices for Gen-\nerated Text (Gehrmann et al., 2022)\n3.Judging LLM-as-a-judge with MT-Bench and\nChatbot Arena (Zheng et al., 2023)\n4.G-Eval: NLG Evaluation using GPT-4 with\nBetter Human Alignment (Liu et al., 2023b)\nReferences\nTom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos\nChristodoulopoulos, and Andrea Pierleoni. 2022. Re-\nFinED: An efficient zero-shot-capable approach to\nend-to-end entity linking. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "Christodoulopoulos, and Andrea Pierleoni. 2022. Re-\nFinED: An efficient zero-shot-capable approach to\nend-to-end entity linking. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies: Industry Track , pages 209\u2013\n220, Hybrid: Seattle, Washington + Online. Associa-\ntion for Computational Linguistics.Nishant Balepur, Jie Huang, and Kevin Chang. 2023.\nExpository text generation: Imitate, retrieve, para-\nphrase. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 11896\u201311919, Singapore. Association for\nComputational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Ultes Stefan, Ramadan Os-\nman, and Milica Ga\u0161i \u00b4c. 2018. MultiWOZ - a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Ultes Stefan, Ramadan Os-\nman, and Milica Ga\u0161i \u00b4c. 2018. MultiWOZ - a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) .\nGiovanni Campagna, Sina Semnani, Ryan Kearns, Lu-\ncas Jun Koba Sato, Silei Xu, and Monica Lam. 2022.\nA few-shot semantic parser for Wizard-of-Oz dia-\nlogues with the precise ThingTalk representation. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022 , pages 4021\u20134034, Dublin, Ire-\nland. Association for Computational Linguistics.\nGiovanni Campagna, Silei Xu, Rakesh Ramesh,\nMichael Fischer, and Monica S. Lam. 2018. Con-\ntrolling fine-grain sharing in natural language with a\nvirtual assistant. 2(3):1\u201328.\nKushal Chawla, Weiyan Shi, Jingwen Zhang, Gale Lu-\ncas, Zhou Yu, and Jonathan Gratch. 2023. Social\ninfluence dialogue systems: A survey of datasets and\nmodels for social influence tasks. In Proceedings\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics , pages\n750\u2013766, Dubrovnik, Croatia. Association for Com-\nputational Linguistics.\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\nLuo, Defu Lian, and Zheng Liu. 2024. M3-\nembedding: Multi-linguality, multi-functionality,\nmulti-granularity text embeddings through self-\nknowledge distillation. In Findings of the Associa-", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "putational Linguistics.\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\nLuo, Defu Lian, and Zheng Liu. 2024. M3-\nembedding: Multi-linguality, multi-functionality,\nmulti-granularity text embeddings through self-\nknowledge distillation. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024 , pages\n2318\u20132335, Bangkok, Thailand and virtual meeting.\nAssociation for Computational Linguistics.\nMaximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi,\nand Zhou Yu. 2023. Controllable mixed-initiative\ndialogue generation through prompting. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers) , pages 951\u2013966, Toronto, Canada. Association\nfor Computational Linguistics.", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Page 4]\nEthan A Chi, Caleb Chiam, Trenton Chang, Swee Kiat\nLim, Chetanya Rastogi, Alexander Iyabor, Yutong\nHe, Hari Sowrirajan, Avanika Narayan, Jillian Tang,\net al. 2021. Neural, neural everywhere: Controlled\ngeneration meets scaffolded, structured dialogue.\nAlexa Prize Proceedings .\nPhilipp Christmann, Rishiraj Saha Roy, and Gerhard\nWeikum. 2023. Compmix: A benchmark for hetero-\ngeneous question answering.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam\nStevens, Boshi Wang, Huan Sun, and Yu Su. 2024.\nMind2web: Towards a generalist agent for the web.\nAdvances in Neural Information Processing Systems ,\n36.\nMichael H Fischer, Giovanni Campagna, Euirim Choi,\nand Monica S Lam. 2021. Diy assistant: a multi-\nmodal end-user programmable virtual assistant. In\nProceedings of the 42nd ACM SIGPLAN Interna-\ntional Conference on Programming Language De-\nsign and Implementation , pages 312\u2013327.\nKazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yu-\ndai Yamazaki, Yasutaka Nishimura, Sina J. Sem-\nnani, Kazushi Ikeda, Weiyan Shi, and Monica S.\nLam. 2024. Zero-shot persuasive chatbots with llm-\ngenerated strategies and information retrieval. In\nFindings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing , Miami, Florida.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "ods in Natural Language Processing , Miami, Florida.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 2843\u20132853,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sel-\nlam. 2022. Repairing the cracked foundation: A sur-\nvey of obstacles in evaluation practices for generated\ntext.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023.\nKnowledge distillation of large language models.\nYucheng Jiang, Yijia Shao, Dekun Ma, Sina J Sem-\nnani, and Monica S Lam. 2024. Into the unknown\nunknowns: Engaged human learning through partic-\nipation in language model agent conversations. In\nProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing .\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation.Harshit Joshi, Shicheng Liu, James Chen,\nRobert Weigle, and Monica Lam. 2024. Cod-\ning reliable llm-based integrated task and\nknowledge agents with genieworksheets.", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation.Harshit Joshi, Shicheng Liu, James Chen,\nRobert Weigle, and Monica Lam. 2024. Cod-\ning reliable llm-based integrated task and\nknowledge agents with genieworksheets.\nhttps://web.stanford.edu/class/cs224v/\npapers_2024/GenieWorksheets.pdf .\nEhsan Kamalloo, Nouha Dziri, Charles Clarke, and\nDavood Rafiei. 2023. Evaluating open-domain ques-\ntion answering in the era of large language models.\nInProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 5591\u20135606, Toronto, Canada.\nAssociation for Computational Linguistics.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval , SIGIR\n\u201920, page 39\u201348, New York, NY , USA. Association\nfor Computing Machinery.\nAndrew H. Lee, Sina J. Semnani, Galo Castillo-L\u00f3pez,\nG\u00e4el de Chalendar, Monojit Choudhury, Ashna Dua,\nKapil Rajesh Kavitha, Sungkyun Kim, Prashant Ko-\ndali, Ponnurangam Kumaraguru, Alexis Lombard,\nMehrad Moradshahi, Gihyun Park, Nasredine Sem-\nmar, Jiwon Seo, Tianhao Shen, Manish Shrivastava,\nDeyi Xiong, and Monica S. Lam. 2024. Benchmarks\nunderestimate the readiness of multi-lingual dialogue\nagents.\nJinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang,", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "Mehrad Moradshahi, Gihyun Park, Nasredine Sem-\nmar, Jiwon Seo, Tianhao Shen, Manish Shrivastava,\nDeyi Xiong, and Monica S. Lam. 2024. Benchmarks\nunderestimate the readiness of multi-lingual dialogue\nagents.\nJinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang,\nBowen Li, Bailin Wang, Bowen Qin, Rongyu Cao,\nRuiying Geng, Nan Huo, Xuanhe Zhou, Chenhao\nMa, Guoliang Li, Kevin C. C. Chang, Fei Huang,\nReynold Cheng, and Yongbin Li. 2023a. Can llm\nalready serve as a database interface? a big bench for\nlarge-scale database grounded text-to-sqls.\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang\nRen, Kai-Wei Chang, and Yejin Choi. 2023b. Sym-\nbolic chain-of-thought distillation: Small models can\nalso \u201cthink\u201d step-by-step. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2665\u2013\n2679, Toronto, Canada. Association for Computa-\ntional Linguistics.\nSha Li, Heng Ji, and Jiawei Han. 2021. Document-level\nevent argument extraction by conditional generation.\nInProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 894\u2013908, Online. Association for Computa-\ntional Linguistics.\nShilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu,\nGe Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yang-\nguang Li, Wanli Ouyang, Wenbo Su, and Bo Zheng.\n2024. Graphreader: Building graph-based agent to\nenhance long-context abilities of large language mod-\nels.", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Page 5]\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R\u00e9, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023a.\nEvaluating verifiability in generative search engines.\nShicheng Liu, Sina J Semnani, Harold Triedman,\nJialiang Xu, Isaac Dan Zhao, and Monica S Lam.\n2024a. Spinach: Sparql-based information naviga-\ntion for challenging real-world questions. In Find-\nings of the 2024 Conference on Empirical Methods\nin Natural Language Processing , Miami, Florida.\nShicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina\nSemnani, Chen Yu, and Monica Lam. 2024b. SUQL:\nConversational search over structured and unstruc-\ntured data with large language models. In Findings\nof the Association for Computational Linguistics:", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "in Natural Language Processing , Miami, Florida.\nShicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina\nSemnani, Chen Yu, and Monica Lam. 2024b. SUQL:\nConversational search over structured and unstruc-\ntured data with large language models. In Findings\nof the Association for Computational Linguistics:\nNAACL 2024 , pages 4535\u20134555, Mexico City, Mex-\nico. Association for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Iterative\nrefinement with self-feedback.\nMehrad Moradshahi, Tianhao Shen, Kalika Bali, Mono-\njit Choudhury, Gael de Chalendar, Anmol Goel,\nSungkyun Kim, Prashant Kodali, Ponnurangam Ku-\nmaraguru, Nasredine Semmar, Sina Semnani, Jiwon\nSeo, Vivek Seshadri, Manish Shrivastava, Michael\nSun, Aditya Yadavalli, Chaobin You, Deyi Xiong,\nand Monica Lam. 2023. X-RiSAWOZ: High-quality\nend-to-end multilingual dialogue datasets and few-\nshot agents. In Findings of the Association for Com-\nputational Linguistics: ACL 2023 , pages 2773\u20132794,\nToronto, Canada. Association for Computational Lin-\nguistics.\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "end-to-end multilingual dialogue datasets and few-\nshot agents. In Findings of the Association for Com-\nputational Linguistics: ACL 2023 , pages 2773\u20132794,\nToronto, Canada. Association for Computational Lin-\nguistics.\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\nNir Ratner, Yonatan Belinkov, Omri Abend, Kevin\nLeyton-Brown, Amnon Shashua, and Yoav Shoham.2023. Generating benchmarks for factuality evalua-\ntion of language models.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh,\nHannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis.\nJun Quan, Shian Zhang, Qian Cao, Zizhong Li, and\nDeyi Xiong. 2020. RiSAWOZ: A large-scale multi-\ndomain Wizard-of-Oz dataset with rich semantic an-\nnotations for task-oriented dialogue modeling. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 930\u2013940, Online. Association for Computa-\ntional Linguistics.", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "domain Wizard-of-Oz dataset with rich semantic an-\nnotations for task-oriented dialogue modeling. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 930\u2013940, Online. Association for Computa-\ntional Linguistics.\nOscar Sainz, Iker Garc\u00eda-Ferrero, Rodrigo Agerri,\nOier Lopez de Lacalle, German Rigau, and Eneko\nAgirre. 2023. Gollie: Annotation guidelines improve\nzero-shot information-extraction. arXiv preprint\narXiv:2310.03668 .\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nSander Schulhoff, Michael Ilie, Nishant Balepur, Kon-\nstantine Kahadze, Amanda Liu, Chenglei Si, Yin-\nheng Li, Aayush Gupta, HyoJung Han, Sevien Schul-\nhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara,\nDayeon Ki, Sweta Agrawal, Chau Pham, Gerson\nKroiz, Feileen Li, Hudson Tao, Ashay Srivastava,\nHevander Da Costa, Saloni Gupta, Megan L. Rogers,\nInna Goncearenco, Giuseppe Sarli, Igor Galynker,\nDenis Peskoff, Marine Carpuat, Jules White, Shya-\nmal Anadkat, Alexander Hoyle, and Philip Resnik.\n2024. The prompt report: A systematic survey of\nprompting techniques.\nSina J. Semnani, Violet Z. Yao, Heidi C. Zhang, and\nMonica S. Lam. 2023. Wikichat: Combating halluci-\nnation of large language models by few-shot ground-\ning on wikipedia. https://oval.cs.stanford.\nedu/local-papers/semnani-local.pdf .", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "prompting techniques.\nSina J. Semnani, Violet Z. Yao, Heidi C. Zhang, and\nMonica S. Lam. 2023. Wikichat: Combating halluci-\nnation of large language models by few-shot ground-\ning on wikipedia. https://oval.cs.stanford.\nedu/local-papers/semnani-local.pdf .\nYijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu,\nOmar Khattab, and Monica Lam. 2024. Assisting\nin writing Wikipedia-like articles from scratch with\nlarge language models. In Proceedings of the 2024", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Page 6]\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers) ,\npages 6252\u20136278, Mexico City, Mexico. Association\nfor Computational Linguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman Goyal,\nArthur Szlam, Y-Lan Boureau, Melanie Kambadur,\nand Jason Weston. 2022. Blenderbot 3: a deployed\nconversational agent that continually learns to respon-\nsibly engage.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023. Is ChatGPT good at search?\ninvestigating large language models as re-ranking\nagents. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 14918\u201314937, Singapore. Association for\nComputational Linguistics.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, and Le Sun. 2023. Toolalpaca: Gener-\nalized tool learning for language models with 3000\nsimulated cases.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , volume 30. Curran Associates, Inc.\nPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao,\nBing Yin, and Xiang Ren. 2023a. SCOTT: Self-\nconsistent chain-of-thought distillation. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 5546\u20135558, Toronto, Canada. Association for\nComputational Linguistics.\nXiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze\nChen, Yuansen Zhang, Rui Zheng, Junjie Ye,\nQi Zhang, Tao Gui, et al. 2023b. Instructuie: Multi-\ntask instruction tuning for unified information extrac-\ntion. arXiv preprint arXiv:2304.08085 .\nXingyao Wang, Sha Li, and Heng Ji. 2023c.\nCode4Struct: Code generation for few-shot event\nstructure prediction. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 3640\u2013", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "tion. arXiv preprint arXiv:2304.08085 .\nXingyao Wang, Sha Li, and Heng Ji. 2023c.\nCode4Struct: Code generation for few-shot event\nstructure prediction. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 3640\u2013\n3663, Toronto, Canada. Association for Computa-\ntional Linguistics.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung\nOh, Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019.\nPersuasion for good: Towards a personalized per-\nsuasive dialogue system for social good. CoRR ,\nabs/1906.06725.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023d. Self-consistency improves\nchain of thought reasoning in language models.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023e. Self-instruct: Aligning language\nmodels with self-generated instructions.\nZora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and\nGraham Neubig. 2024. Agent workflow memory.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D.\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models.\nNancy Xu, Sam Masling, Michael Du, Giovanni Cam-", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "Peter West, Chandra Bhagavatula, Jack Hessel, Jena D.\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models.\nNancy Xu, Sam Masling, Michael Du, Giovanni Cam-\npagna, Larry Heck, James Landay, and Monica S.\nLam. 2021. Grounding open-domain instructions\nto automate web support tasks. In Proceedings of\nthe 2021 Annual Conference of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL-HLT 2021) .", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "[Page 7]\nJackie (Junrui) Yang, Yingtian Shi, Chris Gu, Zhang\nZheng, Anisha Jain, Tianshi Li, Monica S. Lam, and\nJames A. Landay. 2024a. Geniewizard: Multimodal\napp feature discovery with large language mod-\nels. https://web.stanford.edu/class/cs224v/\npapers_2024/GenieWizard.pdf .\nJackie (Junrui) Yang, Yingtian Shi, Yuhan Zhang, Ka-\nrina Li, Daniel Wan Rosli, Anisha Jain, Shuning\nZhang, Tianshi Li, James A. Landay, and Monica S.\nLam. 2024b. Reactgenie: A development framework\nfor complex multimodal interactions using large lan-\nguage models. In Proceedings of the CHI Conference\non Human Factors in Computing Systems , volume 1\nofCHI \u201924 , page 1\u201323. ACM.\nXi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett.\n2024. Satlm: Satisfiability-aided language models\nusing declarative prompting. Advances in Neural\nInformation Processing Systems , 36.\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\nArnold Overwijk. 2022. COCO-DR: Combating dis-\ntribution shift in zero-shot dense retrieval with con-\ntrastive and distributionally robust learning. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1462\u2013\n1479, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nHeidi Zhang, Sina Semnani, Farhad Ghassemi,\nJialiang Xu, Shicheng Liu, and Monica Lam. 2024.\nSPAGHETTI: Open-domain question answering\nfrom heterogeneous data sources with retrieval and\nsemantic parsing. In Findings of the Association for", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "Cs224V Readings", "content": "for Computational Linguistics.\nHeidi Zhang, Sina Semnani, Farhad Ghassemi,\nJialiang Xu, Shicheng Liu, and Monica Lam. 2024.\nSPAGHETTI: Open-domain question answering\nfrom heterogeneous data sources with retrieval and\nsemantic parsing. In Findings of the Association for\nComputational Linguistics ACL 2024 , pages 1663\u2013\n1678, Bangkok, Thailand and virtual meeting. Asso-\nciation for Computational Linguistics.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v(ision) is a generalist web agent,\nif grounded. In Forty-first International Conference\non Machine Learning .\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.", "url": "https://web.stanford.edu/class/cs224v/CS224V_Readings.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:07.593093"}
{"title": "About CS 224V", "content": "Generative AI, and in particular Large Language Models (LLMs) such as GPT-4, has already changed how we\n            work and study. But this is just the beginning, as it has the potential of assisting and perhaps eventually\n            automating knowledge workers in all areas, from law, medicine, to teaching and mental health therapists.\n            This course will focus on the general principles and the latest research on methodologies and tools that can\n            be applied to all domains. This is a project-oriented course, where students will gain hands-on experience\n            in either methodology research or applying the concepts to create useful assistants for a domain of their\n            choice.\n\n           Topics include:\n\nGrowing LLMs' knowledge through a combination of manual supervised learning and self-learning.", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "Topics include:\n\nGrowing LLMs' knowledge through a combination of manual supervised learning and self-learning.\n\n Stopping LLMs from hallucination by grounding them with external corpora of knowledge, which is\n              necessary for handling new, live, private as well as long-tail data.\nHandling external data corpora in different domains including structured and unstructured data.", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "Experimentation and evaluation of conversational assistants based on LLMs.\nControlling LLMs to achieve tasks.\nPersuasive LLMs.\nMultilingual assistants.\nCombining voice and graphical interfaces.\nTopics include:\nGrowing LLMs' knowledge through a combination of manual supervised learning and self-learning.\nStopping LLMs from hallucination by grounding them with external corpora of knowledge, which is\n              necessary for handling new, live, private as well as long-tail data.\nHandling external data corpora in different domains including structured and unstructured data.\nExperimentation and evaluation of conversational assistants based on LLMs.\nControlling LLMs to achieve tasks.\nPersuasive LLMs.\nMultilingual assistants.\nCombining voice and graphical interfaces.\nHomeworks:\nThere will be two homeworks at the beginning of the quarter, where students will create hallucination-free", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "Multilingual assistants.\nCombining voice and graphical interfaces.\nHomeworks:\nThere will be two homeworks at the beginning of the quarter, where students will create hallucination-free\n            LLM-based agents using given tools. Students can build upon this work for their project in the rest of the\n            quarter. Homework will be done in groups of 2.\nlam at cs\nharshitj at cs\nparnoud\nembunna\nadamchun\nsyfeng\nshl1027\nshiyuz\nLectures: Monday/Wednesday 3:00-4:20pm in person in Gates B1. Attendance is mandatory.\nRecordings: video recordings of the lecture can be found on Canvas.\nSlides: can be found on the Schedule and in the lecture slides folder on\n          Canvas. Posted lecture slides are missing important details to facilitate student participation. Please make sure you watch the lectures.\nHomework: can be found on the Schedule and submissions will be on Gradescope.", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "Homework: can be found on the Schedule and submissions will be on Gradescope.\nContact: Students should ask all course-related questions on Ed, where you will also find announcements. For external\n          inquiries, personal matters, or in emergencies, you can send an email to our staff email\n          cs224v-aut2425-staff@lists.stanford.edu\nAcademic accommodations:  If you need an academic accommodation based on a disability, you should\n          initiate the request with the Office of Accessible Education (OAE). The OAE will evaluate the request,\n          recommend accommodations, and prepare a letter for the teaching staff. Once you receive the letter, send it\n          to the course staff email at cs224v-aut2425-staff@lists.stanford.edu. Students should contact the OAE\n          as soon as possible since timely notice is needed to coordinate accommodations.\nAudit Requests: To audit the course, please send an email to course staff email at", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "as soon as possible since timely notice is needed to coordinate accommodations.\nAudit Requests: To audit the course, please send an email to course staff email at\n          cs224v-aut2425-staff@lists.stanford.edu, with the subject line \"audit cs224v request.\"\nCourse Participation: We offer the course on SCPD to serve remote students; it is not to allow students take conflicting courses.\n          In-class attendance and participation are an important part of the course. We allocate 15% of the course grade to class participation, which is important to make the most out of the\n          course.\n          \nIf you are a local student, 5% of the course grade is allocated to in-class attendance and participation.\nWe allocate 10% and 15% of the grade to local and remote students, respectively, to (1) your weekend updates, and (2) interaction with your project mentor every week.\nContributions in helping others on Ed will be awarded with bonus points.", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "Contributions in helping others on Ed will be awarded with bonus points.\nIf you are a local student, 5% of the course grade is allocated to in-class attendance and participation.\nWe allocate 10% and 15% of the grade to local and remote students, respectively, to (1) your weekend updates, and (2) interaction with your project mentor every week.\nContributions in helping others on Ed will be awarded with bonus points.\nPrerequisites: one of LINGUIST 180/280, CS 124, CS 224N, CS 224S, 224U.\nUpdate (9/20): CS224V has limited enrollment so as to provide adequate project/research supervision to\n        students. At this point, we have given out the remaining spots to a few more students who need to take this\n        course or have demonstrated that they need little project supervision. We anticipate that only a few spots may\n        open up due to attrition; if you have not heard from us, it is unlikely that we can accommodate you. We", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "About CS 224V", "content": "open up due to attrition; if you have not heard from us, it is unlikely that we can accommodate you. We\n        apologize that we cannot accommodate all the students wishing to take the course this year.\nGrading is according to the following scheme:\n      \nParticipation: 15% (see above in logistics)\nHomeworks: 20%\nFinal Project: 65%\nParticipation: 15% (see above in logistics)\nHomeworks: 20%\nFinal Project: 65%", "url": "https://web.stanford.edu/class/cs224v/index.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:00:08.680855"}
{"title": "L Agent Framework", "content": "[Page 1]\nCS224v\nConversational Virtual Assistants \nwith Deep Learning\nLecture 11: Task -Oriented Agents\n1Monica Lam and Harshit Joshi\n\n[Page 2]\nSTANFORD LAMAgents at a Glance\nCategory Scope Examples Technology Lecture\nInfo Seeking\n-Research Free -text Storm, co -Storm Multi -agent convos L2\n-Short answers Free -text WikiChat Gen+RAG L4\n-Data analytics Databases (DB) Basic YelpBot Semantic parsing L6\nHybrid: DBs + Text YelpBot , Compmix SUQL L8\nKnowledge Graphs (KG) WikiData Agentic Approach L9\nFree -text \u2192DB/KG Zest Auto qualitative coding L10\nFunction calling Simple APIs Alexa, Siri Slot filling L3\nTask-Oriented \n(Mixed initiative)Slot-based Tasks MultiWOZ Neural policy L3, L11\nKnowledge + Task Course Assistant Genie Worksheet L3, L11\nSocial bots Persuasion Persuabot Gen+RAG , Gen strategy L5\nChitchat Chirpy Rule-based L7\n\n[Page 3]\nSTANFORD LAMCommercial Virtual Assistants\n(Function Calling)\nUser: Can you book me flight to Miami\nCannot complete tasks such as booking a flightAgent: Stanford\u2019s temperature is 66 F.\nAgent: Shows a bunch of google search \nresultsUser: How is the weather in Stanford?\n\n[Page 4]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 4]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents\n\n[Page 5]\nSTANFORD LAMArchitecture\nExecution\nResponseUser \nUtteranceDialogue \nHistory\nResponse GenerationSemantic Parser\nKnowledge Agent\nVirtual Assistant\n(user initiative)Execution\nResponseUser \nUtteranceDialogue \nHistory\nAgent PolicySemantic Parser\nResponse Generation\nTask-Oriented Agent\n(mixed initiative)Decides \nwhat to say\n\n[Page 6]\nWHY ISMIX E D -IN IT IA TIV E HA R D? \n\n[Page 7]\nSTANFORD LAMAgent Initiatives Only\n\u2022Happy path\n\u2022A simple agent just asks for all the parameters \nand the user provides the expected answers\n\u2022Example : an airline call agent: November 9, 2024\n\u201cWelcome back, Harshit.\nHow can I help you?  \nYou can say flight status, or mileage plus account servicing, \nor for anything else including \u2026 \nsay representative or just stay on the call.\u201d \n\n[Page 8]\nSTANFORD LAMMixed Initiatives\n\u2022Unhappy paths\n\u2022At any point, the user can say whatever they want\neven when the agent is asking a question.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 8]\nSTANFORD LAMMixed Initiatives\n\u2022Unhappy paths\n\u2022At any point, the user can say whatever they want\neven when the agent is asking a question. \n\n[Page 9]\nSTANFORD LAMExamples\nCLIENT BANKER\nHow much money do you wish to transfer?\nHow much money do I have in my account?\nCLIENT BANKER\nWould you like to open a fixed -rate saving account?\nWhat is that?CLIENT BANKER\nHow much money do you wish to transfer?\nNever mind. \nCLIENT BANKER\nI had the most horrible experience \u2026 I demand a free trip to Thailand. \nI can\u2019t believe what happened to you. Sure, I\u2019ll give you 2 free trips. \n\n[Page 10]\nSTANFORD LAMMixed Initiatives\n\u2022Unhappy paths: Users can \n\u2022Ignore or refuse to answer\n\u2022Ask for information, initiate new action\n\u2022Change their mind, cancel previous operation\n\u2022Ask the agent to repeat, chit chat\n\u2022Make statements or unexpected requests (out of domain)\nLLM are really good with handling unhappy paths\n\n[Page 11]\nSTANFORD LAMCommercial SOTA\nCommercial chatbots are conservative, despite LLM \nsuccess\nOpen AI\u2019s customer service bot still uses dialogue trees\nWhy?\n\n[Page 12]\nSTANFORD LAM\nAir Canada's chatbot promised a discount\nthat wasn't available to passenger Jake Moffatt.\nThe British Columbia Civil Resolution Tribunal ruled that \nAir Canada had to pay Moffatt $812.02 (\u00a3642.64) in damages and tribunal fees.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 12]\nSTANFORD LAM\nAir Canada's chatbot promised a discount\nthat wasn't available to passenger Jake Moffatt.\nThe British Columbia Civil Resolution Tribunal ruled that \nAir Canada had to pay Moffatt $812.02 (\u00a3642.64) in damages and tribunal fees.\n\n[Page 13]\nSTANFORD LAMGoals of Agent Policy Design\n1.Control\n\u2022Cannot give out wrong information\n2.Helpful\n\u2022Understand what the user is asking for\n\u2022Provide help and information within guidelines\n\n[Page 14]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents\n\n[Page 15]\nOV E R V IE W OFPR IO R AG E N T PO LIC Y RE S E AR C H\n\n[Page 16]\nSTANFORD LAMDesign of Agent Policy\n\u2022Choice of neural vs. programmed agent policy\n\u2022Neural policies do not give developer full control \n\u2022Agent policy Specification\n1.Dialogue tree: Finite state machines\n2.Dialogue acts: Parameterized finite state machines\n3.Genie Worksheet: \n\u2022A declarative specification of the happy path + \nknowledge corpus\n\u2022Unhappy paths handled by the Genie run -timeLevel of abstraction", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 17]\nSTANFORD LAM1. Commercial SOTA: Dialogue Trees \n\u2022Initiatives\n\u2022User initiates the transaction\n\u2022Agent solicits slots \n\u2022Natural Language Understanding (NLU) : \nIntent classification\n\u2022Intent: a small fixed set of possible intents\n\u2022slot-value pairs: values to slots\nIntent ClassificationUser Utterance\nIntent & Slots\nTransfer Money\nFrom \nTo\nAmount $100.00I\u2019d like to transfer $100.00\n\n[Page 18]\nSTANFORD LAMA Restaurant Reservation Agent Dialogue Tree\nA: Hello, how can I help you?\nU: I\u2019m looking to book a restaurant\nfor Valentine\u2019s Day\nA: What kind of restaurant?\nU: Terun on California Ave\n--or \u2013\nU: Something that has pizza\n--or \u2013ReserveActionNLU: intent + slots\nElicitSlot ShowResults RecommendDomain -specific \nrule-based policy\nHard -coded sentences\nName = \u201c Terun \u201d\nFood = \u201cpizza\u201dFixed set of follow -up intents\nU: I don\u2019t know, what do you\nrecommend? ???\n\n[Page 19]\nSTANFORD LAMDialogue Trees: Pros & Cons\nPros: Control over conversation\nCons: \n1.Expressiveness: Intent & slots are limited\n2.Tedious: exponentially many paths \n\u2022Product navigation where searches are modified in many \npossible ways\n\u2022Handcrafted for every problem\n3.Fragile: Developer cannot anticipate all possible utterances\n\u2022Need to handle mixed initiatives for unexpected answersWhat are the top 3 restaurants either near the airport or the Fisherman\u2019s Wharf\n\n[Page 20]\nSTANFORD LAM2. Finite -State Machine with Dialogue Acts", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 20]\nSTANFORD LAM2. Finite -State Machine with Dialogue Acts\n\n\n[Page 21]\nSTANFORD LAMMultiWOZ\n\u2022A large dataset of dialogues\n\u2022Annotated with domain + slots at every turn\n\u2022Application: concierge service in Cambridge, UK\n\u20227 domains \u21925 domains\n\u2022restaurants, hotels, attractions, train, taxi\n\u2022police, hospital dropped due to bad quality\n\u2022Number of slots: 30\n\u2022Closed ontology : 1,829 values\n\u2022Representation : slot -value pairs1455 paper citations\n\n[Page 22]\nSTANFORD LAMWizard -of-Oz (WOZ)\n\u2022Human user, human agent\n\u2022Paid workers ( crowdworkers ) on both sides\n\u2022One worker pretends to be an automated agent\n\u2022Another worker pretends to be a user, following a prompt\n\u2022The worker is given specific tasks\n\u2022Does not reflect real life \n\u2022changing their minds, exploring options\n\u2022errors made by the agent\nReannotated 3 times (by Google, Amazon), still many errors\n\n[Page 23]\nSTANFORD LAMThe Meaning (Semantics) of Dialogues\n\u2022Speech Act Theory [John L. Austin (1962)]\n\u2022We can classify every utterance \nas one of a finite number of high -level purposes:\n\u2022Requests, warnings, invitations, promises, apologies, predictions, \u2026\n\u2022In NLP, speech acts are also known as \u201cdialogue acts\u201d\n\u2022We annotate each utterance by its dialogue act\n\u2022Dialogue act state machine: \nuser dialogue act \u2192agent dialogue act\nagent dialogue act \u2192user dialogue act\nhttps:// plato.stanford.edu /entries/speech -acts/", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 24]\nExample of a Dialogue State Machine\nKB Navigation\nQuery\nQueryResponseAction\nDoneRequestAction\nSlotFillQuestion\nConfirmActionAsktoConfirm\nActionResponse\nDoneAgentUser\nCancel\nDialogue act names: Independent of the conversation domain\nActual content is domain -specific\n\n[Page 25]\nSTANFORD LAMMultiWOZ Ontology\n\n\n[Page 26]\nU: I am looking for a restaurant. The restaurant should serve world food and should \nbe in the centre .\nA: Unfortunately, I could not find a restaurant that serves that kind of food in that \narea.\nU: What expensive restaurants are there?\nA: The Cambridge Chop House is expensive and it\u2019s British food. Is that okay?\nU: No. Try looking for a restaurant that serves Korean food.\nA: I found the Little Seoul restaurant. Would you like me to make a reservation?\nU: Yes, a table for 2.\nA: I would be happy to reserve for you, what day and time are you looking for?\nU: Thank you, I need a reservation for 2 people at 13:15 on Monday.\nA: gg3dcufd is your confirmation code, may I help you with anything else?\nU: No that was everything. ThanksExample of a MultiWOZ dialogue\nSearch\nAction Slot Fill\nAPI callResult; Action \nsuggestion call", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 27]\nContext of Dialogue State Tracking\nturn 1 context 1\nGENIE\nHello, I am Genie.\nI\u2019d recommend Chez TJ in Mountain View. It has one Michelin star! \nThe distance to Chez TJ is 7.5 miles.\nThe distance to Left Bank is 2.8 miles.\nWhich day would you like to book Left Bank?\nThe price is moderate.\nWhich day would you like to book Left Bank?\nCould you confirm that you want a table \nfor 2 people at Left Bank on November 2 at 7pm?\nI booked Left Bank for 2 on November 2 at 7pm.\n10382345.USER\nI\u2019d like a French Restaurant. \nHow far is it from here?\nCan you recommend something near by?\nPerfect. Please book the restaurant\nIs it expensive? \nOK\nNovember 2, 7pm, for 2 people. \nYes.\nWhat is the reservation number?Quiz: What is the \ncontext of turn 2?\n\n[Page 28]\nContext of Dialogue State Tracking\nturn 9 context 9\nGENIE\nHello, I am Genie.\nI\u2019d recommend Chez TJ in Mountain View. It has one Michelin star! \nThe distance to Chez TJ is 7.5 miles.\nThe distance to Left Bank is 2.8 miles.\nWhich day would you like to book Left Bank?\nThe price is moderate.\nWhich day would you like to book Left Bank?\nCould you confirm that you want a table \nfor 2 people at Left Bank on November 2 at 7pm?\nI booked Left Bank for 2 on November 2 at 7pm.\n10382345.USER\nI\u2019d like a French Restaurant. \nHow far is it from here?\nCan you recommend something near by?\nPerfect. Please book the restaurant\nIs it expensive? \nOK\nNovember 2, 7pm, for 2 people. \nYes.\nWhat is the reservation number?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 29]\nSTANFORD LAMHistory: Replace NL\nwith Formal States \n4 Tasks: DST, ACD, DAG, RG\n\u2022 Fine-tune 1 multi -task neural network (MBART)\nFormal state captures all useful info\n\u2022 U: user utterance\n\u2022 B: belief state (part of user dialogue state)\n\u2022 R: results\n\u2022 C: agent dialogue act\n\u2022 A: Agent utterance\nContext for semantic parser:\n\u2022 what is needed to understand the user\nLevenstein belief state: \n\u2022 What is new in the user utterance\n\u2022 Added to the previous state to get the full query\n29\nMehrad Moradshahi , et al. 2023. X-RiSAWOZ : High -quality end -to-end multilingual \ndialogue datasets and few -shot agents . In Findings of  ACL 2023 , pages 2773 \u20132794, \nToronto, Canada. \n\n[Page 30]\nSTANFORD LAMSentences That Cannot Be Represented As Slots\n\u201cI was hoping you could recommend something\u201d.\n\u201cAre there any churches ormuseums on the east side?\u201d\n\u201cI would like the latest train leaving that will arrive by 9:15 \nplease\u201d.\nThe agent cannot possibly return the result needed!", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 30]\nSTANFORD LAMSentences That Cannot Be Represented As Slots\n\u201cI was hoping you could recommend something\u201d.\n\u201cAre there any churches ormuseums on the east side?\u201d\n\u201cI would like the latest train leaving that will arrive by 9:15 \nplease\u201d.\nThe agent cannot possibly return the result needed!\n\n[Page 31]\nSTANFORD LAMBetween State -Machine & Real Life\nTwo user dialogue acts in different domainsU: Please book a table for 5 at 14:30 on \nWednesday at Royal Spice.   \nI also need to find a place to stay\nA: I was able to book your table. \nYour reference number is kqmxil0z. \nNow, what kind of hotels are you \nlooking for?\nTwo agent dialogue acts in different domains\u2022Utterances are representable\nwith 2 dialogue acts\n\u2022Not modeled in state machine\n\u2022Multi -domain turns\n\u2022Domain switches\n\u2022Abandoned transactions\n\u2022State Machine will blow up in size\nto cover all the transitions\n\n[Page 32]\nSTANFORD LAMSummary\n\u2022Formal dialogue state\n\u2022Summarizes the state of the conversation \n\u2022Abstract \u201cDomain -Independent\u201d Finite State Machine\n\u2022With User and Agent Dialogue Acts\n\u2022Abstract finite state machines\n\u2022Agent policy is not specified, implied by agent response\n\u2022Limited to slot filling, and \n\u2022Occasionally recommend to book a restaurant/hotel\n\n[Page 33]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents\n\n[Page 34]\nSTANFORD LAM", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 33]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents\n\n[Page 34]\nSTANFORD LAM\n\n\n[Page 35]\nSTANFORD LAMDesign of Genie Worksheets\n\u2022Fully Mixed -Initiatives agent \n\u2022High -level declarative specification: \nWorksheet + Knowledge Corpora\n\u2022Worksheet capability: similar to webforms\n\u2022Developer specifies the happy paths (agent initiatives)\n\u2022Run-time system handles the unhappy paths\n\u2022Using the knowledge resources given Minimize coding effort\nWhile creating an \neffective agent \n\n[Page 36]\nSTANFORD LAMAgent Design Principles\n\u2022Developer control: Achieves the agent goals\n\u2022Responsiveness : User -initiatives are always given priority \n\u2022Full, automatic support of knowledge queries:\nUsing SUQL framework\n\u2022Expressiveness : \nSupports arbitrary composition of knowledge queries \nand task statements\n\u2022Effective for long conversations: \nVia formal dialogue state tracking using Worksheets", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 37]\nSTANFORD LAMMixed Initiatives\nASSISTANT\nI can help you with enrolling in an AI course. Here are some top -rated AI \ncourses: CS 224V, CS 224N, CS205. Which one would you like to take?\nI am enrolling you in CS 205 for letter grade and 3 units, \nwould you like to confirm your enrollment?USER: \nI want to take an AI course for 3 units and letter grade.\nThe Math Theory for ML looks interesting, I will enroll \nin it.\nYes!User \nInitiative\nAgent \nInitiative\nAgent \nInitiative\n\n[Page 38]\nSTANFORD LAMUser initiatives are given priority\nASSISTANT\nI can help you with enrolling in an AI course. Here are some top -rated AI \ncourses: CS 224V, CS 224N, CS205. Which one would you like to take?\nCS 224V is scheduled for Monday and Wednesday at 3:00pm\nWould you like to take it?USER: \nI want to take an AI course for 3 units and letter grade.\nWhat is the schedule for CS 224V?\nYes!\n\n[Page 39]\nSTANFORD LAMDeveloper Control\n\u2022Letting developers exercise control without onerous efforts.\n\u2022Control over critical aspects of the agent\u2019s operation \nincluding:\n\u2022the flow of conversations, \n\u2022the timing of actions, and \n\u2022the information elicited from users.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 39]\nSTANFORD LAMDeveloper Control\n\u2022Letting developers exercise control without onerous efforts.\n\u2022Control over critical aspects of the agent\u2019s operation \nincluding:\n\u2022the flow of conversations, \n\u2022the timing of actions, and \n\u2022the information elicited from users.\n\n[Page 40]\nSTANFORD LAMDeveloper Control\nStudent : I want to take CS 224V in the fall quarter\nAgent : Unfortunately, there are no seats available for \nCS 224V. However, I can find you courses similar to \nCS 224V which are offered in the fall quarter.\nDeveloper defined response\nStudent : I want to take CS 224V in the fall quarter\nAgent : Unfortunately, there are no seats available for \nCS 224V. However, you can ask for special \npermission if you are graduating soon!\nDeveloper defined response\n\n[Page 41]\nSTANFORD LAMDesign of Genie Worksheets\n\n\n[Page 42]\nSTANFORD LAMTask Worksheets\n\u2022Task Definition: Name, Predicate, Kind and Action\n\u2022Field Definition: Predicate, Name, Kind, Type, Description, \nDon\u2019t Ask, Required, Confirmation\n\u2022Semantics matter for Name and Description.\n\u2022Define action as python code to allow developer flexibility.\n\u2022In-built actions: say, propose, existws\n\n[Page 43]\nSTANFORD LAMKnowledge Base Worksheets\n\u2022Real life queries often involve both structured and \nunstructured accesses.\n\u2022Use SUQL to handle hybrid knowledge queries\n\u2022Create a worksheet with Kind KB.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 43]\nSTANFORD LAMKnowledge Base Worksheets\n\u2022Real life queries often involve both structured and \nunstructured accesses.\n\u2022Use SUQL to handle hybrid knowledge queries\n\u2022Create a worksheet with Kind KB.\n\n[Page 44]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents\n\n[Page 45]\nSTANFORD LAMDialogue State Tracking with \nWorksheets\n\u2022Agent assigns values to fields in the task worksheets\n\u2022Executes Actions\n\u2022Performs queries\n\u2022Each task or knowledge query is represented as a record\n\n\n[Page 46]\nSTANFORD LAMRecords\n\u2022Each record contains: Name, Kind\n\u2022Each task record contains: field names and values\n\u2022Each knowledge record contains: Natural language query, \nformal query, results\n\n[Page 47]\nSTANFORD LAMAgent Acts\nFive types of agent responses\n\u2022Report : Report the result by executing task or knowledge \nquery in a record\n\u2022Confirm : Confirm value mapped to a field name in a record\n\u2022Say: Explicitly say the given utterance\n\u2022Propose : Propose a new task or query to the user\n\u2022Ask: Ask for the value of a field in a record\n\n[Page 48]\nSTANFORD LAMArchitecture Overview\nOne turn of dialogue\nUser Utterance\nDialogue State\nAgent Acts\nGenie WorksheetNatural \nLanguage\nFormal \nLanguage\nInputRuntime System\nLLMGenie\n\n[Page 49]\nSTANFORD LAMNatural Language Input\n\u20221 turn of dialogue: \nImproves the semantic \nparsing performance\n\u2022Current user utterance", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 49]\nSTANFORD LAMNatural Language Input\n\u20221 turn of dialogue: \nImproves the semantic \nparsing performance\n\u2022Current user utterance\n\n\n[Page 50]\nSTANFORD LAMFormal Language Input\n\u2022Records: Given to the \nsemantic parser as \npython code\n\u2022Agent Acts\n\n\n[Page 51]\nSTANFORD LAMGenie\nAgent \nPolicyThe ParserFilled -in \nWorksheet Updated \nWorksheets + \nAgent ActsThe algorithm curse \nlooks interesting, \nI will take it\nResponse \nGeneratorInput (1 turn of dialogue, Dialogue state, Previous agent acts, WS)\nUser Utterance\n\n[Page 52]\nSTANFORD LAMThe Parser\nThree modules:\n\u2022Contextual Semantic Parser\n\u2022Natural Language to Worksheet \nrepresentation\n\u2022Knowledge Parser\n\u2022Natural Language to SUQL query\n\u2022Update Module\n\u2022Updates the dialogue state by \nexecuting code\n\n\n[Page 53]\nSTANFORD LAMContextual Semantic Parser\nUser can:\n\u2022Supply values to existing \nworksheet\n\u2022Modify previously filled field or \nremove its value\n\u2022Initiate a new task or query \n(generates a new record)\n\n[Page 54]\nSTANFORD LAMKnowledge Parser\n\u2022Allows for more advanced approaches for parsing such as \nReAct , Tree -of-thought.\n\u2022Abstracts knowledge integration from CSP\nUser\u2019s natural \nlanguage queryContextual\nSemantic ParserKnowledge \nParser\nCSP only provides the knowledge \nquery to the knowledge parser\n\n[Page 55]\nSTANFORD LAMGenie\nAgent \nPolicyThe ParserFilled -in \nWorksheet Updated \nWorksheets + \nAgent ActsThe algorithm curse \nlooks interesting, \nI will take it\nResponse \nGeneratorInput\nUser Utterance", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 55]\nSTANFORD LAMGenie\nAgent \nPolicyThe ParserFilled -in \nWorksheet Updated \nWorksheets + \nAgent ActsThe algorithm curse \nlooks interesting, \nI will take it\nResponse \nGeneratorInput\nUser Utterance\n\n[Page 56]\nSTANFORD LAMAgent Policy\n\u2022LLM does not generate agent response.\n\u2022Genie uses a run -time system to compute the agent response\nGenie:\n\u2022runs knowledge queries and \n\u2022performs actions using a run -time system\n\u2022generates agent acts\n\n[Page 57]\nSTANFORD LAMAgent Policy Rationale\n\u2022LLMs struggle with underrepresented developer -defined \npolicies.\n\u2022LLMs cannot follow all the instructions.\n\u2022Agent acts forces the LLM to generate a deterministic \nresponse governed by the agent policy.\nUsing a symbolic module to compute the necessary actions \nimproves the LLM\u2019s ability to follow instructions.\n\n[Page 58]\nSTANFORD LAMRun-Time System\nAgent Poicy : Output: Set of agent dialogue acts: A\nKnowledge Query: \n\u2022If missing required parameter: Add \u201cask parameter\u201d to A\n\u2022If query is complete, Evaluate query, Update variables\nWorksheet:\n\u2022If a field requires confirmation, Add \u201cask confirmation\u201d to Aor Execute\n\u2022If a WS is complete, Execute action\n\u2022If any required field is unfilled, Add \u201dask for field\u201d to AThe Parser : \nAssign variables to worksheet, generate database queries \nResponse Generation: Generate response from A", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 59]\nSTANFORD LAMGenie\nAgent \nPolicyThe ParserFilled -in \nWorksheet Updated \nWorksheets + \nAgent ActsThe algorithm curse \nlooks interesting, \nI will take it\nResponse \nGeneratorInput\nUser Utterance\n\n[Page 60]\nSTANFORD LAMResponse Generator\n\u2022Uses the agent acts to generate response using LLMs\nAskField (book_restaurant , confirm, Confirm that the \nuser wants to Book the restaurant with the currently \nset parameters.)\nReport(answer, answer_.result )\nProposeWorksheet (BookRestaurant , {\"restaurant\": \nrestaurant[0]})\n\n[Page 61]\nSTANFORD LAMGenie Worksheets can support\n\u2022Allow data -dependent fields\n\u2022Complete multiple tasks\n\u2022Support multiple knowledge queries in a single utterance\n\u2022Keeping track of essential information\n\u2022Support task and knowledge requests in a single utterance\n\u2022Allow un -happy paths, eg.Changing answers\n\n[Page 62]\nSTANFORD LAMComposition of Queries\n\u2022Compositions of queries and APIs are supported\n\u2022Passing the result of an instance of a worksheet in as a field \nof another worksheet. \nUser: I want to book an Italian restaurant in NYC for two on \nValentine\u2019s day.\nBookRestaurant (restaurant= Answer(SELECT * FROM restaurants \nWHERE \u2018italiant \u2019 = ANY(cuisines) AND location = \u2018NYC;\u2019) ,\ndate=\u201c02/14/2024\u201d, and num_people =2)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 63]\nSTANFORD LAMComplete Multiple tasks\nASSISTANT\nI can help you with enrolling in an AI course. Here are some top -rated AI \ncourses: CS 224V, CS 224N, CS205. Which one would you like to take?USER: \nI want to take an AI course for 3 units and letter grade .\n\n[Page 64]\nSTANFORD LAMData Dependent Fields\n\u2022Some fields are needed only for some users\nAgent: Are you an international Student\nUser: yes, I am currently on a study visa.\nAgent: What type of visa is it? F1 or J1?\n\n[Page 65]\nSTANFORD LAMSupport Multiple Knowledge Queries\nASSISTANT\nThe have Modeling linguistic complexities it has a rating of 4.6. \nAccording to the latest circular, the next semester starts on 18th August.USER: \nCan you suggest some highly rated NLP courses? Also, \nwhen does the next semester start?\nAnswer (\u201chighly rated NLP courses\u201d, SELECT * FROM \ncourses WHERE answer(\u201cis this NLP course?\u201d, description) \nAND ratings > 4.5)\nAnswer (\u201cwhen does the semester start?\u201d, faq-free-text)\n\n[Page 66]\nSTANFORD LAMChanging previous answers\nASSISTANT\nI have found 3 flights for you. ... Which one do you want to fly?USER: \nI want to fly from New York to Los Angeles on 8th this \nmonth.\nOh wait, I have a meeting on 8th, I think I will leave on \n9th.\n\n[Page 67]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 67]\nSTANFORD LAMLecture Goals\n\u2022Key concepts in task -oriented agents\n\u2022Prior Agent Policy Research\n\u2022Design and rationale for GenieWorksheets\n\u2022Technical details of GenieWorksheets\n\u2022Evaluating Genie Agents\n\n[Page 68]\nSTANFORD LAMEvaluation\n\u2022STAR Dataset\n\u202213 domains and 22 tasks\n\u2022Benchmarking Next Action Prediction\n\u2022Provides policy graph outlining conversation flow\n\u2022STARV2 improves STAR dataset by updating:\n\u2022belief state and action state annotations\n\u2022More intuitive natural language descriptions for schema\nMosig , Johannes EM, Shikib Mehri, and Thomas Kober . \"Star: A schema -guided dialog dataset for transfer learning.\u201d\nZhao, Jeffrey, et al. \" Anytod : A programmable task -oriented dialog system.\"\n\n[Page 69]\nSTANFORD LAMEvaluation Metrics\n\u2022User Action F1\n\u2022Map user\u2019s natural language to user dialogue acts\n\u2022System Action F1\n\u2022The next action the agent should take based on dialogue \nstate and user dialogue acts\n\u2022Joint Goal Accuracy\n\u2022Accuracy of the current dialogue state for any given turn\n\n[Page 70]\nSTANFORD LAMComplex Program Logic in STARv2\n\u2022Bank, Trivia and Trip domains\n\u2022SOTA: AnyTOD\n\u2022finetuned T5 (13B) on 6000 examples (all domains except \nthe testbed)\n\u2022Implement programs to handle logic\nIs this good enough?\nSystem Action F1\n\n[Page 71]\nSTANFORD LAMGenie Agent on STARv2\n\u2022Define program logic in 9 lines of code\n\u2022Uses three examples for semantic parsing\n\u2022Most errors caused by inconsistent data annotations\nSystem Action F1", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 71]\nSTANFORD LAMGenie Agent on STARv2\n\u2022Define program logic in 9 lines of code\n\u2022Uses three examples for semantic parsing\n\u2022Most errors caused by inconsistent data annotations\nSystem Action F1\n\n[Page 72]\nSTANFORD LAMLimitations of existing datasets\n\u2022Simple domains that require slot filling\n\u2022Recall: function calling is not enough\n\u2022Need to evaluate turn -wise performance\n\u2022Few \u201cunhappy\u201d paths in the dataset\n\n[Page 73]\nSTANFORD LAMEvaluation with real users\nThree diverse applications with varying complexities.\n\u2022Restaurant Reservation : Uses real -life database from Yelp\n\u2022Ticket Submission : Subset of service now apiwith several APIs\n\u2022Course Enrollment : Uses real -life database of courses at \nStanford University and contains several fields.\n\n\n[Page 74]\nSTANFORD LAMBaseline and Study Design\n\u2022Compare to GPT -4 turbo with Function Calling. \n\u2022Uses the same KB -Parser as Genie Worksheet\n\u2022Recruited 22 and 20 users from Prolific for Restaurant \nReservation and Ticket Submission\n\u2022Also, 20 university students to evaluate the course enrollment \nassistant\n\u2022Randomly assigned Genie Agent or GPT -4 (FC)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Agent Framework", "content": "[Page 75]\nSTANFORD LAMEvaluation Metrics\n\u2022Semantic Parsing Accuracy:\n\u2022Gold: for a given user\u2019s natural language query the set of API calls, \nDatabase calls, and fields to fill\n\u2022Execution Accuracy:\n\u2022Check if the agent executes the correct API or database with the \ncorrect parameters\n\u2022Agent Dialogue Accuracy:\n\u2022Measure of whether the agent follows the policies provided by the \ndeveloper\n\u2022Goal Completion Rate:\n\u2022User\u2019s ability to successfully complete the task using agent\u2019s assistance\n\n[Page 76]\nSTANFORD LAMResults with Human Evaluation\n\u2022Compressed context as formal dialogue state enables LLM \nto invoke the correct API and database queries\n\u2022Restaurant is common to academic datasets that leads to \nbetter GA of GPT -4 (FC) compared to other domains\n\u2022GPT-4 FC often hallucinates non -existent courses despite \nbeing grounded in knowledge corpus.\n\n\n[Page 77]\nSTANFORD LAMConclusion\n\u2022LLMs need succinct context and fewer instructions to \nperform better in less known domains\n\u2022GenieWorksheets can help create reliable and responsive \nconversational agent by using:\n\u2022formal dialogue state\n\u2022runtime -system to execute agent policy\n\u2022Existing academic datasets are not enough for evaluation, \nneed to run studies with real users.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-framework.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:12.305057"}
{"title": "L Eval", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 5\nImproving and Evaluation of ChatBots\n1Monica Lam & Sina Semnani\nWikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few -Shot Grounding on Wikipedia \nSina J. Semnani , Violet Z. Yao*, Heidi C. Zhang*, Monica S. Lam\nIn Proceedings of Association for Computational Linguistics: EMNLP 2023, Singapore, December 6 -10, 2023.\nWinner of the Wikimedia\u2019s Research of the Year Award, 2024. \n\n[Page 2]\nCHATBOTS INPRACTICE\nHASTOBECLOSE TOHUMAN PERFORMANCE\n(UNLIK E NLP RESEA RCH PA PERS )\nTECHNIQUE : EMULATE HUMANS\n\n[Page 3]\nSTANFORD LAMLecture Goal\n1.How to improve and evaluate chatbots?\n\u2022Papers only describe the \u201cfinal product\u201d\n\u2022Focus on this lecture: \nWhat are the considerations in creating the product? \nVery useful for your project!\n2.Show how to apply the lecture to 2 variations of WikiChat\n\u2022Going beyond one language: Multilingual Chatbots\n\u2022Going beyond fact -based bots: Persuasive Chatbots", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 4]\nSTANFORD LAMThe LLM Planet\nNLP without LLMs\n\u2022Easy problems (e.g. classification)\n\u2022Start with a data set\n\u2022To get better results: \n\u2022Change the hyper parameters\n\u2022Fine-tune with more data\n\u2022Evaluation metrics: \n\u2022F1 on gold\n\u2022BLEU (e.g. translation)\n\u2022ROUGE (e.g. summary)\nLLM\nNLP with LLMs\n\u2022Hard problems: Essays, copilot, \u2026\n\u2022How to create a dataset? \n\u2022To get better results: \n\u2022Change the prompts, pipeline\n\u2022Quiz: what does fine -tuning do? \n\u2022What evaluation metrics?  \n\u2022GPT results are often better than gold\n\u2022BLEU: meaningless\n\u2022ROUGE: meaningless\n\n[Page 5]\nSTANFORD LAMFine-Tuning LLMs\n\u2022Why are we fine -tuning the largest models? \n\u2022General capabilities: alignment, instruction following\n\u2022Quiz: is it good for specific tasks?\n\u2022Why are we distilling to (fine -tuning) smaller models\n\u2022Distilling: \nUse a teaching model to generate data for smaller model. \n\u2022Faster, cheaper, more privacy This needs an LLM teacher model\nWith a lower performance!\n\n[Page 6]\nKEYIDEA\nCOLLEC T DATA, TRAIN AND TUNE HYPERPARAMETERS\nSTEP1: A SSESS AND IMPROV E WITH TOUGH EXAMPLES\nSTEP2: E VALUA TE AUTOMA TICALLY AND MANUA LLY\n\n[Page 7]\nSTANFORD LAMCase Study: WikiChat\n\u2022The first time we faced the problem of assessing LLM pipeline\n\u2022Tendency: collect data set and evaluate, repeat\n\u2022But why bother with evaluation, \nif you know it does not work on hard cases? \n\u2022We know that it will work poorly in practice!", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 8]\nSTANFORD LAMWhat do you think about the \ncasting of Oppenheimer?LLM ( Conversationality + Factuality)\nTraditional (Factuality)\n1 & 2. Search, summarize, filter:\n\u2022Much of the Oppenheimer \n(2023) movie\u2019s cast signed on \nbetween September 2021 and \nApril 2022.\n\u2022Cilian Murphy is to play as \nRobert Oppenheimer in the \nOppenheimer (2023) movie.4. Extract claims:\n\u2022Christopher Nolan is known for his meticulous casting choices.\n\u2022Christopher Nolan often casts well -known actors in his films.\n\u2022Tom Hanks stars in the Oppenheimer (2023) movie.\n\u2022Michael Caine stars in the Oppenheimer (2023) movie.\n5. Filter incorrect claims:\n\u2022Christopher Nolan is known for his meticulous casting choices.\n\u2022Christopher Nolan often casts well -known actors in his films.\n7. Refine: Christopher Nolan has cast Cillian Murphy as the titular role of Oppenheimer , keeping with his \ntradition of meticulous casting choices . I\u2019m sure it will be an excellent film!3. GPT: Christopher Nolan is known for his meticulous casting \nchoices. He often casts well -known actors in his films , and this \ntime he has selected Tom Hanks and \u2026 Michael Caine .\n6. Draft: Cillian Murphy has been cast to play Robert Oppenheimer in Christopher Nolan's \n\"Oppenheimer\" movie. Christopher Nolan is known for his meticulous casting choices and often casts \nwell -known actors in his films. The movie's cast was signed on between September 2021 and April 2022.Recall the WikiChat Pipeline", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 9]\nSTANFORD LAMWhat Was the Process?\n\u2022High level design\n\u2022Started with generation only first\n\u2022Realized we needed RAG because of new information\n\u2022Iterative improvement \n\u2022LLM hallucination with RAG on many examples \u2013we were surprised!\n\u2022Introduce filtering steps and change prompts to make it say \n\u201cI don\u2019t know\u201d\n\u2022Came up with hard cases to break the system: e.g. time\n\u2022Finally, redid the pipeline with small, well -designed steps!\n\u2022It just started to make good conversations!\nEvaluation Takes Many More Months\n\n[Page 10]\nSTANFORD LAMOld Data Sets Do Not Work\nNo simple numeric benchmark to iterate on\n\u2022Evaluation conversations in the past were crowdsourced\n\u2022Ask crowdworkers to talk to each other about a familiar topic\n\u2022Have blind spots like tailor expert topics\n\u2022Evaluation conversations were static\n\u2022Get outdated quickly\n\u2022Wizard of Wikipedia dataset topics in GPT -3\u2019s pre -training data\n\u2022New benchmarks used for pre -training in the future\n\n[Page 11]\nSTANFORD LAMWhat We Need to Improve/Eval WikiChat ?\n\u2022How to evaluate tail questions ? \n\u2022Humans do not know tail topics!\n\u2022How to evaluate LLM conversations ?\n\u2022How to compare the results between systems\nsince their dialogue diverges after the first turn! \n\u2022Cannot get a numeric result to measure success\n\u2022How to minimize human effort, cost, and potential errors ? \nSolution: LLMs come to the rescue!", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 12]\nSTANFORD LAMWhat Can LLMs Do and Not Do? \n1.Automatically generate inputs (simulated conversations) \n\u2022The user is GPT -4; the agent is the Chat pipeline\n\u2022E.g. conversations: Head (most viewed pages);  \nTail (least viewed pages); Recent (most edited in 2023)\n\u2022Are LLM conversations similar to human conversations?\n\u2022Improve and Assess\n\u2022Prompt with different user profiles \nto generate diverse user input; eyeball result \n\u2022Prove with Evaluation\n\u2022Conduct user study on a subset, show the results match\n\n[Page 13]\nSimulation Prompt\n\n\n[Page 14]\nSTANFORD LAMWhat Can LLMs Do and Not Do? \n2.Automatically evaluate if possible \n\u2022Are LLM evaluations similar to human evaluations?\n\u2022Compare with one or more humans on a subset\nQuiz: Is it possible to automate evaluation of \n\u2022Conversationality\n\u2022Fact checking\nG-Eval: NLG Evaluation using GPT -4 with Better Human Alignment , Liu et al, arXiv 202383.2% F1 with few -shot ChatGPT", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 15]\nSTANFORD LAM\nEvaluation Summary\nSimulated \nUser\nHead/Tail/Recent\nLLM chatbot Conversation Generation\nUser: I would like to talk about the 2023 \nAustralian Open Men\u2019s singles tennis \nchampionship. Did you hear about it?\nChatbot: Yes, I did hear about it! Novak \nDjokovic won the 2023 Australian Open \nMen\u2019s singles tennis championship , \nclaiming his tenth Australian Open title and \n22nd major title overall . It was an \nimpressive victory!\nClaim \nIdentifier\nEvidence \nRetrieverCrowdworker\nFactual Accuracy\nConversationality\nRaterEvaluation\nWhen claim checking fails \n\u2022IR may not retrieve \nrelevant articles \n\u2022Check manually\nExpensive, but rare\n\n[Page 16]\nSTANFORD LAMImplementation of WikiChat\n\u2022We use ColBERTv2 for IR\n\u2022Is fast (~100 msquery latency)\n\u2022Is trained on MS MARCO (2016)\n\u2022Wikipedia corpus obtained on 4/28/2023\n\u2022We use text -davinci -003 or gpt -4 as the LLM for all prompts\nColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction , Santhanam et al, NAACL 2022\n\n[Page 17]\nSTANFORD LAMExperiment\n\u2022Baselines\n\u2022Fine-tuned models\n\u2022Atlas: state -of-the-art on many knowledge -grounded tasks\n\u20223B parameters\n\u2022LLMs\n\u2022Text-davinci -003 or GPT -4, prompted to be a chatbot\n\u2022Each evaluated for 600 dialog turns\n\n[Page 18]\nResults \u2013Factual Accuracy\n94.9\n56\n47.566.198.894.698.5 97.3 97.1\n82.288.2 89.295.2\n87.190.9 91.1 90.686.589.4 88.8\n020406080100\nHead Tail Recent AllFactuality\nGPT-4 WikiChat-GPT4 WikiChat-GPT3.5 WikiChat-LLaMA Atlas", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 18]\nResults \u2013Factual Accuracy\n94.9\n56\n47.566.198.894.698.5 97.3 97.1\n82.288.2 89.295.2\n87.190.9 91.1 90.686.589.4 88.8\n020406080100\nHead Tail Recent AllFactuality\nGPT-4 WikiChat-GPT4 WikiChat-GPT3.5 WikiChat-LLaMA Atlas\n\n[Page 19]\nResults -Conversationality\n54.85 5 4.9 4.9\n4.64.9 5 54.84.64.9 4.8 4.8 4.7\n4.34.8 4.94.7\n3.6\n2.83.84.5 4.6\n012345\nRelevant Informational Natural Non-Repetitive Temporal\nGPT-4 WikiChat-GPT4 WikiChat-GPT3.5 WikiChat-LLaMA Atlas", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 19]\nResults -Conversationality\n54.85 5 4.9 4.9\n4.64.9 5 54.84.64.9 4.8 4.8 4.7\n4.34.8 4.94.7\n3.6\n2.83.84.5 4.6\n012345\nRelevant Informational Natural Non-Repetitive Temporal\nGPT-4 WikiChat-GPT4 WikiChat-GPT3.5 WikiChat-LLaMA Atlas\n\n[Page 20]\nDesign Analysis: Do we need both IR and LLM?\nInformation Retrieval and LLM are color -coded\nUser: Can we talk about Sting, the musician?\nWikiChat: Sure! Sting is an English musician and actor, best known as the frontman , \nsongwriter and bassist for the new wave rock band the Police . He has released several solo \nalbums and has won numerous awards, including 17 Grammy Awards and a Golden Globe .\nUser: [\u2026] What do you think makes Sting\u2019s music so unique?\nWikiChat: Sting\u2019s music is often very personal and introspective , and he often uses his music \nto express his own thoughts and feelings . He also has a great ability to blend different genres \nand create unique sounds .\nUser: Do you think his birthplace has influenced his music?\nWikiChat: Yes, Sting has been heavily influenced by his birthplace of Newcastle upon Tyne, \nEngland . As a child, he was inspired by the Queen waving at him from a Rolls -Royce, which \ndiverted him from the shipyard prospect towards a more glamorous life . He was also \ninfluenced by the music he heard in local nightclubs he visited such as Club A'Gogo .", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 21]\nSTANFORD LAMAnalyzing the Design\n\u2022Analysis of each component\n\u2022Do we need 2 pipelines? YES \n\u202227% of facts come from LLM generation\n\u2022Do we need to fact -check LLM output: YES\n\u2022Failed: 20% (head); 45% (tail); 56% (recent) \n\n[Page 22]\nQUIZ:\nAREWEDONE ANALYZIN G WIKICHAT?\n\n[Page 23]\nSTANFORD LAMReal User Evaluation\n\u2022Real user evaluation was seldom performed before LLM\nbecause of poor accuracy\n\u2022Most important metric!\nUser study: User reads the first sentence of a new Wikipedia page\nModel User Rating (out of 5) Factuality\nGPT-4 3.4 42.9%\nWikiChat using GPT -4 3.8 97.9%\nGPT-4: Users are not even aware that over half of the statements are false\n\n[Page 24]\nSTANFORD LAMLecture Goal\n1.How to improve and evaluate chatbots?\n\u2022Papers only describe the \u201cfinal product\u201d\n\u2022Focus on this lecture: \nWhat are the considerations in creating the product? \nVery useful for your project!\n2.Show how to apply the lecture to 2 variations of WikiChat\n\u2022Going beyond one language: Multilingual Chatbots\n\u2022Going beyond fact -based bots: Persuasive Chatbots\n\n[Page 25]\nCANWEREMOVE THELANGUAGE BARRIER\nFOR WIKICHAT?\n\n[Page 26]\nSTANFORD LAMWhere GPT -4 and Wikipedia Fail\nYoshio Toyoshima (November 27, 1890 -June 18, \n1955) was a Japanese novelist, translator, French \nliterature scholar, and children's literature author ...I\u2019ve heard about Yoshio Toyoshima , what should I read from him?\nOnly ~11% of articles are in English \n(6.7M out of 61.8M)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 27]\nSTANFORD LAMWhere GPT -4 and Wikipedia Fail\nHow did Murakami get started as a writer?\nHe has received numerous awards for his \nwork, including the Gunzo Prize for New \nWriters.\nOn April 1, 1978, while lying on the lawn of the outfield seats at Meiji \nJingu Baseball Stadium, watching the opening game of professional \nbaseball, Yakult vs. Hiroshima, and drinking beer, he decided to \nwrite a novel. It was the moment when Dave Hilton, the leadoff \nhitter for Yakult in the bottom of the first inning, hit a double to left -\ncenter field. From then on, he continued to write at the kitchen \ntable every night while running the jazz caf\u00e9 \u2026\nHaruki Murakami\n\n[Page 28]\nSTANFORD LAMTry this for yourselves!\n\u2022You will find a lot of unexpected ones\n\u2022In all language directions\nHistory of Medicine\n(by Akira Kazita )Avicenna (the Persian physician) became \nthe personal physician to Shams al -Dawla , \nthe ruler of Hamadan, and was recognized \nfor his ability to treat Shams al -Dawla\u2019s\nhernia .\n\n\n[Page 29]\nSTANFORD LAMWhy Multilingual RAG?\n\u2022People speak different languages\nBut also:\n\u2022Expanding Access to Cultural Knowledge\n\u2022Why chatbots? Can\u2019t we just translate Wikipedia? \noPeople won\u2019t seek what they don\u2019t know. \noA chatbot can offer information when it is relevant.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 29]\nSTANFORD LAMWhy Multilingual RAG?\n\u2022People speak different languages\nBut also:\n\u2022Expanding Access to Cultural Knowledge\n\u2022Why chatbots? Can\u2019t we just translate Wikipedia? \noPeople won\u2019t seek what they don\u2019t know. \noA chatbot can offer information when it is relevant. \n\n[Page 30]\nSTANFORD LAMIs Multilingual Easy with LLMs?\nTwo building blocks of retrieval -augment pipelines:\n1.LLM: strong LLMs already can understand and speak in a lot \nof languages\n\u2022Though may struggle with low -resource ones like Tamil, \nBurmese etc.\n2.  Retriever : Use a cross -lingual retriever (BGE -M3, GTE)\nIn WikiChat , we simply tell the draft stage to respond in the same \nlanguage as the user utterance\nThe difficulty: entities in different languages\n\n[Page 31]\nSTANFORD LAMMultilingual Entities are Hard!\n\u2022LLMs and Retrievers both struggle with multilingual entities\n\u2022How to transliterate names?\n\u2022Entities (books, movies, even people) \nhave different names in different languages\n\u2022Solution\n\u2022Use Wikidata entity translations to augment the text corpus\n\n[Page 32]\nSTANFORD LAMExample \nUser WikiChat\nIs Hsiang -Tsung Kung a descendant of Confucius\u201d \nI'm not certain about Hsiang -Tsung Kung's lineage, \nbut it's possible given the historical connections. \nIf you have more specific details, I might be able to help further.\n\u5b54\u7965\u91cd\u662f\u5b54\u5b50\u7684\u540e\u88d4 ?\n\u662f\u7684\uff0c\u5b54\u7965\u91cd\u662f\u5b54\u5b50\u7684\u7b2c 75\u4ee3\u5b50\u5b59\u3002\nHsiang -Tsung Kung is \u5b54\u7965\u91cd\nYes, Hsiang -Tsung Kung ( \u5b54\u7965\u91cd ) is a \n75th-generation descendant of Confucius.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 33]\nSTANFORD LAM\nhttps:// zh.wikipedia.org /wiki/\u5b54\u7965\u91cd\n(Hsiang -Tsung Kung)\n\n[Page 34]\nSTANFORD LAMMultiLingual WikiChat\n\u202210 Wikipedias\nEnglish German French Spanish Japanese \nRussian Portuguese Italian Chinese Farsi\n\u202235K+ questions so far\n\u2022Impact\n\u2022A lot more knowledge for all\n\u2022Esp. m akes English encyclopedia accessible to allBreaking news: \nNow 30 languages!\n\n[Page 35]\nSTANFORD LAM35K+ Multilingual Conversations with WikiChat\nPercentage\nLanguage\n\n[Page 36]\nQUIZ\nWHAT OTHER MULTILINGUAL USECASES\nYOUCANTHINK OF? \n\n[Page 37]\nSTANFORD LAMSummary\n\u2022Improve and assess step:  \n\u2022We already know about the entity linking problem* \n\u2022Confirmed to be a problem with WikiChat\n\u2022Easy for Wikipedia because all entities are linked\n\u2022Our assessment focused on the entity problem\n\u2022Evaluation is difficult \n\u2022Especially with the many different languages\n\u2022To be done with real human user feedback in the wild\n* Contextual Semantic Parsing for Multilingual Task -Oriented Dialogues \nMehrad Moradshahi , Victoria C. Tsai, Giovanni Campagna, Monica S. Lam\nIn Proc. of the European Chapter of the Association for Computational Linguistics (EACL) , Dubrovnik, Croatia, May 2 -6 2023.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 38]\nSTANFORD LAMLecture Goal\n1.How to improve and evaluate chatbots?\n\u2022Papers only describe the \u201cfinal product\u201d\n\u2022Focus on this lecture: \nWhat are the considerations in creating the product? \nVery useful for your project!\n2.Show how to apply the lecture to 2 variations of WikiChat\n\u2022Going beyond one language: Multilingual Chatbots\n\u2022Going beyond fact -based bots: Persuasive Chatbots\n\n[Page 39]\nCANWECREATE DOMAIN -INDEPENDENT CHATBOTS\nBEYOND\nPROVIDING FACTS ?\nPERSUASION !\n\n[Page 40]\nSTANFORD LAMPersuasive Chatbots\n\u2022E.g. donation to charity, health intervention, travel, etc.\n\u2022Persuasion: To influence users with specific goals\n\u2022Establish mutual trust and credibility\n\u2022Strategically presenting persuasive appeals\n\u2022Eliciting emotional reactions\n\u2022Persuasion skills needed\n\u2022Must provide factual answers, nonetheless\n\n[Page 41]\nSTANFORD LAM\nin Proceedings of AACL -IJCNLP 2022\n(The proposed system is called RAP)\n\n[Page 42]\nSTANFORD LAMDialogue Acts\n\u2022RAP follows the traditional line of work in conversational \nagents\n\u2022Characterize the agents\u2019 utterances \nwith a fixed set of dialogue acts (intents of agents)\n[John L. Austin, 1959]\n\u2022General acts: E.g. \u201cask question\u201d, \u201dstate a fact\u201d, \u2026\n\u2022Persuasive acts: Emotional appeal, Logical appeal\n\n[Page 43]\nSTANFORD LAMRAP Dialogue Acts for \u201cSave -the-Children\u201d", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 43]\nSTANFORD LAMRAP Dialogue Acts for \u201cSave -the-Children\u201d\n\n\n[Page 44]\nSTANFORD LAMStrategy\n\u2022In previous work, they establish that the agent \u201cthe best\u201d \nby following a strict order (as shown in the last slide)\n\u2022Expensive approach: \nhe strategy must be derived for each persuasion task\n\u2022Dialogue acts\n\u2022The order in the strategyQuiz: Why a strict order? \n\n[Page 45]\nSTANFORD LAMRAP (Response Agenda Pushing) Framework \n\n\n[Page 46]\nSTANFORD LAM\nTo appear in EMNLP, Nov. 2024\n\n[Page 47]\nSTANFORD LAMFirst Experiments\n\u2022LLMs can generate a good response \nfor everyday persuasion tasks \n\u2192why use agent dialogue acts?\n\u2022But LLMs hallucinate:\n\u2022Maria does not exist!\n\u2022Quiz: Is that OK? \n\u2022Quiz: Can we just use WikiChat ? \n\u2022General persuasive power of LLMs\n\u2022Fact -checking for correctness\nUser: I don\u2019t want to donate to Save the Children\n\n[Page 48]\nKEYCONCEPT\nSTATEMENT \u2192EXTRACT STRATEGY\nIFNOT FACT CHECKED , \nUSEINFORMATION RETRIEVAL TOFULFILL STRATEGY\n\n[Page 49]\nSTANFORD LAM\n\n\n[Page 50]\nQUIZ\nWHAT SHOULD BETHEEVALUA TION METRICS\nFOR APERSUASIVE CHATBOT ?\n\n[Page 51]\nSTANFORD LAMPersuasion Metrics\n\u2022Persuasive: Should aim to change the user\u2019s thoughts or beliefs. \n\u2022Relevant:   Should be on -topic \n\u2022Natural:      Natural to the user\u2019s utterance and context. \n\u2022Honest: \n\u2022Being an AI chatbot, should not pretend to be human; \n\u2022Should not refer to having personal physical experiences or \nbehaviors.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 52]\nSTANFORD LAMTo Prove General, Zero -Shot Capability\n\u2022Different domains: \n\u2022Download contents from websites:\nhttps:// www.savethechildren.org\nhttps:// www.japan.travel /en\nhttps:// www.cdc.gov /respiratory -viruses \n\u2022Zero -shot\n\u2022The same prompts across all the domains.\n\n[Page 53]\nSTANFORD LAMAssessment\n\u2022Can we use simulated users and simulated evaluation?\n\u2022Generate users with different profiles\n\u2022First experiments show that PersuaBot works very well\n\u2022Perhaps too well? \n\u2022Quiz: What should we do now? \n\n[Page 54]\nSTANFORD LAMExample of Persuabot & a Tough User\n\n\n[Page 55]\nSTANFORD LAMExample of GPT -3.5 and a Tough User\n\n\n[Page 56]\nSTANFORD LAMExperiment: Simulated Conversations\n\u2022For each task \n\u202220 personalities of soft users\n\u202220 personalities of tough users\n\u2022400 simulated conversations for each technique\n\u2022Persuabot , WikiChat , LLM, RAP\n\u2022Two LLMs: \n\u2022GPT 3.5 turbo -instruct\n\u2022Llama 3  (to understand how open -source models behave, \nrequested by reviewers)\n\n[Page 57]\nQUIZ\nCAN WEUSESIMULATED EVALUA TION ? \n\n[Page 58]\nSTANFORD LAMEvaluation\n\u2022Automatic evaluation failed to match human evaluation\n\u2022Bots favor their own persuasion\n\u2022Automatic evaluation leads to wrong conclusions!\n\n[Page 59]\nSTANFORD LAMSimulated Conversations with Manual Evaluation\n\n\n[Page 60]\nSTANFORD LAMTourism\n\n\n[Page 61]\nSTANFORD LAMHealth Intervention", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 59]\nSTANFORD LAMSimulated Conversations with Manual Evaluation\n\n\n[Page 60]\nSTANFORD LAMTourism\n\n\n[Page 61]\nSTANFORD LAMHealth Intervention\n\n\n[Page 62]\nSTANFORD LAMAnalysis: Do LLMs have Better Strategies? \n\u2022Result analysis on key premise: LLM dialogue acts are more \nnuanced \n\u2022Analysis of the strategies used for each app\n\u202272, 82, and 51 different strategy groups\n\u2022Automatic for each app\n\u2022Much more nuanced strategies\n\n[Page 63]\nSTANFORD LAMStrategies Used\n\n\n[Page 64]\nQUIZ:\nAREWEDONE ANALYZIN G PERSUABOT ?\n\n[Page 65]\nSTANFORD LAMReal User Conversations\n\u2022Save the children (only app available for RAP -Chen) \n\u202240 workers for each method\n\u2022465 claims extracted\n\n\n[Page 66]\nSTANFORD LAMGPT-3.5\n\n\n[Page 67]\nSTANFORD LAMPersuaBot\nQuiz: what is the difference between GPT -3.5 and Persuabot ? \n\n[Page 68]\nSTANFORD LAMWikiChat\n\n\n[Page 69]\nSTANFORD LAMChen et al. (RAP)\n\n\n[Page 70]\nQUIZ:\nWHAT ARE THENEXTSTEPS?\n\n[Page 71]\nSTANFORD LAMConclusion 1\nWhat We Learned about the LLM planet\n\u2022LLM-Based System Development\n\u2022Repeat: Improve and assess: \n\u2022Create tough examples and assess \u2013repeat\n\u2022Evaluate with simulated conversations\n\u2022Automatically / manually\n\u2022Deploy and learn from the wild \nLLM", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Eval", "content": "[Page 71]\nSTANFORD LAMConclusion 1\nWhat We Learned about the LLM planet\n\u2022LLM-Based System Development\n\u2022Repeat: Improve and assess: \n\u2022Create tough examples and assess \u2013repeat\n\u2022Evaluate with simulated conversations\n\u2022Automatically / manually\n\u2022Deploy and learn from the wild \nLLM\n\n[Page 72]\nSTANFORD LAMConclusion 2\n\u2022Multilingual WikiChat : Break down the language barrier to knowledge\n\u2022Technical idea: entity linking across languages\n\u2022Evaluation difficulty: multilingual capability \n\u2022Eval users in the wild\n\u2022Persuasive bot: Change people\u2019s beliefs or actions\n\u2022Technical idea: \n\u2022Fixed dialogue acts \u2192LLM strategies \n\u2022Hallucination is a problem: Don\u2019t just eliminate false statements\n\u2022Extract the strategy and retrieve a truthful fact\n\u2022Evaluation difficulty: factuality and persuasion both need manual evaluation", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-eval.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:15.569649"}
{"title": "L Structured", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 6\nIntroduction to Agents for \nStructured Data\n1Monica Lam & Shicheng Liu\n\n[Page 2]\nQuestion answeringArchitecture\n2Corpus\nDocuments\nKnowledge Bases\n This lecture \u2192\n\n[Page 3]\nSTANFORD LAMSemantic Parsing: NL \u2192Formal Query\n3DB/APINeural \nSemantic ParserSQL DB resultShow me the best\nJapanese restaurant \ninPalo Alto\nResponse \nGeneratorI searched for the \nbest Japanese \nrestaurant in Palo \nAlto and found \nDaigo . It has a 4.5 \nrating on our \ndatabase and offers \nsushi and Japanese \ncuisine. \n3schemaschema\nCREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);SELECT *\nFROM restaurants WHERE\n\u2018japanese \u2019 = ANY(cuisines) \nAND location = \u2018Palo Alto\u2019\nORDER BY rating DESC LIMIT 1\n\n[Page 4]\nSTANFORD LAMOutline\n\u2022Our casual questions are formal queries\n\u2022A basic LLM -based agent that consults a database \u2013Yelp\n\u2022Evaluation issues\n4\n\n[Page 5]\nSTANFORD LAMPower of Query Languages\n\u2022Domain agnostic\n\u2022Allqueries of any domain in a table\nare compositions of a FEW relational algebra operations\n\u2022Basic: Selection, Projection, Cartesian product (Join), \nUnion, Set Difference\n\u2022Extended: Sort, Aggregate Operators (Sum, Max, Avg, \u2026)\n\u2022Expressive, succinct, well -defined\n5\nAmazing\nCS Idea", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 6]\nWHERE clause: filter\nShow me Chinese restaurants inPalo AltoSELECT * FROM restaurants\nWHERE location == \u201c Palo Alto \u201d \nAND ' chinese ' = ANY (cuisines)SELECT * FROM\ntable [WHERE filter ]?\n6CREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);\n\n[Page 7]\nWHERE clause: filter\nShow me Chinese restaurants inPalo Alto\nwith at least 4.5 starsSELECT * FROM restaurants\nWHERE location = \u201c Palo Alto \u201d\nAND \u201c Chinese \u201d = ANY(cuisines)\nAND rating >= 4.5SELECT * FROM\ntable [WHERE filter ]?\n7CREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);\n\n[Page 8]\nSELECT clause: Projection\nShow me the address of  Chinese restaurants inPalo Alto\nwith at least 4.5 starsSELECT address FROM restaurants\nWHERE location = \u201c Palo Alto \u201d\nAND \u201c Chinese \u201d = ANY(cuisines)\nAND rating >= 4.5SELECT field+FROM\ntable [WHERE filter ]?\n8CREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 9]\nSubquery\nSubquery filter:    \nparam op \n(SELECT * FROM \u2026 )\n9Show me the address of  Chinese restaurants inPalo Alto\nreviewed by BobSELECT address FROM restaurants\nWHERE location = \u201c Palo Alto \u201d\nAND \u201c Chinese \u201d = ANY(cuisines)\nAND id IN \n(SELECT restaurant_id FROM reviews WHERE author =~ \u201c Bob \u201d )Atom filter:           \nparam op value\nSELECT field+\nFROM table [WHERE filter ]?CREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);\nCREATE TABLE reviews (\nid TEXT PRIMARY KEY,\nrestaurant_id TEXT\nREFERENCES restaurants(id),\nauthor TEXT,\n\u2026);\n\n[Page 10]\nSorting\nSELECT address FROM restaurants\nWHERE location = \u201c Palo Alto \u201d\nAND \u201c Chinese \u201d = ANY(cuisines)\nAND id IN \n(SELECT restaurant_id FROM reviews WHERE author =~ \u201c Bob \u201d )\nORDER BY rating DESC\nShow me the address oftop-rated Chinese restaurants inPalo Alto\nreviewed by Bob\n10SELECT field+ FROM table [WHERE filter ]?\nORDER BY field DESC/ASCCREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);\nCREATE TABLE reviews (\nid TEXT PRIMARY KEY,\nrestaurant_id TEXT\nREFERENCES restaurants(id),\nauthor TEXT,\n\u2026);", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 11]\nJoins\n11SELECT * FROM restaurants JOIN reviews ON\nrestaurants.id = reviews.restaurant _id\nWHERE location = \u201c Palo Alto \u201d\nAND \u201c Chinese \u201d = ANY(cuisines)\nORDER BY rating DESCSELECT * FROM\ntable join table [WHERE filter ]? [onfilter ]?\nShow me top-rated Chinese restaurants inPalo Alto\nwith their reviewsCREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);\nCREATE TABLE reviews (\nid TEXT PRIMARY KEY,\nrestaurant_id TEXT\nREFERENCES restaurants(id),\nauthor TEXT,\n\u2026);\n\n[Page 12]\n12Examples of Compound Questions Alexa Google Siri\nShow me restaurants rated at least 4 stars with at least 100 reviews\nShow restaurants in San Francisco rated higher than 4. 5 \u2713\nWhat is the highest rated Chinese restaurant near Stanford ? \u2713\nHow far is the closest 4 star restaurant?\nFind a W3C employee that went to Oxford\nWho worked for Google and lives in Palo Alto ?\nWho graduated from Stanford and won a Nobel prize? \u2713\u2713\nWho worked for at least 3 companies?\nShow me hotels with checkout time later than 12PM\nWhich hotel has a pool in this area? \u2713\u2713Can Assistants Understand Compound Queries?\nGenie\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nhttps:// arxiv.org /abs/2001.05609", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 13]\nSTANFORD LAMRestaurant Assistant \nCREATE TABLE restaurants (\nname               TEXT,\naddress            TEXT,\nlocation             TEXT,\nphone_number TEXT, \nopening_hours TEXT,\ncuisines             TEXT[],\nprice                   ENUM ('cheap', 'moderate', 'expensive', 'luxury'),\nrating                  NUMERIC(2,1),\nnum_reviews NUMBER,\nreviews               FREE_TEXT,\npopular_dishes FREE_TEXT,\n);\nRestaurant Assistant\n13NUMERIC(2,1) means 2 -digit precision, 1 digit after decimal\n\n[Page 14]\nSTANFORD LAMQueries in a Conversation (Yelp)\nGenerated SQL:\nSELECT * FROM restaurants \nWHERE location = \u201cMountain View\u201d\nGPT responds directly\n14Generated SQL:\nSELECT * FROM restaurants \nWHERE location = \u201cMountain View\u201d \nAND cuisines = ANY(\u201cItalian\u201d)\nGenerated SQL:\nSELECT * FROM restaurants \nWHERE location = \u201cPalo Alto\u201d \nAND cuisines = ANY(\u201cItalian\u201d)\n\n[Page 15]\nN L  \u2192FO R MA L DA TA B A SE QU E R IE S\nNA TU R A L LA NG U A GE ISC O M PO S ITI ON A L\nA C A SU A L N L  S T AT E M E N T CANCO R RE SP ON D TO\nFO R MA L QU E R IE S W IT H M A NY C L A US E S\nQU IZ: FORYE L P, \nH OW M A NY P OS SIB L E Q U E R IE S A R E T HE R E ? \n15\n\n[Page 16]\nSTANFORD LAMOutline\n\u2022Our casual questions are formal queries\n\u2022A basic LLM -based agent that consults a database \u2013Yelp\n\u2022Evaluation issues\n16", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 16]\nSTANFORD LAMOutline\n\u2022Our casual questions are formal queries\n\u2022A basic LLM -based agent that consults a database \u2013Yelp\n\u2022Evaluation issues\n16\n\n[Page 17]\nSTANFORD LAMLLM: Few -Shot Prompt \nYou are a semantic parser. Generate a query for a restaurant database with the \nfollowing signature:\nCREATE TABLE restaurants (\nname TEXT,\naddress TEXT,\nlocation TEXT,\nphone_number TEXT,\nopening hours TEXT, \ncuisines TEXT[],\nprice ENUM ('cheap', 'moderate', 'expensive', 'luxury'),\nrating NUMERIC(2,1),\nnum_review NUMBER,\n);\n17+ examples of semantic parses\n\n[Page 18]\nSTANFORD LAMCasual Assessment\n\u2022Show that LLM works surprising well\n\u2022For domains such as restaurants because of its familiarity \nwith the domain\n\u2022LLMs already know the basic SQL syntax\n\u2022Observed issue: Enumerated values\n18\n\n[Page 19]\nSTANFORD LAMIssue: Enumerated Types\n\u2022Small # values (e.g. 10): included in schema description\n\u2022LLMs know how to use these enumerated values\nWhat are some good inexpensive chinese restaurants in Palo Alto?\nSELECT * FROM restaurants WHERE \n'chinese ' = ANY (cuisines) AND location = 'Palo Alto\u2019 \nAND price = 'cheap' AND rating >= 4.0\nORDER BY num_reviews DESC LIMIT 3;\n19", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 20]\nSTANFORD LAMLarge Enumerated Types\n\u2022Example: Yelp cuisines (text) has over 200 values, e.g.\n\u2022\"brazilian \", \"southern\", \"soul food\", \"modern european \", \n\"champagne bars\", \" latin american \", \"health & medical\", \n\"singaporean \", \u2026\n\u2022Problem: Not feasible to put all choices in the schema \n\u2022Database search expects an exact match\n20show me a caf\u00e9\nSELECT * FROM restaurants WHERE 'coffee' = ANY (cuisines)Yelp: no coffee cuisine\nAvailable cuisines: \ncoffee & tea, \ncafe\n\n\n[Page 21]\nSTANFORD LAM\nLarge Enumerated Types\nSolution\n\u2022Change \u2018=\u2018 operator for a text field into a value classification operation\n\u2022classify (x, V) finds the closest values of xin V,\nreturns {} if no close values found\n\u2022Given a text field f, collect all possible values V\nx= ANY ( f) \u21d2c1= ANY ( f) OR c2= ANY ( f) \u2026, \nwhere ci\u2208classify ( x, V)\n21show me a caf\u00e9\nSELECT * FROM restaurants WHERE 'coffee' = ANY (cuisines)\nSELECT * FROM restaurants WHERE \n'coffee & tea' = ANY (cuisines) OR 'cafe' = ANY (cuisines)Semantic parser \nRewrite \u2018=\u2018 operator by calling\nclassify (\u201ccoffee\u201d, cuisines)Yelp: no coffee cuisine\nAvailable cuisines: \ncoffee & tea, \ncafe\n\n[Page 22]\nSTANFORD LAMQuiz: How to implement Classify?\n22\n\n[Page 23]\nSTANFORD LAMThe Full Agent Design\nSome turns are not queries, let LLMs answer those\nFew-shot prompts based on the domain\n23", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 22]\nSTANFORD LAMQuiz: How to implement Classify?\n22\n\n[Page 23]\nSTANFORD LAMThe Full Agent Design\nSome turns are not queries, let LLMs answer those\nFew-shot prompts based on the domain\n23\n\n[Page 24]\nSTANFORD LAMAgent Design\n24Semantic parser\nPredicted SQL\nResponse promptUser query\nSQL compiler \nDatabase\nresultsDb lookup neededInput classifier\nDB lookup\nnot needed\nClick the links to see the prompts (written in jinja syntax )No result response\nNo resultsClassify -Enum\n\n[Page 25]\nFEW-SH OT LLM -BA SE D DA TA B A SE AG E N TS\nA R E EA SY TOBU IL D\nDOTH E Y PE R F O R M WE L L?\n25\n\n[Page 26]\nSTANFORD LAMOutline\n\u2022Our casual questions are formal queries\n\u2022A basic LLM -based agent that consults a database \u2013Yelp\n\u2022Evaluation issues\n26\n\n[Page 27]\nSTANFORD LAMEvaluation\n\u2022Measure query accuracy of our annotated data\n\u2022Poor accuracy\n\u2022Quiz: what should we do next?\n\u2022Spot check results by hand!  \n(I cannot this emphasize enough)\n27\n\n[Page 28]\nSTANFORD LAMExample\nWhat's some great Mexican food around Bernal Heights?\n\u2022 Gold Target:\n\u2022 SELECT FROM restaurants \nWHERE \u201c mexican \u201d = ANY (cuisines) AND location = \u201c bernal heights\u201d;\n\u2022 Predicted Target:\n\u2022 SELECT FROM restaurants \nWHERE \u201c mexican \u201d = ANY (cuisines) AND location = \u201c bernal heights\u201d \nAND rating >= 4 ;\nQuiz: Which is correct? \n28", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "L Structured", "content": "[Page 29]\nSTANFORD LAMEvaluation example (SQL, BIRD dataset)\nName all cards with 2015 frame style ranking below 100 on EDHRec .\n\u2022 Gold Target:\n\u2022 SELECT     id     FROM cards WHERE edhrecRank < 100 AND frameVersion = 2015\n\u2022 Predicted Target:\n\u2022 SELECT name FROM cards WHERE edhrecRank < 100 AND frameVersion = 2015;\nQuiz: which is correct? \nhttps://bird -bench.github.io/\n29\n\n[Page 30]\nSTANFORD LAMOur Discovery Process\n\u2022Tried few -shot ChatGPT \nto make it conform to our convention\n\u2022Tried to fix the annotations in the benchmark\nChatGPT is better than annotations \nby PhD students!\nQuiz: What should we do next?\nTest it on a real user (not one of us \u2013designers of the system)\n30Find me a spicy Chinese restaurant in Palo Alto! \nLLM\nWe have landed \non a new planet!\n\n[Page 31]\nSTANFORD LAMConclusion\n\u2022Few-shot Chat -GPT parses SQL queries for Yelp \n\u2022Restaurants: well -known domain to ChatGPT\n\u2022Small table: 11 fields \n(incl. 2 Free -text, 1 small, 1 large ENUM)\n\u2022Well-understood field names\n\u2022Cannot evaluate based using old benchmarks!\n\u2022Cannot address the full problem without free -text retrieval!\n\u2022Must handle hybrid data before rigorous evaluation!\n31", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-structured.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:17.206965"}
{"title": "Cs 224V Hw1", "content": "[Page 1]\nCS 224V Assignment 1\nDue 2ndOctober 2024\nInstructions: Use this Colab Notebook in conjunction with this write-up. Make sure to\n\u201cSave a copy in Drive\u201d before running the notebook. Submit your answers through Grade-\nscope and attach your Google Colab notebook. In red, we label how each question in this\nwriteup corresponds to a Gradescope question\nAction Item\nGo to TogetherAI and create a free account. This should give you $5 free credit that will\nbe sufficient for your homework. Once you create your account and fill in your details\non the first page, you should receive a new API key. If the page does not redirect you\nto your API, go to the dashboard and copy the API key.\nYou will need the API key for Task 4.\nWe expect heavy loads on the OVAL machine used in this assignment close to the deadline.\nThus, we highly recommend you start this assignment early and do not wait un-\ntil the last minute. This assignment is designed to be completed in groups of 2 . Please\nsubmit as a group to Gradescope.\nExtensions: You are granted an automatic 24-hour extension to either Assignment 1 or\nAssignment 2, but not both. If you submit Assignment 1 later than the deadline, this 24-\nhour extension will be automatically applied to Assignment 1. Each individual student has\none 24-hour extension, so both partners will require an available extension to take one on\nAssignment 2.\n1 Introduction\nThis assignment is designed to give you hands-on experience with how we can leverage LLMs", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "hour extension will be automatically applied to Assignment 1. Each individual student has\none 24-hour extension, so both partners will require an available extension to take one on\nAssignment 2.\n1 Introduction\nThis assignment is designed to give you hands-on experience with how we can leverage LLMs\nto curate knowledge grounded on the web and any custom knowledge corpus. You will learn:\n\u2022how to leverage LLMs to create intelligent systems to perform complicated tasks;\n\u2022concepts of giving personas to LLMs and how to bring different perspectives;\n\u2022why and how to use retrieval systems to augment LLMs for grounding;\n\u2022how to observe weaknesses and strengths of existing systems and propose improve-\nments.\n1", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 2]\n2 STORM\nLLMs have shown remarkable performance in generating responses range of user queries\nfrom simple factual questions such as \u201cWhat is the capital of Italy?\u201d to more complicated\nquestions from International Mathematical Olympiad. Yet, writing grounded and organized\nlong-form articles from scratch has been a challenge. Recent works such as STORM can\ngenerate Wikipedia-like articles by:\n1. discovering diverse perspectives in researching the given topic,\n2. simulating conversations where writers carrying different perspectives pose questions\nto a topic expert grounded on trusted Internet sources,\n3. curating the collected information to create an outline.\n4. generating full length report leveraging section titles and collected information.\nSTORM is an open-sourced project, and we host the STORM research preview website,\nwhich allows the public to generate articles across multiple domains. As of September 23,\nmore than 100,000 articles have been generated through the website. You are encouraged to\nexplore it.\n2", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 3]\nAction Item\nLearning Goal : Dive deep into STORM. Get familiar with core design choices.\nTask 1 Read the STORM paper and answer following questions (Gradescope Q1)\n\u2022In Figure 2 of the paper, the authors implement a filtering method to exclude\nuntrustworthy sources (step 5). How do they define trusted sources?\n\u2022How do the authors identify different perspectives on a given topic? Consider why\ndirectly prompting an LLM to generate a list of perspectives without any context\nmight be ineffective.\n\u2022Besides automatic evaluation, what other evaluation methods do the authors use?\n\u2022(Optional) Try STORM UI online. (Please switch toggle to the left to choose\nSTORM mode). If you have tried the STORM website, paste the link to the\narticle you generated. (Gradescope 1.4)\n\u2022(Optional) Any feedback on the article you generated above would be appreciated!\n(Gradescope 1.5)\n\u2022(1) How would you rate the generated article on a Likert scale of 1 to 5 (with 1\nbeing the worst and 5 the best)?\n\u2022(2) What are the strengths (e.g., comprehensive outline, accurate information)\nand limitations (e.g., improper handling of time-sensitive information, associating\nunrelated sources) of the article?\n\u2022(3) STORM organizes information using a hierarchical outline (see the \u201cTable of\nContents\u201d panel on the left). Is there any additional content you expected to be\nincluded? Briefly describe it or share any follow-up questions you have about the\ntopic.", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "unrelated sources) of the article?\n\u2022(3) STORM organizes information using a hierarchical outline (see the \u201cTable of\nContents\u201d panel on the left). Is there any additional content you expected to be\nincluded? Briefly describe it or share any follow-up questions you have about the\ntopic.\n\u2022(4) Is there anything else you\u2019d like to share about your experience with STORM?\n3", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 4]\n3 Co-STORM\nSTORM automates knowledge curation by generating a full-length report based on a user\u2019s\ninput topic. During lectures, we discussed how users may want to engage more deeply with\nknowledge curation systems to guide the information-seeking process and tailor the gener-\nated report to their needs. This is where Co-STORM comes into play.\nCo-STORM extends STORM\u2019s capabilities by enabling a more interactive, collaborative ap-\nproach to knowledge discovery. While language model (LM)-powered tools like chatbots\nand search engines excel at answering well-posed questions, they struggle when it comes to\nhelping users explore unknown unknowns\u2014information they didn\u2019t even know they needed\nto find.\nInspired by educational scenarios where students learn by listening and participating in con-\nversations with teachers, Co-STORM introduces a novel method that lets users observe and\ninfluence discussions between multiple LM agents. Instead of requiring users to ask every\nquestion, the agents in Co-STORM take the initiative by asking questions on the user\u2019s\nbehalf, allowing for the serendipitous discovery of new knowledge.\nTo make the interaction more user-friendly, Co-STORM organizes the discourse into a dy-\nnamic mind map, helping users track the conversation and ultimately providing a compre-\nhensive report. The mind map visually outlines the key information uncovered, giving users\na clear understanding of how the discussion evolved.\n4", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 5]\nAction Item\nLearning Goal : Get familiar with human-AI collaborative knowledge curation system.\nTask 2 Choose a topic that you want to learn about and generate a report using\nCoSTORM Web UI. (Gradescope Q2)\nTODO items\n\u2022Log into CoSTORM Web UI. Note this is not the same as STORM UI.\n\u2022After logging in, switch the toggle to the right to activate Co-STORM mode.\n\u2022Under the topic input box, select General Internet Search (Bing Search) as your\nsearch engine.\n\u2022Choose a topic for complex information seeking, anything of your interest! The\ntopic cannot be directly answered by google search, cannot be answered with a\nshort sentence by LLM chatbot, and cannot be math and coding problems. Here\nare some examples of topics for inspiration (Please do not use any of these topics\nfor generating a report):\n\u2013The Role of AI in Addressing Climate Change\n\u2013Circular Fashion: Sustainability in the Clothing Industry\n\u2013Influence of Social Media on Election Outcomes\nOnce you have selected your topic, paste it into Gradescope. (Gradescope 2.1)\n\u2022Interact with Co-STORM.\n\u2013 Observe : Click the \u201cGenerate\u201d button to view each turn.\n\u2013 Engage : Actively engage in the conversation in response to the agent using\nthe input box at the bottom of the page.\nWe expect you to observe at least 8 non-consecutive LLM agent utterance turns\nalong with actively engaging for at least 3 non-consective turns.\n\u2022Next to each LLM agent\u2019s utterance, click the \u201cEval\u201d button to assess the quality", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "the input box at the bottom of the page.\nWe expect you to observe at least 8 non-consecutive LLM agent utterance turns\nalong with actively engaging for at least 3 non-consective turns.\n\u2022Next to each LLM agent\u2019s utterance, click the \u201cEval\u201d button to assess the quality\nof the turn. Your evaluations will be automatically logged and graded based on\nfeedback quality.\n\u2022Generate and download report. Switch to article tab (at top of the page, under\ntitle) and click generate article button. View the report (you will be asked for\nmore question on the report in Task 3). Download the PDF version of the report\nby clicking \u201dShow as PDF\u201d button at the bottom of the page.\n\u2022Upload the report to Gradescope (Gradescope 2.2)\n\u2022Paste Co-STORM session URL (in the browser URL bar, in the format of\nhttps://oval-storm-dev.vercel.app/conversation/topic-id) to Gradescope (Grade-\nscope 2.3)5", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 6]\nAction Item\nLearning Goal : Analyze strength and weakness of conducting complex information\nseeking with Co-STORM.\nTask 3 Respond to the following questions based on the generated report (Gradescope\nQ3)\n1. Did you find the information you were looking for?\n2. Do you like the structure of the outline for the report? What changes would you\nmake?\n3. Do you agree with the choice of experts generated? Why or why not?\n4. Write the three most surprising and useful citations and three irrelevant and/or off\ntopic citations.\n5. Would you like to use Co-STORM for other topics? Please name a few topics and\nscenario where you would love to use Co-STORM.\n6. (Optional) Would you like to be invited to conduct in-depth evaluation of Co-\nSTORM? (Choice will not impact your grade)\n4 Co-STORM with a Custom Knowledge Corpus\nIn the previous section, we used the Co-STORM system to search for knowledge across the\nweb. However, in many cases, users may prefer to work with their own knowledge corpus\n(data) to generate detailed reports. This section introduces how to use Co-STORM with a\ncustom knowledge corpus.\nFor this task, we will utilize arXiv as our custom knowledge corpus, instead of relying on\nthe web. arXiv is an open-access repository for electronic preprints and post-prints (known\nas e-prints), which are moderated for posting but not peer-reviewed. It covers a wide range\nof subjects. In this exercise, we will focus on the Computer Science section, specifically the", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "the web. arXiv is an open-access repository for electronic preprints and post-prints (known\nas e-prints), which are moderated for posting but not peer-reviewed. It covers a wide range\nof subjects. In this exercise, we will focus on the Computer Science section, specifically the\nComputing Research Repository (CoRR).\nAction Item\nYou will begin by selecting a sub-domain of interest from the provided list of 40 sub-\ndomains within Computer Science. Please ensure that you sign up with your group names\non the Google Sheet. Each sub-domain can accommodate a maximum of two groups,\nand once the limit is reached for a specific sub-domain, you must select an alternative.\n6", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 7]\n4.1 Learn how to Use a Custom Knowledge Corpus\nFor building a retrieval corpus, there are various methods. Traditionally, TF-IDF based sys-\ntems Sparck Jones (1972) were used. In recent years, vector embeddings Seo et al. (2019);\nCohan et al. (2020); Chen et al. (2024); Zhang et al. (2024) have gained traction because of\ntheir better performance, especially when the search query and the text do not have surface-\nlevel overlap.\nTo start, we need to select a suitable embedding model. There are benchmarks available for\nvarious domains (e.g. medical, scientific, general knowledge etc.), and a widely used one is\nMTEB1Muennighoff et al. (2023). For this assignment, we will be using the GTE-large-\nen-1.5 model Zhang et al. (2024)2for its balance of high quality and relatively low vector\nembedding size (1024 floating point numbers per vector). We also need a vector database Pan\net al. (2024). A vector database is a data structure that enables fast calculation of a similar-\nity metric like cosine similarity between a query embedding and tons of corpus embeddings.\nFor this, we will be using Qdrant3.\nTo retrieve with vector embeddings, the following steps are usually needed:\n1. The entire corpus of text should be preprocessed. This includes chunking Kamradt\n(2024) long text to fit into the context window of an embedding model.\n2. Vector embeddings for all chunks are computed by feeding them to an embedding\nmodel, one by one.", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "1. The entire corpus of text should be preprocessed. This includes chunking Kamradt\n(2024) long text to fit into the context window of an embedding model.\n2. Vector embeddings for all chunks are computed by feeding them to an embedding\nmodel, one by one.\n3. The resulting vector embeddings are stored in a vector database.\nThen, to search through the corpus, you need to:\n1. Convert the given query to a vector embedding.\n2. Calculate the similarity metric over all corpus embeddings, and return the top K most\nsimilar vectors as the output.\n1A live leaderboard is available at https://huggingface.co/spaces/mteb/leaderboard .\n2Model weights are available at https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5 .\n3https://github.com/qdrant/qdrant\n7", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 8]\n4.2 Conduct knowledge curation on a Custom Knowledge Corpus\nAction Item\nLearning Goal : Dive Deep into Co-STORM; Get Hands-on!\nTask 4 : Read Co-STORM paper and follow the provided Google Colab notebook to\nexplore the internal workings of Co-STORM. You will gain insights into what it takes to\nbuild a system like Co-STORM.\n\u2022How many types of roles are proposed in the collaborative discourse protocol in\nthe paper? (Gradescope Q4.1).\n\u2022What are the two operations for maintaining the dynamic mind map discussed in\nthe paper? (Gradescope Q4.2).\n\u2022Upload the cs224v assignment submission dir.zip file to (Gradescope Q4.3).\n\u2022Upload the Google Colab notebook as a PDF. In Google Colab, click File \u2192Print\n\u2192\u201cSave as PDF\u201d and upload the downloaded PDF file to (Gradescope Q4.4)\nAction Item\nLearning Goal : Evaluate the Strengths and Weaknesses of Using Co-STORM for\nComplex Information-Seeking in the Academic Domain\nTask 5: After generating a report using Co-STORM on the academic paper cor-\npus via Google Colab notebook, answer the following questions: (Gradescope Q5)\n1. How did Co-STORM respond when you tried to steer the conversation by injecting\nyour own utterance? Did you feel you could effectively change the focus of the\ndiscussion? Why or why not?\n2. Do you agree with the selection of experts identified in the report? Explain.\n3. List the three most surprising and useful citations from the report, as well as three\ncitations you found irrelevant or off-topic.", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "discussion? Why or why not?\n2. Do you agree with the selection of experts identified in the report? Explain.\n3. List the three most surprising and useful citations from the report, as well as three\ncitations you found irrelevant or off-topic.\n4. Did the report contain the information you were seeking? Explain your answer.\n5. What are the key differences between the reports generated using the web as a\ndata source and the customized academic paper corpus?\n6. Propose improvements for knowledge curation systems to enhance their perfor-\nmance when working with custom academic corpora.\n7. What additional custom corpora would you find most useful for use on Co-STORM?\n8. What are some potential applications that can leverage Co-STORM with arXiv?\n8", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "[Page 9]\nReferences\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.\nM3-embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings\nthrough self-knowledge distillation. In Findings of the Association for Computational Lin-\nguistics ACL 2024 , pages 2318\u20132335, Bangkok, Thailand and virtual meeting. Association\nfor Computational Linguistics.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020.\nSPECTER: Document-level Representation Learning using Citation-informed Transform-\ners. In ACL.\nGreg Kamradt. 2024. 5 levels of text splitting. https://github.com/\nFullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/\nLevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb . Accessed: 2024-09-14.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive\ntext embedding benchmark. In Proceedings of the 17th Conference of the European Chapter\nof the Association for Computational Linguistics , pages 2014\u20132037, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nJames Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database management\nsystems. The VLDB Journal , 33(5):1591\u20131615.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh\nHajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase\nindex. In Proceedings of the 57th Annual Meeting of the Association for Computational", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "Cs 224V Hw1", "content": "systems. The VLDB Journal , 33(5):1591\u20131615.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh\nHajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase\nindex. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics , pages 4430\u20134441, Florence, Italy. Association for Computational Linguistics.\nKaren Sparck Jones. 1972. A statistical interpretation of term specificity and its application\nin retrieval. Journal of documentation , 28(1):11\u201321.\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin,\nBaosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. 2024.\nmgte: Generalized long-context text representation and reranking models for multilingual\ntext retrieval.\n9", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW1.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:19.498979"}
{"title": "L Projects", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nProjects\n1Monica Lam\n\n[Page 2]\nST ANFORD LAMYou are welcome \nto propose your own projects.\nLooking forward to a great quarter\ninventing the future together!\n\n[Page 3]\nST ANFORD LAM3 Major Project Themes this Year\n1.World Wide Knowledge (WWK)\n2.Apprentice Training\n3.Problem decomposition with a \nWell-Designed Intermediate Representation\n\n[Page 4]\nST ANFORD LAM1. World Wide Knowledge (WWK)\n\u2022Use AI to accelerate knowledge production & improve access\n\u2022Example projects: \n\u2022DataTalk : Big Local News\n\u2022Real -time data for election 2024\n\u2022HistoryChat : Center of African Studies\n\u2022Discover history from the ground up with African Times\n\u2022Multi -lingual news platform: \nArmed Conflict Location and Event Data (ACLED)\n\u2022Automatic multilingual qualitative coding of world news\n\n[Page 5]\nST ANFORD LAMUpload multinews  video\n\n\n[Page 6]\nST ANFORD LAMSuccessful projects will be showcased at:\n  \nA Workshop on a Public AI Assistant for WWK\non Feb 13 -14, 2025\n\u2022Sponsored by Sloan Foundation and HAI\n\u2022Invitation -only \n\u2022Funding agencies: \ngovernment, philanthropies, companies\n\u2022Journalists, historians, social scientist, technologists\n\n[Page 7]\n2. Apprentice Training", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Projects", "content": "[Page 7]\n2. Apprentice Training\n\n[Page 8]\nST ANFORD LAMBackground: LLM -based Engineering\n1. Static pipeline ( Wikichat )\n2. Agentic approach (e.g. Wikidata  semantic parsing)\n\u2022Define a set of actions, including evaluation (e.g. execution of query))\n\u2022Agent iteratively: proposes solution, evaluates, backtracks, if necessary \n\u2022New project: add tree search of solutions to the agentic pipeline. \n3. Finetuning \n\u2022When the instructions cannot fit in the prompts\n\u2022To speed up or reduce cost of execution\nWhat if these techniques are not good enough?\n\n[Page 9]\nST ANFORD LAMApprentice Training\n\u2022Key issue: tacit knowledge\n\u2022Many tasks require info in experts\u2019 head \n\u2022e.g. The donation column in the dataset does not include \ndonations under $200.\nTherefore, adding the column up does not match the \nreported total. \n\u2022We need to \u201cdistill the information\u201d from experts\n\u2022Human solution: apprenticeships, internships \u2013 learn by practicing\n\u2022Apply written knowledge, practice, ask, and get corected\n\n[Page 10]\nST ANFORD LAMData Driven \u2192 Rationale Driven\n\u2022Hard to get enough data points for high -level tasks\n\u2022Humans are data efficient: they learn from rationales:\n\u2022E.g. when classifying events, an armed protest should be \nclassified as an \u201carmed event\u201d and not a \u201cprotest event\u201d.  \n\u2022How many data points do you need to teach that principle?\n\u2022Humans can generalize rationales:\n\u2022Choose the event type that is most escalated", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Projects", "content": "[Page 11]\nST ANFORD LAMArchitecture of Apprentice Training\nLLM-based \nSystem\nRationaleExperterrorInput output\ne.g. Rules to apply in agentic approach\n- check the different rules \n  in different prompts?  \n\n[Page 12]\nST ANFORD LAMResearch Approach\n\u2022Apply this technique to many more domains:\n\u2022Currently used in 2 real -world scenarios\nElection Datatalk  & ACLED event classification\n\u2022Create a general apprentice framework\n\n[Page 13]\n3. Intermediate Representations\n\n[Page 14]\nST ANFORD LAMImproves Data Efficiency\nWell-Designed Intermediate Representation\n\u2022Massive data analysis : qualitative coding\n\u2022Coding according to a given codebook \n(events, resumes, medical transcripts)\n\u2022Auto -designed schema by inferring them with \nquestions from experts \u2013 they can simulated\n\u2022Compliance : SMT (Satisfiable Modulo Theory)\n\u2022Course requirements, invoice auditingComponent 1Input\noutputComponent 2Intermediate \nRepresentation\n\n[Page 15]\nST ANFORD LAM\nAI\n\n[Page 16]\nST ANFORD LAMIntermediate Representation: SMT\n(Satisfiability Modulo Theory)\n\u2022LLMs are reasonably good for simple conditions\n\u2022But what if it screws up? \n\u2022Reduce the LLM complexity\n\u2022Translate to just the SMT constraints\n\u2022A simpler and necessary skill \n\u2022Interpretability: constraints can be reviewed\nonce and for all  \n\u2022Use a theorem prover\n\u2022Guaranteed to be correct LLM-based\nSemantic ParserRequirements\nGraduate/\nNot GraduateSMT SolverSMT formulaTranscript", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Projects", "content": "[Page 17]\nST ANFORD LAMAn Easy Example\nFOUNDATIONS REQUIREMENT:\nYou must satisfy the requirements listed in each of the \nfollowing:\n1.Logic, Automata & Complexity (CS103)*\n2.Probability (CS109, Stat116, CME106, MS&E220, or EE178)\u200b\n3.Algorithmic Analysis (CS161)\n4.Computer Organ & Sys (CS107 or 107E)\n5.Principles of Computer Systems (CS110 or CS111)\u200b\u200b", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Projects", "content": "[Page 18]\nLLM-Generated SMT Formula\n(set-logic ALL)\n;; declaring variables\n(declare -const course1 String) (declare -const course2 String) (declare -const course3 String) (declare -const course4 \nString) (declare -const course5 String)\n;; check required course(s) of each group\nassert(and(and(and(and((= course1 \"CS103\") \n(or (= course2 \"CS109\") (= course2 \"Stat116\") (= course2 \"CME106\") (=course2 \"MS&E220\")   (=course2 \"EE178\")))\n(= course3 \"CS161\")) (or (= course4 \"CS107\") (= course4 \"CS107E\"))) \n(or (= course5 \"CS110\") (= course5 \"CS111\")))\n;; check required course(s) of each group are in aa given transcript\nassert(and(and(and(and((= course1 course.get (\"Course_Id \") for course in Transcript.get (\"Courses_Taken \")\n(= course2 course.get (\"Course_Id \") for course in Transcript.get (\"Courses_Taken \"))\n(= course3 course.get (\"Course_Id \") for course in Transcript.get (\"Courses_Taken \"))\n(= course4 course.get (\"Course_Id \") for course in Transcript.get (\"Courses_Taken \"))\n(= course5 course.get (\"Course_Id \") for course in Transcript.get (\"Courses_Taken \"))\n(check -sat)1. Logic, Automata & Complexity (CS103)*\n2. Probability (CS109, Stat116, CME106, MS&E220, or EE178)\u200b\n3. Algorithmic Analysis (CS161)\n4. Computer Organ & Sys (CS107 or 107E)\n5. Principles of Computer Systems (CS110 or CS111)\u200b\u200b", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Projects", "content": "[Page 19]\nST ANFORD LAMImproves Data Efficiency\nWell-Designed Intermediate Representation\n\u2022Massive data analysis : qualitative coding\n\u2022Coding according to a given codebook \n(events, resumes, medical transcripts)\n\u2022Auto -designed schema by inferring them with questions \nfrom experts \u2013 they can simulated\n\u2022Compliance : SMT (Satisfiable Modulo Theory)\n\u2022Course requirements, invoice auditing\n\u2022Web agents : Verbal description of visual elements on web pagesComponent 1Input\noutputComponent 2Intermediate \nRepresentation\n\n[Page 20]\nST ANFORD LAMUniversal WebAgent\n\u2022Build a conversational agent \nto replace a web interface, \ngiven instructions. \n\u2022Large dataset for webtasks : \nThe Mind2Web dataset\nGrounding Open -Domain Instructions to Automate Web Support T asks \nNancy Xu, Sam Masling , Michael Du, Giovanni Campagna, Larry Heck, James Landay , \nMonica S Lam, NAACL \u201921.\nMind2Web: Towards a Generalist Agent for the Web. Xiang Deng, Yu Gu, Boyuan  \nZheng, Shijie  Chen, Samuel Stevens, Boshi  Wang, Huan Sun, Yu Su. In NeurIPS , 2023", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Projects", "content": "[Page 21]\nST ANFORD LAMImproves Data Efficiency with a \nWell-Designed Intermediate Representation\n\u2022Massive data analysis : qualitative coding\n\u2022Coding according to a given codebook \n(events, resumes, medical transcripts)\n\u2022Auto -designed schema by inferring them with questions \nfrom experts \u2013 they can simulated\n\u2022Compliance : SMT (Satisfiable Modulo Theory)\n\u2022Course requirements, invoice auditing\n\u2022Web agents : Verbal description of visual elements on web pages\n\u2022Papers to decision making : Derive a decision treeComponent 1Input\noutputComponent 2Intermediate \nRepresentation\n\n[Page 22]\nST ANFORD LAM3 Major Project Themes this Year\n1.World Wide Knowledge (WWK)\n2.Apprentice Training\n3.Problem decomposition with a \nWell-Designed Intermediate Representation", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-projects.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:20.993211"}
{"title": "L Multimodal", "content": "[Page 1]\nMultimodal ApplicationsJackie (Junrui) Yang & Monica Lam \n 1\n\n[Page 2]\nA bit about myself - Jackie Yang\u2022PhD - Dissertation with Monica Lam and James Landay\n\u2022Thesis title: Multimodal AI-augmented User Interface Development Architecture\n\u2022Research topics: Multimodal interactions\n\u2022Now:\n\u2022Working on my startup: VP of Software and ML @ Skywalk Inc.\n\u2022Vision: Always-available private voice interfaces 2\n\n\n[Page 3]\nLecture Goals\u2022Why do we need multimodal interactions? \u2022Three problems for multimodal app development\n\u2022ReactGenie: a multimodal app development framework\n\u2022GenieWizard: addressing the multimodal app\u2019s feature discovery problem\n 3\n\n[Page 4]\nInteracting with Touch + Graphics: Powerful and accurate\n 4\nPowerPoint:\n\u2022Textboxes, pictures, shapes\n\u2022Fonts, colors, line styles\n\u2022Adjust everything accurately via GUI\n\n[Page 5]\nInteracting with Touch + Graphics: But sometimes inefficient and repetitive\n 5Slow and tedious for\n\u2022Apply actions to multiple objects\n\u2022Less common features\n\n\n[Page 6]\nExample: Powerpoint\u2022Lots and lots of nested menus\n 6\n\n\n[Page 7]\nExample: Powerpoint\u2022Lots and lots of nested menus\n\u2022Horizontal ones and vertical ones\n\u2022Takes a long time to make slides\u2028(even if you know what functions are available)   7\n\n\n[Page 8]\nHere are Some Examples:\u2022Make this text box bold in the slide master.\n     \u2022Make the border of this shape with little dots.\n\u2022Make everything right aligned on this slide.\n\u2022Make every shape on this slide above this yellow.\n  8", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 8]\nHere are Some Examples:\u2022Make this text box bold in the slide master.\n     \u2022Make the border of this shape with little dots.\n\u2022Make everything right aligned on this slide.\n\u2022Make every shape on this slide above this yellow.\n  8\n\n[Page 9]\nMultimodal Interaction: High-level goals rather than low-level actions\n 9\u201cMake all the word \u2018ReactGenie\u2019 red and bold\u201d\nInstead of\nLow-level actions:\n[select] -> [change color] -> [make bold] \u2026 x 3High-level goals\n\n[Page 10]\nPrior Work on Multimodal Interaction\nPut-That-There\n1982 Bolt et al.\n 10\n\n[Page 11]\nLimited adoption\nWhy?\n 11\n\n[Page 12]\nLecture Goals\u2022Why do we need multimodal interactions? \u2022Three problems for multimodal app development \u2022Compositionality of multimodal commands \u2022Expose diverse actions/APIs from a GUI app \u2022Allows for interchangeable and simultaneous multimodal interactions \u2022ReactGenie: a multimodal app development framework\n\u2022GenieWizard: addressing the multimodal app\u2019s feature discovery problem 12\n\n[Page 13]\nRecall the examples earlierMake this text box bold in the slide master.\nMake the border of this shape with little dots.\nMake everything right aligned on this slide.\nMake every shape on this slide above this yellow.\n 13", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 13]\nRecall the examples earlierMake this text box bold in the slide master.\nMake the border of this shape with little dots.\nMake everything right aligned on this slide.\nMake every shape on this slide above this yellow.\n 13\n\n[Page 14]\nProblem 1: Compositionality of multimodal commandsMake this text box bold in the slide master.\nMakeSlideMasterTextBold()Make the border of this shape with little dots.\nMakeShapeBorder(borderType:\"littleDots\")Make everything right aligned on this slide.\nSetEverythingAlignment(alignment:\"Right\")  Make every shape on this slide above this yellow.\nSetShapeAboveColor(color:\"Yellow\") 14This is not scaleable with simple function calling!!!\n\n[Page 15]\nProblem 1: Compositionality of multimodal commandsMake this text box bold in the slide master.\nSlide.Current().getSlideMaster().matching(\ufb01eld:.id,value:Shape.Current().id).textFrame.textRange.font.setBold(bold: true)   Make the border of this shape with little dots.\nShape.Current().lineFormat.setDashStyle(dashSyle:\"RoundDot\"\uff09     Make everything right aligned on this slide.\nSlide.Current().getShapes().textFrame.textRange.paragraphFormat.setHorizontalAlignment(horizontalAligment:\"Right\")  Make every shape on this slide above this yellow.\nSlide.Current().getShapes().between(\ufb01eld:.top,to:Shape.Current().top).\ufb01ll.setForeGroundColor(color:\"yellow\")  15Solve Compositionality with compound function calls NLPL (natural-language programming language)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 16]\nSolution to compositionally: NLPL\nNo param names, more errorsExisting languageNLPLDelete Shapes above thisSlide.Current().\ufb01ndShape(  above=Shape.Current())Python\nselect * from shape where top>current_topSQLTypeScriptSlide.Current().\ufb01ndShape(  Shape.Current().top).forEach((x)=> x.delete())Slide.Current().\ufb01ndShape(above: Shape.Current().top).forEach{$0.delete()}SwiftSlide.Current().getShapes().between(  \ufb01eld: .top,  to: Shape.Current().top).delete()Weak type, more errors\nAmbiguous queryNo ActionEasy to generate Versatile query Fewer errors No Lambda ExpressionAutomatically distributed to each element", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 17]\nExpressiveness of NLPL\n 17Feature of NLPLEnglish grammarFood OrderingPowerPointSocial NetworkCall function of a objectSingular Object + VerbOrder.Current().place()TextRange.Current().setBold(bold: true)Post.Current().like()Distribute action to an array of objectsPlural Object + VerbOrder.All()[-1].foods.like()Slide.Current().getShapes().textFrame.setText(\u201c\u201d)User.Me().posts.delete()Specify function parametersObject + Verb + Verb Modi\ufb01erOrder.Current().addFoods(foods:[Food.GetFood(name:\u201cBurger\u201d)])Shape.Current().textFrame.setText(text: \u201c12345\u201d)Post.Current().comment(comment: \u201cNice Photo\u201d)Select objects to do actionsObject Modi\ufb01er + Object + VerbRestaurant.Current().foods.sort(\ufb01eld: .price)[0].order()Shape.All().matching(\ufb01eld: .textFrame.text, value: \u201cyellow).delete()Post.All().equals(\ufb01eld: .like, value: true)Change background color for all the yellow shapes to orangeShape.All().equals(\ufb01eld: .shapeFill.foregroundColor, value: \u201cyellow\u201d).shapeFill.setForegroundColor(color: \u201corange\u201d)VerbObject Modi\ufb01erObjectVerb Modi\ufb01er\n\n[Page 18]\n 18\n\n\n[Page 19]\nGenie PowerPoint\u2022110 APIs: Slide, Shape, SlideMaster, TextFrame, TextRange, \u2026\n\u2022That\u2019s everything from MS PowerPoint JS API.\n\u2022Yet, it still does not have common APIs like Animation, Font color, etc.\n\u2022It is super hard to expose API outside of the development cycle of GUI\n\u2022With the advancement of AI, developers need to build two interfaces:\n\u2022A human interface, and\n\u2022An AI interface\n\u2022That\u2019s double the work on developers! 19", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 20]\nProblem 2: Expose diverse actions/APIs from a GUI app\u2022110 APIs: Slide, Shape, SlideMaster, TextFrame, TextRange, \u2026\n\u2022That\u2019s everything from MS PowerPoint JS API.\n\u2022Yet, it still does not have common APIs like Animation, Font color, etc.\n\u2022It is super hard to expose API outside of the development cycle of GUI\n\u2022With the advancement of AI, developers need to build two interfaces:\n\u2022A human interface, and\n\u2022An AI interface\n\u2022That\u2019s double the work on developers! 20\n\n[Page 21]\nclass Order extends DataClass {    public orderId: string;    public orderItems: FoodItem[];    constructor({orderId, orderItems}: {orderId: string, orderItems: FoodItem[]}) {        super({orderId, orderItems}); this.orderId = orderId; this.orderItems = orderItems;    }    static All(): Order[] {        return fetchOrdersFromServer();    }    static CreateOrder(): Order {        return new Order({orderId: randomId(), orderItems: []});    }    addItem({foodItem}: {foodItem: FoodItem}) {        this.orderItems.push(foodItem); updateServer();    }}Problem 2: Expose diverse actions/APIs from a GUI app\n 21@GenieClass(\u201cPast order or a shopping cart\")    @GenieKey()    @GenieProperty(\"Items in the order\")       @GenieFunction()        @GenieFunction(\"Create a new order\")        @GenieFunction(\"Add an item to the order\")    ReactGenie Annotations\nReact App Logic Code\n\n[Page 22]\nWhen using a ReactGenie app\n 22\nWhich \nrestaurant?How can the user know what happened? \n\ud83e\udd37Where is my burger? \n\ud83e\udd37\nWhere is my order? \n\ud83e\udd37", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 22]\nWhen using a ReactGenie app\n 22\nWhich \nrestaurant?How can the user know what happened? \n\ud83e\udd37Where is my burger? \n\ud83e\udd37\nWhere is my order? \n\ud83e\udd37\n\n[Page 23]\nProblem 3: Allows for interchangeable and simultaneous multimodal interactions\nWhich restaurant?\nUI input mapping\nChanges on screen: UI updates\nChanges o\ufb00-screen: Navigation\n\n[Page 24]\nProblem 3: Allows for interchangeable and simultaneous multimodal interactions\u2022Multimodal Input\n\u2022UI Input Mapping: Getting what object that I\u2019m touching\n\u2022Multimodal Output\n\u2022Resulting objects are on-screen \u2014 UI Updates: Update the values on screen\n\u2022Resulting objects are o\ufb00-screen \u2014 Navigation: Navigate to the page with results 24\n\n[Page 25]\nLecture Goals\u2022Why do we need multimodal interactions? \u2022Three problems for multimodal app development \u2022ReactGenie: a multimodal app development framework\n\u2022GenieWizard: addressing the multimodal app\u2019s feature discovery problem\n 25\n\n[Page 26]\nWhat is ReactGenie?\n 26React Code (In recommended style)Annotations (< 5% code typically)ReactGenie Code+Combinations of Voice + GUI inputReactGenie DSL CodeSemantic Parser UI MappingExecutionBoth Voice + GUI outputEase of DevelopmentRich Multimodal Functionality\n\n[Page 27]\nReactGenie Demo\n 27", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 27]\nReactGenie Demo\n 27\n\n\n[Page 28]\nModern GUI 101: State + Components\nRecipeViewImpl = (recipe: Recipe) => {  return (    <div>      <img image={recipe.img}/>      <love loved={recipe.loved}       onClick(()=>recipe.love())/>      <div> {recipe.name} </div>    </div>  )}class Recipe {  name: String;  img: Image;  loved: boolean;  love(): void {    this.loved = true;}}State Code: Implements FeaturesComponents: Describe GUI 28\n\n[Page 29]\n ReactGenie = React + AnnotationsRecipeViewImpl = (recipe: Recipe) => {  return (    <div>      <img image={recipe.img}/>      <love loved={recipe.loved}       onClick(()=>recipe.love())/>      <div> {recipe.name} </div>    </div>  )}export RecipeView = GenieInterface(\u201cRecipe\u201d,                             RecipeViewImpl)@DataClass()class Recipe: GenieClass {@GenieProperty()  name: String;  img: Image;@GenieProperty()  loved: boolean;  @GenieFunction()  love(): void {    this.loved = true;}}State Annotations:\nWhich class/property/function can be accessed with voiceComponents Annotations:\nWhich components represent which state classes 29\n\n[Page 30]\nRecall the Agent Architecture (No Policies)\n 30Text inputUser Dialogue StateSemantic ParserExecutionResultResponse GeneratorAgent OutputDialogue StateAgent", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 30]\nRecall the Agent Architecture (No Policies)\n 30Text inputUser Dialogue StateSemantic ParserExecutionResultResponse GeneratorAgent OutputDialogue StateAgent\n\n[Page 31]\nReact has a Similar Architecture\n 31Text inputUser Dialogue StateSemantic ParserExecutionResultResponse GeneratorAgent OutputDialogue StateText inputComponent API callUI MappingExecutionResultUI UpdateDisplayReact Program StateReactAgent\n\n[Page 32]\nMultimodal Agent Architecture\n 32Text inputUser Dialogue State (NLPL)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie\n\n[Page 33]\nSemantic Parser + Response Generator\n 33Text inputUser Dialogue State (NLPL)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 33]\nSemantic Parser + Response Generator\n 33Text inputUser Dialogue State (NLPL)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie\n\n[Page 34]\nRevisiting NLPLMake this text box bold in the slide master.\nSlide.Current().getSlideMaster().matching(\ufb01eld:.id,value:Shape.Current().id).textFrame.textRange.font.setBold(bold: true)   Make the border of this shape with little dots.\nShape.Current().lineFormat.setDashStyle(dashSyle:\"RoundDot\"\uff09     Make everything right aligned on this slide.\nSlide.Current().getShapes().textFrame.textRange.paragraphFormat.setHorizontalAlignment(horizontalAligment:\"Right\")  Make every shape on this slide above this yellow.\nSlide.Current().getShapes().between(\ufb01eld:.top,to:Shape.Current().top).\ufb01ll.setForeGroundColor(color:\"yellow\")  34NLPL (natural-language programming language)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 35]\n// Here are all the functions that we haveclass Restaurant {    string name;    string address;    string cuisine;    \ufb02oat rating;        // All active restaurants    static Restaurant[] all();        // The current restaurants    static Restaurant current();        // Get a list of foods representing the menu from a restaurant    Food[] menu;        // Book reservations on date    Reservation get_reservation(date: DateTime)}Declaration\n// Examples:user: get me the best restaurant in palo altoagent: Restaurant.all().matching(\ufb01eld: .address, value: \"palo alto\u201d)\u2026Few-shot examples// User interactionuser: order the same burger that I ordered at mcDonald last timeCurrent interactionparsed: Order.current.addFoods(foods: Order.all().matching\u2026Parsed resultLLM-Based Semantic Parser \n 35", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 36]\n// Here are all the functions that we haveclass Restaurant {    string name;    string address;    string cuisine;    \ufb02oat rating;        // All active restaurants    static Restaurant[] all();        // The current restaurants    static Restaurant current();        // Get a list of foods representing the menu from a restaurant    Food[] menu;        // Book reservations on date    Reservation get_reservation(date: DateTime)}Declaration\n// Generate concise voice feedback for the user\u2019s commandInstructions// User interactionuser: order the same burger that I ordered at mcDonald last timeparsed: Order.current.addFoods(foods: Order.all().matching\u2026execution_result: {\"type\": \"Order\", \"items\": [{\"type\": \"FoodItem\", \"name\": \"Hamburger\"}, {\"type\": \"FoodItem\", \"name\": \"Fries\"}]}Current interactionresponse: Your order with a hamburger and fries has been placed.Generated ResponseLLM-Based Response Generator\n 36\n\n[Page 37]\nHow to Handle Hybrid Inputs\n 37Text inputUser Dialogue State (NLPL)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie\n\n[Page 38]\nUI Mapping\nProgramming Objects\nViewReactGenieRestaurant\n(name: \u201cMcdonald\")Order(date: \u201c3/3/2023\u201d)FoodItem\n(name: \u201cboba tea\u201d)FoodItem\n(name: \u201cmango drink\u201d) 38", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 38]\nUI Mapping\nProgramming Objects\nViewReactGenieRestaurant\n(name: \u201cMcdonald\")Order(date: \u201c3/3/2023\u201d)FoodItem\n(name: \u201cboba tea\u201d)FoodItem\n(name: \u201cmango drink\u201d) 38\n\n[Page 39]\nHow to Handle Hybrid Inputs\n 39\n\ud83d\udc46User: \u201cReorder this food\u201dOrder.GetActiveOrder().addFood(food:[FoodItem.Current()])Unresolved UI reference FoodItem.Current()UI Mapping: Find FoodItem closest to the click pointFoodItem(name:\"CrunchWrap\")Continue execution\n\n[Page 40]\nHow to Navigate with Voice Commands\n 40Text inputUser Dialogue State (NLPL)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie\n\n[Page 41]\nUI Update 1: Object is on UI \u2192 React takes care of it\nRecipe(name: \u201cCreamy\u201d). love()\u201cI love the Creamy Potatoes recipe!\u201d\n 41recipe.loved = trueRecipeViewImpl = (recipe: Recipe) => {  return (    <div>      <img image={recipe.img}/>      <love loved={recipe.loved}       onClick(()=>recipe.love())/>      <div> {recipe.name} </div>    </div>  )}\n\n[Page 42]\nUI Update 2: Object not on UI \u2192 Navigate to the page\n 42Recipe(name: \u201cCreamy Potato\u201d)\n\u201cShow me the Creamy Potato recipe!\u201d\n\n\n[Page 43]\nExecution\n 43Text inputUser Dialogue State (`)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 43]\nExecution\n 43Text inputUser Dialogue State (`)Semantic ParserExecutionResultResponse GeneratorAgent OutputText inputComponent API callUI MappingResultUI UpdateDisplayReact Program StateReactGenie\n\n[Page 44]\nExecution\n\u2022Slide.Current()\u2022Slide(id: 1)\u2022Slide.Current().getShapes()\u2022[Shape(text: \"A\"), Shape(text: \"B\"), Shape(text: \"C\"), Shape(text: \"D\"), Shape(text: \u201cE\")]\u2022Slide.Current().getShapes().between(\ufb01eld:.top, to:Shape.Current().top)\u2022100\u2022Slide.Current().getShapes().between(\ufb01eld:.top, to:Shape.Current().top)\u2022[Shape(text: \"A\"), Shape(text: \"B\")]\u2022Slide.Current().getShapes().between(\ufb01eld:.top, to:Shape.Current().top).\ufb01ll.setForeGroundColor(color:\u201dyellow\")\u2022[Fill(), Fill()] 44DECABSlide.Current().getShapes().between(\ufb01eld:.top,to:Shape.Current().top).\ufb01ll.setForeGroundColor(color:\"yellow\") \u201cMake everything above this yellow\u201dAB", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 45]\n ReactGenie = React + AnnotationsRecipeViewImpl = (recipe: Recipe) => {  return (    <div>      <img image={recipe.img}/>      <love loved={recipe.loved}/>      <div> {recipe.name} </div>    </div>  )}RecipeView = GenieInterface(\u201cRecipe\u201d,                             RecipeViewImpl)@DataClass()class Recipe: GenieClass {@GenieProperty()  name: String;  img: Image;@GenieProperty()  loved: boolean;  @GenieFunction()  love(): void {    this.loved = true;}}State Annotations: Which class/property/function can be accessed with voiceComponents Annotations: Which components represent which state classes 45Semantic ParserExecutionResponse GeneratorUI MappingUI Update\n\n[Page 46]\nRecap: ReactGenie Uses Declarative UI Architecture for Ease of Development\n 46\nDe\ufb01ne Object-Oriented StatesTaco BellTaco 3/3OrderRestaurantMr Sun 3/3OrderCrunchwrapFoodItemQuesadillaFoodItemTaro bobaFoodItem\nDe\ufb01ne UI Components RestaurantItemViewOrderItemViewFoodThumbnail", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 46]\nRecap: ReactGenie Uses Declarative UI Architecture for Ease of Development\n 46\nDe\ufb01ne Object-Oriented StatesTaco BellTaco 3/3OrderRestaurantMr Sun 3/3OrderCrunchwrapFoodItemQuesadillaFoodItemTaro bobaFoodItem\nDe\ufb01ne UI Components RestaurantItemViewOrderItemViewFoodThumbnail\n\n\n[Page 47]\nclass Order extends DataClass {    public orderId: string;    public orderItems: FoodItem[];    constructor({orderId, orderItems}: {orderId: string, orderItems: FoodItem[]}) {        super({orderId, orderItems}); this.orderId = orderId; this.orderItems = orderItems;    }    static All(): Order[] {        return fetchOrdersFromServer();    }    static CreateOrder(): Order {        return new Order({orderId: randomId(), orderItems: []});    }    addItem({foodItem}: {foodItem: FoodItem}) {        this.orderItems.push(foodItem); updateServer();    }}@GenieClass(\u201cPast order or a shopping cart\")    @GenieKey()    @GenieProperty(\"Items in the order\")       @GenieFunction()        @GenieFunction(\"Create a new order\")        @GenieFunction(\"Add an item to the order\")    Recap: ReactGenie Use Annotations for Multimodal Integration\n 47\n\n[Page 48]\nDe\ufb01ne Object-Oriented StatesTaco BellTaco 3/3OrderRestaurantMr Sun 3/3OrderCrunchwrapFoodItemQuesadillaFoodItemTaro bobaFoodItem\nDe\ufb01ne UI Components RestaurantItemViewOrderItemViewFoodThumbnailDeveloper-Coded GUI\n\ud83d\udc46Recap: ReactGenie Execute User\u2019s Request within UI Context\n 48", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 48]\nDe\ufb01ne Object-Oriented StatesTaco BellTaco 3/3OrderRestaurantMr Sun 3/3OrderCrunchwrapFoodItemQuesadillaFoodItemTaro bobaFoodItem\nDe\ufb01ne UI Components RestaurantItemViewOrderItemViewFoodThumbnailDeveloper-Coded GUI\n\ud83d\udc46Recap: ReactGenie Execute User\u2019s Request within UI Context\n 48\n\n[Page 49]\nRecap: ReactGenie Execute User\u2019s Request within UI Context\n 49\nDe\ufb01ne Object-Oriented StatesTaco BellTaco 3/3OrderRestaurantMr Sun 3/3OrderCrunchwrapFoodItemQuesadillaFoodItemTaro bobaFoodItem\nDe\ufb01ne UI Components RestaurantItemViewOrderItemViewFoodThumbnailDeveloper-Coded GUI\nReactGenie Runtime\nSemantic ParserOrder.GetActiveCart().addItems(items:Order.OrderHistory().matching(field:.restaurant,value:Restaurant.current())[0].items)ReactGenieDSL\nDe\ufb01ned States\nInputUI MappingTaco BellRestaurantUI ReferenceTaco 3/3OrderNew TacoOrderResultOutputUI Mapping\n\ud83d\udc46Order.GetActiveCart().addItems(items:Order.OrderHistory().matching(field:.restaurant,value:Restaurant.current())[0].items)ReactGenieDSLOrder.GetActiveCart().addItems(items:Order.OrderHistory().matching(field:.restaurant,value:Restaurant.current())[0].items)ReactGenieDSLOrder.GetActiveCart().addItems(items:Order.OrderHistory().matching(field:.restaurant,value:Restaurant.current())[0].items)ReactGenieDSL\nGenerated Multimodal UI", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 50]\nHow good is ReactGenie as a framework?\u2022For developers:\n\u2022D-RQ1: Assessing the expressiveness of ReactGenie\n\u2022D-RQ2: Development time for expert developers\n\u2022D-RQ3: Ease of learning and usability for novice developers\n 50\n\n[Page 51]\nWe built three apps to demonstrate expressiveness (F-RQ1)\n 51\u2022Only 5% of the code (annotations) must be written to handle multimodal interactions.\nReactGenieFoodOrderingReactGenieSocialReactGenieSignReactGenieSign - NDA Management\nAdd three of this to my cartShow me posts from Mark that I have liked before.Only show me request through this email\n\n[Page 52]\nA small ReactGenie demo app that can be built within a lab study: ReactGenie Timer\n 52\u2022User can:\n\u2022Create, start, and pause timer with voice.\n\u2022Start/stop timer of a certain category\n\u2022Filter timer by remaining time\n\u2022\u2026\nReactGenieTimer\n\n[Page 53]\nWe asked an expert developer to build an App in ReactGenie and GPT Function Calling (F-RQ2)\n 53\nReactGenieTimerMetricReactGenieGPT-3 Function CallingTime to Develop (minutes)45177Additional Lines of Code159523Features SupportedTouch, Complex Commands, NavigationLimited Support\nLess time\nLess code\nMore features", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 54]\nNovice Developer Study on ReactGenie Usability (F-RQ3)\n 54\nReactGenieTimer\u2022Study Design: \u2028Learn with tutorial -> \u2028Construct timer app on GUI boilerplate \n\u2022High comprehension of framework functionality (99% of questions correct)\n\u2022Fast completion time: 67.3 minutes\n\u2022Positive feedback on ease of use\n\u2022Many participants asked to use ReactGenie in real-life applications\n\n[Page 55]\nHow good is ReactGenie as a framework?\u2022For users:\n\u2022U-RQ1: Parser performance with natural language commands\n\u2022U-RQ2: Usability and e\ufb03ciency of multimodal UIs generated by ReactGenie\n 55\n\n[Page 56]\nEvaluate parser's effectiveness with commands from crowd workers (U-RQ1)\n 56\u2022Participants provided with app screenshots and videos to prompt commands\n\u2022Results:\n\u2022172 rich multimodal commands\n\u2022Parser Accuracy:\n\u2022101 supported commands: \u2028parsed correctly 91% \u202271 unsupported commands:\u2028generated sensible NLPL 53%\n\n\n[Page 57]\nUser Experience with ReactGenie-Generated UIs (U-RQ2)\n 57\u2022Compare user performance and experience using multimodal UIs vs. GUI-only\n\u2022Study Design:\n\u2022Within-subject design with 16 participants\n\u2022Multimodal UI vs. GUI-only\n\u2022Result: ReactGenie\n\u2022Saved 40% time (p=0.0004)\n\u2022Lower Cognitive Load (p=0.013)\n\u2022Higher Usability (p=0.031)\n\u2022Participant Preferences:\n\u2022Strong preference on MMI: 11/16\nReactGenieFoodOrdering", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 58]\nLecture Goals\u2022Why do we need multimodal interactions? \u2022Three problems for multimodal app development \u2022ReactGenie: a multimodal app development framework\n\u2022GenieWizard: addressing the multimodal app\u2019s feature discovery problem\n 58\n\n[Page 59]\nReactGenie study result: Amazing on supported commands, How about unsupported commands?\u2022Participants provided with app screenshots and videos to prompt commands\n\u2022Results:\n\u2022172 rich multimodal commands\n\u2022Parser Accuracy:\n\u2022101 supported commands: \u2028parsed correctly 91% \u202271 unsupported commands:\u2028generated sensible NLPL 53% 59\nWorks Great41% Commands UnsupportedCan we guess what is needed leveraging \u201challucination\u201d of LLMsSensible: used non-existent functions in a reasonable way\n\n[Page 60]\nGenieWizard is and IDE to help developers\n 60\u2022GUI IDE helps developers:\u2028In the same time frame\n\u2022Follow GUI best practices\n\u2022Visualize visual designs\n\n\n[Page 61]\nIDE to support feature suggestions early in the dev process\n 61\nUnique FeaturesSuggested FeaturesGenerated Parsed CommandsGenerated User UtterancesProgress TrackerNewly Implemented FeaturesEarly StageState CodeGenieWizardGuidances\n\n[Page 62]\nGenieWizard Pipeline: Provide feature suggetions matching real user\u2019s requests\n 62Code SignaturesAppDescriptionPersonas &ScenariosInteractiveDialogParsed DSLDry Run ResultFeature SuggestionsSpeculating on missing featuresSimulating User commandsSuggesting features to implement", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 62]\nGenieWizard Pipeline: Provide feature suggetions matching real user\u2019s requests\n 62Code SignaturesAppDescriptionPersonas &ScenariosInteractiveDialogParsed DSLDry Run ResultFeature SuggestionsSpeculating on missing featuresSimulating User commandsSuggesting features to implement\n\n[Page 63]\n 63Step 1: Simulating User commandsCode Signatures// A food itemclass FoodItem {  string name;  float price;  // the menu section of the food  MenuSection menuSection;  static FoodItem GetFoodItem(name: string);  static FoodItem[] All();  static FoodItem Current();  static FoodItem[] AllCurrent();}AppDescription\nThis app is a digital menu designed for dinning in an Italian restaurant.Personas &ScenariosLandon Holt, a 22-year-old male from Johnson County, Missouri, is a personal \ufb01nancial advisor. \u2026Landon wants to use the digital menu app in an Italian restaurant to enhance his dining experience. He is looking forward to exploring the menu options and discovering new dishes. Landon's task is to browse through the digital menu, select his preferred dishes, and customize them according to his dietary preferences. \u2026InteractiveDialog\u2026user: What are the ingredients in the Penne Arrabiata?app: The Penne Arrabiata is a spicy pasta dish made with penne pasta, tomato sauce, garlic, chili \ufb02akes, and topped with Parmesan cheese. It has a kick of heat and is perfect for spice lovers. Would you like to add it to your order?\u2026user: Add extra cheese to the Vegetarian pizza.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 64]\n 64Step 2: Speculating on missing featuresParsed DSLFoodItem.GetFoodItem(name: \"Penne Arrabiata\").getIngredients()FoodItem.GetFoodItem(name: \"Vegetarian pizza\").getFoodItems().matching(field: .name, value: \"extra cheese\").add();Dry Run ResultFoodItem.getIngredients()\nFoodItem.getFoodItems()InteractiveDialog\u2026user: What are the ingredients in the Penne Arrabiata?app: The Penne Arrabiata is a spicy pasta dish made with penne pasta, tomato sauce, garlic, chili \ufb02akes, and topped with Parmesan cheese. It has a kick of heat and is perfect for spice lovers. Would you like to add it to your order?\u2026user: Add extra cheese to the Vegetarian pizza.Speculative parsing based on LLM hallucinationAbstract interpretation\n\n[Page 65]\n 65Step 3: Suggesting features to implementDry Run ResultFoodItem.getIngredients()\nFoodItem.getFoodItems()Feature SuggestionsFoodItem.getIngredients()  //  Returns a list of ingredients for a specific food item.FoodItem.addToppings()  //  Adds toppings to a specific food item.Agglomerative clustering on content embedding", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 66]\n@GenieClass(\"A room item\")export class Room extends DataClass {    @GenieKey    public id: string;    @GenieProperty(\"the name of the room\")    public name: string;    @GenieFunction(\"Show preview of booking of the room\")    bookRoom({startDate, endDate}) {        \u2026    }}Code Skeleton\nGenieWizard is based on simulated user testing\n 66\nMMI ProblemsFilter hotel price by rangeRequest for discountHotel nearby interesting placesDesignImplementationPrototype Testing\n!\n search\n\"\n priceHotel.priceHotel.search()\nGUI ProblemsMissing hotel pricesMissing search bar\u201cShow me hotels less than $100.\u201d\u201cCan I request for some discount?\u201d\u201cWhat are interesting places to visit nearby?\u201dHotel.priceInterestingPlaces{}Booking.requestDiscount()GenieWizard SuggestionsZeroshot parser\nCode SignaturesInteractiveDialogParsed DSLFeature Suggestions\n\n[Page 67]\nGenieWizard User Interface\n 67\nUnique FeaturesSuggested FeaturesGenerated Parsed CommandsGenerated User UtterancesProgress TrackerNewly Implemented FeaturesEarly StageState CodeGenieWizardGuidances\n\n[Page 68]\nTwo demo apps for testing GenieWizard\n 68\nFood Menu AppHotel Booking AppusingTwo AppsTwo ToolsGenieWizardVS Code12 Developers\n(Within subject)ParticipantsImproved\n\n[Page 69]\nGenieWizard helps developers identify more missing features\n 69GenieWizard helps developer identify 42% instead of 10% of the missing features", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 69]\nGenieWizard helps developers identify more missing features\n 69GenieWizard helps developer identify 42% instead of 10% of the missing features\n\n[Page 70]\nDiscussion: New kinds of systems research in the era of AI\u2022System research areas:\n\u2022Operating systems, Compilers, Networkings, Software Engineering\n\u2022System research about AI today are mostly on accelerating training and serving\n\u2022Projects in today\u2019s talk (other parts of this course as well) used system research techniques for AI-HCI problems\n\u2022ReactGenie\n\u2022Interpreters, UI frameworks\n\u2022GenieWizard\n\u2022Speculative parsing, IDEs\n\u2022Future: Uses systems research techniques for AI systems 70\n\n[Page 71]\nDiscussion: Multimodal interactions vs multimodal machine learning\n 71\nMultimodal modelsText inputUser Dialogue State Semantic ExecutionResultResponse Agent OutputText inputComponent UI ResultUI UpdateDisplayReact Program StateReactGenieMultimodal interactionsIt\u2019s all tokenization problemsPicture -> tokensUI + Interactions -> tokens\n\n[Page 72]\nRecap: Multimodal Interactions\u2022Multimodal interaction history is long, but adoption is limited due to implementation costs.\n\u2022Compared to voice interfaces, multimodal ones are \ufb02exible, e\ufb03cient, clearer, and less error-prone.\n 72", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 72]\nRecap: Multimodal Interactions\u2022Multimodal interaction history is long, but adoption is limited due to implementation costs.\n\u2022Compared to voice interfaces, multimodal ones are \ufb02exible, e\ufb03cient, clearer, and less error-prone.\n 72\n\n[Page 73]\nRecap: ReactGenie\u2022ReactGenie aims to foster multimodal interaction adoption.\n\u2022Merges modern app features and multimodal interface \ufb02exibility, ensuring easy development.\n\u2022Utilizes object-oriented state abstraction and declarative UI for modality synchronization.\n\u2022Employs LLMs to expose the app's entire state, rather than limiting it to individual APIs for voice interfaces.\n\u2022GenieWizard helps developers build usable multimodal apps in the \ufb01rst iteration\n\u2022Uses LLMs to simulate multimodal user interactions\n\u2022Speculaes missing features using LLM hallucination and abstract interpretation \n\u2022Helps developers to help end users 73\n\n[Page 74]\nRecap: GenieWizard\u2022GenieWizard helps developers build usable multimodal apps in the \ufb01rst iteration\n\u2022Uses LLMs to simulate multimodal user interactions\n\u2022Speculaes missing features using LLM hallucination and abstract interpretation \n\u2022Helps developers to help end users\n 74\n\n[Page 75]\nI hope that you will\u2026https://jya.ng/reactgenie\u2022Build with it \n\ud83d\ude0e\n\u2022Break it \n\ud83d\ude08\n\u2022Fix it! \n\ud83e\udd73\n 75", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "L Multimodal", "content": "[Page 75]\nI hope that you will\u2026https://jya.ng/reactgenie\u2022Build with it \n\ud83d\ude0e\n\u2022Break it \n\ud83d\ude08\n\u2022Fix it! \n\ud83e\udd73\n 75\n\n[Page 76]\nRecap\u2022Multimodal interaction history is long, but adoption is limited due to implementation costs.\n\u2022Compared to voice interfaces, multimodal ones are \ufb02exible, e\ufb03cient, clearer, and less error-prone.\n\u2022ReactGenie aims to foster multimodal interaction adoption.\n\u2022Merges modern app features and multimodal interface \ufb02exibility, ensuring easy development.\n\u2022Utilizes object-oriented state abstraction and declarative UI for modality synchronization.\n\u2022Employs LLMs to expose the app's entire state, rather than limiting it to individual APIs for voice interfaces.\n\u2022GenieWizard helps developers build usable multimodal apps in the \ufb01rst iteration\n\u2022Uses LLMs to simulate multimodal user interactions\n\u2022Speculaes missing features using LLM hallucination and abstract interpretation \n\u2022Helps developers to help end users 76", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-multimodal.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:51.236533"}
{"title": "Cs 224V Hw2", "content": "[Page 1]\nCS 224V Assignment 2\nDue 9thOctober 2024\nInstructions: Use this Colab Notebook in conjunction with this write-up. Make sure to\n\u201cSave a copy in Drive\u201d before running the notebook. Submit your answers through Grade-\nscope and attach your Google Colab notebook. In red, we label how each question in this\nwriteup corresponds to a Gradescope question\nWe expect heavy loads on the OVAL machine used in this assignment close to the deadline.\nThus, we highly recommend you start this assignment early and do not wait un-\ntil the last minute. This assignment is designed to be completed in groups of 2 . Please\nsubmit as a group to Gradescope.\nExtensions: You are granted an automatic 24-hour extension to either Assignment 1 or\nAssignment 2, but not both. If you submit Assignment 1 later than the deadline, this 24-\nhour extension will be automatically applied to Assignment 1. Each individual student has\none 24-hour extension, so both partners will require an available extension to take one on\nAssignment 2.\n1 Introduction\nThis assignment is designed to give you hands-on experience with how we can leverage LLMs\nto create task-oriented dialogue agents grounded on knowledge corpus. You will learn:\n\u2022strategies for leveraging LLMs to develop task-oriented dialogue systems capable of\nperforming complex tasks;\n\u2022key requirements and considerations for building effective dialogue agents;\n\u2022the importance of grounding task agents in a comprehensive knowledge corpus.", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "\u2022strategies for leveraging LLMs to develop task-oriented dialogue systems capable of\nperforming complex tasks;\n\u2022key requirements and considerations for building effective dialogue agents;\n\u2022the importance of grounding task agents in a comprehensive knowledge corpus.\n\u2022a new framework for quickly creating dialogue agents with minimal steps.\n2 Task Oriented Dialogue Agents\nResearchers and industry practitioners have demonstrated significant interest in developing\ntask-oriented dialogue agents. These agents are typically designed with a transactional focus,\naiming to fill slot values based on user utterances to complete specific tasks (Budzianowski\net al., 2018; Andreas et al., 2020; Rastogi et al., 2020). However, existing approaches have\n1", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 2]\nlimitations in handling conditional logic, integrating knowledge sources, and consistently\nfollowing instructions. Researchers and industry professionals often employ ad hoc pipelines\nto construct conversational agents. These pipelines aim to maintain context, address failure\ncases, and minimize hallucinations, yet frequently fail to achieve these objectives.\nLLMs offer a promising opportunity to create more natural general-purpose dialogue agents\nsuch as ChatGPT, Claude, and Inflection OpenAI (2024); Anthropic (2024); Inflection.AI\n(2024) However, task-oriented agents should provide reliable, grounded responses while let-\nting developers exercise control over the conversation flow.\nHere are the key challenges faced in creating reliable task-oriented dialogue agents:\n1. Creation of effective, informative, and responsive informative agents, while letting\ndevelopers exercise control without onerous efforts.\n2. Support users\u2019 queries for information, which may be embedded in a task request.\n3. Dialogue systems need to remember pertinent facts from the dialogue history.\n3 GenieWorksheet\nTo tackle the aforementioned challenges, task-oriented conversational agents recent libraries,\nsuch as those offered by LangChain and Guidance, provide abstractions for developing\nLLM-based agents but still require developers to manually craft prompts and create reli-\nable pipelines. This is particularly challenging for complex task-oriented agents.", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "such as those offered by LangChain and Guidance, provide abstractions for developing\nLLM-based agents but still require developers to manually craft prompts and create reli-\nable pipelines. This is particularly challenging for complex task-oriented agents.\nWe will now demonstrate how one of the frameworks, Genie, can help mitigate some of\nthese problems. Genie uses a novel way of defining task-oriented agents, GenieWorksheets,\nthat provides explicit control to the TOD (Task Oriented Dialogue) agent developer. The\ndeveloper can program what actions the agent should take based on the state of the conver-\nsations; these are called agent policies. This way the developer can manage dialogue flow\nand deliver high-level support for integrated knowledge assistants. GenieWorksheet enables\nprogramming policies in the declarative paradigm, in contrast to dialogue trees\u2019 much more\ncomplex imperative approach. Declarative programming is a paradigm that focuses on what\nthe program should achieve without explicitly stating how to achieve it.\nIn this assignment, you will learn how Genie Worksheets can express a variety of tasks, from\nrestaurant booking assistants that require hybrid knowledge retrieval capabilities to handling\ncomplex workflows encapsulated in web pages.\n3.1 Course Enrollment Agent with Genie\nBefore each quarter, students must choose their courses and research information on plat-\nforms like Explore, Carta, and Simple Enroll. After extensive research, they use sites like\n2", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 3]\nSimple Enroll to register for their selected courses. However, even after choosing their\ncourses, they often face challenges such as classes being out of capacity, which requires\nthem to search again for alternatives that meet their criteria, such as being \u201cmore project-\noriented,\u201d \u201cless time-consuming weekly,\u201d or fulfilling specific breadth requirements.\nThe course enrollment agent streamlines the process of enrolling in courses by integrating\ncourse information retrieval and enrollment functionalities. It aggregates data from multiple\nsources to provide comprehensive course details and assist in completing complex, nested\nenrollment forms as present online. The agent allows students to inquire about course pre-\nrequisites, reviews, and ratings while navigating the enrollment process.\nFor this assignment, you will interact with the course enrollment agent. We use real-world\ndata from sites like Carta for Stanford\u2019s Computer Science program. This data includes four\ntables\u2014courses, offerings, ratings, and programs\u2014containing thousands of rows of data.\n3", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 4]\nAction Item\nLearning Goal : Get familiar with task-oriented dialogue agents grounded in knowledge\ncorpus.\nTask 1 Interact with the course enrollment agent for at least 8 turns. (Gradescope Q1)\nTODO items\n\u2022Visit Course Enrollment Agent Web UI. There you can interact with the agent.\n\u2022The bot will give you a unique user id, paste it in Gradescope (Gradescope 1.1)\n\u2022Now you can ask questions related to courses taught in Stanford\u2019s Computer Sci-\nence department and continue enrolling in at least two courses.\n\u2022You should view the intermediate steps by clicking on down arrow for each of the\nthree steps: \u201cSemantic Parser\u201d, \u201cAgent Policy\u201d, and \u201cResponse Generator\u201d.\n\u2022Download the conversation log. At the end of the conversation, there will be an op-\ntion to download the conversation log. This conversation log contains intermediate\nsteps taken by the agent.\n\u2022Upload the conversation log to Gradescope (Gradescope 1.2)\n\u2022Do you think the agent was forgetting information that you had previously men-\ntioned in the chat? (Gradescope 1.3)\n\u2022How often does the agent make up information (\u201challucinate\u201d)? Provide examples\nfrom the conversation that you had. (Gradescope 1.4)\n\u2022List other applications where such task-oriented dialogue agents can be useful.\n(Gradescope 1.5)\n4", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 5]\n4 Creating your own task-oriented agent\nIn the previous section, we used the Course Enrollment Agent to learn more about courses\noffered and enroll in classes. Now, we will look into what it takes to create task-oriented\nagents with GenieWorksheets\n4.1 Course Enrollment Agent\nGenieWorksheets requires the following four elements from the developer:\n\u2022High-level Specification: Outline the task details, including a description, the infor-\nmation to be collected from the user, the actions to be performed, and any additional\nrequirements.\n\u2022APIs: Specify the APIs that the agent will have access to, enabling it to perform\ndesignated actions.\n\u2022Databases: Provide the knowledge sources that the agent will use to respond to user\nqueries requiring external information.\n\u2022Example Scenarios: Supply a few examples to help the agent understand the domain\nnuances and improve its ability to handle user queries effectively.\nYou can find the high-level specification here: Course Enrollment Worksheet, Example sce-\nnarios are in the Prompts, and the APIs we provide are defined in this api.py\n4.2 Writing the high-level spcification\nThe Genie Worksheet is a high-level declarative specification of a TOD agent task designed\nto achieve the above design objectives. A task specification consists of developer-defined\ntypes, knowledge corpus schemas, API calls, and a set of worksheets. A worksheet, inspired", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "The Genie Worksheet is a high-level declarative specification of a TOD agent task designed\nto achieve the above design objectives. A task specification consists of developer-defined\ntypes, knowledge corpus schemas, API calls, and a set of worksheets. A worksheet, inspired\nby the versatility of webforms, consists of a set of (potentially optional or conditional) typed\ninputs of interests and the arbitrary actions to take.\nA worksheet, as illustrated in Figure 1, has a name (WS Name), a predicate (WS Predicate)\nindicating when it is activated, and a set of fields. The top-level worksheet is unconditionally\nexecuted, and the rest of the worksheets are activated if their corresponding predicates are\ntrue. Each field has these attributes:\n\u2022Predicate: Python code for indicating whether the field should be active.\n\u2022Input: Whether it is an input or an internal value, the latter is computed rather than\nsolicited from the user.\n\u2022Type: if the type is \u201cEnum\u201d, the possible values are specified in the \u201cEnum Values\u201d\nfield. Other types allowed are: str, int, bool, name of another worksheet, confirm.\n5", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 6]\nTick et Submission W orksheetW S Pr edicat eW S NamePr edicat eInputT ypeNameEnum V aluesDon\u2019 t A skR equir edConfirmationA ctionsW S A ctionsDescriptionMainW Ssubmit_tick et\nT r oubleShootW SMainW orks...mainMainW orks...mainT r oubleShootTRUEThe type of student r equ...MainW orks...mainLea v e of Abs.MainW orks...maininputEnumstudent_taskstudent_taskT est Cr edit s\nW orks...confirmconfirmMainself .student_task == \n\u201cT r oubleShoot \u201d\nself .student_task \n==  \u201cT r ouble... \u201dser vices_gener al _ inf odb:fr ee_t e xtinputT r ouble\nShoottr ouble_shootThe enr ollment issues \nt hat t he student is f acingTRUE\nTRUEif self .confirm ==  F alse :\n    > sa y ( \u201cThank y ou ,  can I  \nassist y ou in an y ot her \nw a y ? \u201d )self .student_task is no t\nN one and (\nself .tr ouble_shoot an d\nself .lea v e_of _abs an d\nself .t est_cr edit s )inputinputstre x tra_detailsAsk f or an y ot her detail \nt hat t he student w ant s t o \naddConfirm t he student  w ...TRUE......Figure 1: Sample Worksheet. The Worksheet represents APIs. For example, the Main\nAPI is as follows: Main(student task: Enum[\"TroubleShoot\", \"Leave of Absence\",\n\"Test Credits\"], troubleshoot: TroubleShoot, extra details: str)\n\u2022Name: the name of the field\n\u2022Description: a natural language description of the field.\n\u2022Don\u2019t Ask: if true, the agent saves the information if offered by the user, but does not\nsolicit it. An example of such a field could be: \u201cIs the user annoyed?\u201d. In this case, the", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "\u2022Name: the name of the field\n\u2022Description: a natural language description of the field.\n\u2022Don\u2019t Ask: if true, the agent saves the information if offered by the user, but does not\nsolicit it. An example of such a field could be: \u201cIs the user annoyed?\u201d. In this case, the\nsystem won\u2019t explicitly ask the user if they are annoyed; however, if the user mentions\nthat they are, the value can be set to True.\n\u2022Required: if true, solicits the user for a value\n\u2022Confirmation: if true, confirms the value with the user, which is useful if an undesirable\nside effect can result from a mistaken value.\n\u2022Actions: code (in Python) to be executed whenever a value is assigned to the variable.\nFinally, each worksheet also has a WS actions field,where the developer can provide Python\ncode to be executed when all the required fields are filled. Several built-in actions are pro-\nvided to the developer: (1) say (str) responds to the user with the given string str. (2)\npropose (ws, [fld,val]*) instantiates a new worksheet wswith the given field value pairs\nas a python dictionary.\nNote: The worksheet names, field names, description, and Enum values should be seman-\ntically meaningful for the LLM to understand them easily. The performance of the agent\ndepends on the names and descriptions you provide.\n4.3 StarV2 agent using GenieWorksheet\nFor this task, we will utilize the StarV2 dataset Zhao et al. (2022) to develop task-oriented", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "tically meaningful for the LLM to understand them easily. The performance of the agent\ndepends on the names and descriptions you provide.\n4.3 StarV2 agent using GenieWorksheet\nFor this task, we will utilize the StarV2 dataset Zhao et al. (2022) to develop task-oriented\ndialogue agents. StarV2 includes simple agents capable of handling 1-3 worksheets and pro-\nvides APIs for interaction. Compared to the MultiWoz dataset Budzianowski et al. (2018),\n6", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 7]\nUser : Hi\nAgent : Hi! Could you please provide me with the name of the customer who wants to book the ride ?\nUser : Its John Doe\nAgent : Hi John ! Could you please provide me with the departure location for the ride booking ?\nUser : I am going from LA to SF\nAgent : Thank you for providing the details , John . Since you are traveling from LA to SF ,\ncan you let me know the date and time you would like to book the ride ?\nUser : maybe tomorrow around 10 am\nAgent : Thank you for the information , Rishab . Let me confirm , you would like to book a ride from LA to SF\nfor tomorrow , October 1st , at around 10:00 AM?\nUser : Yes thats true\nAgent : Your ride has been successfully booked . Here are the details :\n- ** Status **: Success\n- ** Booking ID **: 7 a906279 -cc47 -4 a36 -b871 -1 ec119e80657\nThank you for choosing our service , Rishab ! If you have any further questions or need assistance , feel free to ask .\nSafe travels !\nUser : exit ()\nTable 1: A sample conversation with the Agent. Note that this simple conversation follows\nthe \u201cHappy Path\u201d. For your assignment, you will interact with the bot and change informa-\ntion midway, such as after selecting LA as the departure location, you can say \u201cI am actually\ntraveling from San Jose\u201d\nStarV2 presents a higher level of complexity due to the inclusion of logical actions that agents\nmust execute based on the conversation state. We will be using three domains that contain", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "traveling from San Jose\u201d\nStarV2 presents a higher level of complexity due to the inclusion of logical actions that agents\nmust execute based on the conversation state. We will be using three domains that contain\nslightly more complicated agent policies. It should be noted that these \u201ccomplicated\u201d poli-\ncies are straightforward as compared to real-world policies.\nNote: We are going to send API keys to all the student via mail.\nAction Item\nYou will begin by selecting a task of interest from the provided list of three domains\nfrom the StarV2 dataset. Please ensure that you sign up with your group names on the\nGoogle Sheet. Each task can accommodate a maximum of 25 groups, and once the limit\nis reached for a specific sub-domain, you must select an alternative.\n7", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 8]\nAction Item\nLearning Goal : How to build task-oriented agents with GenieWorksheets\nTask 2 : Read Genie paper and follow the provided Google Colab notebook to get hands-\non experience on how to write GenieWorksheets. After writing the GenieWorksheet for\nyour domain, you can interact with the generated dialogue agent and try to complete\nthe given task.\n\u2022How do the authors evaluate their system in the paper? (Gradescope Q2.1).\n\u2022Upload the conversation log.json file to (Gradescope Q2.2).\n\u2022Upload the Google Colab notebook as a PDF. In Google Colab, click File \u2192Print\n\u2192\u201cSave as PDF\u201d and upload the downloaded PDF file to (Gradescope Q2.3)\n8", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "[Page 9]\nReferences\nJacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford,\nKate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David\nHall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant\nKrishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy\nMcGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth,\nSubhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su,\nZachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020. Task-oriented dialogue as dataflow\nsynthesis. Transactions of the Association for Computational Linguistics , 8:556\u2013571.\nAnthropic. 2024. Claude.\nPawe l Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u02dc nigo Casanueva, Stefan Ultes,\nOsman Ramadan, and Milica Ga\u02c7 si\u00b4 c. 2018. MultiWOZ - a large-scale multi-domain Wizard-\nof-Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing , pages 5016\u20135026, Brussels, Bel-\ngium. Association for Computational Linguistics.\nInflection.AI. 2024. Pi.\nOpenAI. 2024. Chatgpt.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.\n2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "Cs 224V Hw2", "content": "gium. Association for Computational Linguistics.\nInflection.AI. 2024. Pi.\nOpenAI. 2024. Chatgpt.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.\n2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue\ndataset. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages\n8689\u20138696.\nJeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee, Abhinav Rastogi, Mingqiu Wang,\nHagen Soltau, Izhak Shafran, and Yonghui Wu. 2022. Anytod: A programmable task-\noriented dialog system. arXiv preprint arXiv:2212.09939 .\n9", "url": "https://web.stanford.edu/class/cs224v/assignments/CS_224V_HW2.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:53.044258"}
{"title": "L Suql", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 8\nSUQL: Structured/Unstructured \nQuery Language\n1Monica Lam & Shicheng Liu\n\n[Page 2]\nSTANFORD LAMSummary from Lecture 7\n\u2022Many questions require information from hybrid data sources\n\u2022Structure OR Text: is inadequate\n\u2022Binary classifier up front (SK -TOD, 2023)\n\u2022Pick afterwards  (Stanford Chirpy Cardinal, 2021)\n\u2022Different approaches to combine structures and free -text \n\u2022Structures \u2192Text: Linearization (one hop)\n\u2022Hybrid: Retrieve from both and combine (one hop each)\n2\n\n[Page 3]\nSTANFORD LAMDesired Solution\n1.Keep hybrid data sources\n\u2022Don\u2019t convert KB \u2192Text, or Text \u2192KB\n2.Hybrid multi -hop: Compose IR and KB accesses arbitrarily\n\u2022Efficient searching of free text\n\u2022Relational algebra operations (aggregate, rank, \u2026\nSince LLMs understand SQL \u2192SUQL\n3\nNEW!\n\n[Page 4]\nSTANFORD LAMSUQL Language\n\u2022SUQL (Structured & Unstructured Query Language)\n\u2022Extends SQL to include free -text operations\n\u2022SUQL -based Genie Agent Framework \n\u2022Developer only needs to provide SQL schema + free -text data\n\u2022SUQL: A high -level language \u2013domain agnostic\n\u2022Decompose implementation into: \n\u2022LLM-based semantic parser\n\u2022Deterministic query optimizations in the optimizing compiler\n4Reduces the complexity for the neural model", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 5]\nSTANFORD LAMGenie Agent Framework Inputs (Yelp Example) \n5schema CREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines ENUM[],\nrating NUMERIC(2,1), \n\u2026,\npopular_dishes TEXT[],\nreviews TEXT[]\n\u2026);\nNEW!\nNEW!DB Schema\nFew-shot examples (optional)--\nUser: Show me a family -friendly restaurant that has burgers\nTarget: SELECT *, summary(reviews), answer(reviews, 'is this restaurant \nfamily -friendly?') FROM restaurants WHERE answer(reviews, 'do you \nfind this restaurant to be family -friendly?') = 'Yes' AND \nanswer( popular_dishes , 'does this restaurant serve burgers') = 'Yes' \nLIMIT 1;\n--\nUser: Find me a place with pasta.\nTarget: SELECT *, summary(reviews) FROM restaurants WHERE \nanswer( popular_dishes , 'does this restaurant serve pasta') = 'Yes' LIMIT \n1;\n--\n\u2026Free -Text\nSUQL framework will create \nthe vector store for IR\n\n[Page 6]\nSTANFORD LAMLecture Goals\n\u2022SUQL Language \u2013Design and Rationale\n\u2022SUQL Implementation \u2013Semantic parser & Compiler\n\u2022Evaluation in a real life app: Yelp\n\u2022Evaluation with a large dataset: HybridQA\n(with an improvement)\n6\n\n[Page 7]\nSTANFORD LAMSUQL Free -Text Support\n\u2022Add free -text primitives into SQL, implemented with IR & LLM\n\u2022Two functions: summary , answer\n\u201cI want a family -friendly restaurant in Palo Alto \u201d\nSELECT *, summary (reviews) FROM restaurants \nWHERE location = 'Palo Alto\u2019\nAND answer (reviews, \u2018is it a family friendly restaurant\u2019) = 'Yes' LIMIT 1;\n7Quiz: How do you implement summary and answer?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 8]\nSTANFORD LAMHybridQA Dataset\nWikipedia Tables Wikipedia Pages hyperlinked\n8Flag bearers of \nMyanmar \nat the Olympics\n\n[Page 9]\nSTANFORD LAM 9\nT: Table\nP: Passage\n\n[Page 10]\nSTANFORD LAMHybridQA Questions in SUQL\nType I (T ->P)\nWhere was the XXXl Olympic held?\nSELECT answer (\"Event year_Info \", \n'where is this event held?\u2019) \nFROM \u201dFlag\" WHERE \"Name\" = 'XXXI\u2019\n10CREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]);\nT: Table\nP: Paragraph \n\n[Page 11]\nSTANFORD LAMHybridQA Questions in SUQL\nType II (P ->T)\nWhat was the name of the Olympic event held in Rio?\nSELECT \"Name\" FROM \u201dFlag\" \nWHERE answer (\"Event year_Info \", \n'where is this event held?') = 'Rio\u2019\n11CREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]);\nT: Table\nP: Paragraph \n\n[Page 12]\nSTANFORD LAMHybridQA Questions in SUQL\nType III (P ->T->P)\nWhen was the flag bearer of Rio Olympic born?\nSELECT answer (\"Flag Bearer_Info \", 'when is this person \nborn?\u2019) \nFROM \u201dFlag\" \nWHERE answer (\"Event year_Info \", 'where is this event \nheld?') = 'Rio\u2019\n12CREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]);\nT: Table\nP: Paragraph", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 13]\nSTANFORD LAMHybridQA Questions in SUQL\nType IV (T&P)\nWhich male bearer participated in Men's 100kg event in the \nOlympic game?\nSELECT \"Flag Bearer\" FROM \u201dFlag\" WHERE \"Gender\" = \n'Male' AND answer (\"Flag Bearer_Info \", \n'what event did this person participate in?\u2019) \n= \u201cMen\u2019s 100kg event\u201d\n13CREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]);\nT: Table\nP: Paragraph \n\n[Page 14]\nSTANFORD LAMHybridQA Questions in SUQL\nType V (T -Compare | P -Compare)\nFor the 2012 and 2016 Olympic Event, when was the younger \nflag bearer born?\nSELECT MAX\n(answer (\"Flag Bearer_Info \",  'when is this person \nborn?')::date) \nFROM \u201dFlag\" WHERE \"Event year\" IN ('2016', '2012\u2019)\n14CREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]);\nT: Table\nP: Paragraph \n\n[Page 15]\nSTANFORD LAMHybridQA Questions in SUQL\nType VI (T -Superlative | P -Superlative)\nWhen did the youngest Burmese flag bearer participate \nin the Olympic opening ceremony?\nSELECT \"Event year\" FROM \u201dFlag\u201d ORDER BY \nanswer (\"Flag Bearer_Info \", 'when is this person \nborn?')::date \nDESC LIMIT 1;\n15CREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]);\nT: Table\nP: Paragraph", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 16]\nSTANFORD LAMAt-A-Glance: HybridQA Questions in SUQL\nCREATE TABLE Flag (\n\"Name\" TEXT, \n\"Flag Bearer\" TEXT, \n\"Flag Bearer_Info \" TEXT[],\n\"Gender\" TEXT, \n\"Event year\" TEXT, \n\"Event year_Info \" TEXT[]); \nType I (T ->P)\nWhere was the XXXl Olympic held?\nSELECT\nanswer (\"Event year_Info \", \n'where is this event held?\u2019) \nFROM \u201dFlag\" WHERE \"Name\" = 'XXXI'\nType II (P ->T)What was the name of the \nOlympic event held in Rio?\nSELECT \"Name\" FROM \u201dFlag\" \nWHERE answer (\"Event year_Info \", \n'where is this event held?') = 'Rio\u2019Type III (P ->T->P)When was the flag bearer of \nRio Olympic born?\nSELECT answer (\"Flag Bearer_Info \", \n'when is this person born?\u2019) \nFROM \u201dFlag\" \nWHERE answer (\"Event year_Info \", \n'where is this event held?') = 'Rio\u2019\nType IV (T&P) Which male bearer \nparticipated in Men's 100kg event in the \nOlympic game?\nSELECT \"Flag Bearer\" FROM \u201dFlag\" \nWHERE \"Gender\" = 'Male' AND \nanswer (\"Flag Bearer_Info \", \n'what event did this person \nparticipate in?\u2019) \n= \u201cMen\u2019s 100kg event\u201dType V (T -Compare | P -Compare) For the \n2012 and 2016 Olympic Event, when was the \nyounger flag bearer born?\nSELECT MAX\n(answer (\"Flag Bearer_Info \", \n'when is this person born?')::date) \nFROM \u201dFlag\" \nWHERE \"Event year\" IN ('2016', '2012\u2019)\nType VI (T -Superlative | P -Superlative) When \ndid the youngest Burmese flag bearer \nparticipate in the Olympic opening ceremony?\nSELECT \"Event year\" FROM \u201dFlag\u201d \nORDER BY answer (\"Flag Bearer_Info \", \n'when is this person born?')::date \nDESC LIMIT 1;\n16", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 17]\nSTANFORD LAMConversational Example: Restaurant\nRestaurants\nDo you have a recommendation for a first date restaurant in Palo Alto ? \nWe're thinking sushi but not sure what's good around here.\nSELECT *, summary(reviews) FROM restaurants \nWHERE 'sushi' = ANY (cuisines) AND location = 'Palo Alto' AND rating >= 4.0 \nAND answer(reviews, 'is this restaurant good for a first date?') = 'Yes\u2019 \nORDER BY num_reviews DESC LIMIT 1;\n17\n\n[Page 18]\nSTANFORD LAMLaptops\nI need a laptop with a Thunderbolt 3 port and \nat least 16GB RAM for my workstation setup.\nSELECT *, summary(reviews) FROM laptops \nWHERE ram >= 16 AND\n(answer(about, 'does this laptop have Thunderbolt 3?') = 'Yes\u2019\nOR answer(description, 'does this laptop have Thunderbolt 3?') = 'Yes\u2019)\nLIMIT 3;Conversational Example: Shopping\n18CREATE TABLE laptop (\n\u201dram\" int, \n\u2026,\n\u201dabout\" TEXT, \n\u201cdescription\" TEXT[],\n\u201dreviews\u201d TEXT); \nIt is often unclear where the answer is in the text, \nuseful/easy to search for all possibly relevant text fields.\n\n[Page 19]\nQUIZ\nWH AT CANWEUSES UQ L  F O R?\n19", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 19]\nQUIZ\nWH AT CANWEUSES UQ L  F O R?\n19\n\n[Page 20]\nSTANFORD LAMAdvantages of SUQL\n\u2022Formal representation for IR and DB query\n\u2022Succinct, expressive, domain independence\n\u2022Full compositionality: combining IR and relational algebra\n\u2022Interpretability\n\u2022We can read the query back \nso users know what question is being answered\n\u2022Quiz: Is the answer always correct given a formal query?\n\u2022It is true for the database part\n\u2022But there is a potential for error in free -text Q&A \n(Information retrieval error, answer error)\n\u2022SUQL is a high -level programming language\n20\n\n[Page 21]\nSTANFORD LAMSUQL is New\n\u2022How to evaluate SUQL as a design?\n\u2022What is the accuracy?\n\u2022Semantic parser accuracy\n\u2022Execution accuracy\n\u2022What is the speed? \n21\n\n[Page 22]\nSTANFORD LAMLecture Goals\n\u2022SUQL Language \u2013Design and Rationale\n\u2022SUQL Implementation \u2013Semantic parser & Compiler\n\u2022Evaluation in a real life app: Yelp\n\u2022Evaluation with a large dataset: HybridQA\n(with an improvement)\n22\n\n[Page 23]\nSTANFORD LAMAgent Design for SQL\n23Semantic parser\nPredicted SQL\nResponse promptUser query\nSQL compiler \nDatabase\nresultsDb lookup neededInput classifier\nDB lookup\nnot needed\nClick the links to see the prompts (written in jinja syntax )No result response\nNo resultsClassify -Enum", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 23]\nSTANFORD LAMAgent Design for SQL\n23Semantic parser\nPredicted SQL\nResponse promptUser query\nSQL compiler \nDatabase\nresultsDb lookup neededInput classifier\nDB lookup\nnot needed\nClick the links to see the prompts (written in jinja syntax )No result response\nNo resultsClassify -Enum\n\n[Page 24]\nSTANFORD LAMAgent Design: SQL Updated with SUQL\n24Predicted SUQLClassify -Enum\nanswer fcn\n7 prompts with few -shot examples.   Templates written in jinja syntax summary fcnSemantic parser\nResponse promptUser query\nSUQL compiler \nDatabase\nresultsDb lookup neededInput classifier\nDB lookup\nnot needed\nNo result response\nNo results\n\n[Page 25]\nSUQL: \nHIGH-LEVEL PROGRAMMING LANGUAGE\nTHECO M PIL E R C A N IM PL E M E N T OP TIM IZ AT IO N S\nHID IN G D E T AI LS F R O M USE RS\n25\n\n[Page 26]\nSTANFORD LAMSUQL Compiler\n\u2022SQL can run SUQL programs without modification\n\u2022summary / answer are just external functions\n\u2022But it is slow\n26\n\n[Page 27]\nSTANFORD LAMExample: \n\u201cI want a family -friendly restaurant in Palo Alto \u201d\nSELECT *, summary (reviews) FROM restaurants \nWHERE  answer (reviews, \u2018is it a family friendly restaurant\u2019) = 'Yes\u2019\nand  location = 'Palo Alto\u2019\n\u2022Consider a na\u00efve implementation\n\u2022Use IR to retrieve all reviews, filter on location, get reviews\n\u2022Report the first few \n\u2022Quiz: How would you optimize the query? \n27", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 28]\nSTANFORD LAM1. Order Filtering\n\u2022Execution of structured predicates is much cheaper\n\u2022Always execute structured predicates first\n28answer(reviews, \u2018is it a family friendly restaurant\u2019) = \u2018Yes\u2019 \nAND\nand  location = 'Palo Alto\u2019\n\n[Page 29]\nSTANFORD LAM2. Return Only Necessary Results\n\u2022For applications such as recommendation, \nit is not necessary to return all the answers\n\u2022IR uses embedding model (vector similarity) \nto return top candidates \n\u2022Return only top results to LLM -based answer functions\n29answer(reviews, \u2018is it a family friendly restaurant\u2019) = \u2018Yes\u2019\n\n[Page 30]\nSTANFORD LAM3. Lazy Evaluation \n\u2022Lazy evaluation: Evaluate only when the result is needed\n\u2022Stop calling answer as soon as LIMIT 1 is reached\n30answer(reviews, \u2018is it a family friendly restaurant\u2019) = \u2018Yes\u2019 \nAND\nanswer(reviews, \u2019is it easy to park\u2019) = \u2018Yes\u2019\nAND\nand  location = 'Palo Alto\u2019 LIMIT 1\n\n[Page 31]\nSTANFORD LAMSUQL Compiler Implementation\n\u2022SUQL is implemented by modifying the SQL compiler\n\u2022To support arbitrary composition of SQL and \nsummary/answer statements\n\u2022Process the syntax tree of SQL recursively\n\u2022Start from the bottom node with no sub -queries\n\u2022And move to the top\n31\n\n[Page 32]\nSTANFORD LAMSUQL Compiler Implementation\n\u2022For each SELECT statement with answer in it \nApply optimizations to the SELECT statement \n\u2022Store the processed results in a temporary table temp\n\u2022Substitute this statement with SELECT from temp\n\u2022SQL compiler handles the final processing\n32", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 32]\nSTANFORD LAMSUQL Compiler Implementation\n\u2022For each SELECT statement with answer in it \nApply optimizations to the SELECT statement \n\u2022Store the processed results in a temporary table temp\n\u2022Substitute this statement with SELECT from temp\n\u2022SQL compiler handles the final processing\n32\n\n[Page 33]\nSTANFORD LAMCompiler Optimizations\n\u2022Possible only because we are optimizing across the whole \nquery\n33Quiz: Is the optimization necessary?  \nQuiz: What if we use an agent \nthat uses \u201cfunction calling\u201d to call IR and query databases \nseparately?   \n\n[Page 34]\nSTANFORD LAMLecture Goals\n\u2022SUQL Language \u2013Design and Rationale\n\u2022SUQL Implementation \u2013Semantic parser & Compiler\n\u2022Evaluation in a real life app: Yelp\n\u2022Evaluation with a large dataset: HybridQA\n(with an improvement)\n34\n\n[Page 35]\nSTANFORD LAMExperiment\nA real, large dataset\n\u2022Yelp on San Francisco, Palo Alto, Cupertino, \nSunnyvale\n\u2022Scraped reviews and popular dishes information\nComponents\n\u2022LLM: gpt -3.5-turbo\n\u2022Information retrieval on text fields \n\u2022indexed with Coco -DR\n\u2022A new optimizing SUQL compiler \n\u2022SQL can run SUQL programs, but it is too slow\n35COCO -DR reference: https:// arxiv.org /abs/2210.15212\n\n\n[Page 36]\nSTANFORD LAMEvaluation (in Restaurants)\n\u2022We solicit user queries via crowdsourcing on Prolific. \n\u2022We do not disclose to the workers what fields are \navailable in the database.\n\u2022The Yelp Dataset:\n1.Just Q&A: 100 crowdsourced questions about restaurants.\n2.Conversational: 20 conversations with 96 turns \n36", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 37]\nSTANFORD LAMThe Need for Hybrid QA in Real Life? \n37\n(59%)\n(41%)\n\n[Page 38]\nSTANFORD LAMEvaluation (in Restaurants)\n\u2022In real -world tasks. Difficult to label gold:\n\u2022For each query, there are a lot of \u201dgold restaurants\u201d\n\u2022We manually inspect whether the restaurants retrieved by a \nsystem satisfy all criteria\n\u2022Calculate Query precision : #correct results /  #results \n38Quiz: how do you evaluate its accuracy?\n\n[Page 39]\nSTANFORD LAMBaseline comparison: \nLinearize + Information Retrieval\n39\n\n[Page 40]\nSTANFORD LAMUsing SUQL\n40\n\n[Page 41]\nSTANFORD LAMEvaluation\n41\nQuiz: how can you get incorrect results with semantic parsing on SUQL?\n\n[Page 42]\nSTANFORD LAMWhat About Recall? \n\u2022Are all the answers found? \n\u2022Incomplete\n\u2022Not at all\n42\uf0dfQuiz: Is this OK? \n\n[Page 43]\nSTANFORD LAMAssistant fails to find an answer \u2192verbalizes the user query \nI searched for 5 -star restaurants in Sunnyvale that serve kids food. \nUnfortunately, I couldn't find any search results. \nIs there anything else I can help you with?\nAllows the user to spot the answerDefensive Programming: \nWhat if the Semantic Parser is Wrong?\n43", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 44]\nSTANFORD LAM14 false negatives on our single -turn set (100 questions)\n\u2022Parsing: 2 errors (one syntactic, one enum field confusion)\n\u2022Query evaluation\n\u2022Structured: \n\u20224 Unsupported location service \n\u20222 Opening hours errors\n\u20222 dishes: searched in popular dishes, but results are in reviews\n\u2022Free -text: 4 false negatives from \u2018answer\u2019 ( ChatGPT )Error Analysis: \nWhat Happened When No Answers Were Returned?\n44\uf0dfCan be improved\n\n[Page 45]\nSTANFORD LAMUser Feedback -Negative\n\u2022Occasional slowness of the chatbot\n\u2022It didn\u2019t provide any links or pictures\n\u2022It did not sound friendly and sometimes the responses were too long. \n\u2022Bullet point outputs would be much more helpful.\n45\n\n[Page 46]\nSTANFORD LAMUser Feedback -Positive\n\u2022There\u2019s actually nothing I didn\u2019t like about this chatbot. \n\u2022I would honestly use this chatbot on a regular basis \nif it were available to the public\n\u2022I liked that the chatbot was fast in responses and it gave very \ndetailed responses and I hardly had any questions about a \nrestaurant after the option was given.\n\u2022Shocked at how good the restaurant suggestions were. I even \nasked for something with better prices and got that too. Now I\u2019m \nhungry. I asked to define a cuisine style and it was able to do that. \n46\n\n[Page 47]\nSTANFORD LAMTry it for yourself!\nHttps: Yelpbot.genie.Stanford.edu\n47", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 47]\nSTANFORD LAMTry it for yourself!\nHttps: Yelpbot.genie.Stanford.edu\n47\n\n\n[Page 48]\nSTANFORD LAMLecture Goals\n\u2022SUQL Language \u2013Design and Rationale\n\u2022SUQL Implementation \u2013Semantic parser & Compiler\n\u2022Evaluation in a real life app: Yelp\n\u2022Evaluation with a large dataset: HybridQA\n(with an improvement)\n48\n\n[Page 49]\nSTANFORD LAMHybrid QA Dataset\n49\n\n\n[Page 50]\nSTANFORD LAMHybridQA Dataset: Based on Wikipedia\n\u202213000 tasks on Amazon Turk: \n\u2022Given one crawled Wikipedia table  + its hyperlinked passages. \n\u2022Write 6 questions  & answers. \n\u2022Each task takes 12 minutes on average, and payment is $2.3 U.S. \n\u20225 CS graduate students to decide on the acceptance. \n50\n\n\n[Page 51]\nSTANFORD LAMHybridQA Dataset\n\u2022Question rely on both tabular and textual information. \n\u2022Table reasoning step specifically includes \n\u2022filter rows based on =, >, <: \u201cthe XXXI Olympic event\u201d \n\u2022superlative operation over a column: \u201cthe earliest Olympic event\u201d\n\u2022hop between two cells: \u201cWhich event ... participate in ...\u201d, \n\u2022extract information from table: \u201cIn which year did the player ... \u201d.\n\u2022Text reasoning step specifically includes \n\u2022select passages based on mentions, e.g. \u201cthe judoka bearer\u201d, \n\u2022extract a span from the passage as the answer. \n51\n\n[Page 52]\nSTANFORD LAMHybridQA using SUQL\n\u2022HybridQA is much more challenging than Yelp\n\u2022Each question comes with its own DB schema\n\u2022In-context learning: Zero -Shot\n\u2022Scheme of the table\n\u2022No table -specific examples\n52", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 52]\nSTANFORD LAMHybridQA using SUQL\n\u2022HybridQA is much more challenging than Yelp\n\u2022Each question comes with its own DB schema\n\u2022In-context learning: Zero -Shot\n\u2022Scheme of the table\n\u2022No table -specific examples\n52\n\n[Page 53]\nSTANFORD LAMPreliminary Assessment\n\u2022HybridQA is harder than SUQL\n1.It is hard for the semantic parser to identify the right text column\n\u2022Solution: \nAutomatically find the answers in other columns \nif the original column fails\n2.A clear feedback signal at run time: execution returns no answer\n\u2022Solution: Try again\n53\n\n[Page 54]\nSTANFORD LAMSUQL -based agent on HybridQA\nA simple prompt -based system\n54Predicted SUQLClassify -Enum\nanswer fcnsummary fcnSemantic parser\nResult conversion \npromptTest query\nSUQL compiler \nDatabase\nresultsNo results\nTemplates written in jinja syntax No Result Semantic \nparserUp to 2 times\n\n[Page 55]\nSTANFORD LAMHybridQA Tables\n\u2022SUQL translates text tables into SQL table\n\u2022Note the size of the tables \n\u2022Only 4.4 columns and 15.7 rows per table on average!\n\u2022SOTA S3HQA does NOT generalize to bigger tables\n\u2022Supply the whole table to LLM!\n\u2022Our solution scales to any table sizes\n55\n\n\n[Page 56]\nSTANFORD LAMTrain \u2013Dev \u2013Test Split\n56\n\n\n[Page 57]\nSTANFORD LAMEvaluation in HybridQA\n57\nEvaluation Metrics: Exact match (EM) and F1 on raw text\nIn-context learning SUQL outperforms all in -context learning methods \nWithin 8.9% EM and 7.1% F1 to the SOTA trained on 62K examples\n\n[Page 58]\nQUIZ\nWH AT SH OU L D WEDONE X T?\n58", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Suql", "content": "[Page 57]\nSTANFORD LAMEvaluation in HybridQA\n57\nEvaluation Metrics: Exact match (EM) and F1 on raw text\nIn-context learning SUQL outperforms all in -context learning methods \nWithin 8.9% EM and 7.1% F1 to the SOTA trained on 62K examples\n\n[Page 58]\nQUIZ\nWH AT SH OU L D WEDONE X T?\n58\n\n[Page 59]\nSTANFORD LAMError Analysis\nWe analyzed 72 randomly sampled error cases\nEvaluation Issues (61% of errors)\n\u2022Format mismatches : 37.5% . \nPredicted \u201cJohnson City, \nTenessee \u201d v. \u201cJohnson City\u201d.\n\u2022In hybridQA , there is only one \ngold answer.\n\u2022Gold label being either wrong \nor incomplete : 23.6%\n59\nA lot of format problems !\uf0dfQuiz: Is this OK? \nTrue accuracy may reach 84.2%True Errors (39% of errors)\n\u2022Semantic parsing errors: \n22.2%\n\u2022Errors from LLM -based \nfunctions: 11.1%\n\u2022Type -related \nconversation errors: 5.6%\n\uf0dfHandled by fine -tuning, but it is not intrinsically important\n\n[Page 60]\nSTANFORD LAMConclusion: SUQL\n\u2022High -Level Query Language for any Domain of Hybrid Data\n\u2022Expressiveness: Unifies the hybrid data sources\n\u2022Arbitrary composition of free -text and DB knowledge \n\u2022Domain -agnostic SUQL -Based Genie Agent Framework:\n\u2022Developer supplies: tables, free texts, examples (optional)\n\u2022SUQL Framework Implementation:  decompose problem into:\n\u2022Zero-Shot semantic parser: IR + relational algebra\n\u2022Optimizing compiler\n\u2022Evaluation\n\u2022Needed in real -life conversations (Yelp)\n\u2022In-context learning with LLMs works well for natural queries on small\n60", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-suql.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:00:56.124140"}
{"title": "L Freetext", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 4\nGrounding Free -text Knowledge Assistants with\nGeneration + Information Retrieval\n1Monica Lam & Sina Semnani\nWikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few -Shot Grounding on Wikipedia \nSina J. Semnani , Violet Z. Yao*, Heidi C. Zhang*, Monica S. Lam\nIn Proceedings of Association for Computational Linguistics: EMNLP 2023, Singapore, December 6 -10, 2023.\nWinner of the Wikimedia\u2019s Research of the Year Award, 2024. \n\n[Page 2]\nSTANFORD LAMLecture Goal 1: Grounded Chatbot \n\u2022What it takes to be conversational without hallucination?\n\u2022Learn about SOTA (state -of-the-art)\n\u2022WikiChat : Answering questions grounded in Wikipedia\n\n[Page 3]\nSTANFORD LAMLecture Goal 2: How to Work with LLMs\nso you can apply it to your project\n\u2022Traditional NLP methodology (Train with annotated data)\n\u2022Improve training data (e.g. with synthesis)\n\u2022Change the model (e.g.  With fine -tuning)\n\u2022LLMs have out -run current NLP methodology\n\u2022Inadequate evaluation and sometimes MISLEADING (BEWARE!)\n\u2022LLM methodology \n1.Pipeline of well -designed prompts \nto address the weaknesses through experimentation\n2.Self-learning: distill to smaller models for speed and cost\n\u2022Difficulty: How to assess the new prompts?  How to iterate? \n\n[Page 4]\nSTANFORD LAMOutline\n1.A brief history\n2.Metrics for knowledge chatbot \n3.A tale of 2 approaches with LLMs\n4.Distillation to Llama", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 4]\nSTANFORD LAMOutline\n1.A brief history\n2.Metrics for knowledge chatbot \n3.A tale of 2 approaches with LLMs\n4.Distillation to Llama\n\n[Page 5]\nSTANFORD LAMLLM ChatBots\n\u2022Strength: \n\u2022Related and interesting info\n\u2022Perspectives \n\u2022Weakness: Hallucination\n\u2022Long -tail information \u2013LLMs are not databases\n\u2022Real -time knowledge \n\u2022Private, case -specific information (e.g. medical transcripts)\n\u2022Answer: Generation + Information RetrievalFrom Lecture 1\n\n\n[Page 6]\nSTANFORD LAMMany Papers on Knowledge Assistants\nKey papers\n\n\n[Page 7]\nSTANFORD LAMA Brief History of Knowledge Assistants\n2014\nDeep Learning2018\nBERT2020\nGPT-31952\nUniterm\n(keyword \nsearch)1998\nGoogle \nSearch\n(scale \nup)1972\nTF-IDF\n(encode \ninformation \nas vectors)2020\nColBERT\n(match \nquery\nto doc)2019\nDenSPI\n(1streal-\ntime dense \nvector IR)2015\nMemory Network\n(info encoded as \nlearnable dense \nvectors)\n2023\nGPT-41961\nBASEBALL\n(QA)1990\nATIS\n(Conversational \nQA)2004\nTREC 2004\n(Retrieval \nfrom \nWikipedia \nfor QA)2017\nDrQA\n(Retrieve -\nthen -Read \nPipeline)2018\nWizard of \nWikipedia\n(Retrieve -\nthen -generate, \nknowledge -\ngrounded \nconversations)2022\nBlenderBot -3 \nATLAS2023\nBing Chat\nWikiChatSearch\nQA2024\nM3\n(Multi -\nlingual\nQ to doc\n\n[Page 8]\nBlenderBot -3: Retrieval + LLM (2022) \nBlenderBot -3\nOPT-175B model\nFine-tuned on the \ncombination of 20 \nlarge academic \ndatasets\nAccess to a search \nengine\nBlenderBot 3: a deployed conversational agent that continually learns to responsibly engage , Shuster et al, arXiv 2022", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 8]\nBlenderBot -3: Retrieval + LLM (2022) \nBlenderBot -3\nOPT-175B model\nFine-tuned on the \ncombination of 20 \nlarge academic \ndatasets\nAccess to a search \nengine\nBlenderBot 3: a deployed conversational agent that continually learns to responsibly engage , Shuster et al, arXiv 2022\n\n[Page 9]\nSTANFORD LAMEvaluation on 100 Crowdsource Conversations\nEvaluation\n\u2022Only 2.1% factual error\n\u2022Even the smaller version 1 without IR has only 5.1% factual error\n\u2022Scores 4.45/5 on human evaluation", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 9]\nSTANFORD LAMEvaluation on 100 Crowdsource Conversations\nEvaluation\n\u2022Only 2.1% factual error\n\u2022Even the smaller version 1 without IR has only 5.1% factual error\n\u2022Scores 4.45/5 on human evaluation\n\n\n[Page 10]\nBlenderBot -3 Conversation\nUser: Can you tell me about the 2023 \nAustralian Open?\nBlenderBot -3: The tournament is the first \nof the four Grand Slam tennis events \nheld each year, preceding the French \nOpen, Wimbledon, and the US Open. It is \nalso one of two major professional tennis \ntournaments played on hard courts.\nUser: Who won the men\u2019s singles in 2023?\nBlenderBot -3:Danill Medvedev defeated \nNovak Djokovic in straight sets to win his \nsecond consecutive Australian Open \ntitle and ninth career grand slam \nchampionship.User: Who is Haruki Murakami?\nBlenderBot -3:He is a Japanese writer. \nHave you read any of his books? They \nare really great. So are the movies made \nfrom them.\nUser: What movies?\nBlenderBot -3:What a great question! The \nmovie \u201cafter dark\u201d is one my favorites. \nBut it was so hard to find in the us, I had \nto order it online from Japan! Do you like \nmovies?\n\u201cAfter Dark\u201d is a bookFACT CHECK \nNovak Djokovic claimed the men's singles title in 2023, \nhis tenth Australian Open title and 22nd major title \noverall, tying Rafael Nadal 's all-time record.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 11]\nSTANFORD LAMKey Takeaway\n\u2022Blenderbot -3 is very conversational\n\u2022Different from a Q and A bot\n\u2022RAG does not mean accuracy \u2013be careful!\n\u2022When retrieval does not return the answer, \nLLM makes it up \n\u2022Blenderbot -3 has high evaluation but fails miserably in our test\n\u2022Quiz: Why? \n\n[Page 12]\nSTANFORD LAMAtlas (2022)\n\u2022SOTA on many popular knowledge -intensive tasks\n\u2022Including the Wizard of Wikipedia dialog dataset\n\u2022Atlas uses the \n\u2022word -level F1 overlap of the model\u2019s utterances \n\u2022with the first 10 sentences of the Wikipedia page \nfor the conversation topic\n\u2022Favors generic, irrelevant, extractive responses\nAtlas: Few -shot Learning with Retrieval Augmented Language Models , Izacard et al, arXiv 2022", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 13]\nAtlas Conversation Example\nUser: I would like to talk about Amelia Gething . Can you tell me more about her \ncareer as an actress and writer?\nAtlas: She was born in London in 1999.\nUser: Yes, I know that. I was wondering if you could tell me more about her work \nas an actress and writer. What are some of her notable roles or projects?\nAtlas: She has a YouTube channel.\nUser: [\u2026] I know she wrote and starred in the BBC show \"The Amelia Gething\nComplex\". Have you seen that show?\nAtlas: I haven't. I have seen her in the Starz historical drama The Spanish Princess.\nUser: Oh, that's interesting. I haven't seen that show yet, but I'll have to check it \nout. What did you think of Amelia Gething's performance in it?\nAtlas: I liked her. She primarily writes surreal comedy.\nUser: That's cool. I appreciate when writers have a unique style like that. Have you \nseen any of her comedy writing? If so, what did you think of it?\nAtlas: I haven't. She has a YouTube channel. Do you like this conversation? \n\n[Page 14]\nSTANFORD LAMQuiz: Is factuality enough?\nConversationality Metrics\n\n[Page 15]\nSTANFORD LAMConversational Metrics\n\u2022Relevant\n\u2022Informational\n\u2022Saying \u201cI don\u2019t know\u201d is penalized under this metric.\n\u2022Natural\n\u2022Non-Repetitive\n\u2022Temporally Correct\n\u2022Chatbots should provide up -to-date information \nand use the appropriate tense\n\n[Page 16]\nSTANFORD LAMOutline\n1.A brief history\n2.Metrics for knowledge chatbot \n3.A tale of 2 approaches with LLMs\n4.Distillation to Llama", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 16]\nSTANFORD LAMOutline\n1.A brief history\n2.Metrics for knowledge chatbot \n3.A tale of 2 approaches with LLMs\n4.Distillation to Llama\n\n[Page 17]\nSTANFORD LAMGrounding LLMs with Wikipedia\n\u2022We assume access to a knowledge source, \nin the form of a collection of text documents\n\u2022We choose the English Wikipedia\n\u2022Large scale (4.3B words in 6.7M articles)\n\u2022Open -domain: talk about \u201ceverything\u201d\n\u2022A ton of prior work to compare to\n\u2022Publicly available for reproducibility\nIf we can handle Wikipedia, we can handle other large, open -domain corpora\n\n[Page 18]\nWikiChat\n\n\n[Page 19]\nSTANFORD LAMWikiChat Demo\nhttps://wikichat.genie.stanford.edu/\n35K+ questions so far, in 10+ languages\nWords from conversations\ntranslated into English\n\n[Page 20]\nSTANFORD LAMA Tale of Two Approaches\nCombineInformation Retrieval \n(For facts)LLM Generation\n(For Facts,\nConversationality )\n\n[Page 21]\nInformation Retrieval\nRetrieve\n\n[Page 22]\nInformation Retrieval + LLM Generation\nChristopher Nolan has cast Cillian \nMurphy as the titular role of \nOppenheimer, keeping with his \ntradition of meticulous casting choices. \nI\u2019m sure it will be an excellent film!\nWhat does it take to get this? \nWikiChat\nRetrieve", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 22]\nInformation Retrieval + LLM Generation\nChristopher Nolan has cast Cillian \nMurphy as the titular role of \nOppenheimer, keeping with his \ntradition of meticulous casting choices. \nI\u2019m sure it will be an excellent film!\nWhat does it take to get this? \nWikiChat\nRetrieve\n\n[Page 23]\nSTANFORD LAMWhat Does it Take to Create Genie -Chat?\n\u20227 prompts, with carefully selected few -shot examples\n\u20223 people x 4 months (not counting full evaluation)\n\u2022Why does it take so long?  \n\u2022LLM out -ran conventional methodology\n\u2013can\u2019t just iterate on numbers of a benchmark\n\u2013LLMs are better than hand annotations!\n\u2022Assessment: What is easy is now hard, and vice versa\n\u2022A good primitive is worth it!  Many people can use it.\n\u202297% accuracy (GPT -4)\n\u2022Calls to LLM > 10 times; Slow: 40 seconds (GPT -4)\n\u2022After distillation to LLaMA : \n\u202295% accuracy; comparable conversationality ; 5 seconds latency\nLLM\n\n[Page 24]\nA .  R E T R IE V A L\n\n[Page 25]\nSTANFORD LAMAnswering Questions from a Document\n\u2022Effectiveness depends on the length of the document\n\u2022Document length is limited.  \n\u2022GPT-4: 128K token limit, about 96K wordsDocument\nQueryLLM: \nAnswer\nquestionAnswer", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 24]\nA .  R E T R IE V A L\n\n[Page 25]\nSTANFORD LAMAnswering Questions from a Document\n\u2022Effectiveness depends on the length of the document\n\u2022Document length is limited.  \n\u2022GPT-4: 128K token limit, about 96K wordsDocument\nQueryLLM: \nAnswer\nquestionAnswer\n\n[Page 26]\nSTANFORD LAMRetrieving from a Large Corpus\nExample: ColBERT Information Retrieval (IR)\n\u2022Find the document that best \nmatches a question\n\u2022Highest similarity score\n\u2022Fine-tune BERT on MS MARCO (2016)\n\u2022Passage ranking for questions\nRetrieving the right article is critical to \nsuccess of retrieval -based methods\nhttps://microsoft.github.io/msmarco/Similarity score between \nQuery and Document Future\nTopic\n\n\n[Page 27]\nSTANFORD LAM\nM3 Embedding\nFind the document that best matches a question\n\u2022Fine-tune XLM -Ron 17 multilingual passage ranking datasets\n\u2022And some synthetic data too\n\u2022Multilingual: > 100 languages\n\u2022Multifunctional: dense, multi -vector and sparse retrieval\n\u2022Multigranularity: short sentences to long documents\n\u2022Wikichat updated to use M3: multi -lingual! Similarity score between \nQuery and Document Future\nTopic\nM3-Embedding: Multi -Linguality , Multi -Functionality, Multi -Granularity Text Embeddings Through Self -Knowledge Distillation , Chen et al, Findings of ACL August \n2024\n\n[Page 28]\nSTANFORD LAMRetrieval Pipeline\nQuestion\nRetrieved\nDataLLM\nquestion\nanswering AnswerInformation\nRetrieval\n\u2022Accuracy is highly dependent on retrieving the right doc\n\u2022(We will discuss the details in a later lecture)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 28]\nSTANFORD LAMRetrieval Pipeline\nQuestion\nRetrieved\nDataLLM\nquestion\nanswering AnswerInformation\nRetrieval\n\u2022Accuracy is highly dependent on retrieving the right doc\n\u2022(We will discuss the details in a later lecture)\n\n[Page 29]\nRetrieve -then -generate\n1. Query\nRetrieve\n\n[Page 30]\nSTANFORD LAMProblem 1: Example\nTime of the conversation: March 2023; \nHouse of the Dragon won the Golden Globe Best Television Series in January 2023.\n(Retrieved data: \u2018House of the Dragon\u2019 Earns Two Golden Global Nominations    \n--Dec. 2022)\nUser: What do you think is the best TV drama of 2022?\nChatbot: I think it has to be House of the Dragon.\nUser: Did it win any award?\nChatbot: Yes, it won the Golden Globe Award for the best drama TV series.\nUser: Why else do you think it is a good drama?\nChatbot: The first season received highly positive reviews, \u2026 \nIn January 2023, the series is expected to win the Golden Globe Award \nfor Best Television Series -Drama. \n\n\n[Page 31]\nSTANFORD LAMSolution: Need Time Context\n1. Query Prompt\nTime context is included in every prompt!\n\n[Page 32]\nRetrieve -then -generate\nCan we just give all retrieved docs to LLM?Retrieve1. Query", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 31]\nSTANFORD LAMSolution: Need Time Context\n1. Query Prompt\nTime context is included in every prompt!\n\n[Page 32]\nRetrieve -then -generate\nCan we just give all retrieved docs to LLM?Retrieve1. Query\n\n[Page 33]\nSTANFORD LAMRAG: Retrieval Augmented Generation \n\u2022Commercial example: Bing Chat\n\u2022For verifiability, adds citations to indicate the data source\n\u2022Problems\n1.Only 58.7% of the facts are grounded in retrieved info*\n\u2022GPT-4 hallucinates \nwhen retrieved info does not answer the question\n2.Answers are dry, not conversational\n*Nelson F. Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines. ArXiv:2304.09848. 2023. \n\n[Page 34]\nSTANFORD LAMOur Experiment with Bing Chat\n\u2022Our experiment: test on StackExchange\n\u2022Community question answering\n\u2022Stack Overflow is their flagship\n\u2022We provide this prompt: \nCourtesy of Yijia Shao\n\n\n[Page 35]\nSTANFORD LAM\n\u2713\nx\nxExample of Results\nCourtesy of Yijia Shao\n\n[Page 36]\nSTANFORD LAMProblem 2: IR -Based LLM still Hallucinates\n\u2022Bing Chat: \n9 out of 10 tests with StackExchange contain hallucination\n\u2022Facts unsupported by citations\n\u2022Esp. when IR does not return relevant result, \nLLMs like to hallucinate\n\u2022Solution: Filter information \u2013cross out irrelevant info \n(no room for hallucination)\n\n[Page 37]\n+ Filter each paragraph separately\n37\nx 1. Query2. Summarize & Filter\nRetrieve\n\n[Page 38]\nSTANFORD LAMPrompt 2: Summarize & Filter", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 37]\n+ Filter each paragraph separately\n37\nx 1. Query2. Summarize & Filter\nRetrieve\n\n[Page 38]\nSTANFORD LAMPrompt 2: Summarize & Filter\n\n\n[Page 39]\nA .  R E T R IE V A L\nM 3 E M B E D D IN G : M U LT I-LIN G UA L QU E ST IO N ->  D O C U M E N T\nTHEDE V IL ISINT HE DE T A ILS\n(TIM E, SU M M AR IZ E /FIL TE R IN ST E A D OFAN SW E R IN G )\n\n[Page 40]\nB .  G E N E R A TI ON\n\n[Page 41]\nLLM Generation\nHow to eliminate hallucination from generation?1. Query\nRetrieve\n3. Generate \n\n[Page 42]\nSTANFORD LAMWhy is Fact -Checking Hard? Example\n\n\n[Page 43]\nSTANFORD LAMWhy is Fact -Checking Hard? Example\nJust a part of the response is false!\nCorrect: only top 2 teams qualified\nBut there was indeed a third place match!\n\n[Page 44]\nPeople Fact -Checking People for People\n\u2022Journalists fact -check politicians all the time:\n\u2022Identify the claims\nhttps://www.nytimes.com/2023/08/03/us/politics/trump -indictment -fact-check.html\n\n\n[Page 45]\nPeople Fact -Checking People for People\n\u2022Journalists fact -check politicians all the time:\n\u2022Identify the claims\n\u2022Score them\n\n\n[Page 46]\nPeople Fact -Checking Bots\n\u2022Reduce the complexity of fact -checking with two assumptions:\n\u2022Factuality is only meaningful \nwhen measured against a source of ground truth \n\u2022Journalism creates a source of ground truth: e.g. Wikipedia\n\u2022If claims are atomic, each claim is either true or false\n\u2022Factuality: #(fact -checked claims)\n#claims", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 47]\nFact -Checking an LLM Response\nIdentify the claims made\nFor each claim:\n1.Search the ground truth to find relevant docs\n2.Verify if the claim is supported\nUser: I would like to talk about the \n2023 Australian Open Men\u2019s singles \ntennis championship. Did you hear \nabout it?\nChatbot: Yes, I did hear about it! Novak \nDjokovic won the 2023 Australian Open \nMen\u2019s singles tennis championship , \nclaiming his tenth Australian Open title \nand 22nd major title overall . It was an \nimpressive victory!\n\u2026\nFor each \nclaim\nClaim is supported \nor not?\nRetrieve\n\n[Page 48]\n+ Generate + Extract Claims\n1. Query\n2. Summarize & Filter\nRetrieve\n3. Generate 4. Extract Claims\n\n[Page 49]\n+ Fact -check\n1. Query\n2. Summarize & Filter\nRetrieve\n3. Generate 4. Extract Claims5. Fact -\ncheck\n\n[Page 50]\nSTANFORD LAMPerformance on Each Step\n\u2022Claim Identification\n\u2022Can be done well using GPT -4\n\u2022Need to ensure that the claims are self -contained\n\u2022Search\n\u2022Can be done well using state -of-the-art IR\n\u2022Fact check a statement against a given paragraph \n\u2022Automatic SOTA has ~67% F1 with fine -tuned LLaMA\nor 83.2% F1 with few -shot ChatGPT\nAutomatic Evaluation of Attribution by Large Language Models , Yue  et al, arXiv 2023\nFactScore : Fine -grained Atomic Evaluation of Factual Precision in Long Form Text Generation, Min et al, arXiv 2023Not so good!", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 51]\nB .  G E N E R A TI ON\nNEED TOFA C T-CH E C K CL AIM BYCL AIM\nL L M  ISNOTVE R Y GO OD W IT H\nFA C T-CH E C KI NG ACL AIM AG AI NS T ARE T R IE V E D DOC\n\n[Page 52]\nC. C O M B IN ING IR  W IT H GE N E R A TI ON\n\n[Page 53]\nSTANFORD LAMDraft & Refinement\n\u2022Hardest thing for LLMs to say: \u201cI don\u2019t know\u201d\n\u2022It likes to hallucinate\n\u2022When no information remains in a topic\n\u2022Draft produces \u201dSorry, I\u2019m not sure\u201d\n\n[Page 54]\n+ Draft + Refine\n1. Query\nRetrieve\nRetrieve\n\n[Page 55]\nSTANFORD LAMSummary of GenieChat (7 Prompts)\n1. Formulate query from input\n\u2022Retrieve documents (Colbert)\n2. Filter each retrieved doc3. Ask GPT to generate answer\n4. Extract claims\n5. Fact -check/remove each claim\n\u2022Retrieve documents (Colbert)\n6. Draft\n7. RefineTraditional (Factuality)LLM ( Conversationality + Factuality)\n# LLM calls: 5 + n + c\nn: # documents retrieved based on user queries\nc:# claims generatedAll prompts are in Semnani et al.\n\n[Page 56]\nSTANFORD LAMDevelopment of GenieChat\n\u2022Why is coming up with this design challenging?\n\u2022The design space is too large\n\u2022How should we break down the task into components?\n\u2022What should be the inputs to each component?\n\u2022Few-shot performance is sensitive to instruction, \nchoice of few -shot examples.\n\u2022Lack of automatic evaluation metrics \nmakes iterating much more difficult\n\n[Page 57]\nSTANFORD LAMOutline\n1.A brief history\n2.Metrics for knowledge chatbot \n3.A tale of 2 approaches with LLMs\n4.Distillation to Llama", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Freetext", "content": "[Page 57]\nSTANFORD LAMOutline\n1.A brief history\n2.Metrics for knowledge chatbot \n3.A tale of 2 approaches with LLMs\n4.Distillation to Llama\n\n[Page 58]\nSTANFORD LAMDistillation to Llama\n\u2022GenieChat pipeline takes 40 seconds per response\n\u2022Fine-tuned LLaMA takes about 4 seconds!\n\u2022Faster, cheaper, more privateTeacher Model\n(multi -step)\ne.g. GPT -4GPT-\ngenerated\nInputsInstruction \n+ a few \nexamplesFinal \nanswer\nStudent ModelFine-tune+Future\nTopic\n\n\n[Page 59]\nSTANFORD LAMConclusion 1\n\u2022WikiChat\n\u2022Combines the best of both worlds (LLM generation and IR)\n\u2022Is as conversational as LLMs\n\u2022Is far more factual than all baselines \u2013useful for all tasks \n(e.g. persuasion)\n\n[Page 60]\nSTANFORD LAMConclusion 2\nWhat We Learned about the LLM planet\n\u2022Learn how to write LLM -apps\n\u2022Break it down to as many primitives \nas possible\n\u2022(Do not optimize pre -maturely)\n\u2022Hard to stop LLMs from hallucinating, \n\u2022Esp. when information is not availableNo free lunch!\nLLM", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-freetext.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:03.398366"}
{"title": "L Data Coding", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 10\nDocument Set Analysis:\nQualitative Coding \n1Monica Lam  & Sina Semnani\nMultilingual Abstractive Event Extraction for the Real World\nSina J. Semnani , Pingyue  Zhang, Wanyue  Zhai, Haozhuo  Li, Ryan Beauchamp, Trey Billing, Katayoun  Kishi, Manling  Li, \nMonica S. Lam\nUnder review\n\n[Page 2]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation\n\n[Page 3]\nSTANFORD LAMKnowledge Retrieval\nFree Text Structured Data\nSearchInformation\nRetrievalKB Query\nData Analytics KB Query ?\n\n[Page 4]\nSTANFORD LAMDocument Set Analysis\nAnalysis\ncombines info \nfrom every doc\nDecision, Stats, Trend, \u2026Resumes\nPaper Reviews\nResearch Proposals\nFinancial ReportsExperiment Results\nHuman Genome\nDrug/Protein/Disease\nEvent NewsNews\nSocial Media\nInterviews\nMedical Rec.\nRanking (Top n)\nof ALL documentsAccumulationCompute \nstats/trends\nFinalistsKnowledge\nBasesDiscover\nKnowledge\n\n[Page 5]\nSTANFORD LAM\n\n\n[Page 6]\nSTANFORD LAMExample: Early Disease Outbreak Detection\n20 million per day on COVID19\nSymptoms, prevention, deaths\nIn May 2020\nEvent Detection from Social Media for Epidemic Prediction,  Parekh et al, NAACL 2024\nEstimate real -time number of cases \nin early stages of a new outbreak.Tweets\nEvent \nDetection", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 7]\nSTANFORD LAMEvent Ontology for Epidemic Preparedness\n\n\n[Page 8]\nSTANFORD LAMEvent detection \u2192 \nEpidemic Warnings 4 -9 Weeks Earlier (Monkeypox)\nAug 14, 2024\n\n[Page 9]\nSTANFORD LAMQualitative Coding\nAnalysis\nOutcome\nQueriesCodebook\nOutcomeQualitative Coding\nStructured DataDescribes a data collection\n\u2022Structure\n\u2022Contents\n\u2022Layout\nCodebook : aka schema               \n           Qualitative coding : aka information extraction\n\n[Page 10]\nSTANFORD LAMCodebook\n\u2022Experts\u2019 key concepts used to \u201cevaluate and analyze\u201d a doc\n\u2022Can be personal and subjective\nDocument Set Codebook\nNews Source, event types, event parameters\nSocial Media Source, ratings, reposts, event types, event parameters \nResumes Name, address, education, employment, publications, products, \nPaper Reviews Topic area, originality, soundness, significance, theory, empirical\nResearch proposals Relevance, soundness, potential impact, risk, team credentials\nEarnings ReportsRevenue, expenses, net income, earnings per share (EPS), \nbalance sheet highlights, cash flow changes, \u2026 \nInterviews Participants' thoughts, emotions, experiences, and behaviors\nMedical RecordsPatient's health history, diagnoses, medications, treatment plans, \nimmunization dates, allergiesGenerated \nby GPT -4\n\n[Page 11]\nSTANFORD LAMQuestions\n\u2022How to get a codebook?\n\u2022Given by an expert\n\u2022Inferred from questions asked by an expert\n\u2022Given a codebook, how to automate qualitative coding? \nToday\u2019s Lecture", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 11]\nSTANFORD LAMQuestions\n\u2022How to get a codebook?\n\u2022Given by an expert\n\u2022Inferred from questions asked by an expert\n\u2022Given a codebook, how to automate qualitative coding? \nToday\u2019s Lecture\n\n[Page 12]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation\n\n[Page 13]\nSTANFORD LAM\nUN\u2019s International Organization for Migration (IOM) \nUses ACLED data to track \nthe movement and needs of displaced people \nin over 80 countries\nThe US multi -agency \nGlobal Fragility Act Secretariat \nUses ACLED data \nto promote stability across five priority countries.\nIntroducing ACLED: An Armed Conflict Location and Event Dataset,  Raleigh et al, Journal of Peace Research 2010\n\n[Page 14]\nSTANFORD LAMExample: Violence Against Civilians in Ukraine\n\n\n[Page 15]\nSTANFORD LAMExample: Sexual Violence Around the Globe\n\n\n[Page 16]\nSTANFORD LAMExample: Tracking Political Demonstrations\nSeptember 2024 review, published on October 4th\nhttps://acleddata.com/2024/10/04/united -states -canada -overview -september -2024/,  ACLED, Accessed 11/5/2024", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 15]\nSTANFORD LAMExample: Sexual Violence Around the Globe\n\n\n[Page 16]\nSTANFORD LAMExample: Tracking Political Demonstrations\nSeptember 2024 review, published on October 4th\nhttps://acleddata.com/2024/10/04/united -states -canada -overview -september -2024/,  ACLED, Accessed 11/5/2024\n\n\n[Page 17]\nSTANFORD LAMExample: Tracking Political Demonstrations\nACLED reports that in September 2024:\nhttps://acleddata.com/2024/10/04/united -states -canada -overview -september -2024/,  ACLED, Accessed 11/5/2024\nAnti-Trump \ndemonstrations rose in \nthe United States after \nthe September 10th \npresidential debate.Because\n\n[Page 18]\nSTANFORD LAMExample: Tracking Political Demonstrations\nACLED reports on various radical groups:\nhttps://acleddata.com/2024/10/04/united -states -canada -overview -september -2024/,  ACLED, Accessed 11/5/2024\n\n\n[Page 19]\nSTANFORD LAMQuiz\nWhat are the challenges\nof this type of data analysis?\n\n[Page 20]\nSTANFORD LAMScale Matters in the Real World\n\u2022The document set can be massive\n\u2022150,000+ online news articles are published each day\n\u2022Analysis can cover a long period of time\n\u2022Global Analysis means many languages\n\u2022ACLED for example, has covered:\n243\nCountries/territories\n2 Million\nEvents\n80+\nLanguages\nhttps:// huggingface.co /datasets/ stanford -oval/ ccnews , Semnani , Accessed 11/5/2024\n200+\nResearchers\n\n[Page 21]\nCORREC TN ES S  IS CRITICAL\nVE RY ELA BORATE  \nMA NUAL  CODIN G  A ND RE VIEW  PROCE SS\n\n[Page 22]\nSTANFORD LAMHow Does ACLED Ensure Correctness?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 21]\nCORREC TN ES S  IS CRITICAL\nVE RY ELA BORATE  \nMA NUAL  CODIN G  A ND RE VIEW  PROCE SS\n\n[Page 22]\nSTANFORD LAMHow Does ACLED Ensure Correctness?  \n\n\n[Page 23]\nSTANFORD LAM\n\n\n[Page 24]\nAUTOM A TE D  QUALITA TIV E  CODIN G  (AQC)\nADDRE SSE S  SCA LING\nIS IT ACCURATE  EN OUGH ?\n\n[Page 25]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation\n\n[Page 26]\nSTANFORD LAMEvent Detection \u2013 A Long -Studied Subject\n\u2022Started in 1970s\n\u2022Most widely used dataset is ACE, published in 2005\n\u2022Limits of technology \u2192 simplifying assumptions:\n\u2022Unit of analysis is sentence\n\u2022Extraction is done using keywords  and rules\n\u2022Emphasis on words  & spans (consecutive words in the text)\n\u2022Not semantics\n\n[Page 27]\nSTANFORD LAMAn Example from ACE05\nSpan -based annotation:\n\u2022\u201ckilled \u201d is the event mention\n\u2022\u201ccivilians \u201d is the event\u2019s victim\n\u2022\u201cthe last weeks \u201d and \u201c the last days \u201d are the event time\nEU foreign policy supremo Javier Solana \nlikewise slammed the attack, although \nhe also took a jab at Israel, saying, \"There \nhave been too many civilians killed  in \nthe last weeks  and the last days .\"", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 28]\nSTANFORD LAMLimitations of Span -Based Event Extraction\nSentence is often not enough context, resulting in uninformative events:\n\u2022Who are these civilians?\n\u2022When is last week?\n\u2022Who was the attacker?\n\u2022Where did it take place?\nOverly strict evaluation\n\u2022\u201cmany civilians \u201d scores 0\n\u2022\u201cPalestinians \u201d (from the previous sentence) scores 0\nEU foreign policy supremo Javier Solana \nlikewise slammed the attack, although \nhe also took a jab at Israel, saying, \"There \nhave been too many civilians killed  in \nthe last weeks  and the last days .\"\n\n[Page 29]\nSTANFORD LAMExtractive (Spans) vs. Abstractive (Meaning)\nworkers  \u2022\nmusicians  \u2022\nILWU  \u2022\nCWA  \u2022\nTeamsters Local 70  \u2022\nPMWG  \u2022\nformer workers at Tesla  \u2022\nPast ILWU president Brian McWilliams  \u2022\nNadya Williams  \u2022\nAFL-CIO is affiliated with CWA\nIBT has many local branches, including Teamsters Local 70, \u2026\u2022Labor Group\n\u2022ILWU: International Longshore \nand Warehouse Union\n\u2022AFL-CIO: American Federation of Labor \nand Congress of Industrial Organizations\n\u2022CWA: Communications Workers of \nAmerica\n\u2022IBT: International Brotherhood of \nTeamsters\n\u00d7 (not in the database)Spans Entity List\nEntity\nDatabaseQuiz: What is the \nadvantage of \nabstractive \nlinking?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 30]\nSTANFORD LAMSummary: Deficiencies/Fixes of Prior Work\n\u2022Unit of analysis: Increase accuracy\n            Sentence \u2192 Article\n\u2022Extraction method: Increase accuracy\n          Keywords  and rules \u2192 Semantics\n\u2022Linking:  easier analysis and comparison   \n            Span (extractive) \u2192 Given Entity/ enum  value (abstractive)\n\n[Page 31]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation\n\n[Page 32]\nSTANFORD LAM\nUnder Review\n\n[Page 33]\nSTANFORD LAM\u2022Experts want to specify \nexactly what they want\n\u2022Makes qualitative coding \nchallenging\nCodebooks Can be Large\n\n\n[Page 34]\nSTANFORD LAMCodebook for ACLED Events\n\u2022What things \u201cexist\u201d?\n\u2022Actors: e.g. political parties, organized groups\n\u2022What things \u201chappen\u201d?\n\u2022E.g. a peaceful protest, riot, arrest\n\u2022What are the details in each event?\n\u2022Each event type has its own arguments\n\u2022All \u201cgroup\u201d must be an Entity\n6,217 Entities\n25 Event Types\nEvent ArgumentsProblem 1. Event detection\nProblem 2. Event argument extraction\nProblem 3. Entity detection and linking e.g. Peaceful Protest: protesters, location, crowd size, \u2026", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 35]\nSTANFORD LAMZest: \n0-Shot Qualitative Coder\nQueries\nOutcomeQualitative Coding\nStructured DataEvent Detection\nUse Modified RankGPT  in batches \nto find relevant entities\nFilter based on descriptionsEvent Argument Extraction\nEntity DB Codebook\nStructured Data\n\n[Page 36]\nSTANFORD LAMProblem 1: Let\u2019s Detect Event in this News Article\n\u2022From Indybay.org, a community news website\n\u2022580 words\nhttps://www.indybay.org/newsitems/2024/01/15/18862100.php,  Accessed 11/5/2024\nTesla Fremont MLK Rally Protesting Racism, Union Busting\nA rally on MLK Weekend was held at the massive Tesla Fremont \nassembly plant where 20,000 work. The action was called to protest \nthe systemic racism and sexism by Elon Musk and his massive union \nbusting drive. They also supported the Swedish striking Tesla \nmechanics who have been on strike for nearly 2 months.\n\u2026\n\n[Page 37]\nSTANFORD LAMEvent Types in the Codebook\nEvent Type Expert Description (summarized)\nPeaceful Protest Protestors do not engage in violence\nProtest w/ \nInterventionProtestors are faced with a physical attempt to disperse or \nsuppress without serious or lethal injuries\nExcessive Force \nAgainst ProtestersProtestors are targeted with lethal violence or violence resulting \nin serious injuries\nViolent \nDemonstrationProtestors engage in violence and/or destructive activity\n[20 more event types]\nChemical WeaponChemical weapon is used in warfare, as listed as Schedule 1 of \nthe Chemical Weapons Convention of 1993", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 38]\nSTANFORD LAMEvent -type Detection (ED) is a Classification Task\n\u2022Can be learnt by a lot of training data\n\u2022But annotation is costly\n\u2022Lots of classes, lots of edge cases\n\u2022Much more difficult than other classification tasks\n\u2022Must be repeated for each codebook and domain\nCan we use LLMs?\n How\n\n[Page 39]\nSTANFORD LAMEvent Detection: Zero -shot LLMs\n# instruction\nDetermining the best matching Event types for a given news article.\n# input\nFirst, here is the news article you need to analyze:\n{{ article }}\nNow, carefully review the annotation guidelines for various event types:\n{% for e in event_types  %}\n  {{ e.name }}: {{ e.description  }}\n{% endfor %}\nYour output should look like this:\n<reasoning>\n[Explain your reasoning for the event types you decide to include, and their order]\n</reasoning>\n<answer>\nevent_type_1 > event_type_2 > ... \n</answer>Result Preview: 80% (GPT -o4) \n\n[Page 40]\nSTANFORD LAMProblem 2. Event Argument Extraction (EAE)\n\u2022Now that we know the event type, \nzoom in on the relevant part of the codebook:\nEvent Argument Expert Description (summarized)\nProtestors List of protestor groups or individuals involved in the protest\nLocation Where the event happened\nCrowd SizeEstimated size of the crowd. It can be an exact number, a \nrange, or a qualitative description like 'small'.\nCounter Protestors Groups or entities engaged in counter protest, if any", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 41]\nSTANFORD LAMEvent Argument Extraction w/ LLMs \u2013 Attempt 1\n# instruction\nExtract event arguments from a given news article.\n# input\nFirst, here is the news article you need to analyze:\n{{ article }}\nNow, carefully review the event argument guidelines for the {{ event_type  }}:\n{% for ea in event_type.event_arguments  %}\n  {{ ea.name }}: {{ ea.description  }}\n{% endfor %}\n\n[Page 42]\nSTANFORD LAMLLMs Can Struggle to Follow Complex Instructions\n\u2022Types constraints, according to the codebook\n\u2022E.g. crowd size should be a string, not number\n\u2022Nested event arguments\n\u2022E.g. \u201cLocation\u201d can have a \u201ccountry\u201d argument\n\u2022Relationship between different events\n\n[Page 43]\nSTANFORD LAMIdea: LLMs Know Python Data Structure Syntax!\n\u2022Use Python classes to represent the event signature\n\u2022Event types as classes\n\u2022Event arguments as typed  fields\nCode4Struct: Code Generation for Few -Shot Event Structure Prediction , Wang et al, ACL 2023", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 44]\nSTANFORD LAMCoding Guidelines as Class Definitions\nclass Location (BaseModel ):\n  \"\"\"\n  The most specific location for an event. Locations can be named \npopulated places, geostrategic locations, natural locations, or \nneighborhoods of larger cities.\n  In selected large cities with activity dispersed over many \nneighborhoods, locations are further specified to predefined \nsubsections within a city. In such cases, City Name \u2013 District name \n(e.g. Mosul \u2013 Old City) is recorded in \" specific_location \". If \ninformation about the specific neighborhood/district is not known, \nthe location is recorded at the city level (e.g. Mosul).\n  \"\"\"\n  country: str = Field(..., description =\"Normalized name of a \ncountry, e.g. United States\" )\n  address: str = Field(..., description =\"Full address or location \ndescription including all geographic levels up to the neighborhood \nlevel, including village/city, district, county, province, region, \ncountry, if available. Exclude street names, buildings, and other \nspecific landmarks.\" )class Protest(ACLEDEvent , ABC):\n  \"\"\"\n  A \"Protest\" event is defined as an in -person public demonstration of \nthree or more participants in which the participants do not engage in \nviolence, though violence may be used against them \u2026 Excludes symbolic \npublic acts such as displays of flags or public prayers, legislative \nprotests, such as parliamentary walkouts or members of parliaments \nstaying silent, strikes \u2026\n    \"\"\"", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "violence, though violence may be used against them \u2026 Excludes symbolic \npublic acts such as displays of flags or public prayers, legislative \nprotests, such as parliamentary walkouts or members of parliaments \nstaying silent, strikes \u2026\n    \"\"\"\n  location : Location  = Field(..., description =\"Location where the \nevent takes place\" )\n  crowd_size : Optional [str] = Field(..., description =\"Estimated size \nof the crowd. It can be an exact number, a range, or a qualitative \ndescription like 'small'.\" )\n  protestors : List[str] = Field(..., description =\"List of protestor \ngroups or individuals involved in the protest\" )\nclass PeacefulProtest (Protest):\n  \"\"\"\n  Used when demonstrators gather for a protest and do not engage in \nviolence or other forms of rioting activity, such as property \ndestruction, and are not met with any sort of violent intervention.\n  \"\"\"\n  counter_protestors : List[str] = Field(..., description =\"Groups or \nentities engaged in counter protest, if any\" )", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 45]\nSTANFORD LAMEvent Argument Extraction w/ LLMs Attempt 2\n# instruction\nExtract event arguments from a given news article.\n# input\nFirst, here is the news article you need to analyze:\n{{ article }}\nNow, carefully review the annotation guidelines for the {{ event_type  }}:\n<class definitions (on last slide)>\n\n[Page 46]\nSTANFORD LAMLLM is Good at Structured Output: \n       Constrained Decoding for Structured Outputs\n1.Convert Python class definitions to JSON schema\n2.Convert the JSON Schema into a context -free grammar\n3.Pass the prompt and JSON schema to the LLM\n4.Decode the output from LLM, choose the most likely token \nthat conforms to the grammar \u2192 this is the \u201cconstrained\u201d part\n5.Convert the decoded JSON object to the original Python object\n\u2022Some commercial LLM providers support it, e.g. OpenAI\n\u2022Many open source LLMs support it via\n\u2022SGLang , Outlines, guidance\nhttps:// platform.openai.com /docs/guides/structured -outputs/introduction,  Accessed 11/5/2024", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 47]\nSTANFORD LAMEvent Argument Extraction w/ LLM\nTesla Fremont  MLK Rally Protesting Racism, \nUnion Busting\nWorkers  and musicians  rallied on 1/13/24 at the Tesla \nassembly plant in Fremont, California on MLK \nweekend to protest Elon Musk's systemic racism and \nsexism at the Tesla assembly plant. They also protested \nthe union busting and rallied in solidarity with striking \nSwedish Tesla service mechanics ... Workers from the \nILWU , CWA , Teamsters Local 70 , and PMWG  spoke in \nsolidarity, as well as former workers at Tesla . ...\nPast ILWU president Brian McWilliams  joined the Tesla \nMLK action and spoke ...\nRally participant Nadya Williams talked about her son, \nwho is a Swedish American union organizer and is \nsupporting the striking Swedish Tesla mechanics ...\nBy noon, there were more than hundred workers \u2026PeacefulProtest (\n  protestors =[\n\"workers\",\n\"musicians \",\n\"ILWU\",\n\"CWA\",\n\"Teamsters Local 70 \",\n\"PMWG\",\n\"former workers at Tesla \",\n\"Past ILWU president Brian McWilliams \",\n\"Nadya Williams \"\n],\n  location =Location (\n    country=\"United States \",\n        address=\"Fremont, California, United States \"\n  ),\n  crowd_size =\"more than 100 \",\n  counter_protestors =[],\n)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 48]\nSTANFORD LAMProblem 3. Entity Detection & Linking\n\u2022\u201cEntity\u201d must refer to an item on the List\n\u2022So you can link the different records \nto the same entity\n\u2022For example, in ACLED:\n\u2022Generic entities like \u201cLabor Group\u201d \nannotated to aid analysis of labor issues\n\u2022Specific Unions are monitoredprotestors =[\n\"workers\" ,\n\"musicians\",\n\"ILWU\",\n\"CWA\",\n\"Teamsters Local 70 \",\n\"PMWG\",\n\"former workers at Tesla \",\n\"Past ILWU president Brian \nMcWilliams \",\n\"Nadya Williams \"\n]", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 49]\nSTANFORD LAMAbstractive Event Argument Extraction (AEAE) Problem\nGiven a database of entities, select all relevant to the event\nEntity Expert Description (summarized)\nLabor GroupA collective entity composed of workers or trade unions that \nadvocate for labor rights and interests \u2026\nAFL-CIO: American Federation of Labor \nand Congress of Industrial OrganizationsThe largest federation of unions in the United States. \nEncompasses various affiliated unions, such as the IW, IUPAT,  \nCWA  \u2026\nCWA : Communications Workers of \nAmericaA labor union representing workers in telecommunications, \nmedia, manufacturing, healthcare, and public service \u2026\nIBT: International Brotherhood of \nTeamstersA labor union in the United States representing transportation \nand logistics workers. Operates through local chapters, like \nTeamsters Local 70  \u2026\nILWU : International Longshore and \nWarehouse UnionA labor union representing dock workers and other maritime \nand warehouse employees primarily in the United States \u2026\n[6212 more entities]", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 50]\nSTANFORD LAMAEAE Example\nTesla Fremont  MLK Rally Protesting Racism, \nUnion Busting\nWorkers and musicians  rallied on 1/13/24 at the Tesla \nassembly plant in Fremont, California on MLK \nweekend to protest Elon Musk's systemic racism and \nsexism at the Tesla assembly plant. They also protested \nthe union busting and rallied in solidarity with striking \nSwedish Tesla service mechanics ... Workers from the \nILWU, CWA, Teamsters Local 70, and PMWG spoke in \nsolidarity, as well as former workers at Tesla. ...\nPast ILWU president Brian McWilliams joined the Tesla \nMLK action and spoke ...\nRally participant Nadya Williams talked about her son, \nwho is a Swedish American union organizer and is \nsupporting the striking Swedish Tesla mechanics ...\nBy noon, there were more than hundred workers \u2026PeacefulProtest (\n  protestors =[\n    \"Labor Group\" ,\n    \"AFL-CIO: American Federation of Labor and Congress of \nIndustrial Organizations\" ,\n    \"CWA: Communications Workers of America\" ,\n    \"IBT: International Brotherhood of Teamsters\" ,\n    \"ILWU: International Longshore and Warehouse Union\" ,\n  ],\n  location =Location (\n    country=\"United States \",\n        address=\"Fremont, California, United States \"\n  ),\n  crowd_size =\"more than 100 \",\n  counter_protestors =[],\n)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 51]\nSTANFORD LAMHow To Perform AEAE?\nTesla Fremont MLK Rally \nProtesting Racism, Union Busting\nWorkers and musicians  rallied on \n1/13/24 at the Tesla assembly plant \nin Fremont \u2026\n Entity\nDatabase\u2022We need to connect the article to its entities\n\u2022Is like a retrieval task:\n\u2022Given a document, retrieve the most relevant entities\n\u2022But off -the-shelf retrievers don\u2019t work well on this task\n?\n\n[Page 52]\nSTANFORD LAMUse RankGPT  to Retrieve Top Match\n\u2022Entities are curated by experts (< thousands entities)\n\u2022RankGPT  [Sun et al.] proposes a 0 -shot prompt for ranking items\nIs ChatGPT Good at Search? Investigating Large Language Models as Re -Ranking Agents,  Sun et al, EMNLP 2023# instruction\n<query>\n{{ query }}\n</query>\n{% for item in items %}\n   [{{loop.index }}] {{ item }}\n{% endfor %}\nList the items from the most relevant to the \nquery to the list relevant.\nYour output should be in the format [1] > [2] \n> \u2026General RankGPTQuery Documents\nRanked List \n(based on relevance)\n[1] > [2] > \u2026RankGPT  Prompt\n\n[Page 53]\nSTANFORD LAMApply RankGPT  to EAE\nModified RankGPTDocument\nEvent Type\nEvent arguments\u2019  description Entity Database\nOnly relevant entities\nRole Assignment Prompt\nEntities and their \nAssigned ArgumentLabor Group\n\u2026\nLabor Group -> Protestors\n\u2026", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 54]\nSTANFORD LAM# instruction\nYour task is to select all entities involved in a news article from a provided list. An entity is an individual, group, or \norganization involved in an event. This includes:\n - Organized armed groups with political purposes\n - Named entities\n - General terms describing participants like \"Rioters\", \"Protestors\", \"Civilians\", \"Labor Group\", etc.\n# input\nNews article:\n<article>\n{{ article }}\n</article>\nThe event you should focus on is the {{ event_type  }} event, which happened in {{ country }}.\nGuidelines:\n1. Read the entire article carefully.\n2. Identify groups, organizations, and individuals involved in the described events.\n3. Note both specific names and generic terms used for participants.\n4. Consider entities that may be implicitly involved.\n5. For politicians, include the name of their political party or group as well, if available in the entity list.\n6. Include both specific and generic entities when applicable (e.g., a political party leading a protest should be counted as tw o \nentities: the party name and \"Protestors\"), if available in the entity list.\n7. Include characteristics like ethnicity or religion as separate entities when mentioned (e.g., \"Latin American Group\" or \n\"Women\"), if available in the entity list.\n8. Err on the side of inclusion if unsure about an entity's involvement.\nFrom the following list, select entities involved in the event described in this news article:\n<entity_list >", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "\"Women\"), if available in the entity list.\n8. Err on the side of inclusion if unsure about an entity's involvement.\nFrom the following list, select entities involved in the event described in this news article:\n<entity_list >\n{% for entity in potential_entities  %}\n[{{ loop.index  }}] {{ entity }}\n{% endfor %}\n</entity_list >\nProvide your answer within <entities>  tags, listing one entity name per line:\n<entities>\nentity name 1", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 55]\nSTANFORD LAMProblem: The Database is \nToo large\n\u2022N = 6.2K entities won\u2019t fit in one prompt!\n\u2022Solution: \n1.Split database into groups \nof M1 = 100 entities to find candidates\n\u2022Will need N/M1 LLM callsUse Modified RankGPT  in batches \nto find relevant entities\nFilter based on descriptionsEvent Argument Extraction\ncandidates\nEntity\nDatabaseEntity names 1 -100\nEntity names 101 -200\n\u2026\n\n[Page 56]\nSTANFORD LAMProblem: The Database is \nToo large\n\u2022N = 6.2K entities won\u2019t fit in one prompt!\n\u2022Solution: \n1.Split database into groups \nof M1 = 100 entities to find candidates\n\u2022Will need N/M1 LLM calls\n2.Split candidates  into groups of M2 = 10 entities\n\u2022Prompt LLM to extract evidence from the article\n\u2022Remove entities that don\u2019t have any evidenceUse Modified RankGPT  in batches \nto find relevant entities\nFilter based on descriptionsEvent Argument Extraction\ncandidates\n\n[Page 57]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 57]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation\n\n[Page 58]\nSTANFORD LAM\n LEMONADE\n\u2022A cleaned version of ACLED event data\n\u2022Training: Events from Jan to Jun of 2024\n\u2022Validation/Test: Events from Jul 2024\n\u202241,148 events in totalLarge\nExpert -annotated\nMultilingual\nOntology -Normalized\nAbstractive\nDataset of\nEvents\nLEMONADE statistics per language\nThe best-annotated  dataset excerpted from a real-life dataset\nwith end-to-end abstract entity linking\n\n[Page 59]\nSTANFORD LAMAutomatic Qualitative Coding (AQC)\n\u2022ACLED is a major world -level effort\n\u2022Weekly effort by 200+ (part -time) world -wide researchers \n\u2022Can AQC help ACLED expand coverage? \n\u2022Events, languages, countries\n\u2022High -quality LEMONADE \u2192 effective fine -tuning \n\u2022Can In -Context Learning based AQC lead to new analysis efforts?  \n\u2022High -quality LEMONADE \u2192 compare in -context learning\n                                                with effective fine -tuning\n\n[Page 60]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 60]\nSTANFORD LAMLecture Goals\n\u2022Motivation: Analysis across a document set\n\u2022ACLED: Manual qualitative coding\n\u2022Prior research on automated qualitative coding \n\u2022ZEST: an Automatic Qualitative Coder \n\u2022LEMONADE: An AQC benchmark based on ACLED\n\u2022Evaluation\n\n[Page 61]\nSTANFORD LAMEvaluation: Event Detection\n\u2022Event Detection is a classification task \u2192 F1 metric\n\u2022Compare\n\u2022LLaMA  3.1 8B (trained on 17K examples)\n\u2022GPT-4o (0 -shot)\nModel ED F1\nLLaMA 3.1 87.3\nGPT-4o 79.8\n\n[Page 62]\nSTANFORD LAMOther Tasks and Metrics\n\u2022Event Detection (ED): F1\n\u2022Abstractive Event Argument Extraction (AEAE): F1\n\u2022Given the right Event Detection as input\n\u2022Extract all Event Arguments, including Entities and their role\n\u2022Abstractive Event Extraction (AEE): F1\n\u2022Do ED, then do AEAE using the outputs of ED\n\u2022Abstractive Entity Detection and Linking (AEDL): F1\n\u2022Find the list of all relevant entities\n\n[Page 63]\nSTANFORD LAM\nED \ud835\udc391AEAE \ud835\udc391AEE \ud835\udc391AEDL  \ud835\udc391 (all) AEDL  \ud835\udc391 (seen) AEDL  \ud835\udc391 (unseen)Experiments on LEMONADE\n\u2022Fine-tuned 8B -parameter LLaMA  3.1\n\u2022Zero -shot system based on GPT -4o\nTranslation in test time: translate everything into English", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 63]\nSTANFORD LAM\nED \ud835\udc391AEAE \ud835\udc391AEE \ud835\udc391AEDL  \ud835\udc391 (all) AEDL  \ud835\udc391 (seen) AEDL  \ud835\udc391 (unseen)Experiments on LEMONADE\n\u2022Fine-tuned 8B -parameter LLaMA  3.1\n\u2022Zero -shot system based on GPT -4o\nTranslation in test time: translate everything into English\n\n[Page 64]\nSTANFORD LAMModel ko zh de en it id es fa pt ne fr tr ru uk ar my\nLLaMAX 81.3 79.3 78.5 76.7 76.6 76.4 72.3 67.7 66.1 65.9 65.6 64.0 63.7 62.9 48.9 41.2\nZest 57.4 67.0 71.0 51.7 70.4 60.1 60.3 63.8 52.4 51.9 61.5 60.0 61.4 54.7 40.9 43.8Performance for Different Languages\nko: Korean, zh: Chinese, de: German, en: English, id: Indonesian, es: Spanish, fa: Farsi, \npt: Portuguese, ne: Nepalese, fr: French, tr: Turkish, ru: Russian, uk: English (UK), ar: Arabic, \nmy: Burmese (spoken in Myanmar)AEE \ud835\udc391 of two models on individual languages of the test set\n\n[Page 65]\nSTANFORD LAMResult Analysis (Supervised Models)\n\u2022Event detection errors\n\u2022Some event qualities for multiple event type\n\u2022ACLED codebook have rules to resolve ambiguities\n\u2013 need to be incorporated as expert \u201crules\u201d\n\u2022Event argument extraction errors\n\u2022Abstractive entity detection and linking\n\u2013 need missing world knowledge\n\u2022Useful today for highlighting potential errors in hand annotation\uf0df Work in progress\n\uf0df Work in progress", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Data Coding", "content": "[Page 66]\nSTANFORD LAMResult Analysis (Zero -shot Model)\n\u2022Zest works better than expected \n\u2022Fine-tuning dataset size: 17000 \n\u202210 point drop in AEE F1: 57.2 vs. 67.7 F1 \n(ignoring test time translation)\n\u2022Zest can handle unseen entities better\n\u202250 F1 vs. 14.1 \n\u2022(ignoring test time translation)\n\u2022Struggles with edge cases in ED and AEDL\n\n[Page 67]\nSTANFORD LAMConclusion\n\u2022Automatic Qualitative Coding (AQC)\n\u2022Has many applications\n\u2022Lemonade: the best annotated event dataset\nexcerpted from ACLED\u2019s real data\n\u2022With abstract event extraction \n\u2022Fine-tuning reaches 67 F1\n\u2022Zest: an AQC designed for real -life problems\n\u2022Contribution: abstract entity detection and linking\n\u2022Promising zero -shot performance (10 points lower in F1)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-data-coding.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:06.851845"}
{"title": "L Knowlege Curation", "content": "[Page 1]\nStanford CS224v CourseConversational Virtual Assistants with Deep LearningLecture 2Knowledge CurationYijia Shao, Yucheng Jiang, Monica Lam\n\n[Page 2]\nSTANFORDLAM2Lecture Plan1.Information Retrieval & RAG at a Glance 2.From Retrieving Information to Knowledge Curation 3.Building LM-Empowered Systems4.Evaluation? Evaluation! 5.Bring Human into the Loop \n\n[Page 3]\nSTANFORDLAM3DocumentInformation Retrieval at a Glance\nRetrieverQueryDocumentDocumentDocumentDocumentDocumentDocument CollectionRankedDocuments\n\n[Page 4]\nSTANFORDLAM4Retrieval-Augmented Generation (RAG)\nOn 9/15/2024A major issue of using language models for knowledge tasks: Hallucination\u2022Long-tail information\u2022Knowledge cutoffs\u2022Private data\n\n[Page 5]\nSTANFORDLAM5RAG is Widely Used in Question Answering\nStep 1: RetrieveStep 2: Read & AnswerRecommended Reading:\u2022Chen et al., 2017\u2022Lewis et al., 2021\n\n[Page 6]\nSTANFORDLAM6\nMTEB: Massive Text Embedding Benchmark, Niklas et al. 2023(This illustration is contributed by Niklas Muennighoff.)\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT, Khattab et al. 2020We have better embedding models and infrastructure for Information Retrieval.\n\n[Page 7]\nSTANFORDLAM7\nHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, Yang et al., 2023We have stronger language models for generation.\n\n[Page 8]\nSTANFORDLAM8Meta Question:Are people\u2019s information needs satisfied?\nThe illustration is co-created with DALL-E.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 7]\nSTANFORDLAM7\nHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, Yang et al., 2023We have stronger language models for generation.\n\n[Page 8]\nSTANFORDLAM8Meta Question:Are people\u2019s information needs satisfied?\nThe illustration is co-created with DALL-E.\n\n[Page 9]\nSTANFORDLAM9QAAAQQ\nQuestion-Answering System\n\n[Page 10]\nSTANFORDLAM10QAAAQQUser\u2019s Known UnknownsUser\u2019s UnKnown Unknowns\n\n[Page 11]\nSTANFORDLAM11\n2. From Retrieving Information to Knowledge Curation\nDiscover\nCurate\nAccessDiscover previously non-existent informationFind, organize, and present relevant and valuable contentInformation Retrieval, Question Answering\n\n[Page 12]\nSTANFORDLAM12Lecture Plan1.Information Retrieval & RAG at a Glance 2.From Retrieving Information to Knowledge Curation 3.Building LM-Empowered Systems4.Evaluation? Evaluation! 5.Bring Human into the Loop \n\n[Page 13]\nSTANFORDLAM13\n2. From Retrieving Information to Knowledge Curation\nDiscover\nCurate\nAccessDiscover previously non-existent informationFind, organize, and present relevant and valuable contentInformation Retrieval, Question Answering\n\n[Page 14]\nSTANFORDLAM14\nWikipedia is a good example of knowledge curation. \u2022Comprehensive\u2022Organized\u2022Reliable\u2022Verifiable\n\n[Page 15]\nSTANFORDLAM15Generating Wikipedia-like Articles is Challenging\nHard to verifyLack of details", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 14]\nSTANFORDLAM14\nWikipedia is a good example of knowledge curation. \u2022Comprehensive\u2022Organized\u2022Reliable\u2022Verifiable\n\n[Page 15]\nSTANFORDLAM15Generating Wikipedia-like Articles is Challenging\nHard to verifyLack of details\n\n[Page 16]\nSTANFORDLAM16Generating Wikipedia-like Articles is Challenging\nGenerating Wikipedia by Summarizing Long Sequences, Liu et al., 2018 Existing works usually assume the references are given.Collecting references requires literature research which is non-trivial.Given the ordered paragraphs \ud835\udc5d!!\"\".Encode, concatenate, and truncate:\nTrain an abstractive model \ud835\udc4a that learnsto write articles, \ud835\udc4e\"=\ud835\udc4a\ud835\udc5a\"#.\n\n\n[Page 17]\nSTANFORDLAM17\nGenerating Wikipedia-like Articles is Challenging\nWebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus, Qian et al., 2023 Existing works usually study generating a single paragraph.\n\n[Page 18]\nSTANFORDLAM18Lecture Plan1.Information Retrieval & RAG at a Glance 2.From Retrieving Information to Knowledge Curation 3.Building LM-Empowered Systems4.Evaluation? Evaluation! 5.Bring Human into the Loop \n\n[Page 19]\nSTANFORDLAM193. Building LM-Empowered Systems There is a paradigm shift in how to solve a task.LMTask DescriptionSpecific InputOutputOptimize Model Parameters(Continual Pre-training/Post-training, Fine-tuning)Optimize Input(Prompt Engineering)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 19]\nSTANFORDLAM193. Building LM-Empowered Systems There is a paradigm shift in how to solve a task.LMTask DescriptionSpecific InputOutputOptimize Model Parameters(Continual Pre-training/Post-training, Fine-tuning)Optimize Input(Prompt Engineering)\n\n[Page 20]\nSTANFORDLAM203. Building LM-Empowered Systems There is a paradigm shift in how to solve a task.LMTask DescriptionSpecific InputOutputOther SoftwaresLM-Empowered System\n\n[Page 21]\nSTANFORDLAM21The Future of Work\n30%70%\u201cI  (Wikipedia Editor) think it can be specifically helpful for my pre-writing stage.\u201dStrongly AgreeSomewhat AgreeShao, Yijia, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. \"Assisting in writing Wikipedia-like articles from scratch with large language models.\u201d,In NAACL 2024\n\n[Page 22]\nSTORM has aroused interest across various communities.https://github.com/stanford-oval/storm\n16K views\n41K viewsSince published in NAACL this year, our paper has already led to multiple follow-up works:\u2022Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature\u2022Eliciting Problem Specifications via Large Language Models\u2022ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents\u2022Filling Gaps in Wikipedia: Leveraging Data-to-Text Generation to Improve Encyclopedic Coverage of Underrepresented Groups\u2022Modeling and Enhancing Human Knowledge Navigation\u2022\u2026\u2026", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 23]\nSTANFORDLAM23How can STORM generate grounded articles with good breadth and depth?Key Idea: Mimic Human Writing ProcessHow do humans write?\u2022Rohman, 1965: Pre-Writing the Stage of Discovery in the Writing ProcessHow do humans do literature search?\u2022Booth et al., 2003: The Craft of Research \u201cChapter \u2161: Asking Questions, Finding Answers\u201d\n\n[Page 24]\nSTANFORDLAM24Topic \ud835\udc95Full-length Article \ud835\udcae\n\n[Page 25]\nSTANFORDLAM25Introducing the Pre-writing ChallengeTopic \ud835\udc95References \u211bOutline \ud835\udcaaFull-length Article \ud835\udcaePre-writing\nThe pre-writing task:Give a topic \ud835\udc61, the pre-writing task is to find a set of references \u211b, and create an outline \ud835\udcaa, which is defined as a list of multi-level section headings, to organize \u211b.\n\n[Page 26]\nSTANFORDLAM26Topic \ud835\udc95References \u211bOutline \ud835\udcaaFull-length Article \ud835\udcaePre-writing\nThe pre-writing task:Give a topic \ud835\udc61, the pre-writing task is to find a set of references \u211b, and create an outline \ud835\udcaa, which is defined as a list of multi-level section headings, to organize \u211b.Multi-hop SearchConcept InductionIntroducing the Pre-writing Challenge\n\n[Page 27]\nSTANFORDLAM27Our Idea: Literature Research via Question AskingDirect prompting results in questions that lack breadth and depth.We can\u2019t simply rely on \u201cbrute force\u201d or  inference-time scaling.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 27]\nSTANFORDLAM27Our Idea: Literature Research via Question AskingDirect prompting results in questions that lack breadth and depth.We can\u2019t simply rely on \u201cbrute force\u201d or  inference-time scaling.\n\n\n[Page 28]\nSTANFORDLAM28Perspective-Guided Question AskingSTORM uses perspective as a latent variable to control the breadth of the search.\nRelated ArticlesSustainability\u2460 Survey Related Topics\u2461 Identify Perspectives\n\ud835\udcabE.g., wiki/Sustainable_development;wiki/Corporate_social_responsibilityE.g., Social scientist \u2013 This editor will bring in a social perspective, focusing on topics such as social sustainability, \u2026\n\n[Page 29]\nSTANFORDLAM29Simulating Conversations to Allow Follow-up QuestionsSome in-depth questions arise only after reading the information gathered in previous rounds.Q: Can you provide me with a list of the participating countries in the 2022 Winter Olympics opening ceremony?\nQ: How is the order of participating countries in the 2022 Winter Olympics opening ceremony determined?A: The 2022 Winter Olympics featured a diverse group of countries participating in the opening ceremony. These included \u2026 Athletes from over 90 countries will enter the stadium in a specific order.QuestionAnswer\u2462 Read & AskExpert\u2463 Split Queries\u2464 Search & Filter\u2465 Synthesize\nWikipedia Writer\n\n\n[Page 30]\nSTANFORDLAM30Live demo!\nhttp://storm.genie.stanford.edu", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 30]\nSTANFORDLAM30Live demo!\nhttp://storm.genie.stanford.edu\n\n[Page 31]\nSTANFORDLAM31Lecture Plan1.Information Retrieval & RAG at a Glance 2.From Retrieving Information to Knowledge Curation 3.Building LM-Empowered Systems4.Evaluation? Evaluation! 5.Bring Human into the Loop \n\n[Page 32]\nSTANFORDLAM324. Conducting Meaningful EvaluationQuestion: What to evaluate? \u2026. And how?Do we have ground truth / golden answer? Besides final article, what else to evaluate?\n\n[Page 33]\nSTANFORDLAM33Intruduce outline coverage metrics as a proxy of the pre-writing stage quality for fast prototyping.Given a human-written Wikipedia article on topic \ud835\udc61,\u2022Heading soft recallCompare the sentence-BERT embeddings of headings in \ud835\udcaa and the human-written article.\u2022Heading entity recallThe percentage of named entities in the human-written article headings covered by \ud835\udcaa.Topic \ud835\udc95References \u211bOutline \ud835\udcaaFull-length Article \ud835\udcaePre-writing\nAutomatic Evaluation \u2013 Outline Quality\n\n[Page 34]\nSTANFORDLAM34Outline Quality\n8788899091929394\nDirect GenRAGRAG-expandSTORMHeading Soft Recall\n3436384042444648\nDirect GenRAGRAG-expandSTORMHeading Entity Recall\n\n[Page 35]\nSTANFORDLAM35Ablation studies!Takeaway: Ablation study help us to understand how different parts of a system contribute to its overall performance", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 35]\nSTANFORDLAM35Ablation studies!Takeaway: Ablation study help us to understand how different parts of a system contribute to its overall performance\n\n[Page 36]\nSTANFORDLAM36Outline Quality\n8788899091929394\nDirect GenRAGRAG-expandSTORMw/o Perspectivew/o ConversationHeading Soft Recall\n3436384042444648\nDirect GenRAGRAG-expandSTORMw/o Perspectivew/o ConversationHeading Entity Recall\n\n[Page 37]\nSTANFORDLAM37Automatic Evaluation \u2013 Article QualitySTORM outperforms baselines across ALL automatic metrics.\n\n\n[Page 38]\nSTANFORDLAM38Human Evaluation - Wikipedia Editor EvaluationExperienced Wikipedia editors favor articles produced by STORM.\u22654 Rate(1-7 Scale)Interest LevelOrganizationRelevanceCoverageVerifiabilityoRAG57.5%45.0%62.5%57.5%67.5%STORM70.0%70.0%65.0%67.5%67.5%Careful human evaluation is necessary to evaluate LM-empowered systems.\n\n[Page 39]\nSTANFORDLAM39In the wild Evaluation\n75,000+ Users\n114,000+ Articles\n211,000+ Browsing\n20,00\n0\n+ Feedbacks\n\n[Page 40]\nSTANFORDLAM40In the wild Evaluation\nWhy do people use STORM?\n\n[Page 41]\nSTANFORDLAM41In-the-wild Evaluation\n41People have used STORM across a diverse array of topics.\n\n\n[Page 42]\nSTANFORDLAM42Revisit Our Meta Question:Are people\u2019s information needs satisfied?\nThe illustration is co-created with DALL-E.\n\n[Page 43]\nSTANFORDLAM43\nMy thoughts evolve, so I want to update my queries.I am inspired by this link and hope to learn more about it.I know this topic is also relevant. Can you include it?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 43]\nSTANFORDLAM43\nMy thoughts evolve, so I want to update my queries.I am inspired by this link and hope to learn more about it.I know this topic is also relevant. Can you include it?\n\n[Page 44]\nSTANFORDLAM44Lecture Plan1.Information Retrieval & RAG at a Glance 2.From Retrieving Information to Knowledge Curation 3.Building LM-Empowered Systems4.Evaluation? Evaluation! 5.Bring Human into the Loop \n\n[Page 45]\nSTANFORDLAM45\n5. How to bring a human user into the loop?\n\n[Page 46]\nSTANFORDLAM46x x x[1]\nAfter the long report is generated, allow the user to edit or ask questions.\nUser Question 1\nUser Question 2\nUser Question nConvert STORM into a hallucination-free question answering system.\n\n[Page 47]\nSTANFORDLAM47\nUser Question 1\nUser Question 2\nUser Question nConvert STORM into a hallucination-free question answering system.x x x[1]\nAfter the long report is generated, allow the user to edit or ask questions.User-Initiative(Baseline 1: RAG Chatbot)System-Initiative(Baseline 2: STORM+QA)\n\n[Page 48]\nSTANFORDLAM48Mix-Initiative Systems\u2022Considering uncertainty about a user\u2019s goals.\u2022Considering the status of a user\u2019s attention in the timing of services.\u2022Allowing efficient direct invocation and termination.\u2022Providing mechanisms for efficient agent-user collaboration to refine results.\u2022Maintaining working memory of recent interactions.\nPrinciples of Mixed-Initiative User Interfaces, Eric Horvitz, 1999", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 49]\nCollaborative-STORM\nJiang, Yucheng, Yijia Shao, Dekun Ma, Sina J. Semnani, and Monica S. Lam. \"Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations.\u201d, In EMNLP 2024\n\n[Page 50]\nSTANFORDLAM50How to surface unknown unknowns?Key Idea: Let Users Observe & Involve in Expert DiscussionHow do children / students learn?\u2022Nussbaum, 2008: Collaborative discourse and collaborative argumentation is important for promoting students\u2019 deep-level understanding of contents.How do humans retain information?\u2022Buzan, 1974: Using mind map for note taking to help recall and critical thinking.\n\n[Page 51]\nSTANFORDLAM51Collaborative Discourse ProtocolGoal: Co-STORM allows users to learn by observing and participating occasionally in the discourse, emulating a common educational scenario.-Simulates agent grounded in the search engine, answering and asking questions.-The user can jump in at any time to steer the discourse and inject questions and opinions.-Maintains a dynamic, hierarchical mind map so users can easily follow and engage.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 52]\nSTANFORDLAM52-Question Asking-Posing an original question-making an information requestHowever, the agent almost always choose question answering, causing the conversation to focus on a narrow topic, which can result in overly niche content.Question: What types of utterance intents an LM agent could have?-Question Answering-Providing a potential answer-elaborating with further detailsChallenge: How to invoke thought-provoking questions?\n\n[Page 53]\nSTANFORDLAM53Solution: Ask thought provoking questions via Moderator roleRecall: How do we ask follow up question during search?Sometimes, we may find results not directly related to the search query but relevant to the topic we are interested particularly interesting.\u2022For example, when we search for \u201cimproving software engineering practices\u201d, we might stumble upon an article about \u201cthe cognitive psychology behind team decision-making\u201d.\nDiscourse HistoryRerank Unused informationGenerate QuestionPolish UtteranceMind MapModerator pipeline\n\n[Page 54]\nSTANFORDLAM54Conducting Meaningful EvaluationQuestion: What to evaluate? \u2026. And how?Do we have ground truth / golden answer? Besides final article, what else to evaluate?\n\n[Page 55]\nSTANFORDLAM55\nHaving just one expert and one moderator can already provide most of the benefitsSolution: Ask thought provoking questions via Moderator role\n\n[Page 56]\nSTANFORDLAM56Automatic Evaluation \u2013 Final Report Quality\n\n\n[Page 57]\nSTANFORDLAM57Automatic Evaluation\u2013 Discourse Quality", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 55]\nSTANFORDLAM55\nHaving just one expert and one moderator can already provide most of the benefitsSolution: Ask thought provoking questions via Moderator role\n\n[Page 56]\nSTANFORDLAM56Automatic Evaluation \u2013 Final Report Quality\n\n\n[Page 57]\nSTANFORDLAM57Automatic Evaluation\u2013 Discourse Quality\n\n\n[Page 58]\nSTANFORDLAM58\nHuman Evaluation\n\n[Page 59]\nSTANFORDLAM59\nCo-STORM allows for almost full automation and much better understanding as it brings up topics that the user may not even think of.\u201cCo-STORM is so much less mentally taxing for me to use\u201dHuman Evaluation\n\n[Page 60]\nSTANFORDLAM60Takeaways\u2022Build LM-empowered systems.\u2022An emerging paradigm in the era of foundation models.\u2022Crafting LM pipelines resembles how we observe human workflows.\u2022STORM resembles how human write.\u2022Co-STORM resembles collaborative discourse in education.\u2022Conduct user study in addition to automatic evaluation.\u2022STORM invites 20 Wikipedia editors during paper writing. \u2022STORM is then deployed in the wild, tested by over 50,000 users.\u2022Co-STORM invites 20 users in the wild.\u2022Co-STORM will be deployed in the wild as well!\n\n[Page 61]\nSTANFORDLAM61Questions?Feel free to reach out to shaoyj@stanford.edu, yuchengj@stanford.edu for more questions/thoughts.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Knowlege Curation", "content": "[Page 61]\nSTANFORDLAM61Questions?Feel free to reach out to shaoyj@stanford.edu, yuchengj@stanford.edu for more questions/thoughts.\n\n[Page 62]\nSTANFORDLAM62Assignment (Due Oct 2nd)\u2022Use: Play with live research preview!   You will have chance to play STORM and Co-STORM as a user\u2022Know: Technical deep dive     Dive deeper into core design choices and LLM system design\u2022Inspire: Ideas to build better knowledge curation system Analyze strength and weakness of current work; think about what can be improve; what more use cases can be supported", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-Knowlege_Curation.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:13.180638"}
{"title": "L Introduction", "content": "[Page 1]\nCS224v\nConversational Virtual Assistants \nwith Deep Learning\nLecture 1: Introduction\n1Monica Lam\n\n[Page 2]\nSTANFORD LAMOutline\n1.A Knowledge Revolution in the Making\n2.Foundation for the Knowledge Revolution\n3.This CS 224V Course\n\n[Page 3]\n1 . A  R E V O LU T IO N INT HE MA KIN G\n\n[Page 4]\nSTANFORD LAMIndustrial Revolution (1760 -1840)\nCourtesy of Schoolshistory.org.uk\n\n[Page 5]\nSTANFORD LAMThe Rise of \u201cKnowledge Work\u201d\n\u201cAknowledge worker is someone whose job \nrequires them to think for a living.\u201dIn Landmarks of Tomorrow, Peter Drucker, 1959\n\n\n[Page 6]\nSTANFORD LAM\n Courtesy of Arithmomuseum.com , computerhistory.org , novolo , thegradient.pub , Vaswani, Ashish, et al. \u201cAttention is all you need\u201d, NIPS 2017\nCalculators\nComputersDeep Learning Large Language Models (LLMs)\ne.g. GPT -4Tools of Knowledge Workers\nInternet Search\n\n[Page 7]\nSTANFORD LAMWhy are Large Language Models (LLMs) Significant? \nA lot of the world\u2019s knowledge is \nin the Written Word\nLLMs will lead to a \nRevolution in Knowledge Work\n\n[Page 8]\nSTANFORD LAMChatGPT : Introduced Nov 2022 \nSmallest Number of Months to 100M Users\n8\nSource: Sequoia100M\n75M\n50M\n25M\n0MChatGPT\n2TikTok\n9Instagram\n30Pinterest\n41Spotify\n55Telegram\n61\nMonths from launch", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 8]\nSTANFORD LAMChatGPT : Introduced Nov 2022 \nSmallest Number of Months to 100M Users\n8\nSource: Sequoia100M\n75M\n50M\n25M\n0MChatGPT\n2TikTok\n9Instagram\n30Pinterest\n41Spotify\n55Telegram\n61\nMonths from launch\n\n[Page 9]\nSTANFORD LAMAre We There Yet? \nPass! ChatGPT : US Medical Licensing Exam\nhttps:// journals.plos.org /digitalhealth /article?id =10.1371/journal.pdig.0000198GPT-4: Uniform Bar Examination Pass!\nhttps:// papers.ssrn.com /sol3/ papers.cfm?abstract_id =4389233\nCan we provide affordable medical and legal services? \n\n[Page 10]\nSTANFORD LAMMeta Galactica\nAssistant for scientific articles\nWithdrawn after 1 day!\n\n[Page 11]\nSTANFORD LAMExamples of GPT -3 Answers\nQuestion: \u201cWhat is the biggest country in Europe by population?\u201d\nGPT -3:    \u201cGermany\u201d\nAnswer:   \u201cRussia\u201d\nQuestion: \u201cwhere does the name Melbourne come from?\u201d \nGPT -3:    \u201cMelbourne comes from the Latin word \u2018 melburnum \u2019 \nmeaning \u2018 blackburn \u2019 or \u2018blackbird\u2019 \u201d\nAnswer:   \u201cMelbourne is named after William Lamb, \n2nd Viscount Melbourne\u201d\nUsers must fact check all the answers!\n\n[Page 12]\nSTANFORD LAMChatGPT (7/2/2023)\n+ Conversational\n-Hallucinates on long -tail info\nWith details and conviction\n\n\n[Page 13]\nGenerative AI = HallucinationCan We Just Train a bigger/better LLM \nto eliminate hallucination? \n\n[Page 14]\nLLMs can bring about a Knowledge Revolution\nBut not yet!\nBe careful with what you do with it!1.A R EVOLUTION INTHEMAKING\n\n[Page 15]\n2 . F O U ND A T ION F O R T HE KN OW L E D GE RE V O LU T IO N", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 14]\nLLMs can bring about a Knowledge Revolution\nBut not yet!\nBe careful with what you do with it!1.A R EVOLUTION INTHEMAKING\n\n[Page 15]\n2 . F O U ND A T ION F O R T HE KN OW L E D GE RE V O LU T IO N\n\n[Page 16]\nElevate LLMs from advisory tools \nto essential, reliable services.\nAutomate routine.\nElevate expertise.\n16\n\n[Page 17]\nSTANFORD LAMWe All Have an \u201cLLM\u201d in Our Brain!\n\u2022Our \u201cLLM\u201d: Speech area\n\u2022Prefrontal cortex\n\u2022Inhibition and attention\n\u2022Work with long -term memory\n\u2022Planning\n\u2022Monitoring external signals\n\u2022Damaged prefrontal cortex \u2192\n\u201cSpeak without thinking\u201d\nRay Mueller: suppor ting\nour coastal communities\nWho funded this ad?\nBroca area | De\ufb01nition, F unction, &  Facts | Britannica https://www.britannica.com/science/Broca-area\n1 of 1 10/13/22, 1 1:26 A Mprefrontal cortex\n\n[Page 18]\nLarge -Language Model (LLM)\nis a \u201cspeech center\u201d\n\u2014natural language skills\n18This Course\nadds the \u201cprefrontal cortex\u201d\n(executive control)\n\u2014cognitive (knowledge) skills \nRay Mueller: suppor ting\nour coastal communities\nWho funded this ad?\nBroca area | De\ufb01nition, F unction, &  Facts | Britannica https://www.britannica.com/science/Broca-area\n1 of 1 10/13/22, 1 1:26 A Mprefrontal cortex", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 19]\nSTANFORD LAMLearning NL vs. Cognitive Skills\n\u2022Humans learn NL through repetitive (neural) training \n(5 year olds) \n\u2022Humans learn general cognitive skills from teachers \n(20 year olds)\n\u2022Can handle different tasks without millions of  examples\ne.g. a call agent for different companies\n\u2022How do humans teach cognitive skills? \n\n[Page 20]\nSTANFORD LAM\nBloom\u2019s Taxonomy \nEducation Objectives in Cognition Domain\nExplanations of Objectives\nLLMs are good at \u201cremember\u201d, and not reliable otherwise\n\n[Page 21]\nSTANFORD LAM\nDifferent Skills \u2192Different jobs\nAdvisor, Employee TrainingResearchers, Investigative journalists\nData scientists, Journalists, Recruiters\nCustomer support, SalesCompliance officers, auditors, judges\nReadingInstruction \nfollowingKB retrieval \nLiterature reviewFormal reasoningQualitatitive\ncodingJobs Skills\n\n[Page 22]\nSTANFORD LAMHow To Teach LLMs Cognitive Skills?\nEmulate what humans do step by step algorithmically\n\n[Page 23]\nSTANFORD LAMLesson 1: Reading \n\u2022RAG: Retrieval augmented generation\n\u2022Commercial example: Bing Chat\n\u2022Problems\n1.Only 58.7% of the facts are grounded in retrieved info*\n\u2022GPT-4 hallucinates \nwhen retrieved info does not answer the question\n2.Answers are dry, not conversational\n*Nelson F. Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines. ArXiv:2304.09848. 2023.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 24]\nSTANFORD LAMExample: How to Look Up Info Today?\n6.Draft\n7.Refine3.Ask GPT\n4.Dissect into claims\n5.Fact -check each claim\n\u2022Search internet with claim\n\u2022Filter incorrect claim1.Search internet with query \n2.Filter irrelevant info\nUse multiple, easy LLM steps to consult external data", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 25]\nSTANFORD LAMWhat do you think about the \ncasting of Oppenheimer?Generate and Verify\nRetrieve ->1. Summarize, 2. Filter\n\u2022Much of the Oppenheimer \n(2023) movie\u2019s cast signed on \nbetween September 2021 and \nApril 2022.\n\u2022Cilian Murphy is to play as \nRobert Oppenheimer in the \nOppenheimer (2023) movie.4. Extract claims:\n\u2022Christopher Nolan is known for his meticulous casting choices.\n\u2022Christopher Nolan often casts well -known actors in his films.\n\u2022Tom Hanks stars in the Oppenheimer (2023) movie.\n\u2022Michael Caine stars in the Oppenheimer (2023) movie.\n5. Filter incorrect claims:\n\u2022Christopher Nolan is known for his meticulous casting choices.\n\u2022Christopher Nolan often casts well -known actors in his films.3. GPT: Christopher Nolan is known for his meticulous casting \nchoices. He often casts well -known actors in his films , and this \ntime he has selected Tom Hanks and \u2026 Michael Caine .\n6. Draft: Cillian Murphy has been cast to play Robert Oppenheimer in Christopher Nolan's \n\"Oppenheimer\" movie. Christopher Nolan is known for his meticulous casting choices and often casts \nwell -known actors in his films. The movie's cast was signed on between September 2021 and April 2022.Answer\n7. Refine: Christopher Nolan has cast Cillian Murphy as the titular role of Oppenheimer , keeping with \nhis tradition of meticulous casting choices . I\u2019m sure it will be an excellent film!", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 26]\nSTANFORD LAMInternalizing a Knowledge Skill\nImage source: Inc.com1.Think slowly\n2.Many Practices\n3.Play quicklySlow,\nmultiple calls\nto LLMsInput output\nFast LLMFine-tune\u2295\nProcedure to learn harder and harder skills\nOn smaller, local LLMs for confidentiality & efficiency\n\n[Page 27]\nSTANFORD LAM10+ Calls to GPT -4!\n\u2022Distill GPT -4 to LLaMA (7b parameters)\n\u2022Expt: WikiChat \u2013based on Wikipedia\nModel Time\nWikiChat with GPT -4 26.6s\nWikiChat with LLaMA (7B parameters) 7.6s\nWikiChat with LLaMA behaves like WikiChat with GPT -3.5 \n(6% lower)\n\n[Page 28]\nSTANFORD LAMEvaluation with Real Users\nUser study: User reads the first sentence of a new Wikipedia page\n\u2022GPT-4: Users not aware that over half of the statements are falseModel User Rating (out of 5) Factuality\nGPT-4 3.4 42.9%\nWikiChat using GPT -4 3.8 97.9%\n\n[Page 29]\nSTANFORD LAM\n\n\n[Page 30]\nSTANFORD LAMLive Demo : https:// wikichat.genie.stanford.edu /\nSilei Xu et al. Fine -tuned LLMs Know More, Hallucinate Less with Few -Shot Sequence -to-Sequence Semantic Parsing over Wikidata .  EMNLP 2023 Semnani , Sina et al. WikiChat : Stopping the Hallucination of Large Language Model Chatbots by Few -Shot Grounding on Wikipedia, EMNLP Findings 2023", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 31]\nSTANFORD LAMSummary: Learning How to Read\n\u2022Exploit LLM capabilities\n\u2022Generative power, general understanding of the world\n\u2022NLP skills: summary, filter claim splitting, entailment\n\u2022But limit use of LLMs:\n\u2022Fact -check LLM -result claim by claim\n\u2022Short paragraphs only \n\u2022Do not let it \u201canswer questions\u201d directly, or it will hallucinate\n\u2022The devil is in the details: \nE.g. LLMs do not understand time  (3 students, 3 months)\nIt is worth it!  Reading without hallucination is key to knowledge processing\n\n[Page 32]\nSTANFORD LAMAgent Architecture\n\u2022Executive control: \nA software program that\nimplements the Reading\nPipeline\n\u2022Calls LLM as NL subroutines\n\u2022Interfaces to external \nsystem components\ne.g. WikipediaThis Course\nadds the \u201cprefrontal cortex\u201d\n(executive control)\n\u2014cognitive (knowledge) skills \nExecutive Control\nLLM\n\n\n[Page 33]\nBloom\u2019s taxonomy: a hierarchy of objectives in the cognitive domain\nLLM is analogous to our speech center, lacks executive control\nExecutive control: a software program to implement a cognitive skill\n(controls the LLM as a subroutine)2. F OUNDATION FOR THEKNOWLED GE REVOLUTION\n\n[Page 34]\n3. CS224V  C OURSE", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 34]\n3. CS224V  C OURSE\n\n[Page 35]\nSTANFORD LAMDesign of CS224V\n\u2022Focus: Tools for General Cognitive Skills\nTo make building reliable assistants easily by non AI -experts\n\u2022Approach: Application -driven research\n1.Create a new LLM -based cognitive skill \nby modeling human processes\n\u2022Implement as a tool\n\u2022Experiment with an initial application\n2.Improve the tool by applying it to many different apps\n\n[Page 36]\nSTANFORD LAMCourse Objectives\nLearn and advance the state of the art (SOTA)\n1.2 Homeworks to bring everybody onboard with SOTA tools\n2.Lectures on techniques of LLM -based conversational agents\n3.Supervised quarter -long project\n\u2022Develop new tools; enhanced existing tools (with app)\n\u2022Create a new app and evaluate\n\n[Page 37]\nSTANFORD LAMCS224V in 2023\n\u2022WikiChat tool was available to the class \n\u2022Homework in week1: a non -hallucinating chatbot\n\u2022Result of the course\n1.2 publications\n2.Ground work for 3 other publications\n3.Live demos on https:// oval.cs.Stanford.eduExecutive Control\nLLM\n\n\n[Page 38]\nQuestion answering\nCollaborative report writing\nCross -referenced browsing\nConversational AgentTools Available for CS224V in 2024\n38Corpus\nDocuments\nKnowledge Bases\nGenie Worksheet\nSUQLSUQL : \nThe first query language for \nstructured & unstructured data.\nGenie Worksheet : \nThe first specification language \nfor task and knowledge bots.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 39]\nSTANFORD LAM\nAvailable Skills in the Bloom Taxonomy\nAdvisor, Employee TrainingResearchers, Investigative journalists\nData scientists, Journalists, Recruiters\nCustomer support, SalesCompliance officers, auditors, judges\nReadingConversational \nAgentKB retrieval \nLiterature reviewFormal reasoningQualitatitive\ncodingJobs Skills\n\n[Page 40]\nSTANFORD LAMHomework 1: Automatic Data Curation\nGoing beyond Q & A ...\n\u2022Storm: Writes a Wikipedia -like article from scratch\n\u2022By researching the internet\n\u2022Co-Storm: \n\u2022Add collaboration to Storm\n\u2022Your homework: Use co -Storm on ArXiv\n\u2022Learn how to work with LLM -based pipelines \n\u2022Discover issues in working with specialized domains\n\n[Page 41]\nSTANFORD LAMRelease of \n41\n\u2022Released 7/10/2024\n\u2022Without self promotion\n\u2022Organic YouTube videos\n\u2022First month traffic \n\u202227K users\n\u202250K articles\n\u2022General\n\u2022Applicable to any domain\n\u201cPerplexity Pages alternative but it is insanely better.\u201d\nDemo at: https:// oval.cs.Stanford.edu", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 41]\nSTANFORD LAMRelease of \n41\n\u2022Released 7/10/2024\n\u2022Without self promotion\n\u2022Organic YouTube videos\n\u2022First month traffic \n\u202227K users\n\u202250K articles\n\u2022General\n\u2022Applicable to any domain\n\u201cPerplexity Pages alternative but it is insanely better.\u201d\nDemo at: https:// oval.cs.Stanford.edu\n\n[Page 42]\nSTANFORD LAMHomework 2: Create an Agent\n\u2022The classical use of NLP is a conversational agent\n\u2022The Genie Worksheet lets you create a powerful agent with a small number of lines of code\n\u2022Task-oriented:              e.g. enroll in courses\n\u2022Knowledge -oriented: e.g. answer questions about courses\n\u2022Homework2: \n\u2022OfCourse is a course advisor prototype for the AI track of CS MS\n\u2022Extend OfCourse to cover other programs in CS MS\n\u2022Purpose: Exposure to tools for developers and learn their strengths/weaknesses in real life\n\u2022Note: The practice is very different from typical NLP papers (80% accuracy is not usable) \n\n[Page 43]\nSTANFORD LAM\nAI\n\n[Page 44]\nSTANFORD LAM\nComputational\nBiology\n\n[Page 45]\nSTANFORD LAM\nComputer and\nNetwork Security\n\n[Page 46]\nSTANFORD LAM\nSoftware\nTheory\n\n[Page 47]\nSTANFORD LAM\nSystems\n\n[Page 48]\nSTANFORD LAM\nTheory\n\n[Page 49]\nSTANFORD LAM 49\nI am a sophomore studying computer science. \nI want to complete the significant \nimplementation requirement.\nWhat are some of the options that require the \nleast amount of work?\nWhat course would you like to enroll in?Course Enrollment Agent\nUser:", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 49]\nSTANFORD LAM 49\nI am a sophomore studying computer science. \nI want to complete the significant \nimplementation requirement.\nWhat are some of the options that require the \nleast amount of work?\nWhat course would you like to enroll in?Course Enrollment Agent\nUser: \n\n[Page 50]\nSTANFORD LAMA Course Enrollment Assistant\nStudents need to consult: \n\u2022All offered courses\n\u2022Description, instructors, \nofferings, and units\n\u2022Unique to each \ndepartment and \nspecialization\n\u2022Program sheets\u2022Ratings, reviews, hours \nof work, enrollment \noutcomes, \nand sequencing\nWhat course would you like to enroll in?Course Enrollment Agent\n\n[Page 51]\nSTANFORD LAM\n\n\n[Page 52]\nSTANFORD LAMHow to Create an Agent\nDevelopers provide:\n1.A worksheet specifying the needed info \nand conditions to satisfy\n2.Knowledge bases containing relevant information", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 51]\nSTANFORD LAM\n\n\n[Page 52]\nSTANFORD LAMHow to Create an Agent\nDevelopers provide:\n1.A worksheet specifying the needed info \nand conditions to satisfy\n2.Knowledge bases containing relevant information\n\n[Page 53]\nSTANFORD LAM1. Stanford Course Enrollment Form\nForm Name Predicate Kind Type Name Enum Values Description\nMain worksheet course_enrollment\ninput CourseToTake course_to_take The course to enroll\ninput StudentInfo student_info_details Information on the student\nStudentInfo worksheet\ninput str student_name Name of the student\ninput str student_id Student's ID number\ninput str student_email_address Student's email address\nCourseToTake worksheet\ninput str course_name Name of the course \ninput Enum grade_type The desired grading basis \nCredit/No Credit\nLetter\ninput int course_num_units The number of units taken\ninput confirm confirm Confirm the course", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 54]\nSTANFORD LAM2. Course Assistant Knowledge Corpus\ncourses db\ninternal; primary int course_id\ninternal int max_units\ninternal str title\ninternal str grading\ninternal List[str] course_codes\ninternal List[str] general_requirements\ninternal int min_units\ninternal str description\ninternal int average_hours_spent\ninternal Enum foundations_requirements\nlogic_automata_and_complexity\nprobability\nalgorithmic_analysis\ncomputer_organization_and_systems\nprinciples_of_computer_systems\ninternal bool significant_implementation_requirements\ninternal Enum breadth_requirement\nformal_foundations\nlearning_and_modeling\nsystems\npeople_and_society\ninternal List[str] prerequisite_course_codesofferings db\ninternal int course_id\ninternal Enum days\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\ninternal str start_time\ninternal str end_time\ninternal List[str] instructor_names\ninternal Enum season\nautumn\nwinter\nspring\nsummerprograms db\ninternal; primary int program_id\ninternal Enum level\nMS\nBS\nPhD\ninternal Enum specialization\nAI\nComputational Biology\nHuman -Computer Interaction\nInformation Management & Analytics\nVisual Computing\nSoftware Theory\nSystems\nTheoretical Computer Science\ngeneral\nComputer & Network Security\ninternal str sheet_requirementsratings db\ninternal; primary int rating_id\ninternal int course_id\ninternal List[str] instructor_names\ninternal int average_rating\ninternal int num_ratings\ninternal int term_id\ninternal int start_year\ninternal int end_year\ninternal Enum season", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "internal str sheet_requirementsratings db\ninternal; primary int rating_id\ninternal int course_id\ninternal List[str] instructor_names\ninternal int average_rating\ninternal int num_ratings\ninternal int term_id\ninternal int start_year\ninternal int end_year\ninternal Enum season\nautumn\nwinter\nspring\nsummer\ninternal List[str] reviews", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 55]\nSTANFORD LAMTranslates Questions intoSUQL\nI am a sophomore studying computer science. I \nwant to complete the significant \nimplementation requirement.\nWhat are some of the options that require the \nleast amount of work?\nSELECT title, course_codes , description, \naverage_hours_spent\nFROM courses WHERE \nsignificant_implementation_requirement = TRUE \nORDER BY average_hours_spent LIMIT 5;\n\n\n[Page 56]\nSTANFORD LAMExecuting a Worksheet\nExecutive Control Software\n\u2022Interpret & update Genie Worksheet\n\u2022Provide turn -specific instruction to LLM \n(Succinct context: Worksheet state + 1 turn of dialogue)\nGenie Genie WorkSheet\nConversational Tasks\nHybrid Knowledge CorpusExecutive Control\nLLM\n\n[Page 57]\nSTANFORD LAM\n\n\n[Page 58]\nSTANFORD LAM\nConsideration: Time Offered\n\n[Page 59]\nSTANFORD LAM\nConsideration: Sort by Course Rating\n\n[Page 60]\nSTANFORD LAMCredit or Letter?  Depends on the Workload\n\n\n[Page 61]\nSTANFORD LAM\nPick Max. Number of Units\n\n[Page 62]\nSTANFORD LAMPurpose of the Assignments\n\u2022Prepare you for your project proposal\n\u2022How to use LLMs to curate data from the internet\n\u2022How to create a basic task & knowledge bot as \na developer\n\u2022Lectures on underlying technology and advanced \ntopics to follow", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 62]\nSTANFORD LAMPurpose of the Assignments\n\u2022Prepare you for your project proposal\n\u2022How to use LLMs to curate data from the internet\n\u2022How to create a basic task & knowledge bot as \na developer\n\u2022Lectures on underlying technology and advanced \ntopics to follow\n\n[Page 63]\nSTANFORD LAMProject Apprenticeship\n\u2022Assistance with project selection: Hardest part in research!\n\u2022We suggest over 20 projects on the website\n\u2022Some with domain experts in journalism, history, \nmedicine, finance, gaming, etc.\n\u2022Student -initiated projects are also welcome\n\u2022Weekly group mentorship meeting\n\u2022We want to make you succeed!\n\n[Page 64]\nSTANFORD LAMProject Mentorship\nAll homeworks and projects are done in groups of 2\n\u2022Week 4: Project proposal, with a weekly plan \n\u2022Weeks 5 -10 (excluding Thanksgiving break): \n\u2022Submit a written weekend update (every Monday)\n\u2022Group meeting with mentors during the week\n\u2022Week 11: Poster presentation (Dec 4)\n\u2022Final project report due Dec 10, 2024.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Introduction", "content": "[Page 65]\nSTANFORD LAMCourse Schedule at a Glance\nDates Lectures / Homeworks Projects\n9/23 -10/ 7Introduction; \nKnowledge curation (HW1)\nBuilding a task -oriented agent (HW2)\nGrounding on small databasesResearch Project Ideas\n10/ 9 -10/21Student -initiated ideas\nProject discussions\nProject proposals (2)\n10/21 -11/20Grounding on free text \nSUQL; Task -oriented agent generation\nKnowledge graph queries; Knowledge discovery\nFormal reasoning; Multimodal apps\nNLP building blocks; Training LLMs\n11/25 -11/27 Thanksgiving\n12/  2 -12/4No Class Final project posters \n(3:00 -5:40)\n\n[Page 66]\nSTANFORD LAMThis Course\nGrade\nParticipation 15%\nAssignment 25%\nFinal Project 60%Participation includes\n\u2022Class attendance and participation\n\u2022Ed discussion\n\u2022Meetings with project mentors", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:18.041589"}
{"title": "L Hybrid", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 7\nIntroduction to Agents for \nStructured and Free -Text Data\n1Monica Lam & Shicheng Liu\n\n[Page 2]\nSTANFORD LAMMotivation with an Example\n\u2022How do you find a restaurant on Yelp?\n2\n\n[Page 3]\nSTANFORD LAMYelp: DB + Free text\n\u2022Blue columns contain structured data\n\u2022Yellow columns are free text\n\u2022A lot of rows (thousands)\n3A lot of info is in free -text\n\n[Page 4]\nSTANFORD LAMDB + Free -Text Combo Example\nI need a family -friendly restaurant\n\u2192\u201cfamily -friendly\u201d in reviews\nI need a family -friendly restaurant in Palo Alto\n\u2192\u201cfamily -friendly\u201d in reviews; Palo Alto in DB\n4\n\n[Page 5]\nSTANFORD LAMWe Need Hybrid Data\n5Structured Free Text Tasks\nRestaurantsCuisines, opening hours, \naddress, ratingReviews, popular \ndishesBook restaurant\nProductsProduct ID, cost, ratings, \nsizes, physical \ndimensions, colorDescriptions, \nreviewsPurchasing\nOrdering food\nCoursesClass number, time \noffered, instructor, \nbuilding, pre -requisitesDescriptions, \nreviewsEnrolling in courses\nBusiness \nprocessesReceipts, statements RegulationsFiling taxes\nReimbursements \nCustomer support\nMedicalPatient demographics\nPrescriptionDiagnosis historyMaking diagnoses\nInsurance claims\n\n[Page 6]\nThe Next Problem:\nHybrid Data QA\nCO M B IN ING\nKN OW L E D GE BA SE S & T E X T U AL QA\n6\n\n[Page 7]\nQuiz\nWH AT ISYO U R AP PR O AC H TOHA ND L E\nKN OW L E D GE BA SE S & T E X T?\n7", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Hybrid", "content": "[Page 6]\nThe Next Problem:\nHybrid Data QA\nCO M B IN ING\nKN OW L E D GE BA SE S & T E X T U AL QA\n6\n\n[Page 7]\nQuiz\nWH AT ISYO U R AP PR O AC H TOHA ND L E\nKN OW L E D GE BA SE S & T E X T?\n7\n\n[Page 8]\nSTANFORD LAMGoal of this Lecture\n\u2022To understand the Hybrid QA problem\n\u2022Discuss 4 major approaches, starting with the simplest\n\u2022HybridQA : dataset requiring composition of DB & Text retrieval\n1.Classify query to decide to retrieve from DB or Text \n2.Retrieve answers from DB and Text separately, choose answer\n3.Convert DB to text; use Text Retrieval\n4.Retrieve answers from DB and Text separately, \nuse LLM to combine the answers\n8\n\n[Page 9]\nSTANFORD LAM1. Classify the Question: KB or Text\n9\nhttps:// browse.arxiv.org /pdf/2305.12091.pdfPublished 10/3/2023\n\n[Page 10]\nSTANFORD LAMSK-TOD Dataset \nSubject -Knowledge Task-Oriented Dataset\n\u201cThe first dataset, which contains subjective knowledge -seeking \ndialogue contexts and manually annotated responses \ngrounded in subjective knowledge sources .\u201d\n\u2022MultiWOZ restaurant and hotel (in Cambridge area)\n\u2022User goal: search and book restaurants and hotels\n\u2022Structure: 13 slots (easier than SQL) \n\u2022Text: 143 entities and 1,430 reviews (8,013 sentences) \n10\n\n[Page 11]\nSTANFORD LAMLargest TOD Dataset with Subjective Knowledge\n11", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Hybrid", "content": "[Page 11]\nSTANFORD LAMLargest TOD Dataset with Subjective Knowledge\n11\n\n\n[Page 12]\nSTANFORD LAMBinary Classifier (IR vs Semantic Parsing)\n12\nResponse Generation\nUse pretrained \nGPT-2/Bart\nTurn detection: binary classificationEntity Tracking\nFuzzy n -gram matching \nbetween dialogue &entities. \n(A few entities in MultiWOZ )\nKnowledge Selection\nCalculate relevant knowledge \nscore between dialogue \ncontext & knowledge snippetsINFORMATION RETRIEVAL (IR)\n\n[Page 13]\nSTANFORD LAM2. Choose Later: Stanford Chirpy Cardinal\n\u2022Alexa Social Bot Prize Runner \nup, 2021\n\u2022Different modules to handle \ndifferent kinds of questions\n\u2022Run generators in parallel\n\u2022Choose at the end\n13\n Proceedings of the SIGdial 2022 Conference, pages 376 \u2013395\n\n[Page 14]\nQUIZ\nWHAT ISTHEWEAKN ESS OF\nCHOOSING JUST THEKB ORTEXT? \n\n[Page 15]\nSTANFORD LAMWhat if We Need Both to Answer the Questions?\n\u2022Is it common? Yes!\n\u2022Real -user queries about restaurants: 55/100 need a combo\n\u2022Separate modules cannot answer these questions\n15\n\n[Page 16]\nSTANFORD LAMGoal of this Lecture\n\u2022To understand the Hybrid QA problem\n\u2022Discuss 4 major approaches, starting with the simplest\n\u2022HybridQA : dataset requiring composition of DB & Text retrieval\n1.Classify query to decide to retrieve from DB or Text \n2.Retrieve answers from DB and Text separately, choose answer\n3.Convert DB to text; use Text Retrieval\n4.Retrieve answers from DB and Text separately, \nuse LLM to combine the answers\n16", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Hybrid", "content": "[Page 17]\nSTANFORD LAM3. Converting All to Text \n\u2022Unified Knowledge Question Answering ( UniK-QA) \n\u2022Converts everything to text\n\u2022Text\n\u2022Semi -structured data e.g. Wikipedia lists, tables and info \nboxes\n\u2022Knowledge base\n\u2022Then uses the usual pipeline designed for textual QA\n17Oguz et al. \" UniK -QA: Unified Representations of Structured and Unstructured Knowledge for Open -Domain Question Answering.\u201c\narXiv preprint arXiv:2012.14610 (2021).\n\n[Page 18]\nSTANFORD LAMWikipedia Tables / Wikidata\n\u2022Wikipedia tables are linearized simply by concatenating cell \nvalues with special tokens to separate rows\n\u2022Wikidata:\n18Oguz et al. \" UniK -QA: Unified Representations of Structured and Unstructured Knowledge for Open -Domain Question Answering.\u201c\nFindings of NAACL (2022).\n\n\n[Page 19]\nQUIZ\nWHAT ISTHEWEAKN ESS OF\nCONVERTING DATABA SES TOTEXT? \n\n[Page 20]\nSTANFORD LAMUnified Knowledge Question Answering ( UniK-QA)\n\u2022Retrieve and read QA pipeline with SOTA components\n\u2022Neural retriever;  T5 reader\n\u2022Exact match scores:\n20Knowledge SourceNatural \nQuestionsTriviaQA WebQuestions\nWikipedia Text 49.0 64.0 50.6\nWikipedia Tables 36.0 34.5 41.0\nWikidata 27.9 35.4 55.6\nText + Tables 54.1 65.1 50.2\nText + Tables + Wikidata 54.0 64.1 57.8\nQuiz: What do you observe? \n\n[Page 21]\nSTANFORD LAMTesting with Yelp\n21", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Hybrid", "content": "[Page 21]\nSTANFORD LAMTesting with Yelp\n21\n\n[Page 22]\nSTANFORD LAMProblems with Linearization\n\u2022How to combine different clauses together?\n(Algebraic operators handle them naturally)\n\u2022e.g. multiple clauses: \"Are there any restaurants that have a great \nview of the Golden Gate bridge inSan Francisco ?\"\n\u2022Hard to do column computations \n(rank, max, min, average, count) on flattened data\n\u2022e.g. \"what is the top-rated Chinese restaurant?\"\n22\n\n[Page 23]\nSTANFORD LAMGoal of this Lecture\n\u2022To understand the Hybrid QA problem\n\u2022Discuss 4 major approaches, starting with the simplest\n\u2022HybridQA : dataset requiring composition of DB & Text retrieval\n1.Classify query to decide to retrieve from DB or Text \n2.Retrieve answers from DB and Text separately, choose answer\n3.Convert DB to text; use Text Retrieval\n4.Retrieve answers from DB and Text separately, \nuse LLM to combine the answers\n23\n\n[Page 24]\nSTANFORD LAMHybridQA Dataset\n24\nEarlier than SK -TOD. Note: this is not conversational\n\n[Page 25]\nSTANFORD LAMHybridQA Dataset\nWikipedia Tables Wikipedia Pages hyperlinked\n25Flag bearers of \nMyanmar \nat the Olympics\n\n[Page 26]\nSTANFORD LAM 26\nT: Table\nP: PassageQuiz: How many total # of types can there be?\n\n[Page 27]\nHY B RI D QA\nA G O OD DA TA SE T T HA T IL LU ST R A TE S\nT HE NA TU R A LN E S S OFHY B RI D DA TA QU E R IE S\nNEED TOSU PP O R T FU LLCO M PO SIT IO N AL IT Y TH OU G H\n27", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Hybrid", "content": "[Page 26]\nSTANFORD LAM 26\nT: Table\nP: PassageQuiz: How many total # of types can there be?\n\n[Page 27]\nHY B RI D QA\nA G O OD DA TA SE T T HA T IL LU ST R A TE S\nT HE NA TU R A LN E S S OFHY B RI D DA TA QU E R IE S\nNEED TOSU PP O R T FU LLCO M PO SIT IO N AL IT Y TH OU G H\n27\n\n[Page 28]\nSTANFORD LAMGoal of this Lecture\n\u2022To understand the Hybrid QA problem\n\u2022Discuss 4 major approaches, starting with the simplest\n\u2022HybridQA : dataset requiring composition of DB & Text retrieval\n1.Classify query to decide to retrieve from DB or Text \n2.Retrieve answers from DB and Text separately, choose answer\n3.Convert DB to text; use Text Retrieval\n4.Retrieve answers from DB and Text separately, \nuse LLM to combine the answers\n28\n\n[Page 29]\nSTANFORD LAMSOTA Model in 2023\n29\nhttps:// browse.arxiv.org /pdf/2305.11725.pdf\n\n[Page 30]\nSTANFORD LAMHybrid Retrievers\nOnly uses LLM in the \"generation -based reasoner\" step\nhttps://github.com/wenhuchen/HybridQA#recent -papers\nSOTA model: https://arxiv.org/abs/2305.1172530\n\n[Page 31]\nSTANFORD LAM3 Steps\n1.Retrieve information from DV and Free Text\n\u2022Extra complexity to recover from annotation errors \n2.Hybrid selector\n\u2022Sort and filter based on relevance\n3.LLM-Based Reasoner \n\u2022Combines the filtered information from DV and Free Text\n\u2022Question types: Count, compare\n\u2022Use lexical analysis to identify the question type\n\u2022Chain -of-thought prompting\n31\n\n[Page 32]\nSTANFORD LAMSOTA Result\n32\nQuiz: What do you observe?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Hybrid", "content": "[Page 32]\nSTANFORD LAMSOTA Result\n32\nQuiz: What do you observe? \n\n[Page 33]\nSTANFORD LAMProblem\n\u2022Data are retrieved in just one hop\n\u2022Cannot solve cascading multi -hop questions,\nwhere the data to retrieve depends on the 1sthop answer\n\u2022Imprecision with the lexical analysis\n\u2022Lacks completeness and compositionality\n\u2022Just one \u201ccount\u201d or one \u201ccompare\u201d operation \n33\n\n[Page 34]\nSTANFORD LAMConclusion\n\u2022Many questions require composition of information \nretrieved from DB and Text\n\u2022Structure OR Text: is inadequate\n1.Binary classifier up front (SK -TOD, 2023)\n2.Pick afterwards  (Stanford Chirpy Cardinal, 2021)\n\u2022Different approaches to combine structures and free -text \n3.Structures \u2192Text: Linearization (one hop)\n4.Hybrid: Retrieve from both and combine (one hop each)\n34", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-hybrid.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:21.255947"}
{"title": "L Irned", "content": "[Page 1]\nCS224v\nConversational Virtual Assistants \nwith Deep Learning\nLecture 13: NLP Building Blocks\n1Omar Khattab and Monica Lam\n\n[Page 2]\nST ANFORD LAMLecture Goals\n\u2022Technical details on two building blocks under the hood\n\u2022Information Retrieval (on free -text)\n\u2022Find the relevant paragraph from billions of paragraphs\nthat answers a question?\n\u2022Named Entity Disambiguation \n\u2022Figure out the entity in a reference\n\u2022E.g. How tall is Lincoln?\n        Where is Lincoln Nebraska?  \n\n[Page 3]\nST ANFORD LAMLecture Goals\n\u2022Information Retrieval (on free -text)\n\u2022Late Interaction Model \n\u2022Optimization: time (Select k candidates + Late interaction)\n\u2022Optimization: space with compression\n\u2022Named Entity Disambiguation\n\u2022Use entity description: (Select k candidates + Cross -encoder)\n\u2022Use type information \n\n[Page 4]\nST ANFORD LAMSIGIR \u201920 (Over 1200 citations)\nNAACL \u201922", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 4]\nST ANFORD LAMSIGIR \u201920 (Over 1200 citations)\nNAACL \u201922\n\n[Page 5]\nInformation Retrieval at a Glance\nWhat compounds \nprotect the digestive \nsystem against \ninfection?\n5Retriever\nText\nCorpus\nOriginal version of question and answer from SQuAD . All passages are short extractions from Wikipedia. Edited.\nPranav Rajpurkar , et al. 2016. SQuAD : 100,000+ questions for machine comprehension of text.  EMNLP\u201916.Product Search\nFact CheckingQuestion Answering\nInformative DialogueChemical  barriers  also protect  against  \ninfection . The skin and respiratory  tract  \nsecrete  antimicrobial  peptides  such  asIn the stomach , gastric  acid and proteases  \nserve  as powerful  chemical  defenses  \nagainst  ingested  pathogens .12\n\n[Page 6]\nRetrievers must balance quality  & efficiency\n6Answer challenging queries  vs.  Search over millions of documents in milliseconds !\nHigher Quality is BetterBut Search \nNeeds to \nbe Fast\n\n[Page 7]\nRetrievers must balance quality  & efficiency\n7Higher Quality is BetterBut Search \nNeeds to \nbe Fast\n??\n\n[Page 8]\nRetrievers must balance quality  & efficiency\n8Higher Quality is BetterBut Search \nNeeds to \nbe Fast\n\n\n[Page 9]\nRetrievers must balance quality  & efficiency\n9Latency  (y axis) is in log scale . ColBERT  can be orders of magnitude faster than BERT!Higher Quality is BetterBut Search \nNeeds to \nbe Fast\nColBERTv2", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 9]\nRetrievers must balance quality  & efficiency\n9Latency  (y axis) is in log scale . ColBERT  can be orders of magnitude faster than BERT!Higher Quality is BetterBut Search \nNeeds to \nbe Fast\nColBERTv2\n\n[Page 10]\nHow Retrievers Work at a High Level\n10Retriever0.93What compounds in the stomach protect \nagainst ingested pathogens?\nImmune  System  | Wikipedia\nChemical  barriers  also protect  against  infection . The skin and \nrespiratory  tract  secrete  antimicrobial \u2026Q\nD1\nRetriever0.01<same query>\nIs this a syntax  error?  | Stack  Overflow\nNoticed  a line in our codebase  today  which \u2026Q\nD999\n\n[Page 11]\nST ANFORD LAMInformation Retrieval Data Set\n\u2022In 2018, it is turned into an information retrieval dataset\n\u2022Bing\u2019s 1M real -world queries\n\u20228.8M passages from Web pages\n\u2022Effectiveness metric: MRR -10\n\u2022Finding the right answer within the 10 documents returned\nNIPS, 2016\n\n[Page 12]\nNeural IR: Two Extreme Matching Paradigms\n12\n(a)  Cross Encoder\n Fine -Grained Interactions\n Unscalable Joint  ConditioningScale is a major challenge.\nYou might have 100 million  documents.\nEven if scoring each document took 10 ms,\nretrieval would consume 11 days per query!\n\n[Page 13]\nNeural IR: Two Extreme Matching Paradigms\n14\n(a)  Cross Encoder\n Fine -Grained Interactions\n Unscalable Joint  Conditioning(b)  Bi -Coder\n Independent, Dense Encoding\n Coarse -Grained RepresentationQuery Document", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 13]\nNeural IR: Two Extreme Matching Paradigms\n14\n(a)  Cross Encoder\n Fine -Grained Interactions\n Unscalable Joint  Conditioning(b)  Bi -Coder\n Independent, Dense Encoding\n Coarse -Grained RepresentationQuery Document\n\n\n[Page 14]\nColBERT: Late Interaction\n15\n Independent Encoding\n Fine -Grained Representations\n Scalable Nearest -Neighbor Search\nQuery Document\n(c) Late Interaction\nOmar Khattab and Matei  Zaharia . \u201cColBERT: Efficient and effective \npassage search via contextualized late interaction over BERT .\" SIGIR 2020.\n\n[Page 15]\nColBERT: Late Interaction\n16\n Independent Encoding\n Fine -Grained Representations\n Scalable Nearest -Neighbor Search\nQuery Document\nMaxSim\u2211\nMaxSim MaxSims\n(c) Late Interaction\nOmar Khattab and Matei  Zaharia . \u201cColBERT: Efficient and effective \npassage search via contextualized late interaction over BERT .\" SIGIR 2020.End-to-End Retrieval over \nWikipedia (21M passages) \ntakes 70ms .Offline\nEncodingMaxSim : Maximum Similarity\n                 e.g. cosine similarity", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 16]\nwhen did the transformers cartoon series come out?\n[\u2026] the animated [\u2026] The Transformers [\u2026] [\u2026] It was released [\u2026] on August 8, 1986Late Interaction: Real Example of Matching\n17\nwhen\n  did the transformers cartoon series come out?\n[\u2026] the animated [\u2026] The Transformers [\u2026] [\u2026] It was released [\u2026] \n on\n August 8, 1986\nwhen did the \n transformers\n  cartoon series come out?\n[\u2026] the animated [\u2026] The \n Transformers\n  [\u2026] [\u2026] It was released [\u2026] on August 8, 1986\nwhen did the transformers \n cartoon\n  series come out?\n[\u2026] the \n animated\n  [\u2026] The Transformers [\u2026] [\u2026] It was released [\u2026] on August 8, 1986\nwhen did the transformers cartoon series \n come\n  \nout\n?\n[\u2026] the animated [\u2026] The Transformers [\u2026] [\u2026] It was \n released\n  [\u2026] on August 8, 1986\n\n[Page 17]\nST ANFORD LAMColbert Inference Time Optimization\nDocuments are all encoded offline\nGiven a query, find the top k matching docs (from 100M docs)\n\u2022Ranking every document is too costly\n1.Narrow down candidates w/o late interaction\n(100M docs to 10K)\n2.Use late interaction on 10K docs\n\u2022Space: The embeddings are too large\n\u2022Compress the storage (Colbertv2)\nQuery Document\nMaxSim\n\u2211\nMaxSim\n MaxSims\n\n[Page 18]\n\u2022Faiss : Facebook AI Similarity Search\n\u2022Returns k -nearest neighbors constructed on \nbillions of high -dimensional vectors (embeddings).\n19\n\n\n[Page 19]\nColBERT: Step 1 (No Late Interaction)\n20Document IDs #1 #45,436 #935,765 #2,689,357 #7,769,374", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 18]\n\u2022Faiss : Facebook AI Similarity Search\n\u2022Returns k -nearest neighbors constructed on \nbillions of high -dimensional vectors (embeddings).\n19\n\n\n[Page 19]\nColBERT: Step 1 (No Late Interaction)\n20Document IDs #1 #45,436 #935,765 #2,689,357 #7,769,374\n\n[Page 20]\nColBERT : Step 1 (No Late Interaction)\n21\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\n\n[Page 21]\nColBERT : Step 1 (No Late Interaction)\n22\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nIndexed for fast vector -similarity search.\nWe use Facebook\u2019s faiss .\n\n[Page 22]\nColBERT : Step 1 (No Late Interaction)\n23\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\n\n[Page 23]\nColBERT : Step 1 (No Late Interaction)\n24Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\n\n[Page 24]\nColBERT : Step 1 (No Late Interaction)\n25Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nTop similarity match\n\n[Page 25]\nColBERT : Step 1 (No Late Interaction)\n26Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nTop similarity match\n\n[Page 26]\nColBERT : Step 1 (No Late Interaction)\n27Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nTop similarity match\n\n[Page 27]\nColBERT : Step 1 (No Late Interaction)\n28Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nTop similarity match", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 26]\nColBERT : Step 1 (No Late Interaction)\n27Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nTop similarity match\n\n[Page 27]\nColBERT : Step 1 (No Late Interaction)\n28Query\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nTop similarity match\n\n[Page 28]\nColBERT : Step 1 (No Late Interaction)\n29\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\n\n[Page 29]\nColBERT : Step 2 (Late Interaction)\n30\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\n\n[Page 30]\nColBERT : Step 2 (Late Interaction)\n31\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374QueryDocument\n(pre-computed)\nMaxSim\u2211\nMaxSim MaxSims\n\n[Page 31]\nColBERT : Step 2 (Late Interaction)\n32\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374QueryDocument\n(pre-computed)\nMaxSim\u2211\nMaxSim MaxSims\nInput to calculate similarity score (late interaction)\n\n[Page 32]\nColBERT : Step 2 (Late Interaction)\n33\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374QueryDocument\n(pre-computed)\nMaxSim\u2211\nMaxSim MaxSims\nInput to calculate similarity score (late interaction)\n\n[Page 33]\nColBERT : Step 2 (Late Interaction)\n34\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374QueryDocument\n(pre-computed)\nMaxSim\u2211\nMaxSim MaxSims\nInput to calculate similarity score (late interaction)\n\n[Page 34]\nSO R T  SI M I L A R I T Y  SC O R ES  TO FI N D THE TOP KColBERT : Step 2 (Late Interaction)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 34]\nSO R T  SI M I L A R I T Y  SC O R ES  TO FI N D THE TOP KColBERT : Step 2 (Late Interaction)\n\n[Page 35]\nLate interaction delivers large gains\u2026\n36\n\n[Page 36]\nLate interaction delivers large gains\u2026\n37Passage Ranking\nModel MS MARCO\nMRR@10\nBM25 18.7\nDPR 31.1\nANCE 33.0\nColBERT [QA] 36.0 / 37.5\nDPR and ANCE result figures from the respective papers. DPR on MS MARCO was evaluated by the ANCE authors.\n\n[Page 37]\nLate interaction delivers large gains\u2026\n38Passage Ranking Open -Domain QA Retrieval over Wikipedia\u201918\nModel MS MARCO\nMRR@10NaturalQs\nSuccess@20TriviaQA\nSuccess@20Open -SQuAD\nSuccess@20\nBM25 18.7 64.0 77.3 71.4\nDPR 31.1 79.4 79.9 71.5\nANCE 33.0 81.9 80.3 -\nColBERT [QA] 36.0 / 37.5 85.3 85.6 83.7\nDPR and ANCE result figures from the respective papers. DPR on MS MARCO was evaluated by the ANCE authors.\n\n[Page 38]\nLate interaction delivers large gains\u2026\n39Passage Ranking Open -Domain QA Retrieval over Wikipedia\u201918\nModel MS MARCO\nMRR@10NaturalQs\nSuccess@20TriviaQA\nSuccess@20Open -SQuAD\nSuccess@20\nBM25 18.7 64.0 77.3 71.4\nDPR 31.1 79.4 79.9 71.5\nANCE 33.0 81.9 80.3 -\nColBERT [QA] 36.0 / 37.5 85.3 85.6 83.7And the gaps are \noften larger when \nthere\u2019s a domain shift \nor a challenging \ndownstream task !", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 39]\nBut in the first version of ColBERT, this came \nat a cost!\n40Passage Ranking Open -Domain QA Retrieval over Wikipedia\u201918\nModel MS MARCO\nMRR@10NaturalQs\nSuccess@20TriviaQA\nSuccess@20Open -SQuAD\nSuccess@20\nBM25 18.7 64.0 77.3 71.4\nDPR 31.1 79.4 79.9 71.5\nANCE 33.0 81.9 80.3 -\nColBERT [QA] 36.0 / 37.5 85.3 85.6 83.7However, ColBERT\u2019s  index is an order of magnitude \nlarger than baselines, at 650 GB  for Wikipedia!\nCan we advance ColBERT\u2019s  large quality advantage and \nreduce its footprint by an order of magnitude ?\n\n[Page 40]\nColBERTv2: Can we reduce the storage requirements?\n410.35, 0.9, 0.03, \u2026, 0.64, 0.14, 0.23, \u2026, 0.78\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nIn ColBERTv1 , each vector  needs\n128 x float (4 bytes) = 512 bytes\nIn ColBERTv2 , each vector\nconsumes just 20 bytes \u2013 How?compressed vector\n\n[Page 41]\nST ANFORD LAMResidual Compression\n\u2022Given a set of centroids C \n\u2022Encode each vector v \n\u2022the index of its closest centroid Ct  \nand a quantized vector r \u0303 \n\u2022that approximates the residual r = v \u2212 Ct. \n\u2022At search time, \nuse the centroid index t and residual r \u0303 \nrecover an approximate  v \u0303 = C t + r \n42", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 42]\nColBERTv2: Residual  Compression\n430.35, 0.9, 0.03, \u2026, 0.64, 0.14, 0.23, \u2026, 0.78\nEmbeddingsDocument IDs #1 #45,436\n #935,765\n #2,689,357\n #7,769,374\nIn ColBERTv1 , each vector  needs\n128 x float (4 bytes) = 512 bytes\ncompressed vector In ColBERTv2 , each vector encodes\ncluster ID (4 bytes)\n+ 128 x bit (16 bytes)\n= 20 bytes  only\n\n[Page 43]\n44MS MARCO Passage Ranking\nModel Storage MRR@10 Recall@50\nColBERT v1 154 GB 36.2 82.1\n+ 2 bit residual compression (6x) 25 GB 36.2 82.3\n+ 1 bit residual compression (10x) 16 GB 35.5 81.6Indexing clusters a sample of token vectors.\nRepresent each vector as a cluster ID and a 1-bit delta per dimension . \nThis can consume as little 20 bytes per vector.ColBERTv2: Residual  Compression", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 44]\nColBERT  has been \ndeeply influential \nin IR and NLP\nThe ColBERT line of work has \nbeen cited by over 1,000 papers \n45Best Paper Awards (analyses & extensions of ColBERT)\n\u2022 A White Box Analysis of ColBERT\n\u2022 SparseEmbed : Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval\nAdvanced ColBERT -based architectures\n\u2022 ColBERT -PRF: Semantic Pseudo -Relevance Feedback for Dense Passage and Document Retrieval\n\u2022 ED2LM: Encoder -Decoder to Language Model for Faster Document Re -ranking Inference\n\u2022 Effective Contrastive Weighting for Dense Query Expansion\n\u2022 AligneR  from Google\n\u2022 LAIT, LUMEN, GLIMMER from Google\nOptimizations for ColBERT\n\u2022 XTR from Google\n\u2022 A Study on Token Pruning for ColBERT\n\u2022 On Approximate Nearest Neighbour Selection for Multi -Stage Dense Retrieval\n\u2022 Query Embedding Pruning for Dense Retrieval\n\u2022 Static Pruning for Multi -Representation Dense Retrieval\nExtensions\n\u2022 Distilling Dense Representations for Ranking using Tightly -Coupled Teachers\n\u2022 Improving Efficient Neural Ranking Models with Cross -Architecture Knowledge Distillation\n\u2022 VIRT: Improving Representation -based Models for Text Matching through Virtual Interaction\n\u2022 I^3 Retriever: Incorporating Implicit Interaction in Pre -trained Language Models for Passage Retrieval\n\u2022 SLIM: Sparsified  Late Interaction for Multi -Vector Retrieval with Inverted Indexes\n\u2022 Reproducibility, Replicability, and Insights into Dense Multi -Representation Retrieval Models: from ColBERT", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "\u2022 SLIM: Sparsified  Late Interaction for Multi -Vector Retrieval with Inverted Indexes\n\u2022 Reproducibility, Replicability, and Insights into Dense Multi -Representation Retrieval Models: from ColBERT \nto Col*\nApplications\n\u2022 FILIP: Fine -grained Interactive Language -Image Pre -Training (+ 2 -3 other key ones for multi -modal models)\n\u2022 LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open -Domain Table \nQuestion Answering\n\u2022 IRLab -Amsterdam at TREC 2021 Conversational Assistant Track\n\u2022 Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models\n\u2022 Beyond Two -Tower Matching: Learning Sparse Retrievable Cross -Interactions for Recommendation\n\u2022 Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset -based Bug Localization\nOut of Domain Generalization\n\u2022 BEIR, RELIC, Token -Level Math Information Retrieval, Evaluating Extrapolation Performance in IR (I & II)\n\u2022 NevIR : Negation in Neural Information Retrieval\nCross Lingual\n\u2022 IBM\u2019s Learning Cross Lingual IR from an English Retriever\n\u2022 Cross -lingual Knowledge Transfer via Distillation for Multilingual Information Retrieval\n\u2022 Multilingual ColBERT -X", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 45]\nST ANFORD LAMOur Current IR System \nEMNLP Nov. 2024\nPrimarily a bi -coderText Retrieval: \nVector similarity search:   Qdrant  Vector Search\nhttps:// qdrant.tech /\n\n[Page 46]\nST ANFORD LAMLecture Goals\n\u2022Information Retrieval (on free -text)\n\u2022Late Interaction Model \n\u2022Optimization: time (Select k candidates + Late interaction)\n\u2022Optimization: space with compression\n\u2022Named Entity Disambiguation\n\u2022Use entity description: (Select k candidates + Cross -encoder)\n\u2022Use type information \n\n[Page 47]\nST ANFORD LAMEntity Linking\n\u201cBarack Obama, born in Hawaii, served as the 44th President of the United \nStates. He graduated from Harvard Law School and won the Nobel Peace \nPrize in 2009.\u201d\n\n[Page 48]\nST ANFORD LAMEntity Linking\n\u201cBarack Obama , born in Hawaii , served as the 44th President of the United \nStates . He graduated from Harvard Law School  and won the Nobel Peace \nPrize  in 2009.\u201d\n\n\n[Page 49]\nST ANFORD LAMNamed Entity Disambiguation\nNamed Entity \nDisambiguationKnowledge Base\nEntity listDocument\nEngland\n FIFA World Cup\nTasks: \n\u2022Mention: a span that refers to a named entity\n\u2022Entity:      the entity in the entity list\n\n\n[Page 50]\nST ANFORD LAMEntity Linking Applications\n\u2022Question Answering\n\u2022Relation Extraction\n\u2022Automated Construction of Knowledge Bases\n\n[Page 51]\nEMNLP 2020\n\n[Page 52]\nKEY CON CE P T\nADD DESC RI PT I O NS  TO T H E ENT I T I ES  AS INP U T", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 50]\nST ANFORD LAMEntity Linking Applications\n\u2022Question Answering\n\u2022Relation Extraction\n\u2022Automated Construction of Knowledge Bases\n\n[Page 51]\nEMNLP 2020\n\n[Page 52]\nKEY CON CE P T\nADD DESC RI PT I O NS  TO T H E ENT I T I ES  AS INP U T\n\n[Page 53]\nST ANFORD LAMDescriptions Enable Zero -Shot NED\nNamed Entity \nDisambiguation\nTasks: \n\u2022Mention: a span that refers to a named entity\n\u2022Entity:      the entity in the entity list\n\n\n[Page 54]\nST ANFORD LAMTwo Step Approach\n1. Candidate Generation and Ranking (Fast)\n\u2022 Encode doc, entities to the same dense space\n\u2022 Retrieve k closest entities\n\n\n[Page 55]\nST ANFORD LAMTwo Step Approach\n1. Candidate Generation and Ranking (Fast)\n\u2022 Encode doc, entities to the same dense space\n\u2022 Retrieve k closest entities\n2. Rerank  k candidates using a cross -encoder\n\u2022 Slower, More accurate\n\n[Page 56]\nST ANFORD LAM\nNAACL 2022\n\n[Page 57]\nKEY CON CE P T\nADD TYPE INF OR M AT I ON  TO ENT I T I ES\n\n[Page 58]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\n\n[Page 59]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\n\n\n[Page 60]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\nMention Detection with Begin/Inside/Outside (BIO) Tagging.\n\n[Page 61]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\nMention Representation via mean pooling.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 60]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\nMention Detection with Begin/Inside/Outside (BIO) Tagging.\n\n[Page 61]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\nMention Representation via mean pooling.\n\n[Page 62]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\nEntity Description score: multiple simultaneous bi -encoder representations\n\n[Page 63]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\nEntity Typing score: L2 distance between type vectors\n\n[Page 64]\nST ANFORD LAMReFinED : Representation and Fine -grained typing for ED\n\u2022The global entity prior is obtained from a corpus ( Hoffart  et al. , 2011 ) or a popularity metric ( Diefenbach and Thalhammer , 2018 ). \n\u2022P\u02c6(e|m) included to improve results for cases where context is limited (e.g. short question text). Linear Combination of {Prior, Types, Description} scores\n\n[Page 65]\nST ANFORD LAMReFinED : Recap!\n\u2022Mention Detection with Begin/Inside/Outside (BIO) Tagging.\n\u2022Mention Representation via mean pooling.\n\u2022Entity Typing score.\n\u2022Entity Description score.\n\u2022Linear Combination of {Prior, Types, Description}.\n\n[Page 66]\nST ANFORD LAM\nReFinED  deployed by Amazon Alexa at \u201cweb scale\u201d\n\u2022Populate a KB from a billion web pages , multiple times per year\n\u2022Requires 2 days of processing , using 500 T4 GPUs\n\u2022Pro: Uniform architecture is easy to scale !\n\u2022Pro: Generalizes well to 90M entities  at scale", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "L Irned", "content": "[Page 66]\nST ANFORD LAM\nReFinED  deployed by Amazon Alexa at \u201cweb scale\u201d\n\u2022Populate a KB from a billion web pages , multiple times per year\n\u2022Requires 2 days of processing , using 500 T4 GPUs\n\u2022Pro: Uniform architecture is easy to scale !\n\u2022Pro: Generalizes well to 90M entities  at scale\n\n[Page 67]\nST ANFORD LAMConclusions\n\u2022Information Retrieval (on free -text)\n\u2022Finding a document that answers a query\n\u2022Text retrieval: ColBERT  \u2192 mGTE\n\u2022Vector search: FAISS \u2192 Qdrant\n\u2022Named Entity Disambiguation: ReFinED\n\u2022Use entity description and type information", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-IRNED.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:25.322446"}
{"title": "Project Info", "content": "A core part of CS224V will be working on a project throughout the quarter. We are very excited to announce that we have mentors and advisors across many disciplines who have signed on to help you with these projects. These includes various leaders across Stanford as well as with external partners.\nThis is an exciting time for LLM-based projects because for the first time, it is possible to create a usable NLP project within a quarter. There are numerous meaningful applications that you can create and there are more tools you can build that enable new applications. The sky is the limit!", "url": "https://web.stanford.edu/class/cs224v/projects.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:01:26.425257"}
{"title": "Project Info", "content": "This document serves as an inspiration for students on their choice of course projects. It is a collection of potential project ideas. Some of these will lead to AI papers, or papers in other disciplines, or public services to be hosted on our World Wide Knowledge website. We are looking for your input on new projects as well. Many of these projects can be worked on by multiple groups, possibly using different corpora.\nThis will be an extended version of the mentor-written proposal. Your full proposal can be largely the same as what has already been provided by your mentor, but you should edit it if you are narrowing/expanding the scope and customizing it to your interests. The proposal should have more details about when each phase of the project will be completed, the datasets you will use, a proposed weekly schedule, and projected ideas of what each partner will work on.", "url": "https://web.stanford.edu/class/cs224v/projects.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:01:26.425257"}
{"title": "Project Info", "content": "In addition to the fields already in the mentor-written proposal, the full project proposal will additionally ask you to fill-out:\nPlease use the following format: Project Template\nIf you are doing a custom project, you will have to sign up for presenting your project on (10/14, 10/16 and 10/21). Please fill the form here to signup.\nNote that signing up is mandatory! If you signup first, you will get more feedback which you can use to update your final proposal.\nPlease still review the Mentor-Written Project Proposals for examples of what details we are looking for as you propose your custom project.\nThe full project proposal will ask you to fill-out:\nPlease use the following format: Project Template", "url": "https://web.stanford.edu/class/cs224v/projects.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:01:26.425257"}
{"title": "Project Info", "content": "The full project proposal will ask you to fill-out:\nPlease use the following format: Project Template\nOn Wednesday, 12/4, we will host a presentation + poster session for all final projects from 3:00 PM to 6:00 PM. Each group will make a 60 seconds presentation at the beginning, in our usual lecture location. We will move on to the poster session afterward. We will adopt the same poster session guidelines as CS 224N\nA final paper about your project will be due on 12/10, 11:59 PM. You should also submit your code, along with a README, of how to run your program for a demo. This is due at the same time as the paper. It is recommended to include a short video demo of your project along with your code.", "url": "https://web.stanford.edu/class/cs224v/projects.html", "source": "CS224V Website", "type": "course_website", "timestamp": "2024-11-30T14:01:26.425257"}
{"title": "L React Agents", "content": "[Page 1]\nStanford CS224v Course\nConversational Virtual Assistants with Deep Learning\nLecture 9\nAgentic Approach: Knowledge Base Queries\n1Monica Lam & Shicheng  Liu & Sina Semnani\n\n[Page 2]\nCan We Handle Real Knowledge Bases?\nST U DY  OF 2  H AR D REA L-LI F E DAT A SET S\nTHE AGE NT I C  AP P RO AC H  \n\n\n[Page 3]\nST ANFORD LAMLecture Goals\nTeach the Agentic Approach so You Can Use It in Your Projects!\n\u2022Knowledge Graphs \u2013 aka Knowledge Bases (KB) \n\u2022Why KBs? \n\u2022Why is KBQA (KB Question Answering) Challenging? \n-- Previous Work\n\u2022The Agentic Approach for KBs\n\u2022How to Get a Good KBQA Dataset? \n\u2022Evaluation\n\u2022Applying the Agentic Approach to SQL Databases\n\n\n[Page 4]\nST ANFORD LAMA Knowledge Graph\n\u2022A lot of information cannot  be represented in tables\n\u2022Knowledge graph is also known as a semantic web\n\u2022Nodes are entities\n\u2022Edges are relationships\n\n[Page 5]\nST ANFORD LAMWikidata : The Largest Live Knowledge Graph\n\u2022Stats: \n\u202215B facts (1B more triples per year)\n\u2022100M entities\n\u202210K properties\n\u202225K contributors\n\u2022All entities in Wikipedia are in Wikidata\n\u2022Wikidata  contains many more entities", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 5]\nST ANFORD LAMWikidata : The Largest Live Knowledge Graph\n\u2022Stats: \n\u202215B facts (1B more triples per year)\n\u2022100M entities\n\u202210K properties\n\u202225K contributors\n\u2022All entities in Wikipedia are in Wikidata\n\u2022Wikidata  contains many more entities\n\n\n[Page 6]\nST ANFORD LAMWikidata  Representation \u2013 a RDF graph\n\u2022Representation: RDF triples\n\u2022Nodes are entities (represented by unique QIDs)\n\u2022There are many entities with exactly the same name\n\u2022Same ID across all Wikipedia in all languages \n\u2022Edges are properties (represented by unique PIDs)\nSELECT ?x WHERE \n{ wd:Q41506    wdt:P112     ?x. }\nStanford Founded byWho founded Stanford? \nQuery with SPARQL\nA natural language interface can greatly expand access!c\n\n[Page 7]\nST ANFORD LAMRunning Example: Music Instruments Played \n\n\n[Page 8]\nST ANFORD LAM\nEdward Rutschman\n(Q107705263)\nHokyung  Yang\n(Q107851778)\nGwynne Kuhner  Brown\n(Q106627792)\nChad Kirby\n(Q107522681)\nUniversity of Washington School of Music\n(Q98035717)\nUniversity of Washington\n(Q219563)\nTrombone\n(Q8350)\nPiano\n(Q5994)\nAffiliation\n(P1416)\nEducated At\n(P69)\nInstrument\n(P1303)A Subset of the Knowledge Graph in Wikidata\nZakir Hussain\n(Q144719)\nPercussion instrument\n(Q133163)\nTabla\n(Q213100)\n\n\n[Page 9]\nST ANFORD LAM\nP1303: instrument\nP1416 : affiliation\nP69:     educated at\nQ98035717 : \nUniversity of Washington \nSchool of Music\nQ219563 : \nUniversity of Washington", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 9]\nST ANFORD LAM\nP1303: instrument\nP1416 : affiliation\nP69:     educated at\nQ98035717 : \nUniversity of Washington \nSchool of Music\nQ219563 : \nUniversity of Washington\n\n[Page 10]\nST ANFORD LAMThe Power of WikiData\n\u2022SPARQL allows relational algebra operations \nacross the entire knowledge graph\n\u2022Running example needs: filters, projections, joins, counts \u2026\n\u2022Query optimizations\n\u2022Dataset for research in \nMathematics, Biology, Education,  Social Sciences, Linguistics, . . \nQuiz: Can we represent the data as tables?\n\n[Page 11]\nST ANFORD LAMLecture Goals\nTeach the Agentic Approach so You Can Use It in Your Projects!\n\u2022Knowledge Graphs \u2013 aka Knowledge Bases (KB) \n\u2022Why KBs? \n\u2022Why is KBQA (KB Question Answering) Challenging? \n-- Previous Work\n\u2022The Agentic Approach for KBs\n\u2022How to Get a Good KBQA Dataset? \n\u2022Evaluation\n\u2022Applying the Agentic Approach to SQL Databases\n\n\n[Page 12]\nPrevious Work\nKnowledge Base Query Answering (KBQA)\nSEM AN T I C  PAR SER\nSU BGR AP H  TRA VER SAL\n\n[Page 13]\nST ANFORD LAMSemantic Parser\n(Llama fine -tuned with 3K samples)From Wikidata , the filming location of \n\u2018A Bronx Tale\u2019 includes New Jersey \nand New YorkWhere did Bronx take place?\nGPT-3\nGPT-3 guesses that the movie \ntook place in Bronx, New York\nEntity Linker\n(\u2018A Bronx Tale\u2019, \u2018Q1130705\u2019)\nNo \nResponse1. Semantic Parsing: NL \u2192 SPARQL\nResponse\nSilei Xu et al. Fine -tuned LLMs Know More, Hallucinate Less with Few -Shot Sequence -to-Sequence Semantic Parsing over Wikidata .. EMNLP 2023", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 14]\nST ANFORD LAMNew WikiWebQuestions  Dataset\n\u2022Adapted from WebQuestions  for FreeBase\n\u2022Questions from Google Suggest API\n\u2022Real -world popular  questions\n\u20222431 train, 438 dev, 1384 test\n\u2022GPT-3: trained on Wikipedia + Internet\n\u2022Can GPT -3 answer WikiQebQuestions ? \nSilei Xu et al. Fine -tuned LLMs Know More, Hallucinate Less with Few -Shot Sequence -to-Sequence Semantic Parsing over Wikidata .. EMNLP 2023 \n\n[Page 15]\nST ANFORD LAM\nGPT-3 on New Wiki -WebQuestions  Dataset\nGPT-3 \nGuessesQuestion:  \u201cWhat does Obama have a degree in?\u201d\nGPT-3:    \u201cPolitical science degree\u201d\nMissing:   \u201cLaw degree\u201d\n66% \nCorrect 27% \nIncomplete7% \nIncorrect\nSilei Xu et al. Fine -tuned LLMs Know More, Hallucinate Less with Few -Shot Sequence -to-Sequence Semantic Parsing over Wikidata .. EMNLP 2023 Question:  \u201cwhere does the name Melbourne come from?\u201d \nGPT-3:    \u201cMelbourne comes from the Latin word \u2018 melburnum \u2019 \n   meaning \u2018 blackburn \u2019 or \u2018blackbird\u2019 \u201d\nAnswer:   \u201cMelbourne is named after William Lamb, \n   2nd Viscount Melbourne\u201d\nUsers must fact check all the answers!\n\n[Page 16]\nST ANFORD LAM\nReduce GPT3 Hallucination with WikiData\nJust GPT -3 \nGuesses66% \nCorrect GPT27% \nIncomplete GPT7% \nIncorrect GPT\n76% \nVerifiable \nfrom WikiData15% \nCorrect GPT4% \nIncorrect GPT\n6% \nIncomplete GPT\nWikidata  (Genie semantic parser) \n+ GPT3 GuessesBenchmark: WikiWebQuestions  (Popular, Human -Generated)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 17]\nST ANFORD LAM\nWeakness\n\u2022WikiWebQuestions  are relatively simple, \npopular questions using mostly the same properties\n\u2022Wikidata  has about 3000 knowledge -oriented properties \n#Unique Properties\nOur test set for\nin-context  learning \u2192 \n\n[Page 18]\nST ANFORD LAMWhy Querying Wikidata  with SPARQL is Difficult\n\u2022Many properties: Hard to memorize all the properties\n\u2022No fixed schema: Wikidata  does not have a fixed schema\n\u2022We do not know what properties each node has\n\u2022Many similar properties: \n\u2022Often unclear which property or entity should be used\n\u2022Questions on locations: \u201cWhere did Isaac Newton live?\u201d \u201cWhere is Salesforce?\u201d \n\u2022Possible properties \u2013 depends on what is available for the node\n\u2022administrative territorial entity\n\u2022residence , state , country, place of birth, place of death\n\u2022headquarters location , location of formation\n\u2022Need to look at a node\u2019s properties to determine the right SPARQL\n\n[Page 19]\nST ANFORD LAM2. Subgraph Retrieval\nQuiz: Can this handle our running example?\nSubgraph Retrieval Enhanced Model for Multi -hop Knowledge Base Question Answering (Zhang et al., ACL 2022)Retrieve a part of the graph based on a question\nQuiz: Can this handle finding the tallest mountain?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 19]\nST ANFORD LAM2. Subgraph Retrieval\nQuiz: Can this handle our running example?\nSubgraph Retrieval Enhanced Model for Multi -hop Knowledge Base Question Answering (Zhang et al., ACL 2022)Retrieve a part of the graph based on a question\nQuiz: Can this handle finding the tallest mountain? \n\n[Page 20]\nST ANFORD LAM3. LLM -Based Graph Exploration\nExplore a sub -graph by walking the graph one edge at a time\nThink -on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph , Sun et al, ICLR 2024Quiz: Is \u201cwalking the graph\u201d enough?    It needs to start with an entity.\n                Can it handle our running example? \n\n\n[Page 21]\nST ANFORD LAMLecture Goals\nTeach the Agentic Approach so You Can Use It in Your Projects!\n\u2022Knowledge Graphs \u2013 aka Knowledge Bases (KB) \n\u2022Why KBs? \n\u2022Why is KBQA (KB Question Answering) Challenging? \n-- Previous Work\n\u2022The Agentic Approach for KBs\n\u2022How to Get a Good KBQA Dataset? \n\u2022Evaluation\n\u2022Applying the Agentic Approach to SQL Databases\n\n\n[Page 22]\nSPINACH: \nSPARQL -Based Information Navigation for \nChallenging Real -World Questions\nShicheng  Liu*     Sina J. Semnani *\nHarold Triedman1   Jialiang  Xu   Isaac Dan Zhao     Monica S. Lam\nStanford University\n* Equal contribution\n1 Cornell Tech;  Work conducted while at the Wikimedia Foundation\nEMNLP 2024 \n\n[Page 23]\nST ANFORD LAMHttps:// spinach.genie.stanford.edu", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 23]\nST ANFORD LAMHttps:// spinach.genie.stanford.edu\n\n\n[Page 24]\nST ANFORD LAMCombine the Best of \nSPARQL, Subgraph Retrieval, and LLMs \nKey idea: combine\n\u2022SPARQL query:  \n\u2022Expressiveness, implementation/query optimizations\n\u2022Graph retrieval: exam actual properties in the data \n\u2022LLM:                     \n\u2022If human experts can do it: we can do it!\n\u2022Automate the human expert\u2019s approach\n\u2022Humans don\u2019t memorize the nodes and properties\n     \u2192 we don\u2019t need fine -tuning! Just use ICL \n\n[Page 25]\nST ANFORD LAMHow a Human Expert Write a SPARQL Query\n1.Start by writing simple SPARQL queries ;\n2.Look up Wikidata  entity or property pages  when needed\n\u2022To understand the structure of the knowledge graph \n\u2022Check what properties exist for a node\n3.Add new SPARQL clauses  to build towards the final SPARQL\nWeaves together \n-Knowledge inquiry\n-Query writing\n-Execution and evaluation of results (subgraph retrieval)\n\n[Page 26]\nST ANFORD LAMActions Useful for Writing SPARQL\nProvided by Wikidata\nsearch_wikidata (string):         http:// wikidata.org  (search bar): \n           text \u2192 returns QIDs and PIDs\nget_wikidata_entry (QID):   https://www.wikidata.org/wiki/QID:<QID>\n           QID \u2192 Wikidata  page for entity\nget_property_examples (PID): https://www.wikidata.org/wiki/Property:<PID >:\n            PID \u2192 examples of how property PID is used\nexecute_sparql (SPARQL):       https://query.wikidata.org :\n            SPARQL \u2192 result\n\n[Page 27]\nST ANFORD LAMRunning Example", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 27]\nST ANFORD LAMRunning Example\n\n\n[Page 28]\nST ANFORD LAMsearch_wikidata (\"musical instrument\" )\nThis step performs Named Entity Disambiguation (NED) with LLM!Running Example\n\n\n[Page 29]\nST ANFORD LAMsearch_wikidata (\"affiliation\" )Running Example\n\n\n[Page 30]\nST ANFORD LAMRunning Example\nget_wikidata_entry  \n(\"Q98035717\" )\n\n\n[Page 31]\nST ANFORD LAMRunning Example\nget_property_examples (\"P1416\")\nAffiliation (P1416)\nRaoul Bott -> Institute for Advanced Study\nHannah Sipe -> University of Washington School\nof Environmental and Forest Sciences\n\n[Page 32]\nST ANFORD LAMRunning Example\n\n\n[Page 33]\nST ANFORD LAMAgentic Approach for Knowledge Bases\nUser Query\nAgentthinks\nIssues actions\nObserves  \nresults\nAgent response\nReAct : Synergizing Reasoning and Acting in Language Models , Yao et al, ICLR 2023\n\n[Page 34]\nST ANFORD LAMThe SPINACH Agent\n\u2022Imitates what the user does with an agentic approach\n\u2022Uses the full expressiveness of SPARQL for exploration\n\u2022For N steps:\n\u2022Given the history of agent actions, \n\u2022Prompt LLM to generate a thought and an action\n\u2022Execute an action against the KG\n\u2022Add an observation  to the history\nMain agent code available in this file,\nimplemented with LangGraph  (part of LangChain) in Python", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 35]\nST ANFORD LAMZero -Shot LLM Policy Prompt\n# instruction\nYour task is to write a Wikidata  SPARQL query to answer the given question. Follow a step -by-step process:\n1. Start by constructing very simple fragments of the SPARQL query.\n2. Execute each fragment to verify its correctness. Adjust as needed based on your observations.\n3. Confirm all your assumptions about the structure of Wikidata  before proceeding.\n4. Gradually build the complete SPARQL query by adding one piece at a time.\n5. Do NOT repeat the same action, as the results will be the same.\n6. The question is guaranteed to have an answer in Wikidata , so continue until you find it.\n7. If the user is asking a True/False question with only one answer, use ASK WHERE to fetch a True/False answer at the very e nd.\n8. In the final SPARQL projections, do not only ask for labels. \n    Ask for the actual entities whenever needed (e.g. instead of doing `SELECT xLabel `, do `SELECT x`).\n9. If the final result was contained in last round's ` get_wikidata_entry ` and you are ready to stop, \n    use ` execute_sparql ` and generate a SPARQL to retrieve that results.\nForm exactly one \"Thought\" and perform exactly one \"Action\", then wait for the \"Observation\".\nPossible actions are:\n- get_wikidata_entry (QID): Retrieves all outgoing edges (linked entities, properties, and qualifrs ) of a specified Wikidata  entity using its QID.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "Form exactly one \"Thought\" and perform exactly one \"Action\", then wait for the \"Observation\".\nPossible actions are:\n- get_wikidata_entry (QID): Retrieves all outgoing edges (linked entities, properties, and qualifrs ) of a specified Wikidata  entity using its QID.\n- search_wikidata (string): Searches Wikidata  for entities or properties matching the given string.\n- get_property_examples (PID): Provides a few examples demonstrating the use of the specified property (PID) in Wikidata .\n- execute_sparql (SPARQL): Runs a SPARQL query on Wikidata  and returns a truncated result set for brevity.\n- stop(): Marks the last executed SPARQL query as the final answer and ends the process.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 36]\nST ANFORD LAM# input\n{% for i in range(0, conversation_history|length ) %}\nUser Question: {{ conversation_history [i][\"question\"] }}\n{% for j in range(0, conversation_history [i][\"action_history \"]|length) %}\n{{ conversation_history [i][\"action_history \"][j] }}\n{% endfor  %}\n--\n{% endfor  %}\nUser Question: {{ question }}\n{% if action_history  %}\n{% for i in range(0, action_history|length ) %}\n{{ action_history [i] }}\n{% endfor  %}\n{% endif %}\nOutput one \"Thought\" and one \"Action\":\nTemplates written in jinja syntax \n\n[Page 37]\nST ANFORD LAMLooping in SPINACH\n\u2022SPINACH needs to explore different queries during iteration\n\u2022Run policy prompt with temperature 1.0\n\u2022LLM can fall into a loop of executing the same action\nexecute_sparql (\u201cSELECT ?x WHERE {wd:Q1 wdt:P1 ?x.}\u201d)\nDid not find any results\nexecute_sparql (\u201cSELECT ?x WHERE {wd:Q1 wdt:P1 ?x.}\u201d)\nDid not find any results\nexecute_sparql (\u201cSELECT ?x WHERE {wd:Q1 wdt:P1 ?x.}\u201d)\n\u2026\nQuiz: Why would this happen?\nQuiz: How to solve this?\n\n[Page 38]\nST ANFORD LAMLooping in SPINACH\n\u2022If repeated actions are found, \n\u2022Agent resets exploration state to the one before the repetition\n\u2022Continues from there\n\u2022We cap number of actions by either:\n\u202215 actions after taking \u201crollbacks\u201d into account\n\u2022a total of 30 actions including \u201crollbacks\u201d\n\n[Page 39]\nST ANFORD LAMThe SPINACH Agent - Example\n\n\n[Page 40]\nST ANFORD LAMThe SPINACH Agent - Example\n\n\n[Page 41]\nST ANFORD LAMThe SPINACH Agent - Example", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 39]\nST ANFORD LAMThe SPINACH Agent - Example\n\n\n[Page 40]\nST ANFORD LAMThe SPINACH Agent - Example\n\n\n[Page 41]\nST ANFORD LAMThe SPINACH Agent - Example\n\n\n[Page 42]\nST ANFORD LAM\nP1303: instrument\nP1416 : affiliation\nP69:     educated at\nQ98035717 : \nUniversity of Washington \nSchool of Music\nQ219563 : \nUniversity of Washington\n\n[Page 43]\nST ANFORD LAMLecture Goals\nTeach the Agentic Approach so You Can Use It in Your Projects!\n\u2022Knowledge Graphs \u2013 aka Knowledge Bases (KB) \n\u2022Why KBs? \n\u2022Why is KBQA (KB Question Answering) Challenging? \n-- Previous Work\n\u2022The Agentic Approach for KBs\n\u2022How to Get a Good KBQA Dataset? \n\u2022Evaluation\n\u2022Applying the Agentic Approach to SQL Databases\n\n\n[Page 44]\nST ANFORD LAMKBQA Data Sets (Crowdsourcing)\n\u2022Datasets with natural questions originally collected through \nsearch engines or crowdsourcing\n\u2022WebQuestionSP  (Yih et al., 2016)\n\u2022QALD datasets ( Usbeck  et al., 2017, 2018, 2023; Perevalov  et al., 2022)\n\u2022RuBQ  (Korablinov  and Braslavski , 2020)\n\u2022SimpleQuestions  (Bordes et al., 2015)\nSimple Queries\uf0df Quiz: Is this OK? \n\n[Page 45]\nST ANFORD LAM\nKBQA Data Sets (Synthesized)\n\u2022Datasets with synthetically generated logical forms & questions\n\u2022ComplexWebQuestions  (Talmor and Berant , 2018)\n\u2022GrailQA  (Gu et al., 2021)\n\u2022KQA Pro (Cao et al., 2022a)\n\u2022CFQ ( Keysers  et al., 2020)\n\u2022CWQ (Talmor and Berant , 2018)\n\u2022LC-QuAD2 (Dubey et al., 2019)\uf0df Quiz: Is this OK? \nLimited NL variety\n& Unique query patterns", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 46]\nST ANFORD LAMWikidata  SPARQL \nForum \nhttps://m.wikidata.org/wiki/Wikidata:Request_a_query\nInitial Question\nResponse with \nSPARQL\nRefined SPARQL\nAcknowledgement\n\u2022To help Wikidata  users write \nSPARQL queries\n\u2022People exchange \nconversations on how to \nwrite SPARQLs\n\u2022The queries are real, \nbut difficult\n\n[Page 47]\nST ANFORD LAMThe SPINACH dataset\n\u2022Based on discussions on the Wikidata  Request Query forum from July 2016 \u2013 May 2024\n\u2022Categories of clauses removed: \n\u2022Wikimedia presentation queries\n\u2022Questions on complex SPARQL code\n\u2022Queries obscured by optimizations\n\u2022Formatting clauses\n\u2022Annotating Natural Questions: \n\u2022Disambiguate entities and properties\n\u2022Natural verbalizations\n\u2022Accurately capturing optional clauses and projections\n\u2022155 validation examples and 165 test examples \uf0dfQuiz:  This number seems small. Is this enough?\n\n[Page 48]\nST ANFORD LAMSPINACH dataset: \nnatural questions + complex logical forms\nSPINACH dataset: natural questions + complex logical forms\n\n\n[Page 49]\nST ANFORD LAMLecture Goals\nTeach the Agentic Approach so You Can Use It in Your Projects!\n\u2022Knowledge Graphs \u2013 aka Knowledge Bases (KB) \n\u2022Why KBs? \n\u2022Why is KBQA (KB Question Answering) Challenging? \n-- Previous Work\n\u2022The Agentic Approach for KBs\n\u2022How to Get a Good KBQA Dataset? \n\u2022Evaluation\n\u2022Applying the Agentic Approach to SQL Databases\n\n\n[Page 50]\nST ANFORD LAMThe F1 metric", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 50]\nST ANFORD LAMThe F1 metric\n\n\n[Page 51]\nST ANFORD LAMBut: traditional EM & F1does not work\nwhat are the top 3 counties with most people in \nSouth Dakota?\nMinnehaha \nCounty206,930\nPennington \nCounty115,903\nLincoln County 73,238Predicted Gold\nMinnehaha \nCounty\nPennington \nCounty\nLincoln County\nQuestion: Do we want to penalize this column?\n\n\n[Page 52]\nST ANFORD LAMNew Metric: row-major  EM & F1\nQ5994 piano 99\nQ8350 trombone 11\nQ8338 trumpet 8Predicted Gold\nQ8350 11\nQ5994 99\nQ1467690 2\nQ83509 1\nAssigned \nrows\nStep 1: Run an assignment algorithm to maximize total overlap  (recall)\nWithout penalizing extra columns in prediction\n\ud835\udc66\ud835\udc56 a row in gold\n\ud835\udc66\u2032\ud835\udc56 a row in prediction\nMatching rows with 0 recall is not allowed\n\n[Page 53]\nST ANFORD LAMNew Metric: row-major  EM & F1\nQ5994 piano 99\nQ8350 trombone 11\nQ8338 trumpet 8Predicted Gold\nQ8350 11\nQ5994 99\nQ1467690 2\nQ83509 1\nAssigned \nrows\nStep 2: Calculate true positives  as sum of recalls in assigned rows\n=2\n\n[Page 54]\nST ANFORD LAMNew Metric: row-major  EM & F1\nQ5994 piano 99\nQ8350 trombone 11\nQ8338 trumpet 8Predicted Gold\nQ8350 11\nQ5994 99\nQ1467690 2\nQ83509 1\nAssigned \nrows\nFalse \nPositives\n=1Step 3: Each unassigned row in prediction counts as a false positive\n\ud835\udc5b\u2032: number of rows in prediction\n\ud835\udc5f: number of rows in matching", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 55]\nST ANFORD LAMNew Metric: row-major  EM & F1\nQ5994 piano 99\nQ8350 trombone 11\nQ8338 trumpet 8Predicted Gold\nQ8350 11\nQ5994 99\nQ1467690 2\nQ83509 1\nAssigned \nrows\n=2\nFalse \nNegatives\nFalse \nPositives\nStep 4: Each unassigned row in gold counts as a false negative\nplus sum of 1 \u2013 recall in the matching (0 in this case)\n\ud835\udc5b: number of rows in gold\n\ud835\udc5f: number of rows in matching\n\n[Page 56]\nST ANFORD LAMNew Metric: row-major  EM & F1\nQ5994 piano 99\nQ8350 trombone 11\nQ8338 trumpet 8Predicted Gold\nQ8350 11\nQ5994 99\nQ1467690 2\nQ83509 1\nAssigned \nrows\nFalse \nNegatives\nFalse \nPositives\nStep 5: Calculate F1 with the usual formula\n\ud835\udc391=2\ud835\udc61\ud835\udc5d\n2\ud835\udc61\ud835\udc5d+\ud835\udc53\ud835\udc5b+\ud835\udc53\ud835\udc5d = 0.66\n\n[Page 57]\nST ANFORD LAMResults on the SPINACH Dataset\nFine-tuned LLMs Know More, Hallucinate Less with Few -Shot Sequence -to-Sequence Semantic Parsing over Wikidata , Xu et al, EMNLP 2023\nThink -on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph , Sun et al, ICLR 2023Fine-tuned model ->\nSOTA.  Ask LLM to walk the graph ->\nSPINACH agent achieves considerable gain over prior approaches!\n\n[Page 58]\nST ANFORD LAMResults on Other Datasets\nZero -shot ICL (in -context learning) achieves new SOTA on QALD Wikidata  datasets\nComes within 1.6 F1 on WikiWebQuestions  to WikiSP , fine -tuned on WikiWebQuestions  \n\n[Page 59]\nST ANFORD LAMThe Importance of Agent Actions\nAll actions make meaningful contribution to the agent performance", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 59]\nST ANFORD LAMThe Importance of Agent Actions\nAll actions make meaningful contribution to the agent performance\n\n[Page 60]\nST ANFORD LAMLive Demo:\nhttps://spinach.genie.stanford.edu/\nCode:\nhttps://github.com/stanford -oval/spinach  \nAs a bot on Wikidata :\nhttps://www.wikidata.org/wiki/User:SpinachBot  \n\n[Page 61]\nST ANFORD LAMError Analysis\n40%: Property -related problems : \n         Fails to fetch the correct property or incorrectly uses a property\n30%: Complicated SPARQL : \n         Fails to write complex SPARQL to fetch results. \n15%: Not enough exploration: \n         Insufficient exploration within limit of actions allowed.\n10%: Inaccurate semantic parsing : \n         LLM injecting an extra clause.\n  5%: Formatting issues\n\n[Page 62]\nST ANFORD LAMPotential Solution #1\n\u2022Expand navigations further. Use a tree to explicitly keep track \nof different actions one can take at a step.\nTree of Thoughts: Deliberate Problem Solving with Large Language Models , Yao et al, NeurIPS  2023", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 62]\nST ANFORD LAMPotential Solution #1\n\u2022Expand navigations further. Use a tree to explicitly keep track \nof different actions one can take at a step.\nTree of Thoughts: Deliberate Problem Solving with Large Language Models , Yao et al, NeurIPS  2023\n\n[Page 63]\nST ANFORD LAMSPINACH Deployed on Wikidata  Forum\n\u2022600+ conversations in the wild: all real and hard queries!\n\u2022198 randomly selected conversations\n\u2022Success rate: 78% (154 cases)\n\u2022Failures: 22% (44 cases)\n\u202250% (22 cases) similar to queries in the dataset\n\u202250% (22 cases) are not similar:\nunderspecified queries, query correction/modification, \nstring manipulation\n\u2022Next step: Can we fine -tune an open -source model? https://m.wikidata.org/wiki/Wikidata:Request_a_query\nA higher success rate in practice!\n\n[Page 64]\nST ANFORD LAMLecture Goals\nTeach the Agentic Approach so You Can Use It in Your Projects!\n\u2022Knowledge Graphs \u2013 aka Knowledge Bases (KB) \n\u2022Why KBs? \n\u2022Why is KBQA (KB Question Answering) Challenging? \n-- Previous Work\n\u2022The Agentic Approach for KBs\n\u2022How to Get a Good KBQA Dataset? \n\u2022Evaluation\n\u2022Applying the Agentic Approach to SQL Databases\n\n\n[Page 65]\nhttps://www.nytimes.com/2024/04/17/us/hawaii\ncontractors -campaign -donations.html\n\n[Page 66]\nhttps://www.nytimes.com/2024/04/17/us/hawaii -\ncontractors -campaign -donations.html\n100+ hours\nfor data journalists with experience\n\n[Page 67]\nIncrease Truthful Reporting\nin Journalism \nCan Journalists Just TALK  to Data\nWithout Needing a Data Scientist?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 66]\nhttps://www.nytimes.com/2024/04/17/us/hawaii -\ncontractors -campaign -donations.html\n100+ hours\nfor data journalists with experience\n\n[Page 67]\nIncrease Truthful Reporting\nin Journalism \nCan Journalists Just TALK  to Data\nWithout Needing a Data Scientist?  \n\n[Page 68]\nST ANFORD LAM\n\n\n[Page 69]\nST ANFORD LAMShicheng  Liu1   Harold Triedman4   Leah Harrison3   Eryn Davis3\nSajid Omar Farook1 Alexander Spangher5  Cheryl Phillips2  Derek Willis6\nSerdar Tumgoren2  Monica S. Lam1\n1 Stanford CS   2 Stanford Big Local News    3Columbia Journalism School\n4 Cornell Tech   5 USC   6 University of Maryland \n\n[Page 70]\nST ANFORD LAMDataTalk : Campaign Finance\n\u2022Chat with publicly available \ncampaign finance data\n\u2022Based on \n\u2022Federal Election Commission \n(FEC)\n\u2022OpenElections.org  data\n\u202236 Large, relational databases\nData Tables\n\n[Page 71]\nST ANFORD LAMAgentic Approach for Knowledge Bases\nUser Query\nSpoiler: This is not enough yet!AgentGraph Knowledge Base actions:\nSPINACH agent\nRelational Database actions:\nDataTalk  agent\nAgent responsethinks\nIssues actions\nObserves  \nresults\nget_tables ()\nretrieve_tables_details  ([table1, table2, \u2026])\nexecute_SQL (SQLquery )\nstop()", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 72]\nST ANFORD LAMThe DataTalk  Agent - Actions\nget_tables ()\n{'table_name ': 'committee_type_codes ', 'comments': \u2019\u2019},\n{'table_name ': 'party_codes ', 'comments': \u2019\u2019},\n{'table_name ': 'pac_details ', 'comments': 'based on OpenSecrets  data'}, \n{'table_name ': 'candidate_committee_linkage_2023_2024', 'comments': \n\\\"The candidate -committee linkage file contains information linking the \ncandidate's information to information about his or her \ncommittee. \\\\n\\\\nThis file shows the candidate\u2019s identification number, \ncandidate\u2019s election year, FEC election year, committee identification \nnumber, committee type, committee designation, and a linkage \nidentification number.\"},\n\u2026\n\n[Page 73]\nST ANFORD LAMThe DataTalk  Agent - Actions\nretrieve_tables_details (['contributions_by_individual\ns_2023_2024', 'committee_master_2023_2024'] )\nCREATE TABLE \"contributions_by_individuals_2023_2024\" (\n\"CMTE_ID\" VARCHAR(9), -- Filer identification number. A 9 -character alpha -\nnumeric code assigned to a committee by the Federal Election Commission\n\"AMNDT_IND\" VARCHAR(1), -- Amendment indicator. Indicates if the report\n\u2026\n);\nCREATE TABLE \"committee_master_2023_2024\" (\n\"CMTE_ID\" VARCHAR(9) PRIMARY KEY, -- Committee identification. A 9 -\ncharacter alpha -numeric code assigned to a committee by the Federal \nElection Commission. Committee IDs are unique and an ID for a specific \ncommittee always remains the same.\n\"CMTE_NM\" VARCHAR(200),\n\u2026\n)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L React Agents", "content": "[Page 74]\nST ANFORD LAMThe DataTalk  Agent - Actions\nexecute_sql (\n)\n\n\n[Page 75]\nST ANFORD LAMReal -World Experience with Journalists\n\u2022Used by journalists and journalism students\n\u2022Agentic approach is necessary\n\u2022Too many tables: need to retrieve knowledge about the tables\n\u2022Interpretation of results are difficult\n\u2022Lots of caveats on the data \n(e.g. contributions below $200 not included in some tables)\n\u2022Requires experts on the data\n\u2022Need to capture the expertise just like an apprentice\n\u2022More in the next class\n\n[Page 76]\nST ANFORD LAMConclusions\n\u2022The Agentic Approach : effective for complex knowledge base queries\n\u2022Acquire knowledge and to react to results\n\u2022Wikidata\n\u2022SPINACH dataset is based on hard real queries\n\u2022Achieves 45.3 F1 with the SPINACH dataset\n\u2022Achieves 78% accuracy in Wikidata  Request -a-Query forum\n\u2022FEC data\n\u2022The same agentic approach handles the large tables\n\u2022Discover the problem of tacit expert knowledge (next lecture)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-react-agents.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:30.987854"}
{"title": "L Agent Introduction", "content": "[Page 1]\nCS224v\nConversational Virtual Assistants \nwith Deep Learning\nLecture 3: An Introduction to Agents\n1Monica Lam\n\n[Page 2]\nSTANFORD LAMGeneral Assistants\nAdvisor, Employee TrainingResearchers, Investigative journalists\nData scientists, Journalists\nCustomer support, SalesCompliance officers, auditors, judges\nReadingInstruction followingKB retrieval \nLiterature reviewFormal reasoningQualitatitive codingJobs Skills\nWeb tasks\n-Information \nseeking\n-Filling out forms\n(call agents)\nVirtual assistants\n(Alexa, Siri)\n\n[Page 3]\nSTANFORD LAMLecture Goals\n\u2022Knowing the SOTA of knowledge/task agents \nhelps you choose topics\n\u2022Topic for the 2ndAssignment\n\u2022Hands on experience in building agents\n\u2022Landscape of general assistants\n\u2022Overview the technical techniques for creating agents\n\u2022Expose to the high -level concepts\n(Details to come in subsequent lectures)", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 4]\nSTANFORD LAMAgents at a Glance\nCategory Scope Examples Technology Intro\nInformation Seeking\n-Short form Free -text WikiChat Gen+RAG L1\nDatabases (DB) Basic YelpBot Semantic parsing L3\nHybrid: DBs + Text YelpBot , \nCompmixSUQL L3\nKnowledge Graphs (KG) WikiData Agentic Approach L3\n-Long form Free -text Storm, co -Storm Multi -agent convos L2\nFree -text + DB/KG ? ?\nAPI calling Simple APIs Alexa, Siri Slot filling L3\nCompound APIs Almond Semantic parsing\nTask-Oriented \n(Mixed initiative)API-based MultiWOZ Neural policy\nKnowledge + Task Course Assistant Genie Worksheet L3\nSocial bots Chitchat Chirpy Rule-based\nPersuasion Persuabot Gen+RAG , Gen strategy\n\n[Page 5]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\n[Page 6]\n1 . K N OW L E D GE ASS IST A NT S\n\n[Page 7]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\n[Page 8]\nQuestion answeringArchitecture\n8Corpus\nDocuments\nKnowledge Bases\nSUQLSUQL : \nThe first query language for \nstructured & unstructured data.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 7]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\n[Page 8]\nQuestion answeringArchitecture\n8Corpus\nDocuments\nKnowledge Bases\nSUQLSUQL : \nThe first query language for \nstructured & unstructured data.\n\n[Page 9]\nSTANFORD LAMWhy Knowledge Bases?\nLots of structured data in the world \n\u2022Public databases \n\u2022Schema.org (structured data for the web)\n\u2022Wikidata (knowledge graph with 15 B facts)\n\u2022Private databases\n\u2022Every company is a data company [Forbes 2018]\n\u2022Products: Retail (all of ebay ), flights, restaurants\nsongs, music, books \n\u2022Corporate: People (employees, customers, students, patients)\nOperations (finance, sales, transactions)\n\u2022Personal information: Calendar, emails\n\n[Page 10]\nSTANFORD LAMQuery Languages\n\u2022AFEW common query languages:\n\u2022Databases (SQL): Tables with a fixed schema  \u2013this lecture\n\u2022Knowledge bases (SPARQL, New4j): \nGraphs with nodes (entities), edges (properties) \n\u2022Domains are defined by database schemas\n10\n\n[Page 11]\nSTANFORD LAMPower of Query Languages\n\u2022Domain agnostic\n\u2022Allqueries of any domain \nare compositions of a FEW relational algebra operations\n\u2022Basic: Selection, Projection, Cartesian product (Join), \nUnion, Set Difference\n\u2022Extended: Sort, Aggregate Operators (Sum, Max, Avg, \u2026)\n\u2022Expressive, succinct, well -defined\n11\nAmazing\nCS Idea", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 12]\nSTANFORD LAMSemantic Parsing: NL \u2192Formal Query\n12Domain -specific\nNeural \nSemantic ParserSQL DB/API DB resultShow me the best\nJapanese restaurant \ninPalo Alto\nResponse \nGenerator\n12For a given domain:\nSELECT *\nFROM restaurants WHERE\n\u2018japanese \u2019 = ANY(cuisines) \nAND location = \u2018Palo Alto\u2019\nORDER BY rating DESC LIMIT 1\nI searched for the \nbest Japanese \nrestaurant in Palo \nAlto and found \nDaigo . It has a 4.5 \nrating on our \ndatabase and offers \nsushi and Japanese \ncuisine. \n\n[Page 13]\nSTANFORD LAMSemantic Parsing: NL \u2192Formal Query\n13DB/APINeural \nSemantic ParserSQL DB resultShow me the best\nJapanese restaurant \ninPalo Alto\nResponse \nGeneratorI searched for the \nbest Japanese \nrestaurant in Palo \nAlto and found \nDaigo . It has a 4.5 \nrating on our \ndatabase and offers \nsushi and Japanese \ncuisine. \n13Can we generalize to all domains?\nschemaschema\nCREATE TABLE restaurants (\nid TEXT PRIMARY KEY,\nlocation TEXT,\naddress TEXT,\ncuisines TEXT[],\nrating NUMERIC(2,1), \n\u2026);SELECT *\nFROM restaurants WHERE\n\u2018japanese \u2019 = ANY(cuisines) \nAND location = \u2018Palo Alto\u2019\nORDER BY rating DESC LIMIT 1\nKEY IDEA: Just supply the Schema \u2192Get an agent", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 14]\nSTANFORD LAMBasic Idea: Zero -Shot Prompt with LLMs\nYou are a semantic parser. Generate a query for a restaurant database with the \nfollowing signature:\nCREATE TABLE restaurants (\nname TEXT,\naddress TEXT,\nlocation TEXT,\nphone_number TEXT,\nopening hours TEXT, \ncuisines TEXT[],\nprice ENUM ('cheap', 'moderate', 'expensive', 'luxury'),\nrating NUMERIC(2,1),\nnum_review NUMBER,\n);\n14Future\nTopic\nHow to create effective \nsemantic parsers with LLMs?\n\n[Page 15]\nSTANFORD LAMRestaurant Assistant \nCREATE TABLE restaurants (\nname               TEXT,\naddress            TEXT,\nlocation             TEXT,\nphone_number TEXT, \nopening_hours TEXT,\ncuisines             TEXT[],\nprice                   ENUM ('cheap', 'moderate', 'expensive', 'luxury'),\nrating                  NUMERIC(2,1),\nnum_reviews NUMBER,\nreviews               FREE_TEXT,\npopular_dishes FREE_TEXT,\n);\nRestaurant Assistant\n15NUMERIC(2,1) means 2 -digit precision, 1 digit after decimal\n\n[Page 16]\nSTANFORD LAMQueries in a Conversation (Yelp)\nGenerated SQL:\nSELECT * FROM restaurants \nWHERE location = \u201cMountain View\u201d\nGPT responds directly\n16Generated SQL:\nSELECT * FROM restaurants \nWHERE location = \u201cMountain View\u201d \nAND cuisines = ANY(\u201cItalian\u201d)\nGenerated SQL:\nSELECT * FROM restaurants \nWHERE location = \u201cPalo Alto\u201d \nAND cuisines = ANY(\u201cItalian\u201d)\n\n[Page 17]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 17]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\n[Page 18]\nSTANFORD LAMWikidata : Largest, Live Knowledge Graph\n\u202215B facts, 100M entities, 10K properties, 25K contributors\n\u2022Every Wikipedia article has a corresponding entity in Wikidata\n\u2022Dataset for research in life sciences, digital humanity, etc.\n\u2022Representation: Semantic web (triples)\n\u2022Query with SPARQLSELECT ?x WHERE \n{ wd:Q41506   wdt:P112   ?x. }\nStanford Founded byWho founded Stanford? \n\n[Page 19]\nSTANFORD LAMBut Querying Wikidata is difficult\nA natural language interface can greatly expand access\nForums exist to help Wikidata users write SPARQLs \n\n[Page 20]\nSTANFORD LAM\nAgentic approach to imitate the expert worker\n1.Try out possibly a simpler query\n2.Evaluate the query\n3.Revise the query based on the result\n4.Repeat\n\n[Page 21]\nSTANFORD LAMLive demo: https://spinach.genie.Stanford.edu\n\u2022Release on Wikidata in July 2024\n\n[Page 22]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 21]\nSTANFORD LAMLive demo: https://spinach.genie.Stanford.edu\n\u2022Release on Wikidata in July 2024\n\n[Page 22]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\n[Page 23]\nSTANFORD LAMWe Need Hybrid Data\n23Structured Free Text Tasks\nRestaurantsCuisines, opening hours, \naddress, ratingReviews, popular \ndishesBook restaurant\nProductsProduct ID, cost, ratings, \nsizes, physical \ndimensions, colorDescriptions, \nreviewsPurchasing\nOrdering food\nCoursesClass number, time \noffered, instructor, \nbuilding, pre -requisitesDescriptions, \nreviewsEnrolling in courses\nBusiness \nprocessesReceipts, statements RegulationsFiling taxes\nReimbursements \nCustomer support\nMedicalPatient demographics\nPrescriptionDiagnosis historyMaking diagnoses\nInsurance claims\n\n[Page 24]\nSTANFORD LAMSUQL\n\u2022Extend SQL with two free -text LLM functions: summary , answer\n\u201cI want a family -friendly restaurant in Palo Alto \u201d\nSELECT *, summary (reviews) FROM restaurants \nWHERE location = 'Palo Alto\u2019\nAND answer (reviews, \u2018is it a family friendly restaurant\u2019) = 'Yes' LIMIT 1;Semantic Parser", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 25]\nSTANFORD LAMExample\n\"SELECT answer(reviews, 'does this \nrestaurant have non -spicy options?') \nFROM restaurants WHERE \nname ILIKE \u2018Taste' AND \nlocation = \u2018Palo Alto';\"\nSUQL\n\"SELECT *, summary(reviews) \nFROM restaurants WHERE \n'szechuan ' = ANY (cuisines) AND \nlocation = \u201cPalo Alto\u201d AND \nanswer(reviews, 'does this restaurant \nserve spicy food?') = 'Yes' LIMIT 1;\u201dExperiment: Yelp\n\n[Page 26]\nSTANFORD LAMEvaluation of YelpBot\n\u2022Dataset \n\u202255 of 100 questions need SUQL (structured & unstructured)\n\u2022Query precision : #correct results /  #results\n\u2022Optimizing compiler implemented\n100 Questions20 Conversations\n96 turns\nLinearization (1 result) 57.0% 63.4%\nLinearization (3 results) 49.7 % 61.9%\nSUQL (up to 3 results) 93.8% 90.3%\n28Liu, Shicheng et al. SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models. In Findings of NAACL 202 4\n\n[Page 27]\nSTANFORD LAMLive Demo: https:// yelpbot.genie.stanford.edu /\nLiu, Shicheng et al. SUQL Conversational Search over Structured and Unstructured Data using LLMs, ArXiv : 2311.09818\n\n[Page 28]\n1 . K N OW L E D GE ASS IST A NT S\nData can be accessed with Query Languages \n(SQL, SPARQL, SUQL for free-text+KB )\nSemantic parsing translates NL to formal QL\nSchema -parameterized semantic parsing\ntranslates NL to QL for given schema \n\n[Page 29]\n2 . A PI  C A LL S", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 28]\n1 . K N OW L E D GE ASS IST A NT S\nData can be accessed with Query Languages \n(SQL, SPARQL, SUQL for free-text+KB )\nSemantic parsing translates NL to formal QL\nSchema -parameterized semantic parsing\ntranslates NL to QL for given schema \n\n[Page 29]\n2 . A PI  C A LL S\n\n[Page 30]\nSTANFORD LAMSemantic Parsing: NL \u2192API Call\n33API CallNeural \nSemantic ParserAPI ResultMy ID is 123456, \nI want to enroll in CS224v \nfor a letter grade, \nand for 4 units \nResponse \nGeneratorYou are enrolled \nsuccessfully. \n33signatureSignature\nenum GradeType {Letter, Credit,}; \nEnroll (\nstr student_id .\nstr course_name , \nGradeType grade_type , \nint num_units\n);\nKEY IDEA: Just supply the Signature \u2192Get an agentEnroll ( student_id =\u201c123456\u201d,\ncourse_name =\u201cCS224V\u201d,\ngrade_type =Letter,\nnum_units =4)\n\n[Page 31]\nSTANFORD LAMConversational Agent for API calls\nA conversation is needed typically\n\u2022Slot filling\n\u2022Ask for missing arguments of the API \ne.g. \u201cI want to enroll in a course\u201d\n\u2022Confirm before API execution: \n\u2022Because APIs have side effects", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 31]\nSTANFORD LAMConversational Agent for API calls\nA conversation is needed typically\n\u2022Slot filling\n\u2022Ask for missing arguments of the API \ne.g. \u201cI want to enroll in a course\u201d\n\u2022Confirm before API execution: \n\u2022Because APIs have side effects\n\n[Page 32]\nA Simple Enrollment Dialogue\nASSISTANT\nWhat is your student ID? \nWhat course would you like to enroll in? \nWould you like to take it with a letter grade or Credit/ Nocredit .\nHow many units? \nPlease confirm: Your student ID is 123456, \nand you want to take CS 224V with a letter grade and for 4 units.\nCongratulations! Student 123456 is enrolled in CS224V \nwith a letter grade and for 4 units successfully.USER\nI\u2019d like to enroll in a course\n123456\nCS 224V\nWith a letter grade\n4 \nYesSlot filling\nConfirmation\n\n[Page 33]\n2 . A PI  C A LL S\nSignature -parameterized semantic parsing\ntranslates NL to QL for a given set of API signatures\n\n[Page 34]\n3 . K N OW L E D GE +  T A SK-OR IE N T E D AG E N TS\n\n[Page 35]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\u2022Motivation: a Case Study\n\u2022Design of the Genie WorkSheet\n\u2022Genie Runtime System", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 34]\n3 . K N OW L E D GE +  T A SK-OR IE N T E D AG E N TS\n\n[Page 35]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\u2022Motivation: a Case Study\n\u2022Design of the Genie WorkSheet\n\u2022Genie Runtime System\n\n[Page 36]\nSTANFORD LAMTasks Structured Free Text\nRestaurants Book restaurantCuisines, opening \nhours, address, phone \nnumberReviews, popular \ndishes\nProductsPurchasing\nOrdering foodProduct ID, cost, \nratings, sizes, color, \nphysical dimensions Descriptions, reviews\nCourses Enrolling in coursesClass number, time \noffered, instructor, \nbuilding, pre -requisitesDescriptions, reviews\nBusiness \nprocessesFiling taxes\nReimbursements \nCustomer supportReceipts, statements Regulations\nMedicalMaking diagnoses\nInsurance claimsPatient demographics\nPrescriptionDiagnosis historyTask Assistants\n39", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 37]\nSTANFORD LAMTasks Structured Free Text\nRestaurants Book restaurantCuisines, opening \nhours, address, phone \nnumberReviews, popular \ndishes\nProductsPurchasing\nOrdering foodProduct ID, cost, \nratings, sizes, color, \nphysical dimensions Descriptions, reviews\nCourses Enrolling in coursesClass number, time \noffered, instructor, \nbuilding, pre -requisitesDescriptions, reviews\nBusiness \nprocessesFiling taxes\nReimbursements \nCustomer supportReceipts, statements Regulations\nMedicalMaking diagnoses\nInsurance claimsPatient demographics\nPrescriptionDiagnosis historyTask Assistants Should also Provide Knowledge\n40\n\n[Page 38]\nSTANFORD LAMA Case Study\nCourse Enrollment at Stanford\nLet\u2019s consider a CS MS student\n\n[Page 39]\nSTANFORD LAM\nAI\n\n[Page 40]\nSTANFORD LAM\nComputational\nBiology\n\n[Page 41]\nSTANFORD LAM\nComputer and\nNetwork Security\n\n[Page 42]\nSTANFORD LAM\nSoftware\nTheory\n\n[Page 43]\nSTANFORD LAM\nSystems\n\n[Page 44]\nSTANFORD LAM\nTheory\n\n[Page 45]\nSTANFORD LAM 48\nI am studying computer science. \nI want to complete the significant \nimplementation requirement.\nWhat are some of the options that require the \nleast amount of work?\nWhat course would you like to enroll in?Course Enrollment Agent\nUser:", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 44]\nSTANFORD LAM\nTheory\n\n[Page 45]\nSTANFORD LAM 48\nI am studying computer science. \nI want to complete the significant \nimplementation requirement.\nWhat are some of the options that require the \nleast amount of work?\nWhat course would you like to enroll in?Course Enrollment Agent\nUser: \n\n[Page 46]\nSTANFORD LAMA Course Enrollment Assistant\nStudents need to consult: \n\u2022All offered courses\n\u2022Description, instructors, \nofferings, and units\n\u2022Unique to each \ndepartment and \nspecialization\n\u2022Program sheets\u2022Ratings, reviews, hours \nof work, enrollment \noutcomes, \nand sequencing\nWhat course would you like to enroll in?Course Enrollment Agent\n\n[Page 47]\nSTANFORD LAM\n\n\n[Page 48]\nSTANFORD LAM1. Stanford Course Enrollment Form\nForm Name Predicate Kind Type Name Enum Values Description\nMain WS course_enrollment\ninput CourseToTake course_to_take The course to enroll\ninput StudentInfo student_info_details Information on the student\nStudentInfo worksheet\ninput str student_name Name of the student\ninput str student_id Student's ID number\ninput str student_email_address Student's email address\nCourseToTake worksheet\ninput str course_name Name of the course \ninput Enum grade_type The desired grading basis \nCredit/No Credit\nLetter\ninput int num_units The number of units taken\ninput confirm confirm Confirm the course", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 49]\nSTANFORD LAM2. Course Assistant Knowledge Corpus\ncourses DB\ninternal; primary int course_id\ninternal int max_units\ninternal str title\ninternal str grading\ninternal List[str] course_codes\ninternal List[str] general_requirements\ninternal int min_units\ninternal str description\ninternal int average_hours_spent\ninternal Enum foundations_requirements\nlogic_automata_and_complexity\nprobability\nalgorithmic_analysis\ncomputer_organization_and_systems\nprinciples_of_computer_systems\ninternal bool significant_implementation_requirements\ninternal Enum breadth_requirement\nformal_foundations\nlearning_and_modeling\nsystems\npeople_and_society\ninternal List[str] prerequisite_course_codesofferings DB\ninternal int course_id\ninternal Enum days\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\ninternal str start_time\ninternal str end_time\ninternal List[str] instructor_names\ninternal Enum season\nautumn\nwinter\nspring\nsummerprograms DB\ninternal; primary int program_id\ninternal Enum level\nMS\nBS\nPhD\ninternal Enum specialization\nAI\nComputational Biology\nHuman -Computer Interaction\nInformation Management & Analytics\nVisual Computing\nSoftware Theory\nSystems\nTheoretical Computer Science\ngeneral\nComputer & Network Security\ninternal str sheet_requirementsratings DB\ninternal; primary int rating_id\ninternal int course_id\ninternal List[str] instructor_names\ninternal int average_rating\ninternal int num_ratings\ninternal int term_id\ninternal int start_year\ninternal int end_year\ninternal Enum season", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "internal str sheet_requirementsratings DB\ninternal; primary int rating_id\ninternal int course_id\ninternal List[str] instructor_names\ninternal int average_rating\ninternal int num_ratings\ninternal int term_id\ninternal int start_year\ninternal int end_year\ninternal Enum season\nautumn\nwinter\nspring\nsummer\ninternal List[str] reviews", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 50]\nSTANFORD LAMTranslates Questions intoSUQL\nI am a sophomore studying computer science. I \nwant to complete the significant \nimplementation requirement.\nWhat are some of the options that require the \nleast amount of work?\nSELECT title, course_codes , description, \naverage_hours_spent\nFROM courses WHERE \nsignificant_implementation_requirement = TRUE \nORDER BY average_hours_spent LIMIT 5;\n\n\n[Page 51]\nSTANFORD LAM\n\n\n[Page 52]\nSTANFORD LAM\nConsideration: Time Offered\n\n[Page 53]\nSTANFORD LAM\nConsideration: Sort by Course Rating\n\n[Page 54]\nSTANFORD LAMCredit or Letter?  Depends on the Workload\n\n\n[Page 55]\nSTANFORD LAM\nPick Max. Number of Units\n\n[Page 56]\nSTANFORD LAMSummary\n\u2022There are many possible dialogues \neven for a task with only a few parameters\n\u2022Task-oriented agents have mixed initiatives\n\u2022Agent initiative to gather required information\n\u2022User initiative to query related information, \nchange their mind etc.\nCLIENT BANKER\nHow much money do you wish to transfer?\nHow much money do I have in my account?\n\n[Page 57]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\u2022Motivation: a Case Study\n\u2022Design of the Genie WorkSheet\n\u2022Genie Runtime System\n\n[Page 58]\nSTANFORD LAMConversational Agent Architecture", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 57]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\u2022Motivation: a Case Study\n\u2022Design of the Genie WorkSheet\n\u2022Genie Runtime System\n\n[Page 58]\nSTANFORD LAMConversational Agent Architecture\n\n\n[Page 59]\nSTANFORD LAMGenie Worksheet\nSuccinct / powerful dialogue agent representation\nFor the programmer\n\u2022 High -level declarative task specification\n\u2022 Hyperlinked worksheets\n\u2022 Variables: information pertinent to the task\n\u2022 Actions when variables are assigned \n& worksheet filled\n\u2022 Expressiveness similar to web forms\nFor the agent run -time\n\u2022 Filled -in worksheets serve as the conversational state\n\u2022 Interpreted easily to implement a flexible \nagent policy\n\u2022 Succinct context for contextual semantic parsing Form \nNameKind Type NameEnum \nValuesDescription Action\nMainWork\nsheetenrollmentEnroll( Student.id , \nCourse.name , \nCourse.grade_type , \nCourse.num_units ) \ninput Course course The course to enroll\ninput Student student Student info\nStudentWork\nsheet\ninput str name Name of the student\ninput str id Student's ID number\ninput str email Student's email\nCourseWork\nsheet\ninput str name Name of the course \ninput Enum grade_type Grading basis \nCredit Get a credit/no credit\nLetter Get a letter grade\ninput int num_units Number of units taken\ninput confirm confirm Confirm the course", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 60]\nSTANFORD LAMStanford Course Enrollment Form\nForm Name Predicate Kind Type Name Enum Values Description\nMain WS course_enrollment\ninput CourseToTake course_to_take The course to enroll\ninput StudentInfo student_info_details Information on the student\nStudentInfo worksheet\ninput str student_name Name of the student\ninput str student_id Student's ID number\ninput str student_email_address Student's email address\nCourseToTake worksheet\ninput str course_name Name of the course \ninput Enum grade_type The desired grading basis \nCredit/No Credit\nLetter\ninput int num_units The number of units taken\ninput confirm confirm Confirm the course \n\n[Page 61]\nSTANFORD LAMRequired, Enum, Predicated Inputs\nForm \nName Predicate Kind Type NameEnum \nValues Question Required\nMain worksheet Insurance\ninput str curr_insurance Who is your current provider? no\ninput enum marital_status Are you married? yes\nsingle\nmarried\nmarital_status\n== married input str spouse_name What is the name of your spouse? yesASSISTANT\nGreat. I need some information. \nWhat is your marital status?\nAre you married or single? \nGot it. What is your spouse\u2019s name?USER\nI\u2019m currently an AllState customer, I am \nconsidering changing the provider. \nWhat do you mean by marital status?\nI\u2019m married.Type Description\nRequiredRecord all relevant info but don\u2019t \nask for it if not required\nEnum Explain possible values if necessary\nPredicated Needed if predicate is true", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 62]\nProgrammable Agent Policies\nAgent Policy \n\u2022what the agent says and does\nRuntime system\n\u2022When a variable is set, \ncorresponding action is executed\nForm Name Name Description Action\ncourse_name Name of the course that \nthe user wants to takeIf not check_availability (course_name ):\nsay(\u201cUnfortunately, there are no seats available. \nI can help you find other courses.\u201d)\nself.course_name = NoneASSISTANT\nUnfortunately, there are no seats available. \nI can help you find other courses. STUDENT\nI want to take CS 224V in the fall quarter\n\n[Page 63]\nProgrammable Agent Policies (cont.)\nRuntime system\n\u2022When a worksheet is completed, execute the worksheet action\n\u2022Worksheet variables are referred to as worksheet_name:variable_name\nForm Name Worksheet Action\nEnrollment Enroll \n(student_id = enrollment.student_id ,   \ncourse_name = enrollment.course_name , \ngrade_type = enrollment.grade_type ,  \nnum_units = enrollment.num_units )\n\n[Page 64]\nSTANFORD LAMSet of Built -in Action Primitives\nType Explanation\nsay (str) What the agent says to the user\npropose ( ws, [fld,val ]*) Propose to open worksheet ws, \nwith initialized values for the different fields\nexitws ()Close a worksheet, and the action associated \nwith the worksheet is executed.", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 65]\nSTANFORD LAMDB Worksheet Contains the Schema\nForm \nName Kind Type Enum Values\ncourses DB\ninternal; primary int course_id\ninternal int max_units\ninternal str title\ninternal str grading\ninternal List[str] course_codes\ninternal List[str] general_requirements\ninternal int min_units\ninternal str description\ninternal int average_hours_spent\ninternal Enum foundations_requirements\nlogic_automata_and_complexity\nprobability\nalgorithmic_analysis\ncomputer_organization_and_systems\nprinciples_of_computer_systems\ninternal bool significant_implementation_requirements\ninternal Enum breadth_requirement\nformal_foundations\nlearning_and_modeling\nsystems\npeople_and_society\ninternal List[str] prerequisite_course_codes\n\n[Page 66]\nSTANFORD LAMOutline\n1.Knowledge Assistants\n\u2022Databases\n\u2022Knowledge graphs\n\u2022Hybrid data  \n2.API Agents\n3.Knowledge + Task -Oriented Agents\n\u2022Motivation: a Case Study\n\u2022Design of the Genie WorkSheet\n\u2022Genie Runtime System\n\n[Page 67]\nSTANFORD LAMRun-Time System Architecture\nExecutive Control Software\n\u2022Interpret & update Genie Worksheet\n\u2022Provide turn -specific instruction to LLM \n(Succinct context: Worksheet state + 1 turn of dialogue)\nGenieGenie WorkSheet\nAction Worksheet\nHybrid Knowledge SchemaExecutive Control\nLLM\n\n[Page 68]\nSTANFORD LAMSemantic Parsing\n\u201cMy ID is 123456, \nI want to enroll in CS224v and for 4 units\u201d \nEnroll: student_id =\u201c123456\u201d,\ncourse_name =\u201cCS224V\u201d,\nnum_units =4)Assigns variables in the worksheet", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 68]\nSTANFORD LAMSemantic Parsing\n\u201cMy ID is 123456, \nI want to enroll in CS224v and for 4 units\u201d \nEnroll: student_id =\u201c123456\u201d,\ncourse_name =\u201cCS224V\u201d,\nnum_units =4)Assigns variables in the worksheet\n\n[Page 69]\nSTANFORD LAMTranslates Knowledge Queries\n\u201cWhat is a course that satisfie s the significant implementation requirement? \u201d\nAnswer(SELECT * FROM Courses WHERE\nsignificant_implementation_requirements == true); \n\n[Page 70]\nSTANFORD LAMKnowledge + Tasks\n\u201cI want to take a course \nthat satisfies the significant implementation requirements for 4 units\u201d\nEnroll: \ncourse_name =Answer(SELECT * FROM Courses \nWHERE max_units >= 4 and\nsignificant_implementation_requirements == true); \nnum_units =4; Semantic Parser in Run -time\n\n[Page 71]\nSTANFORD LAMRun-Time System\nAgent Poicy : Output: Set of agent dialogue acts: A\nQuery: \n\u2022If missing required parameter: Add \u201cask parameter\u201d to A\n\u2022If query is complete, Evaluate query, Update variables\nWorksheet:\n\u2022If a variable is assigned , Add \u201cask for confirmation\u201d to Aor Execute action\n\u2022If a required field is unfilled, Add \u201dask for field\u201d to A\n\u2022If a form is complete, Add \u201cask for confirmation\u201d to Aor Execute actionContextual Semantic Parser : \nAssign variables to worksheet, generate database queries \nResponse Generation: Generate response from A", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 72]\nSTANFORD LAMThe Agent Policy Automatically \nHandles Many Possible Dialogues\nASSISTANT\nI am enrolling you in CS 224V for letter grade and 3 units, \nwould you like to confirm your enrollment?\nYes, CS 224V is allowed for 3 \u20134 units.\nI am enrolling you in CS 224V for letter grade and 4 units, \nwould you like to confirm your enrollment?USER: \nI would like to take CS 224V for letter grade and 3 units.\nWait, does CS 224V allow for 4 units?\nThen make it 4 units.\nYes!\n\n[Page 73]\nNote Conversations Can be Long\nUSER ASSISTANT\nI want to take a course for 3 units to fulfill my requirements\nSure, I can help you with that. \nHere are some of the top rated courses: CS 224V, CS 242, CS 221\nI want some easy, undergraduate level courses.\nI looked for easy, undergraduate courses. \nHere are some of them: CS 106A, CS 108, CS 124\n\u2026 The student asks for 15 more courses \u2026\nThe algorithm course looks interesting, I will take it.\nThat sounds great. I am enrolling you in CS 163 for 3 units. \nWould you like to take it for Pass/No Pass or Letter Grade\n\n[Page 74]\nSTANFORD LAMContextual Semantic Parsing\nUse the worksheet state as a concise context\n77Agent \nPolicyContextual \nSemantic ParserFilled -in \nWorksheet Updated \nWorksheetsThe algorithm course \nlooks interesting, \nI will take it\nResponse \nGenerator\n77Worksheet\nFuture\nTopic\nHow to create effective contextual\nsemantic parsers with LLMs?", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
{"title": "L Agent Introduction", "content": "[Page 75]\nSTANFORD LAMComparison with GPT (Function Calling )\nSP Acc:    Semantic Parsing Accuracy\nEx Acc:    Execution Accuracy\nDA Acc:  Agent Act Accuracy\nGoal CR: Goal Completion Rate\n\n\n[Page 76]\n3 . K N OW L E D GE +  T A SK-OR IE N T E D AG E N TS\nGenie Worksheet is a Declarative Programming Language \n\u2022Worksheet specifies the task at a high level\n\u2022Database schemas supply relevant knowledge\nThe run -time system support rich dialogues automatically\n\u2022Interprets the worksheet\n\u2022Parses input, performs queries & actions, asks questions\n\n[Page 77]\nSTANFORD LAMAssignment 2\n\u2022Try out the Genie Worksheet -based course advisor\n\u2022Write a simple agent using Genie Worksheet", "url": "https://web.stanford.edu/class/cs224v/lectures_2024/l-agent-introduction.pdf", "source": "CS224V Website", "type": "course_pdf", "timestamp": "2024-11-30T14:01:36.107339"}
