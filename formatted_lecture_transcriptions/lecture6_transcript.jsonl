{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Sharing a screen before the presentation", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Sharing a screen before the presentation\n\nContent: Dr. Xia: Do you want to share the. Screen or the slides from your end? Yes. Let me show you. You're currently muted right now. Once we start the presentation. I'll make it big once.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Sharing_a_screen_before_the_presentation", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 13000, "end_ms": 109205}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Projects in CPD", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Projects in CPD\n\nContent: Today we are going to talk about projects. We are soliciting feedback from everybody in the class. Everybody has to write down what you think about every single project. Class participation is mandatory for people other than those who are taking cpd.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Projects_in_CPD", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 109665, "end_ms": 222695}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Projects in Cardiology with Dr. Xia", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Projects in Cardiology with Dr. Xia\n\nContent: Dr. Zhao from the Mayo Clinic has an exciting project to share with you. The project is going to be an AI based mini cardiologist to enhance the patient care in the cardiology field. I believe this will be a very exciting project that to inspire people in this classroom.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Projects_in_Cardiology_with_Dr._Xia", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 223195, "end_ms": 292411}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Cardio Agent System", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Cardio Agent System\n\nContent: The cardio agent system is designed to have to be a self improving genetic framework. The goal would be for the system to provide evidence based recommendations in the field of cardiovascular medicine. The main goal is to see with the system's help can we really improve the time and quality on our cardiac patient care.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Cardio_Agent_System", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 292563, "end_ms": 626485}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > RAG Cardiology System", "content": "Title: CS224V Lecture 6 > Chapter Summaries > RAG Cardiology System\n\nContent: I'm open to any questions. Are the responses for the RAG system going to be grounded in any large medical databases like PubMed or how are you expecting to ground this information? And just one more question is that you mentioned that this could be deployed in like more rural settings.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_RAG_Cardiology_System", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 627425, "end_ms": 715645}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Good Medicine or Bad Medicine?", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Good Medicine or Bad Medicine?\n\nContent: In practice there are some ways or some things we do that has strong evidence to support. But in areas that's more vague, sometimes it's actually like case by case approach and there is no true right or wrong answer. Let us know if you want to be involved in this.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Good_Medicine_or_Bad_Medicine?", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 724705, "end_ms": 871065}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Projects for Computers: The Receptionist", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Projects for Computers: The Receptionist\n\nContent: Stanford student Ilya proposes a hands free design user interface to ChatGPT or any other LLM based agents. It's AI agents for better content filtering. The challenges is multi user conversation.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Projects_for_Computers:_The_Receptionist", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 874405, "end_ms": 1622275}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > An Exploiting an LLM", "content": "Title: CS224V Lecture 6 > Chapter Summaries > An Exploiting an LLM\n\nContent: How do you balance flexibility of the agent to allow, oh, a site could be useful in some cases, but not others. But also prevent users from exploiting it. Maybe using cognitive behavioral insights or psychological evidence, try to make the app more human centered.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_An_Exploiting_an_LLM", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 1627255, "end_ms": 1793239}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Using Learning-Based Measures in Robotics", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Using Learning-Based Measures in Robotics\n\nContent: Allison John and Emily park are pitching an idea about using LLMs in robotics. One application of LLMs to robotics is called autonomous perception. If there are any mentors who have some experience in robotics, please let us know.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Using_Learning-Based_Measures_in_Robotics", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 1793287, "end_ms": 1877313}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Projects for the Multimodal World", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Projects for the Multimodal World\n\nContent: Project is like for perceiving like a scene or something like that. Do you plan on fine tuning or using your own image segmentation model? Or do you plan to take the multimodal data and feeding it to the agent system? Everybody will be assigned a mentor from our wonderful teaching staff.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Projects_for_the_Multimodal_World", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 1877409, "end_ms": 2088435}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > GNU Worksheets Project Pitch", "content": "Title: CS224V Lecture 6 > Chapter Summaries > GNU Worksheets Project Pitch\n\nContent: A new project pitches is to generate synthetic data using a teacher model. The idea is to fine tune the model using this data for having a faster semantic parsing. The challenges that might occur in this work are how do you generalize to multiple APIs.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_GNU_Worksheets_Project_Pitch", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2091975, "end_ms": 2222533}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Projects in the first semester", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Projects in the first semester\n\nContent: Next three classes for people to talk about projects. You have to make a proposal by Wednesday. Every project will be mentored. I'm really looking forward to this quarter where we invent this future together.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Projects_in_the_first_semester", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2222669, "end_ms": 2567101}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > What kind of projects are available?", "content": "Title: CS224V Lecture 6 > Chapter Summaries > What kind of projects are available?\n\nContent: All right, so I want to talk a little bit about what kind of projects are available. There are really three themes. One is the concept of apprentice training and the other one is the introduction of intermediate representations. It's important when you try to scope your project whether you can handle this in this, the seven weeks.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_What_kind_of_projects_are_available?", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2567253, "end_ms": 2641911}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Three things we can do to improve access to data", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Three things we can do to improve access to data\n\nContent: There is a lot of things we can do to accelerate the generation of knowledge and improve access. We have already talked about the Data Talk project. We also have talked about History Chat. Sina has talked about how we are able to deal with old copra.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Three_things_we_can_do_to_improve_access_to_data", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2642023, "end_ms": 2700305}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Projects for Multilingual News Platform", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Projects for Multilingual News Platform\n\nContent: Tiffany Luo: We are proposing a cross lingual, multi perspective news platform. We have made a proposal on this and we actually have a video. Can you hear? Do you mind if I try?", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Projects_for_Multilingual_News_Platform", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2700915, "end_ms": 2760375}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Cross-Linguistic Multi- Perspective News Platform", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Cross-Linguistic Multi- Perspective News Platform\n\nContent: We focus on multiple perspectives in salient global events. Our goal is to create a cross lingual multi perspective news platform. It will source from 10 different languages. With our platform, the general public can become better informed. Please consider supporting our initiative.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Cross-Linguistic_Multi-_Perspective_News_Platform", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2771675, "end_ms": 2947705}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > If You Would Join the Project", "content": "Title: CS224V Lecture 6 > Chapter Summaries > If You Would Join the Project\n\nContent: The Brown Institute is looking for more people to join the project. The project has already prototyped the first prototype. A lot of people outside of the United States is actually very interested in this topic. Please let us know if you're interested.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_If_You_Would_Join_the_Project", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 2947855, "end_ms": 3092855}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > How to find misinformation on Twitter", "content": "Title: CS224V Lecture 6 > Chapter Summaries > How to find misinformation on Twitter\n\nContent: Is it possible to make it fit seven weeks to partner with a communications agency perhaps? In our country we have already built up a lot of infrastructure. Can people don't enroll this graph skill? Maybe we need a next Twitter.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_How_to_find_misinformation_on_Twitter", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 3093395, "end_ms": 3214165}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > A New Encyclopedia supported by AI", "content": "Title: CS224V Lecture 6 > Chapter Summaries > A New Encyclopedia supported by AI\n\nContent: Sloan foundation has reached out to ask us to create a workshop to talk about the opportunities of AI. There is room for a bigger version of encyclopedia supported by AI. The workshop will be held February, in the middle of February next year.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_A_New_Encyclopedia_supported_by_AI", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 3215355, "end_ms": 3437801}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Advocacy of the Agent-based Process", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Advocacy of the Agent-based Process\n\nContent: A new style of engineering, I call this the LLM based engineering. We building an agent that observes what is being done and adjusts the actions accordingly. We announced kind of like formalizing this concept of apprentice training. But what if these techniques are not good enough?", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Advocacy_of_the_Agent-based_Process", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 3437953, "end_ms": 4076685}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > The LLM: Error Analysis and Improvement", "content": "Title: CS224V Lecture 6 > Chapter Summaries > The LLM: Error Analysis and Improvement\n\nContent: We find an error, we get the expert to tell us what the issue is and we write them down as rules. These rules can be applied at different stages of my LLM system. What we think we need to do next is to apply this to many more domains.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_The_LLM:_Error_Analysis_and_Improvement", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 4077305, "end_ms": 4210777}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Deep Learning: The Problem of Intermediates", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Deep Learning: The Problem of Intermediates\n\nContent: If you have a complex problem, you have input and you have output. It is important for you to break them down into steps. It allows you to focus on a sub problem at a time.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Deep_Learning:_The_Problem_of_Intermediates", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 4210921, "end_ms": 4349299}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > AI and the Code Book", "content": "Title: CS224V Lecture 6 > Chapter Summaries > AI and the Code Book\n\nContent: The concept is applicable to resumes, medical transcripts and what have you. Can be coded up a little bit so we can do some filtering without reading each article. In order to reduce the LLM complexity and improve interpretability, you want to introduce these intermediate representation.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_AI_and_the_Code_Book", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 4349387, "end_ms": 4863395}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Web Agents and the future of AI", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Web Agents and the future of AI\n\nContent: GPT4O Vision is a general web agent if grounded. The right thing to do is to introduce an intermediate representation. Once you break it down then you can just do machine learning on identifying visual elements. In co Storm right now is there like a multimodal setting?", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Web_Agents_and_the_future_of_AI", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 4863895, "end_ms": 5137481}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Generating Code in C#", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Generating Code in C#\n\nContent: There is easy code and then there's hard code. If you wanted to generate a website, it's very reasonable because you can tell when a website fails. But if you want to write something really hard, be careful. Just be really careful.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Generating_Code_in_C#", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 5137513, "end_ms": 5250085}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Chapter Summaries > Late Night Update", "content": "Title: CS224V Lecture 6 > Chapter Summaries > Late Night Update\n\nContent: We will pick up again on Monday. And please sign up if you can, on Monday because we want to give you early feedback. All right? It.", "block_metadata": {"id": "CS224V_Lecture_6_>_Chapter_Summaries_>_Late_Night_Update", "document_type": "chapter summary", "lecture_number": 6, "start_ms": 5250245, "end_ms": 5267375}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 13000 ms - 118865 ms", "content": "Title: CS224V Lecture 6 > Transcript > 13000 ms - 118865 ms\n\nContent: Hello. Oh, hey, Emily. I think you're muted. We're just a puzzle start. Yeah, I think Monica will introduce us a little bit. And then you're up on the screen. So people can curious you. Sounds great. Yep. And do you want to share the. Screen or the slides from your end? Yes. Oh, probably from my end, so I can have control on the. On the progression of the slide. Okay. Yeah. Let me show you. Okay. You're currently muted right now. So. Can you hear me, Dr. Xia? Yes. All right, wonderful. Hi. So we're going to wait for a minute. Of course. We have 11 seconds before the class starts. Oh, no problem at all. You're very precise. Can you see that my screen from your end? I don't know who is sharing screens. We are sharing screen. We can see your screen. Yeah. Okay, then you want to make it big, right? I'll make it big once. Once we start the presentation. Yeah, I gotcha. All right. There are a lot of classes finishing just before hours, so people are late. So. But we're going to kick it off", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_13000_ms_-_118865_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 13000, "end_ms": 118865}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 103385 ms - 184693 ms", "content": "Title: CS224V Lecture 6 > Transcript > 103385 ms - 184693 ms\n\nContent: right? I'll make it big once. Once we start the presentation. Yeah, I gotcha. All right. There are a lot of classes finishing just before hours, so people are late. So. But we're going to kick it off because we want to start on time. All right, sounds great. So today what we're going to do is to talk about projects. And we have a number of pitches from people who have been thinking about these projects. Some of it is, for example, from experts like Dr. Zhao. Some of these talks are given by students in the class. After the pictures, I want to give you a little bit of guidelines or help with designing your project. We will talk about that after we finish with the individual projects and we are soliciting feedback from everybody in the class. So if you go to the webpage, to the main class webpage, on the very top there is a link to a Google Doc. We want it to be in real time. You listen to the projects. Everybody has to write down what you think about every single project. It's going to", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_103385_ms_-_184693_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 103385, "end_ms": 184693}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 170221 ms - 240699 ms", "content": "Title: CS224V Lecture 6 > Transcript > 170221 ms - 240699 ms\n\nContent: on the very top there is a link to a Google Doc. We want it to be in real time. You listen to the projects. Everybody has to write down what you think about every single project. It's going to be short, just to say a little bit like what you like about the project and some suggestions. So this is how we can. Given that we have such a large class, we want everybody to participate, and we also want to take advantage of that and get feedback from tons and tons of people. All right, so please open your webpage right now and get set up. And it is also a very easy way for us to figure out who is coming to class. As you know that class participation is mandatory for people other than those who are taking it as cpd. Okay, so with this, let me kick this off. And today the first speaker is Dr. Zhao from the Mayo Clinic, and he has an exciting project to Share with you. All right, please take it away, Dr. Xia. All right, well, thank you, Professor Dam, and just want to make sure that everyone", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_170221_ms_-_240699_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 170221, "end_ms": 240699}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 230435 ms - 297301 ms", "content": "Title: CS224V Lecture 6 > Transcript > 230435 ms - 297301 ms\n\nContent: from the Mayo Clinic, and he has an exciting project to Share with you. All right, please take it away, Dr. Xia. All right, well, thank you, Professor Dam, and just want to make sure that everyone can see our slides from here. Is it good? Sorry, I can't hear anyone. Hello? You're all good? We're all good. Sounds great. Yeah. All right, well, so thank you everyone for coming to class. And then we're trying to pitch a COBRA project that is called cardio agent, which is going to be an AI based mini cardiologist to enhance the patient care in the cardiology field. So I'm an assistant professor of medicine at the Mayo Clinic and then I have been at Stanford for the past two years and we're running collaborative projects between cardiovascular medicine and on the AI field. We're trying to explore this healthcare AI revolution. And then I believe this will be a very exciting project that to inspire people in this classroom. All right, so. So why do we need a cardio agent? So the rationale is", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_230435_ms_-_297301_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 230435, "end_ms": 297301}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 283915 ms - 348567 ms", "content": "Title: CS224V Lecture 6 > Transcript > 283915 ms - 348567 ms\n\nContent: this healthcare AI revolution. And then I believe this will be a very exciting project that to inspire people in this classroom. All right, so. So why do we need a cardio agent? So the rationale is that we need an always available cardiologist. So as clinicians or cardiologists or clinicians, we're busy. And then there are more questions for more patients to serve than we can really handle. So that actually comes with increased burden of the cardiovascular disease, which happens in the past 10 years. People are getting older, the population getting more aged, and then that actually comes to a longer waiting time and then also heavier workloads for cardiologists. And then over time with the technology advancements, there are more and more knowledge gap between our specialty and then other providers. And that becomes a little bit difficult for us to really communicate with other specialty doctors or providers. And then that takes a longer time to communicate and then to for the team is", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_283915_ms_-_348567_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 283915, "end_ms": 348567}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 336375 ms - 398591 ms", "content": "Title: CS224V Lecture 6 > Transcript > 336375 ms - 398591 ms\n\nContent: providers. And that becomes a little bit difficult for us to really communicate with other specialty doctors or providers. And then that takes a longer time to communicate and then to for the team is to really reach a final decision making. And then this actually compromised the patient care quality and actually can make more patients dying from cardiac conditions which they shouldn't be. And then this condition is even worse for hospitals in rural areas. So imagine that you're a doctor, internal medicine doctor, who cares of patients in a rural area, and then midnight, your patient had a chest pain, and then you don't have access to any cardiologists around you. And the only ethics you could have is to call out to get a cardiologist opinion who is probably 100 miles away. And then at this time you probably want to have someone like a AI assistant that can help you to sort out the first few steps and then to think, okay, is there any workup I have to complete before I send this", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_336375_ms_-_398591_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 336375, "end_ms": 398591}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 387239 ms - 457071 ms", "content": "Title: CS224V Lecture 6 > Transcript > 387239 ms - 457071 ms\n\nContent: at this time you probably want to have someone like a AI assistant that can help you to sort out the first few steps and then to think, okay, is there any workup I have to complete before I send this patient out? Or should I even call a cardiologist to wake someone up in the night and then yeah, so this is to be helpful for doctors especially, they don't have direct access to specialists. And then so we can help them with our AI system to sort out what to do instead of stuck in with your patient and like I don't know what to do like this one over here. And so our cardio agent system is designed to have to be a self improving genetic framework. So the goal would be for the system to provide in context also as evidence based recommendations in the field of cardiovascular medicine. And we're looking into several stages. First we want to have a reg agent that can answer questions appropriately. We want to make sure it's answering guideline based questions that pretty accurate. And then we", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_387239_ms_-_457071_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 387239, "end_ms": 457071}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 442199 ms - 513655 ms", "content": "Title: CS224V Lecture 6 > Transcript > 442199 ms - 513655 ms\n\nContent: looking into several stages. First we want to have a reg agent that can answer questions appropriately. We want to make sure it's answering guideline based questions that pretty accurate. And then we want to integrate this reg agent into a self improving agent with a apprentice training paradigm that takes experts feedback and then try and improve the system itself in its responding. And after these two stages we're going to implement the system or running a system in a clinical implication study. And the main goal is to see with the system's help can we really improve the time and quality on our cardiac patient care. So the data set we have right now we're using a 200 question data set of cardiology board preparing questions. And these are set to be multiple choice questions with curated answer and there are also written explanations as well as the scientific paper reference to it. And the next stage we're going to revise some of the questions, the more clinically relevant questions", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_442199_ms_-_513655_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 442199, "end_ms": 513655}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 500693 ms - 572453 ms", "content": "Title: CS224V Lecture 6 > Transcript > 500693 ms - 572453 ms\n\nContent: answer and there are also written explanations as well as the scientific paper reference to it. And the next stage we're going to revise some of the questions, the more clinically relevant questions into open ended questions and then for the system to give us an open ended response on clinical practice recommendations. For example, we have a patient with chest pain in the midnight what would you do and what kind of workup or lab test you want to order as your next step. And then do you want to send this patient out to see a cardiologist? And just an FYI, this question set actually has several multimodal questions. These are medical image interpretation questions that would challenge the model to or the system to see if it can really interpret the medical images and use that information for your decision making. Yeah, while most of the questions were knowledge or text based, these parts maybe 20 or 30% of the questions are involving medical images at the end. I want to mention about", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_500693_ms_-_572453_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 500693, "end_ms": 572453}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 556403 ms - 628217 ms", "content": "Title: CS224V Lecture 6 > Transcript > 556403 ms - 628217 ms\n\nContent: for your decision making. Yeah, while most of the questions were knowledge or text based, these parts maybe 20 or 30% of the questions are involving medical images at the end. I want to mention about highlights in the study timeline. The highlights on the study is that we're trying to really deal with a real world problem that's to take care of challenges in cardiology care delivery. Then you will be working with experts closely in the field of cardiology. Then there is also a strong clinical potential that we can put this into practice with relatively short time. And we're looking into hoping at least we'll have one or two CS student that would be willing to work with us. And more is better. Definitely we can look into more different perspectives of the project and then we want to have a prototype for our step one study probably in this 4, 20, 24 quarter. And then by working with us, we're hoping that this is a project that can inspire you to like save lives with us. I'm open to any", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_556403_ms_-_628217_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 556403, "end_ms": 628217}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 613049 ms - 697697 ms", "content": "Title: CS224V Lecture 6 > Transcript > 613049 ms - 697697 ms\n\nContent: prototype for our step one study probably in this 4, 20, 24 quarter. And then by working with us, we're hoping that this is a project that can inspire you to like save lives with us. I'm open to any questions. Thank you. Other questions I'll give. Hi. Thank you for your presentation. I was just wondering, are the responses for the RAG system going to be grounded in any large medical databases like PubMed or how are you expecting to ground this information? So we're hoping to ground that either in PubMed or in the up to date. That's another database that we usually use in clinical practice. Okay, got it. And just one more question is that you mentioned that this could be deployed in like more rural settings. And I was wondering are there different cardiology practices or considerations that you need to make for different settings if you were to deploy this in more rural hospitals? We can probably decide that when we're running. So the first, the step one goal is to make sure that it", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_613049_ms_-_697697_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 613049, "end_ms": 697697}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 685489 ms - 757291 ms", "content": "Title: CS224V Lecture 6 > Transcript > 685489 ms - 757291 ms\n\nContent: you need to make for different settings if you were to deploy this in more rural hospitals? We can probably decide that when we're running. So the first, the step one goal is to make sure that it has certain knowledges that required for addressing cardiology questions. And then I think your question is probably more in the step two that we have to adjust the model or the system's response according to the practice setting. Got it. Thank you. Thank you. It's a really interesting project. Something that occurred to me was the fact that medicine is a practice. Like if you have a patient present with a certain set of symptoms, you could have two or three or ten equally qualified cardiologists have different opinions on how to proceed. So I was curious how you were thinking about how to constrain like how to define what's a good model response, like whether what the model's response is good medicine or bad medicine. How do we approach that? Well, that's a really sharp question and we", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_685489_ms_-_757291_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 685489, "end_ms": 757291}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 746329 ms - 822069 ms", "content": "Title: CS224V Lecture 6 > Transcript > 746329 ms - 822069 ms\n\nContent: like how to define what's a good model response, like whether what the model's response is good medicine or bad medicine. How do we approach that? Well, that's a really sharp question and we actually just had some exchanges with Professor Lam early in the morning about this. So right now in practice there are some ways or some things we do that has strong evidence to support. So in our term we call like Brendan clinical trials. And so those are data with strong evidence. So usually it's like either, okay, you can definitely do that in this kind of condition. Then these questions are more like a black or white answer. We would assume these are easier for the models. Then there are some areas are more gray zone then we sometimes call it the art of medical practice, those are questions or some data with less strong evidence in clinical practice. So in those areas we could judge the model really based on our expert opinion. But sometimes in areas that's more vague, sometimes it's actually", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_746329_ms_-_822069_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 746329, "end_ms": 822069}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 804045 ms - 878765 ms", "content": "Title: CS224V Lecture 6 > Transcript > 804045 ms - 878765 ms\n\nContent: data with less strong evidence in clinical practice. So in those areas we could judge the model really based on our expert opinion. But sometimes in areas that's more vague, sometimes it's actually like case by case approach and there is no true right or wrong answer. So I think we'll probably try to address to the level of questions with some but not very strong evidences. So at least we have some references to look into and then not to those like very individual and questions. Yes. Cool. Thank you. Well, thank you, Dr. Zhou. I think that as you can tell from the Q and A, it is really amazing that we get this partnership with Dr. Xiao because we really need an expert in the house. All right, so this is an exciting project. So let us know if you want to be involved in this. All right, thank you. All right, thank you. Yeah, so feel free to get in touch with us for questions just by email or Slack. Yes, thank you. Thank you. Ilya, Does Ilya want to come? Perfect. I have your slides up", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_804045_ms_-_878765_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 804045, "end_ms": 878765}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 861471 ms - 1198883 ms", "content": "Title: CS224V Lecture 6 > Transcript > 861471 ms - 1198883 ms\n\nContent: you. All right, thank you. Yeah, so feel free to get in touch with us for questions just by email or Slack. Yes, thank you. Thank you. Ilya, Does Ilya want to come? Perfect. I have your slides up here so. You can use this. Are we done with Zoom? Yes, we're done with Zoom. Okay, perfect. It. It. Hi everybody. My name is Ilya and I'm Stanford professional development program student. I'm creator and CTO of the company behind this product we had it's tabletop user interface for talking to agents face to face. Yeah. And I'm proposing the project. Yeah. Just a few words behind my kind of agent. I want to propose that like we use self service kiosks quite a lot. We can order some food in McDonald's. Yeah. Or to print the air ticket but for kind of if you go to more complicated, more complex user flows like chicken and hotel it get too complicated to use the touch screen. So we kind of working on more intuitive hands free design user interface to ChatGPT or any other LLM based agents. Yeah.", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_861471_ms_-_1198883_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 861471, "end_ms": 1198883}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1179653 ms - 1272179 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1179653 ms - 1272179 ms\n\nContent: flows like chicken and hotel it get too complicated to use the touch screen. So we kind of working on more intuitive hands free design user interface to ChatGPT or any other LLM based agents. Yeah. And the case I propose is receptionist. You have it here in Stanford Research park with company inflection AI this is their office. They don't have people on reception. So instead of the receptionist person they have the device there. And so the idea of the agent that like normal receptionist role job is to greet people to be very welcoming. While it should have access to calendar so the calendar access should be integrated and then once visitor comes so it should recognize face either ask name, compare it to the potential meetings in calendar and then notify the person in the company who has this meeting that the visitor is on the reception. Also the receptionist job is to take care of delivery People, some of them don't speak English. Yeah. So it should automatically switch into most time", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1179653_ms_-_1272179_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1179653, "end_ms": 1272179}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1254107 ms - 1366171 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1254107 ms - 1366171 ms\n\nContent: meeting that the visitor is on the reception. Also the receptionist job is to take care of delivery People, some of them don't speak English. Yeah. So it should automatically switch into most time Spanish. So generally, yeah, I can also show how it works. But like for the purpose of the project, the challenges is multi user conversation because like in one context there could be different users approaching it that one challenge. The other challenge is, you know, it should notify the right people and not not notify wrong people about the meetings. I don't know if it will work. Yeah, just to give you oops, I don't know if the sound will. Hello and welcome to Inflection. Nice to see you. Hi, what's your name? My name is Kim. How can I assist you today? My name is Brandon. I'm looking for April. Hi Brandon. April is the executive assistant to our CEO Sean White. You can find her by going through the door on the right and down the hallway. Is there anything else I can help you with? Thank", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1254107_ms_-_1366171_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1254107, "end_ms": 1366171}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1354715 ms - 1478683 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1354715 ms - 1478683 ms\n\nContent: Hi Brandon. April is the executive assistant to our CEO Sean White. You can find her by going through the door on the right and down the hallway. Is there anything else I can help you with? Thank you. Yeah, so generally this is it. Thank you. Questions? Which one are you? Oh, go ahead. I'm just curious on kind of like the look and feel of the thing. Like what's the decision behind putting I guess like five mobile phone sized square there to piece together face. So in order to give person feeling that it sees him or her. So kind of it should be, I mean it moves and it pretend to see you. So once you get it as a human sized head, it's very easy to, you know, assume that it will talk to you as a human. We can do it pretty much same with tablet on gimbal. Right. But it will be like tablet on gimbal, so kind of you don't. So you will see it like a zoom call rather than of the Presence kind of 3D object gives you a feeling of presence. Like the form. Yeah, kind of. Thank you. Thank you. A", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1354715_ms_-_1478683_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1354715, "end_ms": 1478683}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1441483 ms - 1533197 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1441483 ms - 1533197 ms\n\nContent: gimbal, so kind of you don't. So you will see it like a zoom call rather than of the Presence kind of 3D object gives you a feeling of presence. Like the form. Yeah, kind of. Thank you. Thank you. A little like. Hello everybody. I haven't really fleshed this out, but I had an idea, thought it was pretty cool when it's presented. So it's AI agents for better content filtering. Basically I get distracted all the time when I go to YouTube, Reddit, Twitter. It starts out super like oh, let me watch some super educational video. And then I end up not watching some super educational video like 30 minutes later. So in some ad blockers you can limit the amount of time you spend on YouTube for instance, or you could tell yourself not to watch YouTube at all. And this can be kind of effective if there's like a decidedly time wasting site. But most sites are like sometimes productive and sometimes time wasting. So an LLM based system could help. It could understand what kind of site that you", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1441483_ms_-_1533197_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1441483, "end_ms": 1533197}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1519469 ms - 1589837 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1519469 ms - 1589837 ms\n\nContent: there's like a decidedly time wasting site. But most sites are like sometimes productive and sometimes time wasting. So an LLM based system could help. It could understand what kind of site that you were on and the content on the site and determine whether that's useful or not. The additional advantages would be, since it's an LLM based system, you could encode your preferences in there. So if you're like a jewelry designer or something, it's probably productive for you to be watching YouTube videos of, I don't know, jewelry design. But if you're just like some girl who wants to buy jewelry, it's probably not productive. So little things like that you could encode with an LLM system. So here's a workflow I kind of had in mind. This might be like a browser extension similar to your adblock. And it could work for you to limit your own less productive activities or for you to limit your children's less productive activities. And so if you enter a site, it's determined to be productive or", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1519469_ms_-_1589837_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1519469, "end_ms": 1589837}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1575325 ms - 1650153 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1575325 ms - 1650153 ms\n\nContent: And it could work for you to limit your own less productive activities or for you to limit your children's less productive activities. And so if you enter a site, it's determined to be productive or unproductive. And depending on how you set up your system, this could notify a parent. Or you could be like, no, please. For some reason I really need to watch Twitch for a homework assignment. And yeah, there would be a way for you to negotiate with your LLM agent. Maybe even like upload what kind of rubric your teacher gave you or something like that. So lots of possibilities here. And I would really like this product, which is why I'm pitching it. I do need a partner, but I joined the class Wednesday night. So, like, everybody already has a partner. If you don't have a partner, reach out. I'm sure there's a lot of people who don't. So does anyone have any questions or. Feedback. As a point of feedback? How does it stop me from turning on incognito? Actually, there's ways to not get", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1575325_ms_-_1650153_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1575325, "end_ms": 1650153}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1635025 ms - 1701005 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1635025 ms - 1701005 ms\n\nContent: I'm sure there's a lot of people who don't. So does anyone have any questions or. Feedback. As a point of feedback? How does it stop me from turning on incognito? Actually, there's ways to not get yourself incognito already on max. Like you can like some terminal. But I have to do that, which I did because I constantly look for ways to avoid my own, like, blockers. Yeah. I'm curious. This type of problem seems like the person, like, let's just see if a kid, right, they're gonna try to exploit the system and like argue like, oh, I'm using Twitch for homework. So how do you balance, like the flexibility of the agent to allow, oh, a site could be useful in some cases, but not others. But also prevent users from exploiting it. I think that's where the notify parent thing comes along. If you're a kid, you have just a super high tolerance for like, oh, I really want to do this and I'm going to keep trying to exploit the system. So maybe you're willing to Spend hours doing this and then", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1635025_ms_-_1701005_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1635025, "end_ms": 1701005}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1690861 ms - 1754211 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1690861 ms - 1754211 ms\n\nContent: a kid, you have just a super high tolerance for like, oh, I really want to do this and I'm going to keep trying to exploit the system. So maybe you're willing to Spend hours doing this and then it'll notify your parent and then your parent can come discipline you. But for an adult, there's some sort of threshold of like, all right, I'm not really going to spend an hour trying to red team an LLM agent just so I can, like, get my dopamine hit. I suppose that's the idea. Maybe using cognitive behavioral insights or psychological evidence, try to make the app more human centered. Interesting. Okay, one more question. This is a suggestion. I think with some of your evaluation. It would be interesting if you built. Like an LLM red teaming thing. Like, there's some papers on using LLMs to generate attacks for other LLMs to jailbreak them. And so you could, like, simulate the person wanting to watch Mr. Beast or whatever and see, like, what creative ways it can try to break your system and", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1690861_ms_-_1754211_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1690861, "end_ms": 1754211}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1744893 ms - 1835961 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1744893 ms - 1835961 ms\n\nContent: attacks for other LLMs to jailbreak them. And so you could, like, simulate the person wanting to watch Mr. Beast or whatever and see, like, what creative ways it can try to break your system and see how robust it is. Nice. Thanks for the feedback. Thank you, everybody. Good. Next. Rihan, do you still. Are you here, Rihanna? Okay. Hi, everyone. My name is Allison John. I'm a senior in computer science on the AI track, and my partner, Emily park, is a master's student in computer science. And I'm pitching an idea about using LLMs in robotics, which is a field that I just learned existed last week. So I don't know very much about it, but it seems like a very new and exciting area of research. So one application of LLMs to robotics is called autonomous perception, which is the ability to collect information and decide how to act. So, for example, if a robot knows that it needs to clean the dishes in order to do that, it also needs to know that it should, like, go to the sink, find the", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1744893_ms_-_1835961_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1744893, "end_ms": 1835961}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1823377 ms - 1899509 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1823377 ms - 1899509 ms\n\nContent: information and decide how to act. So, for example, if a robot knows that it needs to clean the dishes in order to do that, it also needs to know that it should, like, go to the sink, find the sponge and the soap, and then use those to clean each dish. And so to get those subsequent steps for a task, you could ask an LLM for those steps, and so that can fill in the information. And then it could also be used to do things like if a robot perceives that there are dirty dishes in the sink, the LLM could be used to tell it that, oh, you should go and clean the dishes. Yeah, so that's pretty much my ideas so far. If there are any mentors who have some experience in robotics or anything like this, please let us know. Our emails, our Stanford emails are on the slide. And, yeah, that's it. Thanks. So I'm Mike stoker. I'm a PlayStation. We're also working with somebody who does. Something called Virtual home. And in my case, if you search. Virtual, they have a simulated environment, actual.", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1823377_ms_-_1899509_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1823377, "end_ms": 1899509}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1885305 ms - 1959413 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1885305 ms - 1959413 ms\n\nContent: So I'm Mike stoker. I'm a PlayStation. We're also working with somebody who does. Something called Virtual home. And in my case, if you search. Virtual, they have a simulated environment, actual. Robots that do things like put together. Breakfast and working with language models. So maybe I can give you a bit more details if you want, after class. Anyone else have questions? I just want to add that we encourage everybody to make proposals like yours. This is wonderful. And we will give you a mentor. Everybody will be assigned a mentor from our wonderful teaching staff. Okay, so we are familiar with the literature. We will help to connect you with other sources as well. So don't worry about that. Okay, great. Thank you. Yes, about this project is like for perceiving like a scene or something like that. Do you plan on fine tuning or using your own image segmentation model? Or how exactly do we plan on. Or do you plan on taking the multimodal data and feeding it to the agent system?", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1885305_ms_-_1959413_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1885305, "end_ms": 1959413}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 1946927 ms - 2033707 ms", "content": "Title: CS224V Lecture 6 > Transcript > 1946927 ms - 2033707 ms\n\nContent: like that. Do you plan on fine tuning or using your own image segmentation model? Or how exactly do we plan on. Or do you plan on taking the multimodal data and feeding it to the agent system? Yeah, I feel like we would work on the next step after the computer vision parts, the inferring and decisions, not the image segmentation type stuff. I feel like. On that note, I've seen people like taking a video and chop it out. Into frames and sending it to a multimodal power lamp. Like a couple seconds. Yeah, that too. Great, thank you. So I think with a lot of the multimodal, like robotic applications, I've seen your AI. How are you keeping in track, like the state of the environment and like how we've interacted with it in the past? Because like, yeah, the robot can very easily look at this frame and tell the objects, but then sort of seeing like modeling the environment as sort of a time series. I don't know, large language models are good at that or are a good application of that. I'm", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_1946927_ms_-_2033707_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 1946927, "end_ms": 2033707}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2017433 ms - 2125871 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2017433 ms - 2125871 ms\n\nContent: frame and tell the objects, but then sort of seeing like modeling the environment as sort of a time series. I don't know, large language models are good at that or are a good application of that. I'm also very curious too, because that's just one challenge. Yeah, definitely something to think about. There is a lot of research in this area. So I think that we can take this offline and for people interested, we can have discussions on this topic. Okay, then I guess it's just hard. Yeah, it's just like one slide. So all of you have been working with the GNU worksheets assignment, and here's a new, like an extension to the project that we have been doing with GNU worksheets. The idea is that you kind of like. So right now what happens, we have multiple calls or like two calls for semantic parsing. When a user utterance comes, we want to map that to the formal representation that we have. So one of the new project pitches that I have is you kind of like generate synthetic data using, let's", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2017433_ms_-_2125871_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2017433, "end_ms": 2125871}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2111215 ms - 2181117 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2111215 ms - 2181117 ms\n\nContent: When a user utterance comes, we want to map that to the formal representation that we have. So one of the new project pitches that I have is you kind of like generate synthetic data using, let's say a teacher model. Like we have already learned about distillation with the wiki chat work. So it's kind of the same where you generate a lot of synthetic data and then you try to fine tune the model using this data for having a faster semantic parser. The reason why it could work is that it's a narrow domain. You have a mapping from natural language to a restricted domain specific language. And the challenges that might occur in this work are basically how do you generalize to multiple APIs, because for instruction, like for the larger LLMs, it's easier to adapt to newer APIs. But here one of the challenges could be that when a new API comes in, how do you adapt to it and how do you manage out of domain data? And that's the project. So I think this should be an interesting one if you're", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2111215_ms_-_2181117_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2111215, "end_ms": 2181117}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2168997 ms - 2268685 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2168997 ms - 2268685 ms\n\nContent: the challenges could be that when a new API comes in, how do you adapt to it and how do you manage out of domain data? And that's the project. So I think this should be an interesting one if you're interested in fine tuning models, generating synthetic data and learning more about it. Okay, thank you. So I in this room has seen this and I don't want somebody to tell me who are in this room is like, oh, I didn't fill in the form because I didn't notice this message. Okay, so what I want to do today is that there are a number of projects that have been pitched. And on Monday I was talking to some of you in my office hours and it seems like it is a good idea to talk a little bit about how you go about designing and doing your projects. Okay, so this is kind of like a mini lecture on this topic. I really want you guys to ask me questions. If you have, you know, if you're thinking about the projects on running into difficulties or you know, you're wondering if your ideas are good or not,", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2168997_ms_-_2268685_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2168997, "end_ms": 2268685}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2257465 ms - 2330455 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2257465 ms - 2330455 ms\n\nContent: I really want you guys to ask me questions. If you have, you know, if you're thinking about the projects on running into difficulties or you know, you're wondering if your ideas are good or not, feel free to talk to chime in. I just want it to make it to make. We want to make this interactive. We have been presenting some of the projects that come out of our research. But with this class, we really want to encourage everybody who are interested to propose your own as well. And some students came to me and asked, can I do this, Can I do that? So what I require everybody to do is that when they write their project proposal, you take the problem, you try it on GPT first. If the GPT gives you a good answer already, with very little bit of work on designing your prompt, that's not a project for the quarter. Okay? But most likely you come up with a problem where the response looks okay if you give it easy input. If you think harder, make it a little bit harder, it will fail. And that's very", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2257465_ms_-_2330455_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2257465, "end_ms": 2330455}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2315461 ms - 2390041 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2315461 ms - 2390041 ms\n\nContent: the quarter. Okay? But most likely you come up with a problem where the response looks okay if you give it easy input. If you think harder, make it a little bit harder, it will fail. And that's very likely. And that is fine as long as you can identify why GPT as is does not work, that is a good project because we know that there's always this gap. Let's put it this way, if the GPT does not already solve part of the problem. It is really hard to make it work 100%. So we expected that it has a good starting point. But the question now is, how do you turn this into something that can be used so that would make it a good project? I've chatted with some people already and it looks like there are a lot of interesting ideas. So I'm really looking forward to this quarter where we invent this future together. So if you have ideas, talk to us. So the constant here is that today you hand in the 6 second homework and in this one week we ask you to formalize the project and you have to make a", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2315461_ms_-_2390041_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2315461, "end_ms": 2390041}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2374445 ms - 2450751 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2374445 ms - 2450751 ms\n\nContent: future together. So if you have ideas, talk to us. So the constant here is that today you hand in the 6 second homework and in this one week we ask you to formalize the project and you have to make a proposal by Wednesday. Next week we have office hours, our CAs have office hours, talk to each other, use ed, discuss ideas, you pitch some ideas, and if you have comments on other people's ideas, go ahead. But we have to finish the definition of your proposals by the end of, I mean, in one week's time. It is Wednesday. Okay? So when we make a proposal, it is our best guess. It doesn't mean that you cannot change the topic, but of course, the longer you take to decide the topic, the less time you have. So it is always a game that we have to manage. And because every project will have a mentor, you can work with the mentor on making changes to your projects. But as you make changes and so forth, the mentors are aware of it week by week. So we want to make sure that by the time you finish,", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2374445_ms_-_2450751_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2374445, "end_ms": 2450751}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2437391 ms - 2509777 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2437391 ms - 2509777 ms\n\nContent: you can work with the mentor on making changes to your projects. But as you make changes and so forth, the mentors are aware of it week by week. So we want to make sure that by the time you finish, you have a good project to show. Okay. As I mentioned, every project will be mentored. And so the basic idea is that we will connect you. We don't know everything, but we can try to connect you with other groups or tell you what the related work is and so forth. So this proposal is very important because you put it in and then we'll give you feedback on whether that project works or sometimes we have to, sometimes you have to expand the scope, but more than likely we're going to reduce the scope. So this is an important part of that process. So I really believe that we can learn from each other's projects. So it is not just you submit and we give you feedback. So we are devoting next three classes for people to talk about projects. So the project is due on. The proposal is due next", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2437391_ms_-_2509777_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2437391, "end_ms": 2509777}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2496401 ms - 2566541 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2496401 ms - 2566541 ms\n\nContent: other's projects. So it is not just you submit and we give you feedback. So we are devoting next three classes for people to talk about projects. So the project is due on. The proposal is due next Wednesday. So by Wednesday, everybody is ready to present their projects. But we also offer slots on Monday for people who are, you know, who want to get some feedback before the proposals. Any projects that are presented before Wednesday, you know, need not to be presented again. Okay, so you are. You will get to sign up. It's either Monday next week or the next two classes when everybody should be prepared. So that is the plan. It's a short proposal. We don't have enough time, but we have a big class. But we really want to see if we can hear from everybody. If you are working with a proposal that's been presented already, you don't have to present this round. We just have to take it to the final project because we have already heard of the ideas. Are there any questions about this? All", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2496401_ms_-_2566541_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2496401, "end_ms": 2566541}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2550013 ms - 2625959 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2550013 ms - 2625959 ms\n\nContent: that's been presented already, you don't have to present this round. We just have to take it to the final project because we have already heard of the ideas. Are there any questions about this? All good. All right, so I want to talk a little bit about what kind of projects are available. And I think that there are really three themes. Okay. So we talked about the fact that we are now. We now have AI and we can accelerate the generation as well as dissemination of information. And I lump it under the concept of the worldwide knowledge concept. And the second set. The next two topics have to do with the fact that you try all the things that people have done and it still doesn't work. And I want to introduce you to two ideas that we found useful and we are still working on them. You know, it's not like the GENIE worksheet, where we already have a platform for you to build upon, but these are two general concepts and I want to run over that today because that might be useful for your", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2550013_ms_-_2625959_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2550013, "end_ms": 2625959}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2613755 ms - 2693037 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2613755 ms - 2693037 ms\n\nContent: it's not like the GENIE worksheet, where we already have a platform for you to build upon, but these are two general concepts and I want to run over that today because that might be useful for your project. So it's important when you try to scope your project whether you can handle this in this, the seven weeks you have. So one is the concept of apprentice training and the other one is the introduction of intermediate representations. Okay, so these are the three things that I want to talk about. So the concept here is that we have present is that there is a lot of things we can do to accelerate the generation of knowledge and improve access. And we have already talked about the Data Talk project. Just this Tuesday, George Liu, who presented earlier, they just released a an agent that can let you talk about FEC data, which is all the election campaign data, and that has now been released to the journalists. And we will see how that goes. So we've talked about that. We also have talked", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2613755_ms_-_2693037_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2613755, "end_ms": 2693037}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2678573 ms - 2776627 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2678573 ms - 2776627 ms\n\nContent: let you talk about FEC data, which is all the election campaign data, and that has now been released to the journalists. And we will see how that goes. So we've talked about that. We also have talked about History Chat. Sina has talked about how we are able to deal with old copra. And I want to show you another project that is under this setting that we have not got into the details. And this is the concept of a multilingual news platform. So we have made a proposal on this and we actually have a video. Hello, everyone, My name is Tiffany Luo. Together with Jia Liang, Monica Tai and Jennifer, we are proposing a cross lingual, multi perspective news platform. Can you hear There is a volume sign was right there like the volume or here. Do you mind if I try? I think it's this one. Oh, you made it go pipe through the yeah. Is that okay? Is it done? It should work then do you want to try? Hello everyone, my name is Tiffany Le. Together with Jia Liang, Monica Tai and Jennifer, we are", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2678573_ms_-_2776627_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2678573, "end_ms": 2776627}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2753665 ms - 2821559 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2753665 ms - 2821559 ms\n\nContent: you made it go pipe through the yeah. Is that okay? Is it done? It should work then do you want to try? Hello everyone, my name is Tiffany Le. Together with Jia Liang, Monica Tai and Jennifer, we are proposing the cross lingual multi perspective news platform. We focus on multiple perspectives in salient global events. In Gaza, Hamas claims they are fighting a war as a residency through Israeli occupation and blockades, while Israel contains the war is in defense of their people. In Russia's narrative, the Ukrainian war is a fight back at NATO's expansion, while from American side the war is an unjustified invasion of another country. And there are more perspectives in different languages on the same news which show how complicated international geopolitical relationships are. That's why it is extremely important to have an analysis and comparison of different perspectives on the same news topic today. Traditional commercial systems have fostered clickbaits, technology creates", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2753665_ms_-_2821559_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2753665, "end_ms": 2821559}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2809511 ms - 2872061 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2809511 ms - 2872061 ms\n\nContent: why it is extremely important to have an analysis and comparison of different perspectives on the same news topic today. Traditional commercial systems have fostered clickbaits, technology creates generative echo chambers to news consumers, many countries build state run censorship and even crossing firewalls doesn't help readers understand other languages. We motivate our proposal by the can we provide a reliable and efficient source of news? Our goal is to create a cross lingual multi perspective news platform which translates, compares and analyzes global breaking news. To achieve this goal, we propose clmp, which leverages large language models to break down the language barriers. CLMP enables automatic analysis and comparisons of different languages all in real time. It first translates the news articles into English, breaks them down claim by claim, and then compares them on a claim level. The outcome would be a summary with claim level similarities and differences between the", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2809511_ms_-_2872061_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2809511, "end_ms": 2872061}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2861279 ms - 2920121 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2861279 ms - 2920121 ms\n\nContent: the news articles into English, breaks them down claim by claim, and then compares them on a claim level. The outcome would be a summary with claim level similarities and differences between the news article's perspectives. This has never been done before. It's simply not feasible to build a manual pipeline to achieve the same goal. Here is an illustrative example of what CLMP will be capable of. We can see that even when quoting a statement from the same spokesperson, the coverage by news agencies from different countries in different languages can vary subtly yet significantly. Haarez, an Israeli news agency, says no significant damage was reported, while mih, an Iranian news agency, says further damage was caused. We have implemented a prototype for clmp. The prototype takes in two news articles on the same news topic in different languages. Here we have two articles covering the Biden and xi's meeting. Our system automatically generates analysis provided differences and summarizes", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2861279_ms_-_2920121_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2861279, "end_ms": 2920121}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2909177 ms - 2968461 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2909177 ms - 2968461 ms\n\nContent: articles on the same news topic in different languages. Here we have two articles covering the Biden and xi's meeting. Our system automatically generates analysis provided differences and summarizes them in a conclusion. Our proposed CLMP will be the first cross lingual multi perspective news platform that Makes global news more accessible for users and informs them of the multiple perspectives out there. And it will source from 10 different languages. With our platform, the general public can become better informed, news producers can source stories more quickly, and the news outlets can get better exposure. Please consider supporting our initiative and thank you for your consideration. All right, so this was a video made to the Brown Institute when they asked for proposals and the project has been funded. Okay, so this is. We have already prototyped it, but there is still a whole lot of work to do when you have to build it and you can see what's going on and then you have to have to", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2909177_ms_-_2968461_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2909177, "end_ms": 2968461}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 2955893 ms - 3038875 ms", "content": "Title: CS224V Lecture 6 > Transcript > 2955893 ms - 3038875 ms\n\nContent: has been funded. Okay, so this is. We have already prototyped it, but there is still a whole lot of work to do when you have to build it and you can see what's going on and then you have to have to improve it. So this is a project if you're interested. We're very, very happy to get more people involved. Any questions about that? Yeah, sorry I was going to ask. I mean it sounds like it's an amazing project and sounds like a lot of people are working on it already. So if we were to join the project, could you maybe give us an example of something that we might be working on? Actually we don't have anybody working on it very this moment and the Siena actually has done a lot of work on this and building the first prototype, but he is now busy with another project and so we know enough to carry out the project. But we actually need a lot of manpower from every, you know, every piece here. Ty and Tiffany, they are journalists from Columbia and so they are really not helping with the CS", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_2955893_ms_-_3038875_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 2955893, "end_ms": 3038875}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3020411 ms - 3100459 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3020411 ms - 3100459 ms\n\nContent: carry out the project. But we actually need a lot of manpower from every, you know, every piece here. Ty and Tiffany, they are journalists from Columbia and so they are really not helping with the CS part, but they can give you a lot of help with the subject domain. And also Jennifer Pan, she's from the political science, giving a lot of can give us a lot of expertise. Jia Lang is a student. Unfortunately he has other things that he is doing and moving on. So as you can see, we are looking for help but we have done enough to know. And also Sina can give some high level supervision in the meantime along with me on this project. Okay, so please let us know if you're interested. I talk to people from the outside. A lot of people outside side of the United States is actually very interested in this topic. As you can imagine, it is really a global project. So if there are people here who know other languages as well, you know, that always helps. Okay, Any other questions? Yes, David, Find", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3020411_ms_-_3100459_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3020411, "end_ms": 3100459}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3083091 ms - 3165311 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3083091 ms - 3165311 ms\n\nContent: this topic. As you can imagine, it is really a global project. So if there are people here who know other languages as well, you know, that always helps. Okay, Any other questions? Yes, David, Find misinformation. Is it possible to make it fit seven weeks to partner with a communications agency perhaps? In our country we have already built up a lot of infrastructure. So for example, we're using RSS feeds and this is how we get the news stories. So we have a lot of infrastructure so you can concentrate on the AI Part in a sense you can just work with a smaller number of feeds, like a couple of countries to begin with. But the scaling part is either done or we can just do it later. Yes. So we have the basic idea of the little of the, of the claim checks, fact checks and actually we were just discussing new ideas of improving that. It actually is related to the consistency project. The whole basic idea is how are they different. But the one difference in this project is that there is a", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3083091_ms_-_3165311_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3083091, "end_ms": 3165311}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3150905 ms - 3233403 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3150905 ms - 3233403 ms\n\nContent: discussing new ideas of improving that. It actually is related to the consistency project. The whole basic idea is how are they different. But the one difference in this project is that there is a difference in perspectives. They are not just factual, they take different points of view. Right. And then we also want to want to include that in the summary. There's a lot of opinions involved in that. Okay, so this is, this is, it's very interesting because I think there are a lot of people interested in the result. Yes. Can people don't enroll this graph skill. Well, maybe we do need a next Twitter now that Twitter is very much controlled by a single feed by the owner. Yeah. Okay. So love to have people help with this. All right. The more the merrier actually, because this is really tricky. So. Hello everyone, my name. So what is also what I also want to tell you about is that because of our research work, the Sloan foundation has reached out to ask us to create a workshop to talk about", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3150905_ms_-_3233403_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3150905, "end_ms": 3233403}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3215355 ms - 3295351 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3215355 ms - 3295351 ms\n\nContent: So. Hello everyone, my name. So what is also what I also want to tell you about is that because of our research work, the Sloan foundation has reached out to ask us to create a workshop to talk about the opportunities of AI. Okay. You see the concepts like for example, this multilingual news platform is very, very different from for example, Wikipedia. Okay. Today if you look at public knowledge, open source, public knowledge, Wikipedia is definitely the beacon in a sense of what can be done. Imagine this world without Wikipedia, without wikidata. Okay? Where are we getting trusted information from? So what we are saying here is that Wikipedia, its encyclopedia style knowledge, it just. There's still plenty to do, news being an obvious example. And there are many more domains, I mean more expert domains. And you really want to create an environment where you have the availability of all the AI tools and the communities such as the historians to get together to build up the knowledge", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3215355_ms_-_3295351_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3215355, "end_ms": 3295351}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3281217 ms - 3360167 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3281217 ms - 3360167 ms\n\nContent: expert domains. And you really want to create an environment where you have the availability of all the AI tools and the communities such as the historians to get together to build up the knowledge and make it publicly available. Obviously it costs money to make such a platform available. I don't know if you know, for example, that Wikipedia is running at a rate of $100 million a year. Okay, that's what it takes. But we do support it. It is through a combination of sponsorships by philanthropy organizations as well as consumers and individuals. And in other words, and we believe that there is room for a bigger version of encyclopedia supported by AI. Without the AI, it's too costly. Okay. People thought about, can we put more knowledge together, making all the trusted source of knowledge easily available through linking them to the different sources and so forth. But the point is that it was too expensive. But now with AI, we have a shot at accelerating that process. So this is why", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3281217_ms_-_3360167_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3281217, "end_ms": 3360167}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3344309 ms - 3415377 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3344309 ms - 3415377 ms\n\nContent: easily available through linking them to the different sources and so forth. But the point is that it was too expensive. But now with AI, we have a shot at accelerating that process. So this is why we're creating this workshop. It is going to be held February, in the middle of February next year. And it is an invitation only workshop because we really want to make it work. We want to get together and talk about how we make it work. It is not just a conference where we listen to a bunch of talks and go away. And we are including not just the CS people, but the funding agencies, the journalists, historians, social scientists and so forth. And what I would like to see is that work that we are doing in our group or work being done here in these projects, if it works out, we can showcase that in this workshop. So if you put the multi, multilingual news platform together, we showcase that. Okay? And then we show it to the journalists and we say this is what we can all create. Between the", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3344309_ms_-_3415377_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3344309, "end_ms": 3415377}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3401719 ms - 3475423 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3401719 ms - 3475423 ms\n\nContent: in this workshop. So if you put the multi, multilingual news platform together, we showcase that. Okay? And then we show it to the journalists and we say this is what we can all create. Between the technologists, they, you know, between all the different people. This is one of the reasons why a lot of our projects involve the experts. It is not just CS people doing their projects. And frankly, by doing so we are advancing the CS project, CS technology because we have discovered some really difficult problems which I will discuss next. Okay, so one of them is the need for apprentice training. So let's take a look at the background. If we look at this kind of a new style of engineering, I call this the LLM based engineering. LLM gives you, you the natural language, the knowledge of the world and so forth. Okay, so given that you have LLM, we can build many things with it. What we have talked about in depth in the last class is how we build a static pipeline. And that is, for example,", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3401719_ms_-_3475423_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3401719, "end_ms": 3475423}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3462255 ms - 3543349 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3462255 ms - 3543349 ms\n\nContent: and so forth. Okay, so given that you have LLM, we can build many things with it. What we have talked about in depth in the last class is how we build a static pipeline. And that is, for example, wiki chat. We found that a static pipeline is not good enough. And so we have been creating other pipelines and using this agentic approach. What agentic approach means is that this we building an agent that observes what is being done and adjusts the actions accordingly. So for example, this is what we use in the Wikidata semantic parser. The platform, I mean the framework is that you start by defining a set of actions such as, you know, you can look up the properties or you can execute a query. Okay, you define a set of these primitives, the agent. You don't tell the agent what the algorithm is. The agent looks at the primitives and choose the actions. It performs the action and it looks at its result and it decides what action to take next. So this is very, this is working out really well", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3462255_ms_-_3543349_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3462255, "end_ms": 3543349}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3528929 ms - 3608861 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3528929 ms - 3608861 ms\n\nContent: The agent looks at the primitives and choose the actions. It performs the action and it looks at its result and it decides what action to take next. So this is very, this is working out really well for some of the Harder problems for which we cannot just say, do these seven steps in this static pipeline. And the, one of the problem here is that sometimes you're at a, you know, you reach a dead end, you have to backtrack. So right now we just randomly back track and we just turn up the temperature and we can try different paths. One of the projects that we proposed in the list of ideas is a tree search of solutions. So you can systematically go through the possibilities. And we believe that this is important. For example, if you want to talk about a gaming coach and so forth, where you cannot just probabilistically traverse the space. Okay, so that's the agentic approach that we have found useful for some of our projects and may be useful for some of your projects. Okay, so besides", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3528929_ms_-_3608861_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3528929, "end_ms": 3608861}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3588629 ms - 3665571 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3588629 ms - 3665571 ms\n\nContent: cannot just probabilistically traverse the space. Okay, so that's the agentic approach that we have found useful for some of our projects and may be useful for some of your projects. Okay, so besides that, we also do fine tuning. So if you have a problem where the instructions just cannot pass, possibly fit in the prompts, one possibility is that you just generate tons of inputs and outputs, you know, to span the space and then we fine tune with that. So that's an obvious solution that most people are familiar with what we use. We use fine tuning in the wiki chat just to speed our execution up and just to reduce the cost. But in some cases you may want to use this because you can just cannot squeeze the information into the instructions. So these are three top techniques I think that you'll be using. Some of you will be using these three techniques. But what if these techniques are not good enough? This is what we discovered with some of the projects that we are doing lately. So we", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3588629_ms_-_3665571_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3588629, "end_ms": 3665571}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3652131 ms - 3732511 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3652131 ms - 3732511 ms\n\nContent: be using. Some of you will be using these three techniques. But what if these techniques are not good enough? This is what we discovered with some of the projects that we are doing lately. So we announced kind of like formalizing this concept of apprentice training. The key issue here is that when you look at any jobs that people are doing, that experts are doing, they often have tacit knowledge, they know something about the job. It was never written down anywhere. And there is just no way an LLM can do it because it's just simply not written down. And you really have to distill the information out of the expert's head. So how do we do that? Let me give you a concrete example. We talked about the fact that we create an agent for fec, the donation to the election. They have to disclose all this information. And we downloaded the information from FEC and we tick the donations, we sum it up and it turns out that it doesn't match the summary that was reported for the individual", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3652131_ms_-_3732511_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3652131, "end_ms": 3732511}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3718653 ms - 3796913 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3718653 ms - 3796913 ms\n\nContent: all this information. And we downloaded the information from FEC and we tick the donations, we sum it up and it turns out that it doesn't match the summary that was reported for the individual candidates. So what do we do? So it turns out that we connected with the expert of the FEC data, Professor Willis, and he explained to us that in the raw data, the donation does not include anything under $200. Okay? So when you add them up, it doesn't match. So this is an example of a caveat on the data set that was, that was in his head. We could not figure out why when we summit, it doesn't work. So the point here is that this is just one example. A lot of jobs that we are trying to automate have this problem. So we need to get the information from the experts. And if you are able to take this information from the experts, put it into the system, then everybody can benefit from it. So that's the basic idea. So if you look at what humans do is that we set up these apprenticeships or", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3718653_ms_-_3796913_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3718653, "end_ms": 3796913}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3780505 ms - 3857937 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3780505 ms - 3857937 ms\n\nContent: this information from the experts, put it into the system, then everybody can benefit from it. So that's the basic idea. So if you look at what humans do is that we set up these apprenticeships or internships. You learn, you work and you make mistakes. And you have a supervisor, your advisor can provide you with feedback. And the feedback is not through you making mistakes a million times and then observing on your own. You have an expert to actually tell you the reason why you have to do things a certain way. So for example, you have a rule like this, how am I supposed to learn that? If you give me a bunch of numbers, when you add up, it doesn't match. And that is because of missing data. Okay, There is just, I don't even know how to give you a data set that teaches you this, you know, this observation or this principle. And in other cases you could give you a data set, but it takes a lot of data. So the point here is that with LLM, I think that we can, it is very important for us to", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3780505_ms_-_3857937_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3780505, "end_ms": 3857937}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3841381 ms - 3916807 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3841381 ms - 3916807 ms\n\nContent: observation or this principle. And in other cases you could give you a data set, but it takes a lot of data. So the point here is that with LLM, I think that we can, it is very important for us to think about a different paradigm. Instead of everything being data driven, it has to be rationale driven. If I give you the rationale, I mean, LLM understand knowledge, in a sense, you should be able to apply it because it's really hard to get enough data points for high level tasks. I want to emphasize why this is different from chain of thoughts. Okay? Chain of thoughts means that every time I give you a problem, you try to figure out the answer. You don't accumulate knowledge from one input to another. What is really important if you want to do these tasks is that you learn from your experience and you keep improving yourself. And so having a self improvement loop is really important. And this is what we mean by we need an apprentice. We get the advice, high level advice, we accumulate it", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3841381_ms_-_3916807_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3841381, "end_ms": 3916807}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3900403 ms - 3984695 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3900403 ms - 3984695 ms\n\nContent: and you keep improving yourself. And so having a self improvement loop is really important. And this is what we mean by we need an apprentice. We get the advice, high level advice, we accumulate it and we make it applicable to new inputs that we see. Okay, so let me give you another example. We were doing classification. This is for the ACLID project that we mentioned. So it's a classification of an event across a bunch of possible types of events. But if I have an event, sometimes I can legitimately classify them under different types. So for example, if I have an armed protest, I can classify it as an armed event or a protest event. And when people try to code up this information, they have these rules about what you're supposed to do. And you have to learn these rules. So you can learn this rule by observing many, many examples being coded one way, or you can read what those rules are, or you can ask the expert what are the rules? And you can feedback feed it back into the system.", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3900403_ms_-_3984695_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3900403, "end_ms": 3984695}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 3971085 ms - 4043799 ms", "content": "Title: CS224V Lecture 6 > Transcript > 3971085 ms - 4043799 ms\n\nContent: this rule by observing many, many examples being coded one way, or you can read what those rules are, or you can ask the expert what are the rules? And you can feedback feed it back into the system. Okay, so if I go the data driven way, you need lots and lots of data points. So this is why we care about having rationales. The third point here is that humans can generate, can generalize rationales. So for example, in this case, having an armed event is a more escalated event, so it is preferred over the protest. And that principle can be applied to resolve priorities between any pair. So this is the kind of principles that you can generalize and apply. So this is, this is the architecture that we think is necessary to solve some of these problems, is that when you take an input, you go to through the LLM based system, you get some output, you observe an error. Okay, might, you know, it's just really hard to collect tons and tons of data like this. But when you observe an error, at that", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_3971085_ms_-_4043799_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 3971085, "end_ms": 4043799}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4030563 ms - 4106147 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4030563 ms - 4106147 ms\n\nContent: the LLM based system, you get some output, you observe an error. Okay, might, you know, it's just really hard to collect tons and tons of data like this. But when you observe an error, at that point we can consult with the expert, get the rationale and capture it and feed it back into the LLM. Okay, Rather than try to generate more data or fine tune the system, you say, look, I'm going to capture it and put it into the LLM just to be very concrete. An example here, which is what we did in both of the classification project and the semantic parser project, where we just don't know the caveats in the data. We find an error, we get the expert to tell us what the issue is and we write them down as rules. These rules can be applied at different stages of my LLM system. One of them is that once I generate a result, you check each of the rule. You know, each can be a call to the LLM. You check if the rule applies. If so, you revise and you continue. And in some cases the rule says the", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4030563_ms_-_4106147_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4030563, "end_ms": 4106147}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4091611 ms - 4170283 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4091611 ms - 4170283 ms\n\nContent: once I generate a result, you check each of the rule. You know, each can be a call to the LLM. You check if the rule applies. If so, you revise and you continue. And in some cases the rule says the numbers may not add up. What we will do is that when we tell the user the answer, we add the caveat is like, these may not add up. It is not cut and dry. Here is the number. Okay, so this is the kind of thing that we really need to do for the system to actually be usable in real life. You do not want these journalists to report on numbers with caveats that they are not aware of. Okay. That is an absolute. No, no. Okay. But we have to turn into a system. What we are driving at is that I don't want to work so hard for every application. The concept here is that we put this together in a framework and we let other people, each domain developer, capture the feedback in this way, because we think that this is absolutely necessary, because there's a lot of knowledge that is not written down", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4091611_ms_-_4170283_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4091611, "end_ms": 4170283}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4154763 ms - 4234139 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4154763 ms - 4234139 ms\n\nContent: and we let other people, each domain developer, capture the feedback in this way, because we think that this is absolutely necessary, because there's a lot of knowledge that is not written down anywhere. So what we think we need to do next is to apply this to many more domains. So if you guys have topics you want to apply it to, finance, to legal, to whatever, keep this in mind, because this is one way for you to do error analysis and improvement. You can try it and see what kind. How do we provide the feedback back into the LLM. And I hope that by the end of this quarter, we will learn enough to generate a. To create a general apprentice framework that can be applicable to many more fields. Okay, so this is the second theme. Here's a third thing that we found that is very useful, and that is the concept of intermediate representations. So the problem here is that if you have a complex problem, you have input and you have output. If it is complex, then you need tons and tons of data.", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4154763_ms_-_4234139_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4154763, "end_ms": 4234139}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4219177 ms - 4287209 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4219177 ms - 4287209 ms\n\nContent: is the concept of intermediate representations. So the problem here is that if you have a complex problem, you have input and you have output. If it is complex, then you need tons and tons of data. So, for example, you have a terribly difficult problem of doing robotics work. Okay? You cannot get enough data if you don't try to do something with. What I'm doing here is that you have to break the problem down because it's like driving, you know, auto driving. At the beginning, people say, here is the input. It's like, okay, control the wheels, all right? But this is too big. And you have to. I mean, theoretically, you can potentially generate enough training data for that deep learning system to work. But what I have discovered is that it is important for you to break them down into steps. You have to identify the obstacles. Now you can tune the identification of obstacles. And by the way, when you see the obstacles, and you now know that if you don't see the obstacles, you need to", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4219177_ms_-_4287209_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4219177, "end_ms": 4287209}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4272877 ms - 4348587 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4272877 ms - 4348587 ms\n\nContent: You have to identify the obstacles. Now you can tune the identification of obstacles. And by the way, when you see the obstacles, and you now know that if you don't see the obstacles, you need to improve that part of the system. Okay? It allows you to focus on a sub problem at a time. So the concept of introducing intermediate representation is just really important. Okay, So I want to show you some examples of it. So we talked about coding. Remember the ACLID example? We are trying to take all the news and we say, okay, where are all the. What are. Where are the hot regions where you see a lot of violence against women? Okay, you're going to read every single article, or you code each one, and then you can, you know, you can put in A query, find all the records that have such events and then you say plot their countries. Okay, so this is a concept of qualitative coding. This coding has nothing to do with cs. This is an old word that is used by, you know, in journalism, in data", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4272877_ms_-_4348587_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4272877, "end_ms": 4348587}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4334379 ms - 4414753 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4334379 ms - 4414753 ms\n\nContent: and then you say plot their countries. Okay, so this is a concept of qualitative coding. This coding has nothing to do with cs. This is an old word that is used by, you know, in journalism, in data analysis. Okay, so one topic here is that I give you a code book such as the ACLed events. Can you automatically classify them? So we talked about events, but it can be used for resumes. Okay. I get, you know, we get hundreds of students applying to our grad school and many more hundreds applying to the Stanford School for undergraduates can be coded up a little bit so we can do some filtering without reading each article. I mean, each resume very, very carefully. Okay, so that whole concept is applicable to resumes, to medical transcripts and what have you. If you have tons of data, how do we take a first cut? So that's coding. Now, in the example of acled, we already given a code that has been developed over 10 years. Okay, but a lot of times you don't have a code. As a matter of fact, a", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4334379_ms_-_4414753_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4334379, "end_ms": 4414753}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4397921 ms - 4478745 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4397921 ms - 4478745 ms\n\nContent: take a first cut? So that's coding. Now, in the example of acled, we already given a code that has been developed over 10 years. Okay, but a lot of times you don't have a code. As a matter of fact, a lot of times individuals have personalized codes. I mean, I look at my PhD applications maybe differently from my colleagues. So how do we automatically create that code? And we think that one way is that you create, you give to the expert a tool, such as a tool that allows you to ask questions about the documents. And as the expert works on the tool, you know, you help them browse the information. Once you can locate the excerpts of the paragraphs that I'm looking for example, so as you capture their questions, you can actually infer what the code is. And once you can infer the code, you can use the code and analyze tons of documents ahead of time. So when you ask a question, it is much faster. I can talk about the, you know, find me all the people who have a degree from Stanford. Okay.", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4397921_ms_-_4478745_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4397921, "end_ms": 4478745}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4464573 ms - 4537905 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4464573 ms - 4537905 ms\n\nContent: use the code and analyze tons of documents ahead of time. So when you ask a question, it is much faster. I can talk about the, you know, find me all the people who have a degree from Stanford. Okay. It has already been coded. It will be fast. But why is this interesting? You can tell from the questions that experts are asking, even if you. This is when you don't have a code, we can automatically extract the code. So that's one of the important intermediate representation for analyzing massive amount of data, aggregating and comparing them. Another example I want to bring up is compliance. There are lots of compliance issues. So for example, you have a transcript. Somebody has to decide whether you're going to get your bachelor degree in computer science. But they have requirements. We mentioned that before. So we say this is the AI requirements. A lot of you have seen this, there's a lot of English in this. And Then the question is, can I tell from a transcript whether that transcript", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4464573_ms_-_4537905_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4464573, "end_ms": 4537905}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4524561 ms - 4601395 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4524561 ms - 4601395 ms\n\nContent: that before. So we say this is the AI requirements. A lot of you have seen this, there's a lot of English in this. And Then the question is, can I tell from a transcript whether that transcript can satisfy this requirement? It turns out that LLMs are reasonably good for the simple conditions. But can we afford for it to screw up? Okay, it's not as bad if you can you manage to graduate without and you miss one, but you really don't want to not graduate students because we screw up the LLM. Okay, so this is just one example. I mean, compliance is just in everything. So in order to reduce the LLM complexity and improve interpretability, you want to introduce these intermediate representation. So here's an example where I can tick the requirements. I turn them into, for for example, SMT formula, which stands for satisfiability modulo theory formula. Once you put it into the formula, then I can absolutely just run a solver which is guaranteed to be correct if the constraints are correct", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4524561_ms_-_4601395_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4524561, "end_ms": 4601395}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4587787 ms - 4660377 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4587787 ms - 4660377 ms\n\nContent: which stands for satisfiability modulo theory formula. Once you put it into the formula, then I can absolutely just run a solver which is guaranteed to be correct if the constraints are correct and I can give you the answer. Okay, so with LLM, I may not be able to put it into the constraints perfectly. But the good news is that I only have to do that for each requirement document, not for each transcript. So I take the document, turn on the formula, and you can inspect it, you can even go fix them and do whatever. And once you approve it, then you can just run this theorem prover and you're guaranteed to be correct. So you factor the problem into an iffy part, which is hard because you're dealing with English, and then you can have an algorithm to take care of it. So here is an example. You say that you have to satisfy these different requirements. This is very simple example. Okay, so what we can do is that you take all these statements and turn them into this formula. It's a little", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4587787_ms_-_4660377_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4587787, "end_ms": 4660377}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4649145 ms - 4717325 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4649145 ms - 4717325 ms\n\nContent: say that you have to satisfy these different requirements. This is very simple example. Okay, so what we can do is that you take all these statements and turn them into this formula. It's a little code, it is a domain specific language. It is actually easier, I believe it is much easier to generate than arbitrary Python code from an English description. Okay, but it's kind of a form of semantic parsing before we talk about queries. And now you are writing SMT formula, but you can easily kind of get C the correspondence. Even if you are not familiar with smt, you declare the variables and then you say five. You have five rows here looking for five courses that you are taking. So for each one it's a clause to say that course one has to be one of these course two, course three, course four, course five. And then the second section here is that these courses have to exist in your transcript. And now it is a fully executable program to give you perfect compliance testing. Okay, so the", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4649145_ms_-_4717325_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4649145, "end_ms": 4717325}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4701367 ms - 4770805 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4701367 ms - 4770805 ms\n\nContent: course five. And then the second section here is that these courses have to exist in your transcript. And now it is a fully executable program to give you perfect compliance testing. Okay, so the project here is how do we take English statements and turn them into this formula? And this is easier than trying to prove. To just use LLM to say whether a transcript is accurate or is correct or not is satisfiable whether it satisfies a degree or not. Yes, you have a question. Something I'm wondering is that like I actually built the same system because I'm too lazy to go in and check all the requirements myself. And the method I used was I had it write Python code to create a set of these five and then as it parses my transcript, it will use the Python to check off the set rather than for me. And so this allows the LLM to have a lot less repetition in the code. And I wonder if. Which of course the difference is that this is a declaration program. You write down the constraints and", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4701367_ms_-_4770805_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4701367, "end_ms": 4770805}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4756211 ms - 4825645 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4756211 ms - 4825645 ms\n\nContent: for me. And so this allows the LLM to have a lot less repetition in the code. And I wonder if. Which of course the difference is that this is a declaration program. You write down the constraints and sometimes the constraints are actually circular and they're much harder than execute the code line by line. This is the tiny example, okay, where you don't need the full. You can easily translate them into Python. But when you have constraints like you can take this and this, but then together they cannot be this and so forth, it actually catches it in. This is called declarative programming as opposed to you're translating them into executable imperative programs. This is much more powerful because of the theorem prover. A simple example here is that I have constraints I can take some courses I can assign. If I have a choice in assignment such that one is correct and one is not, I mean one satisfied and one doesn't, then this will save you a lot of time from your coding, okay, because", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4756211_ms_-_4825645_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4756211, "end_ms": 4825645}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4810179 ms - 4880485 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4810179 ms - 4880485 ms\n\nContent: I can assign. If I have a choice in assignment such that one is correct and one is not, I mean one satisfied and one doesn't, then this will save you a lot of time from your coding, okay, because sometimes you can decide, I can use this course to satisfy this requirement or that requirement. Which one should you pick? Depends on what courses you are taking. And this is what it can do. This will solve this without you thinking about it. It will evaluate all the possibilities. It's just a stronger language because you have a theorem proverbs. Okay, good question. So this is. I'm just showing you course requirements. It applies to many things. I think in the project proposals we talk about invoice auditing and it is one of. It is exactly the same type of problem. Finally, I want to talk about web agents. There is a lot of popular. It's a popular topic. It's like how can we get an agent to go on the web and do things for you? And that turns out to be. So there is a data set mind to web", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4810179_ms_-_4880485_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4810179, "end_ms": 4880485}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4865175 ms - 4958733 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4865175 ms - 4958733 ms\n\nContent: web agents. There is a lot of popular. It's a popular topic. It's like how can we get an agent to go on the web and do things for you? And that turns out to be. So there is a data set mind to web and the authors just wrote a paper that says GPT4O Vision is a general web agent if grounded. But the if grounded is the hard part. What does grounding mean here is that which button Are you supposed to push? Because this is very visual and we actually had a paper in 2021 that shows you the right thing to do is to introduce an intermediate representation. And the representation is a description of the visual elements in words. So for example, you say enter the user's email in the text field under email address or username. Humans can understand that, no problem. Okay, but now you have to push the right button on the screen. Okay, now that is the grounding part. So what we have done is to show that you take expert sentences like this, you break them down into a combination of a few actions", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4865175_ms_-_4958733_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4865175, "end_ms": 4958733}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 4942973 ms - 5023935 ms", "content": "Title: CS224V Lecture 6 > Transcript > 4942973 ms - 5023935 ms\n\nContent: the right button on the screen. Okay, now that is the grounding part. So what we have done is to show that you take expert sentences like this, you break them down into a combination of a few actions such as go to ask and retrieve. And when the retrieve, the description is that it's an email address or username and then you. And so forth. So you break them down into these description. Now you have to find the thing on the web. So then we actually use a machine learning module to match the description to the actual HTML operation. Once you break it down then you can just do machine learning on identifying visual elements rather than from the very top to the very end, which is what the mind to web project is. Yes. In co Storm right now is there like a multimodal setting? Like if I want to like please add one. Okay, we do. We. We have not done multimodal on Storm. This is, this is, this is obviously a multimodal project and this was done before LLM. This is 2021. There's a. This is time", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_4942973_ms_-_5023935_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 4942973, "end_ms": 5023935}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 5007231 ms - 5079761 ms", "content": "Title: CS224V Lecture 6 > Transcript > 5007231 ms - 5079761 ms\n\nContent: please add one. Okay, we do. We. We have not done multimodal on Storm. This is, this is, this is obviously a multimodal project and this was done before LLM. This is 2021. There's a. This is time to revisit that concept of intermediate representation. We have two questions. Yeah, go ahead. I thought it was interesting, you know, sort of seeing a connection between this web agent and the improving side, which is that there's also. There's already some automated tools like the Lean programming language. They're improving programming languages and likewise there's like a bunch of architecture for testing websites like Selenium is something. So I'm curious, like you can use. All these tools but the problem here is. How do I say it in natural language? That's the leap that we are taking. So Theorem Prover is a project that has gone on for a couple of decades and we have amazing theorem provers use it. I can tell you that there is no way you can expect an LLM to be as good as a theorem", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_5007231_ms_-_5079761_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 5007231, "end_ms": 5079761}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 5063753 ms - 5141769 ms", "content": "Title: CS224V Lecture 6 > Transcript > 5063753 ms - 5141769 ms\n\nContent: Theorem Prover is a project that has gone on for a couple of decades and we have amazing theorem provers use it. I can tell you that there is no way you can expect an LLM to be as good as a theorem proverb because it is an NP complete problem problem that it is actually solving. You have a question similar test cases. Because we know the older system are building on the machine base, not human based. And if we use with visual, I think it could be one direction. But is there any possible that to change the machine system to make it understand AI part. If I don't have to deal with the web, I'll do it in the GENIE worksheet way. I just bypass the visual. I know that I need this information. I just ask and I put it in. I don't need the web. But there is a lot of legacy problems that why we want the web. It's all a legacy problem. I think unfortunately that's true that you have them. Okay, so this is it. So one last question. Yeah, I sort of had a follow up. What I was getting at is that", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_5063753_ms_-_5141769_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 5063753, "end_ms": 5141769}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 5127173 ms - 5205011 ms", "content": "Title: CS224V Lecture 6 > Transcript > 5127173 ms - 5205011 ms\n\nContent: the web. It's all a legacy problem. I think unfortunately that's true that you have them. Okay, so this is it. So one last question. Yeah, I sort of had a follow up. What I was getting at is that like if there's tools to sort of programmatically do some of these things already and I know that like LLMs can be really, really good at generating code. They are not very good at generating code. They are generating code. The problem here is that there, there is easy code and then there's hard code. If you wanted to generate a website, it's very reasonable because you can tell when a website fails. If you wanted to generate security code, I would say no. Okay, so this is what I mean. So I have a student who loves this and he has been using those copilots and stu stuff like that for writing code. And finally a little bit after a little bit of that, he came to me and he said, you know, the bugs that the GPT introduces are really hard to find. Okay, just keep that in mind. In software", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_5127173_ms_-_5205011_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 5127173, "end_ms": 5205011}}
{"document_title": "CS224V Lecture 6", "section_title": "CS224V Lecture 6 > Transcript > 5189051 ms - 5267375 ms", "content": "code. And finally a little bit after a little bit of that, he came to me and he said, you know, the bugs that the GPT introduces are really hard to find. Okay, just keep that in mind. In software engineering, we all know that the time you write the first code is a tiny fraction to the lifetime maintenance of the software. It's debugging that is hard. So I always say find the best programmer that do not introduce bugs. I don't mind if they even tick slower, but you often are not. They do not introduce the bugs and you have an easier time. I love using GPT to help me remember which APIs to call and stuff like that. It is a really good development environment. But if you want to write something really hard and you care about the correctness, be careful. Okay? Just be really careful. All right? Okay, so that wraps it up today and then we will pick it up again on Monday. And please sign up if you can, on Monday because we want to give you early feedback. All right? It.", "block_metadata": {"id": "CS224V_Lecture_6_>_Transcript_>_5189051_ms_-_5267375_ms", "document_type": "transcript", "lecture_number": 6, "start_ms": 5189051, "end_ms": 5267375}}
