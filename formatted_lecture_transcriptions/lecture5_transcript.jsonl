{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Projects in the Technology Course", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Projects in the Technology Course\n\nContent: Projects are a really big part of this course. We are devoting a few lectures to it and we are. It is another round of project proposals. Please consider signing up for next Wednesday, this coming Wednesday.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Projects_in_the_Technology_Course", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 120905, "end_ms": 219975}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Getting From Research to a Working System", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Getting From Research to a Working System\n\nContent: NLP is never finished. You get feedback from in the wild. It is better than anything else out there. What that means is that you can now get user feedback and go to the next round of research.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Getting_From_Research_to_a_Working_System", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 220395, "end_ms": 277495}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > What does it take to create a chatbot with LLMs?", "content": "Title: CS224V Lecture 5 > Chapter Summaries > What does it take to create a chatbot with LLMs?\n\nContent: Today we are going to talk about grounding on free text and we're going to use generation and IR information retrieval. The second goal here is how do you engineer with LLMs. Wiki chat is really our first example. It is probably the easiest among the things that we are doing.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_What_does_it_take_to_create_a_chatbot_with_LLMs?", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 277615, "end_ms": 608597}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Languages and conversational agents", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Languages and conversational agents\n\nContent: The strengths of the LLM is that it is not just a Q and a problem. It will tell you all related facts, interesting facts that you are not even asking. The weaknesses is hallucination. Long tail information, you cannot trust it. You really want to extend it to cover all these important use cases.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Languages_and_conversational_agents", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 608781, "end_ms": 851925}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Deep Learning and the future of search", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Deep Learning and the future of search\n\nContent: 2014 is the beginning of deep learning. Then 2018, that's BERT. By the time we reach 2020 with GPT3 and 2023, now we have GPT4. And this large language models just changed tons of things. Things just keep improving.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Deep_Learning_and_the_future_of_search", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 853185, "end_ms": 1099725}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Blender Bot 3.1 and Crowdsourcing", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Blender Bot 3.1 and Crowdsourcing\n\nContent: Blender Bot 3 with 175 billion parameters are better in consistency, knowledge, factuality and so forth. Even the smaller version without IR has only 5.1% factual error. Being able to read papers, analyze and understand it is part of the goal of this class.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Blender_Bot_3.1_and_Crowdsourcing", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1100745, "end_ms": 1299085}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Australian Open: Human Evaluation", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Australian Open: Human Evaluation\n\nContent: The 2023 Australian Open is the first of the four Grand Slam tennis events held each year. It is also one of the two major professional tennis tournaments played on hot courts. But it was actually Djokovic who claimed the men single in that year. And it just lied.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Australian_Open:_Human_Evaluation", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1299475, "end_ms": 1413197}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Interviewing Haruki Murakami", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Interviewing Haruki Murakami\n\nContent: Who is Haruki Murakami? He's a Japanese writer. Have you read any of his books? So are the movies made from them? What a wonderful conversation.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Interviewing_Haruki_Murakami", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1413341, "end_ms": 1449975}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Anomaly in the LLM", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Anomaly in the LLM\n\nContent: A typical crowdsource worker is not going to be thinking about things that will be hard for the agent. The evaluations don't mean anything unless you dig into what the test set is. Be very, very careful.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Anomaly_in_the_LLM", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1450595, "end_ms": 1606789}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Atlas", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Atlas\n\nContent:  Atlas has been the state of the art in 2022 for many popular knowledge intensive tasks. It uses word level F1 to overlap the model's utterances. It favors generic, irrelevant and extractive responses.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Atlas", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1606897, "end_ms": 1659081}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Chatbot: Funky, Informative", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Chatbot: Funky, Informative\n\nContent: Conversational bot should bring in information beyond just a dry answer. What other characteristics do you think we should see from a conversational bot besides factuality?", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Chatbot:_Funky,_Informative", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1659273, "end_ms": 1863785}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Basic criteria for learning from the brain", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Basic criteria for learning from the brain\n\nContent: The relevancy, you don't want to talk about things that are off the topic. Informational. They should provide up to date information and use the appropriate tense. They have to be temporally correct. These are some of the metrics that we have noticed that to be of importance.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Basic_criteria_for_learning_from_the_brain", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1864605, "end_ms": 1937675}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Building a good conversational bot on Wikipedia", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Building a good conversational bot on Wikipedia\n\nContent: The conversational bot is only as good as your sources. It is now sourcing from sourcing from 10 Wikipedias. It can also talk in multiple languages. There is a lot of missing information, that's for sure.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Building_a_good_conversational_bot_on_Wikipedia", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 1938095, "end_ms": 2189313}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Two pipelines of Natural Language Models", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Two pipelines of Natural Language Models\n\nContent: Wiki Chat can retrieve relevant information for factuality, but for conversationality. We are also letting it generate so that we can pick up information that is relevant but not as it is. What Wiki Chat does is that it turns into an answer that is both factual and interesting.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Two_pipelines_of_Natural_Language_Models", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 2189489, "end_ms": 2582455}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Information retrieval, generation and the", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Information retrieval, generation and the\n\nContent: The effectiveness depends on the length of the document. If it is short, it's easy but if it's long, it is harder. We have to look at the data corpus so that when I ask a question, I can find the document from which I can get the information from.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Information_retrieval,_generation_and_the", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 2583715, "end_ms": 2780385}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Google Brain: The Problem with the Search", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Google Brain: The Problem with the Search\n\nContent: The first problem we discovered is time. The query has to be self contained, decontextualized. How do you make the improvement in assessment? Currently we are trying to break it.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Google_Brain:_The_Problem_with_the_Search", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 2782255, "end_ms": 3086625}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Bing Chat: Retrieval Augmented Generation", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Bing Chat: Retrieval Augmented Generation\n\nContent: Less than 60% of the facts are actually grounded in the citations that Bing Chat gives you. Only 60% is checked by the citations. The answers are dry and it's not conversational. We did our own experiment on Stack Exchange.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Bing_Chat:_Retrieval_Augmented_Generation", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 3087245, "end_ms": 3366655}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Bling Search: Do LLMs Get It Right?", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Bling Search: Do LLMs Get It Right?\n\nContent: When it comes to stack exchange, 9 out of the 10 results from Bing contains contain hallucination. If the information retrieval does not return relevant result, LLMs do like to hallucinate. Solution is don't even ask the LLM to come up with the answer. Just ask if this information is relevant.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Bling_Search:_Do_LLMs_Get_It_Right?", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 3367235, "end_ms": 3647445}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Fact-Checking Hard", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Fact-Checking Hard\n\nContent: John Avlon: What we do is that we take the generated answer and we fact check it. He says fact checking is actually very hard. Avlon says it's complicated, hard to say true, but misleading and just ultimately false.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Fact-Checking_Hard", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 3648105, "end_ms": 4047231}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Generating a Wikipedia page with a single claim", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Generating a Wikipedia page with a single claim\n\nContent: In Wikipedia it's mostly consistent. If you give me a single sentence, a single atomic sentence, it is either true or false. If it has multiple claims in them, then we have to do a fraction of it to figure out whether which claims are true and false.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Generating_a_Wikipedia_page_with_a_single_claim", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 4047343, "end_ms": 4113745}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > How to fact check a document in GPT", "content": "Title: CS224V Lecture 5 > Chapter Summaries > How to fact check a document in GPT\n\nContent: In generation you have to fact check claim by claim. It can be done quite well so far for this kind of topics using state of the art. But the fact checking part is the weakest part of our pipeline.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_How_to_fact_check_a_document_in_GPT", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 4118605, "end_ms": 4387027}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > What makes GPT4 so challenging?", "content": "Title: CS224V Lecture 5 > Chapter Summaries > What makes GPT4 so challenging?\n\nContent: The rate the things that you can do is very large. There's a lack of automated metrics. The key here is to speed up the performance and lower the cost. The fine tuning step here is not to improve the result, it's just to make it faster and cheaper.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_What_makes_GPT4_so_challenging?", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 4387091, "end_ms": 4672245}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Bing Chat and the OpenAI Model", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Bing Chat and the OpenAI Model\n\nContent: Jay: Why do you think the Bing chat had so many like inaccurate responses? Jay: If you test it on the regular data sets, you get into trouble. Compared to today, NLP two years ago is like in the Stone Ages. It just gets better when everybody puts out their open source models.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Bing_Chat_and_the_OpenAI_Model", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 4672365, "end_ms": 4933701}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Chapter Summaries > Projects on a Wednesday", "content": "Title: CS224V Lecture 5 > Chapter Summaries > Projects on a Wednesday\n\nContent: We are opening up the Wednesday schedule for people to, to pitch their projects. I encourage you to speak as soon as you know what you are doing. So this is this Wednesday.", "block_metadata": {"id": "CS224V_Lecture_5_>_Chapter_Summaries_>_Projects_on_a_Wednesday", "document_type": "chapter summary", "lecture_number": 5, "start_ms": 4933853, "end_ms": 4958805}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 120905 ms - 200651 ms", "content": "Title: CS224V Lecture 5 > Transcript > 120905 ms - 200651 ms\n\nContent: That would help. All right. Good afternoon. So how did you like the projects learned last? You know, projects presented last week? Some of you like them? Yeah. So we're going to do more. I mean, projects are a really big part of this course. So we are devoting a few lectures to it and we are. It is another round of project proposals and I really want to encourage people who have an idea that you want to work on, whether you are looking for a partner or not. You know, come and pitch it. So the class is a little bit big. I don't really have the time to have every person get up during the formal project proposal stage. But we are very happy to take the people who already know what they want to do, get them to speak earlier, and we can give them early feedback and get them going. Okay. And listening to other people's projects is a great way of figuring out what other projects, you know, what projects you may want to do. So we have shared a sign up sheet and please consider signing up for", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_120905_ms_-_200651_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 120905, "end_ms": 200651}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 188203 ms - 257811 ms", "content": "Title: CS224V Lecture 5 > Transcript > 188203 ms - 257811 ms\n\nContent: to other people's projects is a great way of figuring out what other projects, you know, what projects you may want to do. So we have shared a sign up sheet and please consider signing up for next Wednesday, this coming Wednesday. Okay. It's a five minute things. You saw what happened last time and it was. I mean, there are so many different things you can do with this technology. So think about it. Sign up and we'll have a good session on Wednesday. So today we're going to start with getting into the details, lecturing. Like, what do you really need to do to build working systems, systems that actually can be used by users. I don't know for how many years people have been working on NLP projects and they don't really see the light of day. They are papers and they say, I have a conversational agent. And then you try it, it just doesn't work. I mean, we were in the same boat two years ago. We did a lot of work. It is better than anything else out there. I still could not put it up, but", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_188203_ms_-_257811_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 188203, "end_ms": 257811}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 246803 ms - 317613 ms", "content": "Title: CS224V Lecture 5 > Transcript > 246803 ms - 317613 ms\n\nContent: agent. And then you try it, it just doesn't work. I mean, we were in the same boat two years ago. We did a lot of work. It is better than anything else out there. I still could not put it up, but now we could. So that's exciting for us because what that means is that you can now get user feedback and go to the next round of research. Okay. It is never finished. It's not like, oh, I'm finished with this research. But the point here is that you get feedback from in the wild. You do different things. Okay, so today we are going to talk about grounding on free text and we're going to use generation and IR information retrieval. How many people have heard of information retrieval? Ah, okay. So there are two goals in this lecture. The first one is what does it take to create a chatbot? I want to emphasize I'm not doing just question and answering. I'm doing conversational, okay? I want it to be conversational. And of course we are not allowing hallucination. And what we want to do is to", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_246803_ms_-_317613_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 246803, "end_ms": 317613}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 304501 ms - 378981 ms", "content": "Title: CS224V Lecture 5 > Transcript > 304501 ms - 378981 ms\n\nContent: to emphasize I'm not doing just question and answering. I'm doing conversational, okay? I want it to be conversational. And of course we are not allowing hallucination. And what we want to do is to learn about the state of the art. And we will be talking about WikiChat. We introduced it in the first class, but we didn't talk about what it took to build it. And that's what you're going to hear. And the second goal here of this lecture is how do you engineer with LLMs? Basically? Okay, it is, it is. We learned a lot. We learned a lot. Okay? So one thing that I have noticed is that if I have a student who doesn't believe that this problem can be solved using LLMs, it will not work. Okay? They try it, they say, see, it doesn't work. I told you so. It doesn't work. Okay? But the most important thing is that you should have a good educated guess on whether it works or doesn't work. And at this point I'm very good at it. Okay? And you have it. Sometimes it just doesn't work when you try it", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_304501_ms_-_378981_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 304501, "end_ms": 378981}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 365117 ms - 449671 ms", "content": "Title: CS224V Lecture 5 > Transcript > 365117 ms - 449671 ms\n\nContent: thing is that you should have a good educated guess on whether it works or doesn't work. And at this point I'm very good at it. Okay? And you have it. Sometimes it just doesn't work when you try it for strange reasons. It's not inherent reasons. So there are lots of things that we have learned and I'm going to try to talk about it in this particular use case to illustrate the details. So what we learn is that there is a new shift. It is, you know, we have tons of people trained with, for example, the 224N style classes. And it is a traditional deep learning methodology, which is that you give me input and annotated output and then you say, here is my training data. We fine tune a language model. And actually before people were actually making up new models. Now more or less, people are using fine tuning large language models. And then, you know, that is the methodology. But the truth of the matter is that the LLMs have outrun the current NLP methodology. We've seen this over and over", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_365117_ms_-_449671_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 365117, "end_ms": 449671}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 434403 ms - 517633 ms", "content": "Title: CS224V Lecture 5 > Transcript > 434403 ms - 517633 ms\n\nContent: fine tuning large language models. And then, you know, that is the methodology. But the truth of the matter is that the LLMs have outrun the current NLP methodology. We've seen this over and over again. And the worst thing that has happened that we discover is that we just use existing benchmarks and annotated data and we run it and then the result is bad. And then what we realize is that LLMs are actually better than the annotations. Okay? So that completely changes this concept of taking these existing annotation benchmark and using it to tune your system if it is already worse than the LLMs. I mean, we're in a different world. So when we build our system now with LLM technology, metallic methodology is that we're going to be building these pipelines where we use LLMs as subroutines and you have to design your prompt very Carefully. How do you do that? It depends on the weakness of your basic LLM pipeline with respect to the problem that you are looking for. What we discovered is", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_434403_ms_-_517633_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 434403, "end_ms": 517633}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 499721 ms - 572853 ms", "content": "Title: CS224V Lecture 5 > Transcript > 499721 ms - 572853 ms\n\nContent: you have to design your prompt very Carefully. How do you do that? It depends on the weakness of your basic LLM pipeline with respect to the problem that you are looking for. What we discovered is that you don't know what the weakness is until you try it. You try it, you figure it out and then you say, ah, I have to solve this problem, this problem, that problem, and so forth. And then you got to the point where it works and then you say, oh, but it takes too long. So we use a distillation method and we distill it down to the smaller models for speed and cost. Okay, so this has been our experience. This is the basic. It just gets more and more sophisticated as we work on harder and harder problems. And Wiki chat is really our first example. It is probably the easiest among the things that we are doing. So this is why we are talking about it first. And there's a lot of difficulty here is how do you assess your new pipeline, your prompts, and how do you iterate? So here's the outline of", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_499721_ms_-_572853_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 499721, "end_ms": 572853}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 557149 ms - 631957 ms", "content": "Title: CS224V Lecture 5 > Transcript > 557149 ms - 631957 ms\n\nContent: we are doing. So this is why we are talking about it first. And there's a lot of difficulty here is how do you assess your new pipeline, your prompts, and how do you iterate? So here's the outline of the talk. We're going to start with a brief history of a lot of work. The history is large, but we're going to make it very, very brief. We talk about the metrics for the knowledge chatbot, we talk about how do you build what is the idea that makes up the wiki chat. It's a pipeline of LLMs. As we discussed, we did the distillation on the llama, and then we talk about the assessment evaluation. And at the end of that, I'm going to talk about going from the single language wiki chat to multilingual wiki chat if we have the time. Okay, so let's talk about LLM chatbots. I want to remind you what we talked about in lecture one. And it shows you a lot of weaknesses, but let's talk about the strengths first. The strengths of the LLM is that it is not just a Q and a problem. It's not a search", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_557149_ms_-_631957_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 557149, "end_ms": 631957}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 614381 ms - 687963 ms", "content": "Title: CS224V Lecture 5 > Transcript > 614381 ms - 687963 ms\n\nContent: we talked about in lecture one. And it shows you a lot of weaknesses, but let's talk about the strengths first. The strengths of the LLM is that it is not just a Q and a problem. It's not a search engine. The reason is that you say something, it will tell you all related facts, interesting facts that you are not even asking. You know, in the second class we talk about the unknown unknowns. Okay, I don't even know what to ask because I don't know about it. And conversational agent is very important in that respect. It would tell you things that you did not even think about or aware of and you wouldn't be asking those questions. And so that's a strength of chatbots. And as we work on factuality, we don't want to lose sight of that. Okay, the second part is that it brings in perspectives. Is this a good movie? Is it Whatever. You know, there are things that are just high level. It's not just dry facts. One after the other. The weaknesses is hallucination. And there are various reasons", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_614381_ms_-_687963_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 614381, "end_ms": 687963}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 673451 ms - 749383 ms", "content": "Title: CS224V Lecture 5 > Transcript > 673451 ms - 749383 ms\n\nContent: Is this a good movie? Is it Whatever. You know, there are things that are just high level. It's not just dry facts. One after the other. The weaknesses is hallucination. And there are various reasons people think that the number one, the only problem is that it doesn't understand real, I mean recent information, information that was created after training that is obvious that it doesn't know. But that is not the only problem we have. Number one is long tailed information. There are so many facts that are in the literature, it cannot even code it all up. Okay, so we have some examples over there. You just have to ask questions that are not the most popular ones. It's going to fail. The curious thing is that most people think that the GPTs work very well because they keep testing it with things that they know. If they know, GPT knows. If they don't know, they don't know that GPT actually doesn't know. This is the fallacy. I mean this is one of the major issues with GPTs. So long tail", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_673451_ms_-_749383_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 673451, "end_ms": 749383}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 735743 ms - 807663 ms", "content": "Title: CS224V Lecture 5 > Transcript > 735743 ms - 807663 ms\n\nContent: things that they know. If they know, GPT knows. If they don't know, they don't know that GPT actually doesn't know. This is the fallacy. I mean this is one of the major issues with GPTs. So long tail information, you cannot trust it. Real time knowledge, clearly you know, you cannot, you don't have that information. You can say, oh, let us keep training, doing continuous training. But if I want to find out about the earthquake that just happened, okay, I need to know it, right? I mean this is really important. I'm not going to wait till you finish training with a new earthquake information. No choice. I have to be able to get to real time knowledge. The third thing here is that you have a very powerful system that can get information out of all kinds of documents. There are tons of documents that are private, confidential. For example, if I'm a doctor, I want to be able to say, look, here is the data, here is my, this is a patient coming in and I want to get the background, okay? And", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_735743_ms_-_807663_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 735743, "end_ms": 807663}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 792315 ms - 871365 ms", "content": "Title: CS224V Lecture 5 > Transcript > 792315 ms - 871365 ms\n\nContent: that are private, confidential. For example, if I'm a doctor, I want to be able to say, look, here is the data, here is my, this is a patient coming in and I want to get the background, okay? And that information cannot possibly be used in pre training for LLMs. So these are all the reasons that you cannot just rely on find, you cannot rely on pre training. In a sense it is, you know, the LLMs are so powerful already. You really want to extend it to cover all these important use cases. And the answer is using a combination, not just information retrieval, but also take advantage of the generation because I want to retain the strength. Okay? I'm not just doing Q and A. So therefore I need to work generation in together with information retrieval. All right, so that's the brief history. There are many, many papers. And here is just a selection of the papers. I mean that was just the introduction. There are lots and lots of papers. So we have actually Put it into a timeline just to show", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_792315_ms_-_871365_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 792315, "end_ms": 871365}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 856169 ms - 940037 ms", "content": "Title: CS224V Lecture 5 > Transcript > 856169 ms - 940037 ms\n\nContent: are many, many papers. And here is just a selection of the papers. I mean that was just the introduction. There are lots and lots of papers. So we have actually Put it into a timeline just to show you the high level development. The most important thing is what's happened to the language models. 2014 is the beginning of deep learning. Then 2018, that's BERT. It's a language model and it gets larger and larger. It makes a huge difference. By the time we reach 2020 with GPT3 and 2023, now we have GPT4. Okay. And this large language models just changed tons of things. So when we retrieve information, obviously I have to work with my knowledge corpus and they have been. This problem of retrieving information existed for a long time under the title of search, right? You go to Google, you want to learn about X, it is going to retrieve information. And there are all kinds of techniques even before they were developed, even before deep learning, such as keyword search or further on, we are", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_856169_ms_-_940037_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 856169, "end_ms": 940037}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 921333 ms - 1009689 ms", "content": "Title: CS224V Lecture 5 > Transcript > 921333 ms - 1009689 ms\n\nContent: to learn about X, it is going to retrieve information. And there are all kinds of techniques even before they were developed, even before deep learning, such as keyword search or further on, we are encoding information as vectors. And then Google search is based on TF IDF and. And they of course scaled it up and then it continues the memory network in 2015. And what is interesting is that as we go on, the Colbert system is the one that actually does embedding by matching a query to a document. Okay. I'm not just looking for things from a document. We got to a point that says when you're asking different questions, you want to pick up different documents. It's not just mentioning that keyword. And when you start using deep learning to match queries to documents, we are a lot better. That was in 2020. At the same time people have been doing question answering in the early days, even before deep learning. And then we started to do. Once you have deep learning, then we're using machine", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_921333_ms_-_1009689_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 921333, "end_ms": 1009689}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 992371 ms - 1081619 ms", "content": "Title: CS224V Lecture 5 > Transcript > 992371 ms - 1081619 ms\n\nContent: was in 2020. At the same time people have been doing question answering in the early days, even before deep learning. And then we started to do. Once you have deep learning, then we're using machine learning techniques in 2017, 18, for example, the wizard of Wikipedia, it is knowledge grounded conversations. This is when we started seeing conversations as we have gotten better with the search and better language models. There were two pretty interesting chatbots and Atlas. And we will talk about that. And of course the big. You've all heard of Bing Chat, for example. So that is in 2023, the early 2023 and WikiChat came out. Actually the publication of WikiChat was end of 2023. And on top of that, after we finished the paper, there is a new embedding that was released in August 2024. That means two months ago. Okay. And what it does is that it takes Colbert, which match, which matches queries to documents to multilingual. Okay, now we don't even care what language it is coming in from.", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_992371_ms_-_1081619_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 992371, "end_ms": 1081619}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1065715 ms - 1147543 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1065715 ms - 1147543 ms\n\nContent: two months ago. Okay. And what it does is that it takes Colbert, which match, which matches queries to documents to multilingual. Okay, now we don't even care what language it is coming in from. And it is just, you know, things that just keep improving. So for example, our system used to use Colbert. And now it has been upgraded and now it is M3. And now it speaks in many different languages. It is really amazing. So let's talk about the some of these techniques starting from this point, starting with Blender Bot 3. So it was using an Opt 175 billion model. It's a very large language model and it has this pipeline. There's you retrieve information from the Internet and it goes through doing various things and look up information and then you finally generate the answers. And it is built by fine tuning on the combination of 20 large academic data sets. Okay. These are conversational data sets. And of course it accesses a search engine. So they did an evaluation and they compared it", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1065715_ms_-_1147543_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1065715, "end_ms": 1147543}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1131673 ms - 1227047 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1131673 ms - 1227047 ms\n\nContent: by fine tuning on the combination of 20 large academic data sets. Okay. These are conversational data sets. And of course it accesses a search engine. So they did an evaluation and they compared it with previous state of the art on 100 crowdsourced conversations. And as you see here, the numbers of course got better and better. The bold is the best result. And so the BB3, which is Blender Bot 3 with 175 billion parameters, are better in consistency, knowledge, factuality and so forth. The factuality here is now at 2.1% error. And even the smaller version without IR has only 5.1% factual error. What does that mean? That's a little bit surprising. Okay, so the, if you look at the factuality rate, it went from 5.1 to 2.1. It goes from low to even lower. And this is without ir. What does that say to you? What does that say to you? One of the things that I want to do in this class is to talk to you during the lecture. Because most of these things, what is important is to think is to learn", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1131673_ms_-_1227047_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1131673, "end_ms": 1227047}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1212741 ms - 1286689 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1212741 ms - 1286689 ms\n\nContent: say to you? What does that say to you? One of the things that I want to do in this class is to talk to you during the lecture. Because most of these things, what is important is to think is to learn how to think about these things. So I want to kind of work these things with you. Being able to read papers, analyze and understand it is part of the goal, my teaching goal for this class. Okay, what does that mean if it doesn't have like a lot of confidence in the answer? It just says like I don't know or give something. No, it doesn't. Do I? That's a good guess most of the time, actually. I don't know. It's the hardest thing for you to get out of an LLM chapter. No, it doesn't do that. What is the reason? Any guess? Yeah. Michael, does it have to do with how they evaluated factual how? Okay, so that's one possibility. Other ideas? I mean, I know, I mean, of course you don't know anything about it, but if I just present you with this, what questions would you ask? I will reveal this", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1212741_ms_-_1286689_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1212741, "end_ms": 1286689}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1274529 ms - 1347163 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1274529 ms - 1347163 ms\n\nContent: Okay, so that's one possibility. Other ideas? I mean, I know, I mean, of course you don't know anything about it, but if I just present you with this, what questions would you ask? I will reveal this later. What question would you ask? Yes, maybe the topic in these conversations are part of the Training data. Very interesting. Okay, so you are now questioning the benchmark itself, which is part of the evaluation. But a lot of times we take the benchmarks as given. So now we are zooming in. So what does the benchmark say? So it scores very, very high on human evaluation, by the way. So we tried it. Okay, so we start by saying, can you tell me about the 2023 Australian Open? Okay. It's a new event. Right. But it is an ir. It's supposed to retrieve information, so no problem. It's not like I'm tricking it. And Blender Blot says the tournament is the first of the four Grand Slam tennis events held each year. This is the conversational part. Very good. Preceding the French Open, Wimbledon", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1274529_ms_-_1347163_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1274529, "end_ms": 1347163}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1335051 ms - 1421053 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1335051 ms - 1421053 ms\n\nContent: tricking it. And Blender Blot says the tournament is the first of the four Grand Slam tennis events held each year. This is the conversational part. Very good. Preceding the French Open, Wimbledon and the US Open, because the other three are really more famous. So that was. It's doing a good job and giving you the perspective. It is also one of the two major professional tennis tournaments played on hot courts. So of course you want to know who won the men's singles. And it went on and said, Daniel Medvedev defeated Djokovic. Djokovic. Djokovic. I'm not a tennis person, as you can tell any tennis fans here? Yeah. What do you think of the answer? Well, he didn't win. Lie. He didn't win it. He didn't win it. It was actually Djokovic who claimed the men single in that year. It is just all a lie. Okay. And his 10th Australian Open title and so forth. This is what you're supposed to. This is the fact. And it just lied. Okay, here's another question. Who is Haruki Murakami? He's a Japanese", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1335051_ms_-_1421053_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1335051, "end_ms": 1421053}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1401241 ms - 1477947 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1401241 ms - 1477947 ms\n\nContent: lie. Okay. And his 10th Australian Open title and so forth. This is what you're supposed to. This is the fact. And it just lied. Okay, here's another question. Who is Haruki Murakami? He's a Japanese writer. Have you read any of his books? Very, very nice. I mean, he's, you know, they're really great. So are the movies made from them? I said, what movies? Is what a great question. The movie After Dark is one of my favorites, but it was so hard to find in the US I had to order it online from Japan. Do you like movies? What a wonderful conversation. Do you know this person in this movie? After Duck is a book. It's not a movie. Okay, so it is doing retrieval. And the things that we just tried. It's not very, very hard question. I mean, I'm not doing anything totally weird, but the answers are wrong. Okay, so they claim that they have 2% error. And we see this kind of result. And what is the reason? So we. So this is just a summary, right? We see that it is very conversational, different", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1401241_ms_-_1477947_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1401241, "end_ms": 1477947}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1461859 ms - 1543795 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1461859 ms - 1543795 ms\n\nContent: wrong. Okay, so they claim that they have 2% error. And we see this kind of result. And what is the reason? So we. So this is just a summary, right? We see that it is very conversational, different from a Q and A. And we see that even with Rag it doesn't mean that you have accuracy. So be very, very careful. Don't just do them. Just don't do the thing that blender Bob3 did. Okay. Which is of course not a, like a quick student project. And when the major issue here is that when the retrieval does not return the answer, the LLM makes it up. This is one of the reasons why it can get it wrong. Okay, but we just saw that it has very high evaluation. It just fails miserably in our test. Why? Okay, now I give you some examples of why it failed. So why. So what has happened? Why do they report 2%? This is like a lie according to our test. So of course they didn't lie. So where is the discrepancy between these two. Yeah. Related to what you're saying. It's overfitting. Like they were training", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1461859_ms_-_1543795_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1461859, "end_ms": 1543795}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1526635 ms - 1602097 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1526635 ms - 1602097 ms\n\nContent: This is like a lie according to our test. So of course they didn't lie. So where is the discrepancy between these two. Yeah. Related to what you're saying. It's overfitting. Like they were training on a corpus that system already knew. So one answer, one person, I think we heard that it's like it is good only for things that they have been trained on. As a matter of fact, you have to look at where did they get these dialogues from? They crowdsource it. They pay somebody and they say, okay, you talk to each other, you ask me some question, I go look up the web and then I give you the answer. And what do these crowdsource workers do? They ask about things that they already know. That is really easy. Everybody knows. LLMs have been trained with it for sure. But the point here is that a typical crowdsource worker is not going to be thinking about things that will be hard for the agent. So be very careful. The evaluations, the numbers don't mean anything unless you dig into what the test", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1526635_ms_-_1602097_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1526635, "end_ms": 1602097}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1582601 ms - 1658305 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1582601 ms - 1658305 ms\n\nContent: crowdsource worker is not going to be thinking about things that will be hard for the agent. So be very careful. The evaluations, the numbers don't mean anything unless you dig into what the test set is. And you're going to see this over and over again. Okay, so that's Blender 3. So Atlas. So Atlas has been the state of the art in 2022 for many popular knowledge intensive tasks, including this dialogue set from wizard of Wikipedia. And it uses word level F1 to overlap the model's utterances. I don't want to get into the details. I just tell you what it is because we're going to move on to more fancier techniques after 2022 with the first 10 sentences of Wikipedia page for the conversation topic. So now it has a good spread. Okay, it's pages. It's information from Wikipedia pages. Not asking some random person, what do you want to talk about? Okay. Because a random person talks about the things that everybody knows. So it is. It favors generic, irrelevant and extractive responses,", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1582601_ms_-_1658305_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1582601, "end_ms": 1658305}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1645249 ms - 1708427 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1645249 ms - 1708427 ms\n\nContent: asking some random person, what do you want to talk about? Okay. Because a random person talks about the things that everybody knows. So it is. It favors generic, irrelevant and extractive responses, actually. All right, so give you an example. I Would like to talk about Amelia Gething. Can you tell me more about her? She was born in London. Nice. I know that. I was wondering if you could tell me more about her work. Oh, she has a YouTube channel. I know she wrote and starred in the BBC show. Have you seen that show? I haven't. I have seen her in the other show. It's all good. It's all good. The conversation is all good. Oh, that's interesting. I haven't seen that show yet, but I have to check it out. What do you think of her performance in it? I liked her. She primarily writes surreal comedy. A little bit off the topic and then the user says, that's cool. Appreciate when writers have a unique style. Have you seen any of her comedy writing? If so, what did you think of it? He says, I", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1645249_ms_-_1708427_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1645249, "end_ms": 1708427}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1696523 ms - 1779221 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1696523 ms - 1779221 ms\n\nContent: A little bit off the topic and then the user says, that's cool. Appreciate when writers have a unique style. Have you seen any of her comedy writing? If so, what did you think of it? He says, I haven't. She has a YouTube channel. So it was good. Maybe up to the last couple of turns. Okay, so what's wrong with this conversation? Do you like this conversation? What's wrong with it? Yeah. You have a name? Oh, Elizabeth. Sorry? Elizabeth. It feels very dry and like Atlas is a carry on conversation. Good catch. It's very dry. You ask me something, I say something a little bit different from Blenderbot. Right. It doesn't offer you anything. What else? It's very dry. Yes. Your name? Oh, I'm Flora. Flora. Alice keeps repeating the same idea of. She has a YouTube channel. Yes. Advice that is annoying. Humans don't do that usually. Not in few turns. Okay, so that's another problem. So that brings us to the question that what you talked about are things that are beyond factuality. Okay. Whether", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1696523_ms_-_1779221_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1696523, "end_ms": 1779221}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1762223 ms - 1838763 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1762223 ms - 1838763 ms\n\nContent: Humans don't do that usually. Not in few turns. Okay, so that's another problem. So that brings us to the question that what you talked about are things that are beyond factuality. Okay. Whether the conversation is good and one of them is that it is whether it is informational. You bring in information that is beyond just a dry answer. So it's informational repetition. We don't want that. Okay. What other characteristics do you think we should see from a conversational bot besides factuality? Yeah, maybe just some like joyful words, I guess or like enthusiasm in the responses they give us. Oh, that's very good. I like that. What's your name? Stephen. Stephen. Thank you, Stephen. Laura, you have another suggestion? I'm also thinking about some level of depth because I believe part of the thing that person from GPT can offer when you are trying to ask about a very specific technical question is that it may be able to solve really important hard problems so you can explain it in a way", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1762223_ms_-_1838763_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1762223, "end_ms": 1838763}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1827739 ms - 1898477 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1827739 ms - 1898477 ms\n\nContent: thing that person from GPT can offer when you are trying to ask about a very specific technical question is that it may be able to solve really important hard problems so you can explain it in a way that the user feels satisfied with the answer. That's very good point. I mean, it is not just facts. Right. You actually are asking why and so forth. Okay, we are not going to get into this for this part of this lecture, mainly because we are dealing with Wikipedia style chatbot. Okay, but that's a very good point. So anything else? So here are some of the things that we put together. I think we kind of mentioned it, the relevancy, you don't want to talk about things that are off the topic. Informational. What is interesting is that under this category, if I say I don't know, that's considered a negative. But I don't know is still better than lying. Okay. It has to be natural. I mean that's taken for granted if it is human. But of course we're a bot. That's important. It has to be non", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1827739_ms_-_1898477_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1827739, "end_ms": 1898477}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1885197 ms - 1954767 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1885197 ms - 1954767 ms\n\nContent: a negative. But I don't know is still better than lying. Okay. It has to be natural. I mean that's taken for granted if it is human. But of course we're a bot. That's important. It has to be non repetitive. And another thing that you will see which you may not be aware of, is that they have to be temporally correct. They should provide up to date information and use the appropriate tense. Okay. This is because when you pre train with a corpus, the whole corpus is in the past. Okay? Everything is something, it's flat. It just happened before you get trained. And it has no sense. It doesn't have a good sense of time. So that's one of the basic things that we have learned. So these are some of the metrics that we have noticed that to be of importance. So now let's talk about how do you build a good conversational chat that is factual? So we're going to ground it. With Wikipedia we assume that there is a source of knowledge. And the, you know, everybody asks me is like, how do you know", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1885197_ms_-_1954767_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1885197, "end_ms": 1954767}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1940983 ms - 2007655 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1940983 ms - 2007655 ms\n\nContent: a good conversational chat that is factual? So we're going to ground it. With Wikipedia we assume that there is a source of knowledge. And the, you know, everybody asks me is like, how do you know you are correct. I was like, I actually don't. I just take whatever is in Wikipedia. Okay? You trust Wikipedia. The conversational bot is only as good as your sources. And the other thing about Wikipedia is that they are very, they are consistent. Each person can have only one birthdate. Okay? So if you go on the web, you may find that there are people reporting different dates with different persons and so forth. There's a lot of contradictions and consistencies, but for the most part the basic information in a Wikipedia page is consistent. But of course we are talking about looking for inconsistencies, but that is kind of like the more extreme end. And we're trying to improve the quality, but for the most part it is consistent. We're going to use English media. Wikipedia to start it is", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1940983_ms_-_2007655_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1940983, "end_ms": 2007655}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 1996335 ms - 2079086 ms", "content": "Title: CS224V Lecture 5 > Transcript > 1996335 ms - 2079086 ms\n\nContent: but that is kind of like the more extreme end. And we're trying to improve the quality, but for the most part it is consistent. We're going to use English media. Wikipedia to start it is very large, 4.3 billion words, 6.7 million articles. This is an open domain knowledge system and you can talk about just about everything. And another reason why we like to use English Wikipedia is that there's A lot of prior work and it's publicly available for reproducibility. But the truth of the matter is that the work is independent of what your corpus is. You guys have seen us using Storm on other corpora. But the same thing with Wikipedia. I mean, with wiki chat you can supply it with a different corpus and it will talk with that corpus. So this is a WikiChat system. It is publicly available. You can go to WikiChat, Genie, Stanford. Edu. So since the paper was published, Sina Sabnani, the student who spoke on Wednesday multiple times he upgraded it to talk multiple languages. It is now sourcing", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_1996335_ms_-_2079086_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 1996335, "end_ms": 2079086}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2052932 ms - 2141125 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2052932 ms - 2141125 ms\n\nContent: go to WikiChat, Genie, Stanford. Edu. So since the paper was published, Sina Sabnani, the student who spoke on Wednesday multiple times he upgraded it to talk multiple languages. It is now sourcing from sourcing from 10 Wikipedias, the top, the most popularly, the Wikipedias that are Most actively updated. 10 languages. And you get asked questions in all kinds of languages, not just those 10. And it will find you information from all the 10 Wikipedias. Okay, so one thing that is interesting is that English is by far the largest, but there is tons of information in all the other Wikipedia pages. And this is the first time where you only need to know one of these languages and you can tap into all the different languages without doing any Google Translate yourself. Okay, yes. Question. Does it happen that the information that exists in different languages are contradictory? You can get more contradictions because of that. And to some extent, Wikipedia pages, you know, talking about two", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2052932_ms_-_2141125_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2052932, "end_ms": 2141125}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2128069 ms - 2196713 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2128069 ms - 2196713 ms\n\nContent: it happen that the information that exists in different languages are contradictory? You can get more contradictions because of that. And to some extent, Wikipedia pages, you know, talking about two different people may end up describing a third person that contradicts each other too. This is what I was trying to allude to. For the most part it is not contradictory, but there is contradiction and we don't do anything about it here. But that's why we have another project. And when you go to multiple languages, that's very, very interesting. There is a lot of missing information, that's for sure. And some pages, some Wikipedia pages are translated from other pages and the translation could be wrong. All kinds of things happen when you do this. Multilingual. It is a very large. There's a very. I forgot how big the Wikipedia is beyond Eng. You can look it up. I should put that in. So let's get into the details. So we said that, look, we need IR because things are there for all the new", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2128069_ms_-_2196713_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2128069, "end_ms": 2196713}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2179517 ms - 2258889 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2179517 ms - 2258889 ms\n\nContent: very. I forgot how big the Wikipedia is beyond Eng. You can look it up. I should put that in. So let's get into the details. So we said that, look, we need IR because things are there for all the new things or private information and so forth. I cannot have LLM generate the answer. If it doesn't know about an earthquake, it cannot tell you about. It cannot even pretend to generate information about that. Like the tsunami that happened in Japan. It just has no clue. I mean, it doesn't know how to do retrieval. So I mean, through. It cannot. Whatever it generates is meaningless. So that's why we have Two pipelines. One is to retrieve relevant information for factuality, but for conversationality. We are also letting it generate so that we can pick up information that is relevant but we don't take it as it is. We're going to make sure it is factual and then we combine them. So this is really a tale of two approaches. So we start with information retrieval. We have this example, I think I", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2179517_ms_-_2258889_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2179517, "end_ms": 2258889}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2244113 ms - 2320127 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2244113 ms - 2320127 ms\n\nContent: take it as it is. We're going to make sure it is factual and then we combine them. So this is really a tale of two approaches. So we start with information retrieval. We have this example, I think I mentioned this before, is like have you heard about this upcoming movie, Oppenheimer? And you ask about the casting and it does this look up of the information and find that Cillian Murphy, for example, is playing Oppenheimer. So that's retrieval. And at the same time we also do generation. And here's example where LLM would say Christopher Nolan is known for his meticulous casting choices. This is the color, okay? We're not adding color to the answers, which is what we want. But then it went on to make up the actors. We discussed this because if he likes to use well known actors, they have to, as it does Nick's word prediction has to predict some popular well known actors. So it just picked Tom Hanks and Michael Caine. Okay? And so at that point we take all these raw information. What", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2244113_ms_-_2320127_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2244113, "end_ms": 2320127}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2301399 ms - 2384811 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2301399 ms - 2384811 ms\n\nContent: to, as it does Nick's word prediction has to predict some popular well known actors. So it just picked Tom Hanks and Michael Caine. Okay? And so at that point we take all these raw information. What Wiki Chat does is that it turns into, into the answer that is both factual and interesting. Okay? So as shown in that example. So let's take a look at how we do it. It turns out that this project needed seven prompts. And they are few shots prompts. They're examples to illustrate the things that it needs to take care of. And it took three people four months, not counting the full evaluation. It was started as a project in this class two years ago. There were two students with SINA trying to do this. And they think that they could. They thought that they could finish it in one quarter. It was generating something. And then it took actually another quarter to finish this project. And why does it take so long? I'm just doing rag. Everybody knows how to do rag. You just put retrieval in front", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2301399_ms_-_2384811_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2301399, "end_ms": 2384811}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2371475 ms - 2445839 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2371475 ms - 2445839 ms\n\nContent: something. And then it took actually another quarter to finish this project. And why does it take so long? I'm just doing rag. Everybody knows how to do rag. You just put retrieval in front of it and you say generate. What is the big deal, right? So this is what we have discovered, that the world doesn't have just the eight planets. There's a new one. It's like the LLM planet. This is what senior calls it, the LLM planet. It doesn't obey all the old rules, okay? It's like you used to rely on gravity. It's different here. So all the things we know about NLP are different. When we try to use LLMs, it really just outran the conventional methodology. I kind of started Telling you about this, I want to reemphasize it. You just cannot iterate on the number of benchmarks and change the model and see what which one is better. It does not work that way. So everything is different because we are all well trained, we know all the NLP techniques, but it doesn't seem to work right off the bat. And", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2371475_ms_-_2445839_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2371475, "end_ms": 2445839}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2430499 ms - 2509015 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2430499 ms - 2509015 ms\n\nContent: see what which one is better. It does not work that way. So everything is different because we are all well trained, we know all the NLP techniques, but it doesn't seem to work right off the bat. And when the hard part is like the assessment, we discovered everything is upside down. What used to be easy is now hard. And what is hard is now easy. It never used to hallucinate, okay? It's just not impossible for it to make up things before large language models. But now, I mean, it doesn't know anything unless you stuff it. But LLMs know all kinds of things. Okay? So everything is kind of upside down. And this LLM planet is quite weird. So we said that we spent three people four months. That's 12 man months. That's not counting my time. And the thing I want to emphasize is that if you put all this energy into an important primitive, it's worth it because we're just sitting on this. We just use wiki chat everywhere. I mean, if I look at a pipeline, there are so many things I'm doing.", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2430499_ms_-_2509015_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2430499, "end_ms": 2509015}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2495923 ms - 2568347 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2495923 ms - 2568347 ms\n\nContent: all this energy into an important primitive, it's worth it because we're just sitting on this. We just use wiki chat everywhere. I mean, if I look at a pipeline, there are so many things I'm doing. Obviously I need retrieval. And now I'm just using this. We made it open source, everybody else benefits from it and people are applying it, using it as a module. So the question really is, can you come up with the right primitive, put time into it, it's worth it. So this primitive does not assume anything about the domain. It's open domain, you can supply with different corporates, a different corpus, and it works. And so that's the basic idea. We get to the 97% accuracy on all kinds of topics, not just the popular ones. It is very slow if you call the LLM many times because I have seven prompts. But we do a fine tuning using distillation to the Lama, we got to 95% accuracy. It's lower, as you kind of expect, but it is much faster and it has comparable conversationality. And what I want to", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2495923_ms_-_2568347_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2495923, "end_ms": 2568347}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2550383 ms - 2628899 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2550383 ms - 2628899 ms\n\nContent: But we do a fine tuning using distillation to the Lama, we got to 95% accuracy. It's lower, as you kind of expect, but it is much faster and it has comparable conversationality. And what I want to show you is that SGPT4, the parent model, gets better. The LLAMA will also get better. Okay? And LLAMA is getting better on its own too. So that's the picture, that's the story that we're going to tell today. So we said there are three things, the retrieval, the generation and the combination. Okay, so three pieces of the puzzle here. So now you have to answer questions from a document and the effectiveness depends on the length of the document. If it is short, it is easy but if it's long, it is harder. Okay, so this is why we need to index the information. We have to look at the data corpus so that when I ask a question, I can find the document from which I can get the information from. You know, Wikipedia is very large. I mean, I cannot just take the whole Wikipedia and give it to the LM", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2550383_ms_-_2628899_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2550383, "end_ms": 2628899}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2616363 ms - 2692997 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2616363 ms - 2692997 ms\n\nContent: that when I ask a question, I can find the document from which I can get the information from. You know, Wikipedia is very large. I mean, I cannot just take the whole Wikipedia and give it to the LM and say, answer the question. It's not possible. That's why we need information retrieval. I'm not going to get into the details of information retrieval in this class. It's topic in the future. But I just want to give you the subroutine. What does it do and how we are using it. What it does is machine learning where you take the queries and documents and you are trying to figure out, given a new query, what document you are supposed to pick up. After the system is trained, we take the quer for a particular query, we come up with a, you know, we summarize it, we get the score for the query and then we compare, we check the similarity score between the queries and all the documents that you have in the corpus. Okay. And this is done by fine tuning Bert. And there's a lot of details because", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2616363_ms_-_2692997_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2616363, "end_ms": 2692997}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2679557 ms - 2753501 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2679557 ms - 2753501 ms\n\nContent: then we compare, we check the similarity score between the queries and all the documents that you have in the corpus. Okay. And this is done by fine tuning Bert. And there's a lot of details because the corpus is huge. So it has to be very efficient in building this Colbert system. And since then we have the M3 embedding, I introduced that earlier. And this is fine tuning XLMR on 17 multilingual passage ranking data sets and some synthetic data. It's multilingual. It goes beyond 100 languages. It's M3, multilingual, multifunctional, which is that it has dense multivector and sparse retrieval multigranularity that handle short questions and long documents. And the original W3, I mean the original wiki chat was done with Colbert. We upgraded to M3 and it was just released two months ago. Okay. And I just want to emphasize this is what happens when you have open source software. You can take advantage of each other's codes. And this is just beautiful. So what it means now in terms of the", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2679557_ms_-_2753501_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2679557, "end_ms": 2753501}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2741517 ms - 2819795 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2741517 ms - 2819795 ms\n\nContent: And I just want to emphasize this is what happens when you have open source software. You can take advantage of each other's codes. And this is just beautiful. So what it means now in terms of the pipeline is that if you give me a question, I go through IR information retrieval, I retrieve the data that I need, and then we use the LLM module that we're going to describe and then we get the answer. And if you don't receive the right information, it's over. All right? Because we're grounded. I don't say things that I cannot retrieve. This is the rule. All right, let's start with the retrieval part. So, for example, in this question here, you say, what do you think about the casting, the first thing is that I give you a query. The query has to be self contained, decontextualized. So instead of saying, what do you think about the casting? It turns into the cast of the Oppenheimer Christopher Nolan film. Okay. And then it goes through retrieval. It picks up the various documents here.", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2741517_ms_-_2819795_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2741517, "end_ms": 2819795}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2803303 ms - 2879839 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2803303 ms - 2879839 ms\n\nContent: of saying, what do you think about the casting? It turns into the cast of the Oppenheimer Christopher Nolan film. Okay. And then it goes through retrieval. It picks up the various documents here. There's something about Christopher Nolan, Oppenheimer, the movie and the person, the actor, Cillian Murphy. And then you generate. So that's the basic idea. I'm going to start with the basic and then I keep adding pipelines to it. All right. I'm just going to more and more stages as you see the problems. So what we did is exactly that. You tried the basic technique and it fails. And then you say, oh no, I need to split the problem up and make each part more accurate. So the first problem we discovered is time. So here's an example. The time of the conversation is March 2023 and January 2023. The TV series House of the Dragon won the Golden Globe best TV series. And when we did the retrieval, we retrieved this data which is about the House of the Dragon earning two golden gold glow, golden", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2803303_ms_-_2879839_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2803303, "end_ms": 2879839}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2862665 ms - 2936343 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2862665 ms - 2936343 ms\n\nContent: TV series House of the Dragon won the Golden Globe best TV series. And when we did the retrieval, we retrieved this data which is about the House of the Dragon earning two golden gold glow, golden global nominations. Okay. That article was written in December. It has no clue whether it, you know, who is winning the award. So here's the conversation. What do you think is the best TV drama? Oh, I think it has to be the House of the Dragon. Well, that's very, very good guess, right? No matter what. I mean, you are just asking what it thinks it's like. Oh, it has to be House of the Award of the Dragon. And did you win any award? It says, yes, it won the award. Okay. So it probably, if it is not hallucinating, it picked up. Oh, it couldn't be. So it picked up a piece of document that says that it has won the award. What else do you think is. Why else do you think it is a global drama? It's just the first reason, blah, blah, blah blah. In January 23rd. Now it is elaborating. And then it", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2862665_ms_-_2936343_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2862665, "end_ms": 2936343}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2922847 ms - 2992227 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2922847 ms - 2992227 ms\n\nContent: it has won the award. What else do you think is. Why else do you think it is a global drama? It's just the first reason, blah, blah, blah blah. In January 23rd. Now it is elaborating. And then it starts by. And then it went on to say this. The series is expected to win this award. Okay. It kind of forgot that it has a document that says, oh yeah, it has won the best drama series. But in the meantime, while I'm talking about whether it's good or not, I pick up information and I just report whatever I happen to have gotten. And it just went back to say, oh, it is expected to win. So it is wrong. And the point is that we are very used to. You're very, very sensitive to time. You know, this is now March and you just told me that it has won the award. Why are you telling me that? It's expected to win the award. So it is all screwed up. So what we have to do is to make sure that in the prompt you teach it about time. Okay? So it says that you are both located in this location. You want to", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2922847_ms_-_2992227_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2922847, "end_ms": 2992227}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 2976443 ms - 3056721 ms", "content": "Title: CS224V Lecture 5 > Transcript > 2976443 ms - 3056721 ms\n\nContent: to win the award. So it is all screwed up. So what we have to do is to make sure that in the prompt you teach it about time. Okay? So it says that you are both located in this location. You want to be contextual both in time and space. Today's date is today. Okay? Then you say, what do you type in the search box? We are trying to tell the system how to generate this query. You say, what date do you want the search results to be? Enter reason if you're looking for the newest result. Enter none. If the date is not important, then we give it a example. This is, this is very important because we're used to time. You don't want to mess up. But the problem is how did we discover that this is a problem? This is all because we are trying the system and discover that it just screws up time. And you have to put. And you have to fix that. But it goes back to the question, how do you make the improvement in assessment? Currently we are just trying it by trying to. We try to break it. Okay? Coming", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_2976443_ms_-_3056721_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 2976443, "end_ms": 3056721}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3039299 ms - 3119765 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3039299 ms - 3119765 ms\n\nContent: have to put. And you have to fix that. But it goes back to the question, how do you make the improvement in assessment? Currently we are just trying it by trying to. We try to break it. Okay? Coming up with the things that is likely to break it because we don't have a data set to test it with and nor can we learn from that. So that's the experience. And so going back to this problem, they're going back to this pipeline. So we add the better query prompt in here. All right. To put in time. Then can we just give all the retrieved documents to the LLM and expect to be correct? So let's talk about RAG now. Retrieval Augmented generation. So we tried out Bing Chat. It's a commercial system. This is Microsoft series, very serious piece of work. And what Bing Chat does is that for verifiability, it always adds the citations to tell you what the data source is. So that's very useful. We always want that because, you know, we're dealing with AI. We want to have interpretability, verifiability.", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3039299_ms_-_3119765_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3039299, "end_ms": 3119765}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3107369 ms - 3184731 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3107369 ms - 3184731 ms\n\nContent: it always adds the citations to tell you what the data source is. So that's very useful. We always want that because, you know, we're dealing with AI. We want to have interpretability, verifiability. So it turns out that in this paper, Nelson Liu et al. With Percy Language, they discovered that only if you take the Bing Chat results only, I mean, less than 60% of the facts are actually grounded in the citations that it gives you. It doesn't mean that the answers are necessarily incorrect, but they don't necessarily. They are not. 40% are not coming from the citations itself. Okay, so it adds to it. What it adds sometimes is correct, sometimes it is not. But it fails the citation test. Okay? They say this is the citation, but it is not even true. All right, so the second problem is that the answers are dry and it's not conversational. So that's what happens if you use a well polished rack system. So we did our own experiment. I just want to give you some experience is like, what does", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3107369_ms_-_3184731_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3107369, "end_ms": 3184731}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3170131 ms - 3239013 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3170131 ms - 3239013 ms\n\nContent: answers are dry and it's not conversational. So that's what happens if you use a well polished rack system. So we did our own experiment. I just want to give you some experience is like, what does that mean that it is not. It is not grounded. So we did this experiment on the. On Stack Exchange. Well, you all know what Stack Exchange is. It's a community question answering system. The Stack Overflow is one of the products of Stack Exchange. Stack Exchange actually answers questions across many, many things. Of course, the most famous one for us is Stack Overflow and we provided this prompt. I am browsing cooking Stack Exchange. Please find me the related information from the website and form an answer. Remember, your claim must be supported by information from this website and you shouldn't make it up. This is what we gave to Bing Chat. So here is an example of the result. You say, hello, this is Bing. I say, okay, hey, do you know any tricks to figure out if my chocolate is properly", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3170131_ms_-_3239013_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3170131, "end_ms": 3239013}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3227611 ms - 3301391 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3227611 ms - 3301391 ms\n\nContent: make it up. This is what we gave to Bing Chat. So here is an example of the result. You say, hello, this is Bing. I say, okay, hey, do you know any tricks to figure out if my chocolate is properly tempered? And Bing answers the question. It has citations and it looks really good. Okay, but then we look into the citations. The first one, it is true. That is the good answer. But the second one, you know, it's just wrong. Okay, and the third one, you give me the citation. I read the citation and check if that is what they say, but they are not true. Okay, so there is a mismatch between even the cited sources and the information that is given back to us. Okay. And that is kind of disturbing, wouldn't you say? Yes. Stephen, what do you mean by the information is not true? Is it because it's just like someone answering a question but then their answer was completely false? No. No. Okay, good question. So you see the tick. The second one, another is to read the label and see if the primary", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3227611_ms_-_3301391_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3227611, "end_ms": 3301391}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3289551 ms - 3361563 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3289551 ms - 3361563 ms\n\nContent: like someone answering a question but then their answer was completely false? No. No. Okay, good question. So you see the tick. The second one, another is to read the label and see if the primary fat is real cocoa butter and the bar is at least 30% fat by MAS. Okay, that's sentence with that citation too. You look up the citation too, and you couldn't find this piece of information, so they made it up from that source. Something. I don't know. I mean, that's hard to interpret. They claim that that sentence comes from that citation. It does not. Where did they get it from? I don't know, but it is not from that citation. Is it correct? I don't know, but it is. The point here is that they say it to cite it, but it isn't. Okay. Normally when you see a number like that, do you go check it? No. You just assume it. Right? But if you go check it, this is what the paper found. Only 60% is checked by the citations. And this is a real life example of it. Okay. Because you just, you can look it", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3289551_ms_-_3361563_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3289551, "end_ms": 3361563}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3344803 ms - 3433575 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3344803 ms - 3433575 ms\n\nContent: You just assume it. Right? But if you go check it, this is what the paper found. Only 60% is checked by the citations. And this is a real life example of it. Okay. Because you just, you can look it up, but it doesn't say anything about the sentence that was given. This is very odd. Okay, so in other words, in our experiment, when it comes to stack exchange, 9 out of the 10 results from Bing contains contain hallucination. 9 out of 10. Okay. Partly because Stack Exchange is not your most common dataset. Okay. We are limiting it. And you know, it doesn't have maybe a lot of information from the web and you know, not like you're searching from the web. But no matter what, nine out of 10 are wrong. And what we see here is that if the. If the information retrieval does not return relevant result, LLMs do like to hallucinate. And what is the solution? The solution is don't even ask the LLM to come up with the answer. You are retrieving information. Just ask if this information is relevant.", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3344803_ms_-_3433575_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3344803, "end_ms": 3433575}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3414555 ms - 3499129 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3414555 ms - 3499129 ms\n\nContent: LLMs do like to hallucinate. And what is the solution? The solution is don't even ask the LLM to come up with the answer. You are retrieving information. Just ask if this information is relevant. Relevancy is easy for LLMs. Answering questions is not. Okay, so the point here is that we are just going to take the information we retrieve, we summarize it. That is done well. And we ask if it is relevant. If it is not relevant, we kick it out. All right? Because you don't want to have random stuff in there. And if you ask them to do those two things, it has no room to hallucinate. That is the key observation. You know what LLMs can do? You only use things that it can do. Well, if you can, yes. Does the summarization have knowledge of the question? You give it the question and then you. But you are asking to. Oh, no, no, no, sorry. I'm just asking it to summarize. I don't. I do the summarization first and then I filter. The filter gives you the relevance. Okay. It's what it is. We are very", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3414555_ms_-_3499129_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3414555, "end_ms": 3499129}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3480321 ms - 3560893 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3480321 ms - 3560893 ms\n\nContent: are asking to. Oh, no, no, no, sorry. I'm just asking it to summarize. I don't. I do the summarization first and then I filter. The filter gives you the relevance. Okay. It's what it is. We are very keen on getting the accuracy. But you are right. If I give it the question and as I summarize, and it has a very strong urge to summarize it with respect to the question and that you don't want. That's what I recall. And it's all open source, you could go check it out. Yes. So if the LLM is like not directly asked to answer the question. Not at all, then how did. What does it give a response that sounds like it's answering the question? You will see. Okay. That's why we have seven prompts. Okay. This is just making sure that information retrieval does not get you wrong facts. Okay? Nine out of ten, don't forget that. Just keep that in mind. Whatever you do, it can be really wrong. So we filter each paragraph. So for example, here, you know, you give me this paragraph, it says much, much of", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3480321_ms_-_3560893_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3480321, "end_ms": 3560893}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3544591 ms - 3619557 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3544591 ms - 3619557 ms\n\nContent: ten, don't forget that. Just keep that in mind. Whatever you do, it can be really wrong. So we filter each paragraph. So for example, here, you know, you give me this paragraph, it says much, much of the Oppenheimer movies cast signed on that date and so forth. And then you just filter out the information that is not correct. And then you get this much of the information back out. So this is summarize and filter. Um, so here is the. Oh, oh, sorry, sorry. You are right. We did give it the question. We did give it the question. The query. Okay, I'm looking at the code here. We know the question. And then here is the title. Here is the article. And then what we say here is to extract the verbatim parts of this article, okay? That's when we tighten it up so that it is possible for your paragraph to say many more things you're pulling out. That is the verbatim parts of this article that are related to search query. So that gives you the better answers. Okay, good, good, good. So this is", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3544591_ms_-_3619557_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3544591, "end_ms": 3619557}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3606973 ms - 3675677 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3606973 ms - 3675677 ms\n\nContent: to say many more things you're pulling out. That is the verbatim parts of this article that are related to search query. So that gives you the better answers. Okay, good, good, good. So this is the way we approach it. So in other words, here is that the retrieval. You have to be very careful. We are using the M3 embedding at this point so that you can do multilingual. It matches questions to the document and you have to look at the details. You have to worry about the time we talk about the summary and the filter. All the, all the details are in the prompts. I mean, it's not that many lines of code, frankly. Okay, so let's talk about generation. So we say that that's a retrieval part. Let's do the generation part. And we know that there is lots of hallucination when you ask alarms to generate. But we need to do this because otherwise it would be very boring and factual. Right? That was the question that was raised just now. So what we do here is that we take the generated answer and", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3606973_ms_-_3675677_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3606973, "end_ms": 3675677}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3663397 ms - 3746843 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3663397 ms - 3746843 ms\n\nContent: But we need to do this because otherwise it would be very boring and factual. Right? That was the question that was raised just now. So what we do here is that we take the generated answer and we fact check it. And fact checking is actually very hard. You see, I really want to get into the details because this is what you need to do to do a good job in your project. Okay? Don't gloss over anything. So why is fact checking hard? So here is an example. It says that who are the top three in this tournament? It is this. Oh, it's the women's Olympic qualifying tournament concept. So it determines which national teams from the north and Central America and the Caribbean region would qualify for the Summer Olympics in soccer. Okay, so that's conc. Not the most popular thing on earth. I mean, GPT is not going to remember that. Perhaps. So anyway, so this is the information that you can retrieve. And it says, oh, this is from. We can retrieve. And it says, oh, it can generate. Sorry, sorry. It", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3663397_ms_-_3746843_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3663397, "end_ms": 3746843}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3731443 ms - 3810677 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3731443 ms - 3810677 ms\n\nContent: is not going to remember that. Perhaps. So anyway, so this is the information that you can retrieve. And it says, oh, this is from. We can retrieve. And it says, oh, it can generate. Sorry, sorry. It does actually generate. It. It says the top two teams from this tournament qualified for the Olympics. United States and Canada. There wasn't an official third place in this tournament since only the top two teams advance to the Olympics. So this comes from GPT. Okay, so you want to fact check this. Is this correct? Well, it turns out that the United States and Canada do represent the north and middle America and all that, but that sentence is actually incorrect. It says that there wasn't an official third place in this tournament since only the top two teams advanced to the Olympics. It turns out that there is a playoff game. After the number one and number two, there is number three and number. You know, the people who didn't make the final playoff, they have a separate competition. And", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3731443_ms_-_3810677_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3731443, "end_ms": 3810677}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3795549 ms - 3874865 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3795549 ms - 3874865 ms\n\nContent: out that there is a playoff game. After the number one and number two, there is number three and number. You know, the people who didn't make the final playoff, they have a separate competition. And this is Mexico and Costa Rica. Costa Rica and Mexico won. So we actually have a third place finish. Okay, so that statement is actually incorrect that there wasn't a third place finish. All right? You see all the subtleties? It's not that easy. It's like, is it true? Is it not true? It's really not that easy to figure out how to fact check. And it is not all the answer is wrong. Part of it is wrong. So fact checking is old. It is not just in cs. It is actually in real life. For example, journalists are always fact checking when they publish an article. They have fact checkers that look at the article and split them up into one claim at a time. Seriously. We met a person, a journalist, who has been working as a fact checker for like 20 years. Okay? She was explaining to me, you take the", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3795549_ms_-_3874865_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3795549, "end_ms": 3874865}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3861007 ms - 3939697 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3861007 ms - 3939697 ms\n\nContent: article and split them up into one claim at a time. Seriously. We met a person, a journalist, who has been working as a fact checker for like 20 years. Okay? She was explaining to me, you take the paragraphs, you take one sentence at a time, and you look up the information before they go to print. This is what people in journalism actually do. So there's a difference between the press and social media. Social media, who knows? I'm just, I'm just sending you whatever somebody else sends me. Okay? But actually, when they publish in the paper, they have to have human checkers on every single fact. And one of the things that people like to do is to fact check politicians. And so, for example, here is a. Here's a statement, and then, you know, and then you tick each statement and then you turn them into. Into pieces to check. So as it turns out that this is big business, okay? There are really places where they fact check politicians. And what do you think of this chart here? It goes from", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3861007_ms_-_3939697_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3861007, "end_ms": 3939697}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 3919435 ms - 4025527 ms", "content": "Title: CS224V Lecture 5 > Transcript > 3919435 ms - 4025527 ms\n\nContent: them into. Into pieces to check. So as it turns out that this is big business, okay? There are really places where they fact check politicians. And what do you think of this chart here? It goes from true to false and then in between it goes from true to mostly true. It's complicated, hard to say true, but misleading and just ultimately false. So here is the, you know, so CNN Politics is this kind of a fact checking site and it is actually a meter. All right, so in Politifact, Politifact it actually rates things from true, mostly true, half true, mostly false, false. And guess what, there's one more pants on fire. Ultimately, I mean extra, extra, not false. So the whole concept here is that there, you know, it is big business, it is not easy and we do have to figure out what the claims are and for each one try the best we can to do fact checking. I want to emphasize this because it is actually very hard to do for humans and for computers. So what are we doing here? So, so when you talk", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_3919435_ms_-_4025527_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 3919435, "end_ms": 4025527}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4003997 ms - 4085403 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4003997 ms - 4085403 ms\n\nContent: and for each one try the best we can to do fact checking. I want to emphasize this because it is actually very hard to do for humans and for computers. So what are we doing here? So, so when you talk about fact checking in our project, we simplify it, okay? We are only fact checking against Wikipedia if it is not in Wikipedia. We say we don't know, we cut it out. I'm not saying it is false, but I just cut it out. So Wikipedia are very factual. So it helps us a lot to make it easier. But I just want to make sure that, you know, as you go into getting sources from the Internet and so forth, some people say it's yes, some people say it's no, but in Wikipedia it's mostly consistent. And so if you give me a single sentence, a single atomic sentence, it is either true or false. If it has multiple claims in them, then we have to do a fraction of it to figure out whether which claims are true and false. So what we need to do here is that we take the each sentence here, we identify each claim", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4003997_ms_-_4085403_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4003997, "end_ms": 4085403}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4069195 ms - 4152055 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4069195 ms - 4152055 ms\n\nContent: claims in them, then we have to do a fraction of it to figure out whether which claims are true and false. So what we need to do here is that we take the each sentence here, we identify each claim and then we have to search the ground truth, meaning that I'm going to look into Wikipedia in our simpler use case and verify if the claim is supported. Okay, so here is all the sentences. We break them up and we go to retrieve and we check. And so what it means now is that when it comes to generate, we have to extract claims. Once we extracted them, we go into generate. And now let's talk about the fact check that we have to go in in order to filter out the un fact checked claims. So this is this part we are just asking GPT. It's like we retrieve the Wikipedia articles. We say here is the claim and you ask GPT is this supported by the article we just retrieved? But it is done claim by claim? Yes. Laura, I was wondering how does this fact checking work for more technical tasks? For example,", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4069195_ms_-_4152055_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4069195, "end_ms": 4152055}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4136783 ms - 4213105 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4136783 ms - 4213105 ms\n\nContent: and you ask GPT is this supported by the article we just retrieved? But it is done claim by claim? Yes. Laura, I was wondering how does this fact checking work for more technical tasks? For example, solving a math problem that's more than fact checking, there is reasoning behind it. I mean seriously, Wikipedia is nothing but a bunch of facts, okay? And I'm just going to concentrate on that because it is when you're talking about math, you don't retrieve the answer. You have to compute the answer based on first principles. Okay? That's a different ballgame. I mean there is maybe there is a table that says the multiplication table or something. But typically you have to compute the answer and that goes into reasoning. So we're just right now working on the easier problem. But you see, we are already spending so much time just to get basic facts right. Okay, so you see that there are many, many prompts each time I'm calling GPT. Am I doing a good job? It turns out that claim", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4136783_ms_-_4213105_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4136783, "end_ms": 4213105}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4193871 ms - 4274507 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4193871 ms - 4274507 ms\n\nContent: see, we are already spending so much time just to get basic facts right. Okay, so you see that there are many, many prompts each time I'm calling GPT. Am I doing a good job? It turns out that claim identification can be done. Well, this is all experimental. I mean, nobody says it does a good job. We have to figure it out. It turns out that the claim identification is good and it can be used. You can use GPT4 on it. And we just have to make sure in the prompts to make sure that the claims are self contained. Okay? It refers to all the entities mentioned in the conversation that is relevant to that particular claim. You don't just say oh, what is the time of that? Okay, no, you have to substitute with the whole phrase that defines what that is. Even though the user is just saying that co reference the search. It can be done quite well so far for this kind of topics using state of the art. The problem is really fact checking and we found that if you look at the literature it just says", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4193871_ms_-_4274507_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4193871, "end_ms": 4274507}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4255983 ms - 4339701 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4255983 ms - 4339701 ms\n\nContent: reference the search. It can be done quite well so far for this kind of topics using state of the art. The problem is really fact checking and we found that if you look at the literature it just says that the automatic Fact checking is limited. 67% F1 with fine tuned llama 83% with few shot chatgpt. In other words, the fact checking part is the weakest part of our pipeline. And this is now this is also part of the reason why I'm not 100%. Okay, so in summary here is that in generation you have to fact check claim by claim, okay? Because a sentence can have part true and part not true and LLM is just not very. We are using LLM for fact checking. We retrieve and say this is supported, we're using LLM, but it is a little bit weak, it is not perfect. So finally we do the combination and it turns out that I kind of said that already. The hardest thing is for the LLM to say, I don't know, it likes to please, it likes to give you an answer. So what we do is that when no information remains", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4255983_ms_-_4339701_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4255983, "end_ms": 4339701}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4326781 ms - 4399117 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4326781 ms - 4399117 ms\n\nContent: out that I kind of said that already. The hardest thing is for the LLM to say, I don't know, it likes to please, it likes to give you an answer. So what we do is that when no information remains in the topic, we have to produce the draft will produce. Sorry, I'm not sure. Okay, so this is explicit. This is something we have to work in. So I go from fact check, then I take that, I go through draft and I come up with a bunch of piece, you know, a few pieces of things to be assembled and refined into a good sentence. So that's the basic idea. And in this example we call LLMs many times. There's five as the basic where and then N for the number of documents retrieved and C is the number of claims generated. Okay. That's what it takes to answer a question. So I gave you the final answer. I just want to talk a little bit about, you know, since we did this work, we have hands on experience. I want to talk about what makes this design challenging. The rate the things that you can do is very", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4326781_ms_-_4399117_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4326781, "end_ms": 4399117}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4387411 ms - 4462381 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4387411 ms - 4462381 ms\n\nContent: want to talk a little bit about, you know, since we did this work, we have hands on experience. I want to talk about what makes this design challenging. The rate the things that you can do is very large. How do we break down the task in the components? It looks reasonable to you perhaps when I present it. But at the beginning we don't have those pieces. We just say beginning and the end. It's like, oh no, it doesn't work. It's like, let me introduce this and this and so forth. Okay? And then you have to decide what are the inputs. And then the few shot performance is sensitive to the instruction and the choice of few shots examples. And there's a lack of automated metrics. The key here is that you don't have a set of pre canned tests and say, oh, it works or let gets to this level. Let me change a few things and go back. You know, we don't have this. So what did we do in this process? What happened is that every week I met with the team, the team says, look, it looks pretty good. And", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4387411_ms_-_4462381_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4387411, "end_ms": 4462381}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4449993 ms - 4507683 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4449993 ms - 4507683 ms\n\nContent: change a few things and go back. You know, we don't have this. So what did we do in this process? What happened is that every week I met with the team, the team says, look, it looks pretty good. And I say, how about this? And it doesn't look very good. And then they go back and they fix it and they say, oh, it looks pretty good. I say, how about this? Okay, so I'm not saying I'm the only ones who come up with the hard questions. They were coming up with the hard questions. They fix it. But when they come to me, I throw in a few more questions. And every time we try, we just keep running into problems week after week after week. And one week, finally I still remember it was pretty interesting and I just said, look, you have got this, this, this, this problem. They went away this time for two weeks and they come back and all of a sudden it just works like a charm, okay? Because we have put all the pieces together and you know, and that was the first time we actually have a good", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4449993_ms_-_4507683_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4449993, "end_ms": 4507683}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4494865 ms - 4572227 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4494865 ms - 4572227 ms\n\nContent: for two weeks and they come back and all of a sudden it just works like a charm, okay? Because we have put all the pieces together and you know, and that was the first time we actually have a good conversation. And then it just kind of, you know, we tune a little bit more and that's what we've got. So as far as wiki chat is concerned, which is mostly factual conversation, we were able to identify the tricky spots. And we know that every piece of the prompt, every part of the pipeline is needed in order to generate the answers that we want. Okay. So after that we do the distillation because it is too slow. The whole idea is that we're using GPT4 as a teacher model, so we have a long pipeline. Right. It seems to work well. So what we are just trying to do is to speed up the performance and lower the cost. We are not trying to improve the result. The fine tuning step here is not to improve the result, it's just to make it faster and cheaper. Because GPT4, using the GPT4, we are able to", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4494865_ms_-_4572227_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4494865, "end_ms": 4572227}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4557803 ms - 4628193 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4557803 ms - 4628193 ms\n\nContent: lower the cost. We are not trying to improve the result. The fine tuning step here is not to improve the result, it's just to make it faster and cheaper. Because GPT4, using the GPT4, we are able to take the inputs and the inputs and the outputs and it looks pretty good. And then we take that and we generate many, many inputs, get the many, many answers. And you use that to fine tune a student model. And after that it went down from the 40 seconds to 4 seconds I think we have, you know, and it is just faster, cheaper and more private. But this kind of distillation, because we're distilling on a much smaller model and we're just using the results from GPT4, we are not expecting it to actually get better with fine tuning. Okay. This is something that we should know. So in other words, you don't need to worry about distillation until you get the full pipeline to work for this kind of problem. Okay. There may be other kinds of problem where I could not possibly get the prompts to work", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4557803_ms_-_4628193_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4557803, "end_ms": 4628193}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4616873 ms - 4686455 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4616873 ms - 4686455 ms\n\nContent: don't need to worry about distillation until you get the full pipeline to work for this kind of problem. Okay. There may be other kinds of problem where I could not possibly get the prompts to work that's safe for another day. Okay. So you get the problems, right? Don't worry about how long it takes. If it needs to have multiple passes, put it in. Okay. Don't pre optimize. And then you can try to distill it down and then you can speed it up in the end. So that's the basic idea. I think I'm going to stop here and not talk about the assessment. It is a long story because everybody will be, will have to deal with assessment. And it was kind of tricky. So we ended up using both automatic and manual techniques. You cannot skip either of those two and we'll talk about it next time. All right. Any questions? Yes. What's your name? Jay. Why do you think the Bing chat had so many like inaccurate responses? Do you think it was a game like training evaluation? They were evaluating the wrong test", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4616873_ms_-_4686455_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4616873, "end_ms": 4686455}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4673397 ms - 4744131 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4673397 ms - 4744131 ms\n\nContent: questions? Yes. What's your name? Jay. Why do you think the Bing chat had so many like inaccurate responses? Do you think it was a game like training evaluation? They were evaluating the wrong test cases? I don't know. I really don't know. I mean the citations, you don't have to do much. You go to stack exchange with 9 out of 10. I don't know why they didn't catch. First of all, we did that experiment. I think it was last year. This time things could have improved. But I just want to show that even last year, this time, Bing Chat, it still has a problem like that. But I cannot speculate why that is the case. As a matter of fact, I just recently, I was talking to a Google person recently and the person was asking me, it's like, why? How do you make sure that it doesn't give people answers based on Reddit? And I don't know why this person asked me this question. I said, you have Google search, you can tell it not to use Reddit, right? So if you look at Storm, I mean, we said we can only", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4673397_ms_-_4744131_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4673397, "end_ms": 4744131}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4730453 ms - 4808901 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4730453 ms - 4808901 ms\n\nContent: on Reddit? And I don't know why this person asked me this question. I said, you have Google search, you can tell it not to use Reddit, right? So if you look at Storm, I mean, we said we can only search on the websites that are approved by Wikipedia. This is just fundamental because you know that the Wikipedia, I mean, the Internet is just full of bad knowledge, right? So all I'm saying here is I don't know why they are not doing it, but in the end they are just engineers in these companies. And what did they test? What did they do? I don't know. I think that if you test it on. I can tell you this. If you test it on the regular data sets, you get into trouble. The bottom line here is that compared to today, NLP two years ago is like in the Stone Ages. It is that much of a difference. And the datasets are usually very dumb, they're very simplistic, and you can be tuning against that and you're just missing the whole boat. I always wonder about the fact that the Google models, they spend", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4730453_ms_-_4808901_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4730453, "end_ms": 4808901}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4793013 ms - 4861469 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4793013 ms - 4861469 ms\n\nContent: datasets are usually very dumb, they're very simplistic, and you can be tuning against that and you're just missing the whole boat. I always wonder about the fact that the Google models, they spend a lot of time on tuning it on academic datasets, whereas OpenAI, they just took the model and put it out in the open and you have lawyers, accountants, or random students, whatever, and they were using it for whatever they want. That wild data set is very, very different from all the academic data sets, okay? So you have to be very, very careful that you can. You don't want to be misled. You know, you're not tuning for the academic data set performance because the old academic data set was built in the Stone Ages and they only test those things that can possibly be more or less done a little bit. You know, the conversations, everything are very, very lame compared to what we are doing in real life. And that's probably one of the reasons why OpenAI's model got so much faster. Got so much", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4793013_ms_-_4861469_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4793013, "end_ms": 4861469}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4847173 ms - 4922701 ms", "content": "Title: CS224V Lecture 5 > Transcript > 4847173 ms - 4922701 ms\n\nContent: bit. You know, the conversations, everything are very, very lame compared to what we are doing in real life. And that's probably one of the reasons why OpenAI's model got so much faster. Got so much better, faster. It's in the wild. But I'M not saying that, you know, I don't know what Bing or what the other people did by just something that the academics should know because we are very used to those data sets. Okay. I have, I will show you so many papers that we have to. We did. We have to hand analyze the result because we are better than the annotated data sets. If you tune with that, you're in trouble. Any other questions? So you can take a look at the prompts. I mean, I cannot emphasize enough that the devil is in the details. And the trick for my group is that we pick the most important things to clean up so that you can build on top of it and it is worth it. And so we also benefit from other people's modules like the empty. I mean, we knew that there was this problem that we", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4847173_ms_-_4922701_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4847173, "end_ms": 4922701}}
{"document_title": "CS224V Lecture 5", "section_title": "CS224V Lecture 5 > Transcript > 4907657 ms - 4958805 ms", "content": "things to clean up so that you can build on top of it and it is worth it. And so we also benefit from other people's modules like the empty. I mean, we knew that there was this problem that we cannot handle multilinguality with a, with the Colbert index. Right. But hey, it just gets better when everybody puts out their open source models. All right. Okay, I'll see you on Wednesday. Just one more thing for people who show up late. We are opening up the Wednesday schedule for people to, to pitch their projects. And even if you know what you're doing, you've got a group. I encourage you to speak as soon as you know what you are doing so that we can give you early feedback. So this is this Wednesday.", "block_metadata": {"id": "CS224V_Lecture_5_>_Transcript_>_4907657_ms_-_4958805_ms", "document_type": "transcript", "lecture_number": 5, "start_ms": 4907657, "end_ms": 4958805}}
