{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > Oh My!", "content": "Title: CS224V Lecture 16 > Chapter Summaries > Oh My!\n\nContent: Make all words react. Genie, red and blue. It's good. Okay, perfect. Yeah.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_Oh_My!", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 257845, "end_ms": 303015}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > Guest Lecture", "content": "Title: CS224V Lecture 16 > Chapter Summaries > Guest Lecture\n\nContent: Jackie Young is a PhD but dissertation, haven't submitted my dissertation yet. My thesis is about multimodal AI augmented user interface development architecture. I will talk about two of my work which I think will be very relevant to you folks when you're building virtual assistant stuff.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_Guest_Lecture", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 303055, "end_ms": 404155}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > Mixed-Media Interactions", "content": "Title: CS224V Lecture 16 > Chapter Summaries > Mixed-Media Interactions\n\nContent: Currently it's slow for applying actions to multiple objects. For less common features such as all of the stuff in those text boxes, you have to go through multiple clicks. The most important issue is development cost. If you want to implement multimodal interaction, you must tackle all of them.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_Mixed-Media_Interactions", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 406055, "end_ms": 936523}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > React Genie in Natural Language Programming (NLPR)", "content": "Title: CS224V Lecture 16 > Chapter Summaries > React Genie in Natural Language Programming (NLPR)\n\nContent:  React Genie is a new programming language for natural language programming. We ask LLMs to translate what the user wants into a bunch of actions using method training. All of the examples I've shown before are actually real, so here's a demo for that.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_React_Genie_in_Natural_Language_Programming_(NLPR)", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 936639, "end_ms": 1454585}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > React Genie: The Intelligence of the User", "content": "Title: CS224V Lecture 16 > Chapter Summaries > React Genie: The Intelligence of the User\n\nContent: React Genie tries to solve two problems. One is the ease of development by heavily relying on annotations on top of existing React code. The second part is I want to have very rich multimodal functionality. I want user to be able to say a very complex command and show the result accordingly.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_React_Genie:_The_Intelligence_of_the_User", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 1455205, "end_ms": 1847795}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > React Genie 2.8: The Semantic parser and", "content": "Title: CS224V Lecture 16 > Chapter Summaries > React Genie 2.8: The Semantic parser and\n\nContent: The semantic parser and the response generator of React genie. How do you prompt LLM to generate the thing on the right? Give you like potential options like functions. Some potential situations where information ratio will be super crucial.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_React_Genie_2.8:_The_Semantic_parser_and", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 1848975, "end_ms": 2125495}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > UI Mapping and Scripting", "content": "Title: CS224V Lecture 16 > Chapter Summaries > UI Mapping and Scripting\n\nContent: Next up is UI mapping. If specific objects specific value in the state is being updated, then your UI automatically reflects that changes. We tested React GENIE extensively for developers. Is it easy to learn and easy to test?", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_UI_Mapping_and_Scripting", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 2126035, "end_ms": 2645845}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > React 2.8: Not Enough Features for Multimod", "content": "Title: CS224V Lecture 16 > Chapter Summaries > React 2.8: Not Enough Features for Multimod\n\nContent: Next project is called Genie wizard, which is how to address this basically not enough feature problem about multimodal interactions. The way we simulate user commands is like hallucination of LLMs. Tell developers what are the functions they need to implement.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_React_2.8:_Not_Enough_Features_for_Multimod", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 2648145, "end_ms": 2903291}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > How to Build a Human-Like App with LLM", "content": "Title: CS224V Lecture 16 > Chapter Summaries > How to Build a Human-Like App with LLM\n\nContent: We ask LLM to first generate the app description. From that we generate interactive dialogue. We parse interactive dialogue into the dsl. When the function is not supported then it will hallucinate the right function. We built two demo apps for testing Genie wizard.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_How_to_Build_a_Human-Like_App_with_LLM", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 2903403, "end_ms": 3268269}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > Systems research in the New Age of AI", "content": "Title: CS224V Lecture 16 > Chapter Summaries > Systems research in the New Age of AI\n\nContent: A lot of the stuff in this course is kind of about using systems research for those AI HCI problems. I try to make multimodal interaction easier to build. In this new age of AI you can truly use those AI to build better human systems. Using the techniques have been invented long time ago by computer system researchers.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_Systems_research_in_the_New_Age_of_AI", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 3268357, "end_ms": 3432805}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > Mixed-Modality Machine Learning", "content": "Title: CS224V Lecture 16 > Chapter Summaries > Mixed-Modality Machine Learning\n\nContent: The same thing can work with both voice and images. The multimodal models convert other modalities into tokens. Can you map Uber to a specific token? I think it's going to be hard to tokenize apps.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_Mixed-Modality_Machine_Learning", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 3436185, "end_ms": 3850395}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > React Genie: Multisodal Interaction", "content": "Title: CS224V Lecture 16 > Chapter Summaries > React Genie: Multisodal Interaction\n\nContent: React Genie aims to foster multimodal interaction adoption. It merges modern app features with multimodals interfaces for flexibility and easy development. If you want to make it into industry, do you want consider like multi human universities?", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_React_Genie:_Multisodal_Interaction", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 3851775, "end_ms": 4135125}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > DeepMind on Voice Interfaces", "content": "Title: CS224V Lecture 16 > Chapter Summaries > DeepMind on Voice Interfaces\n\nContent: Voice interfaces are sometimes inaccurate, especially in noisy environments. I 100% believe voice interface will be the future. There's paper showing voice interfaces are four times faster than typing. How can we enable private voice interactions and accurate interactions?", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_DeepMind_on_Voice_Interfaces", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 4142735, "end_ms": 4308247}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Chapter Summaries > Chemical Science Lecture", "content": "Title: CS224V Lecture 16 > Chapter Summaries > Chemical Science Lecture\n\nContent: Cool. I guess that's it for this lecture. Thanks a lot for being here. It.", "block_metadata": {"id": "CS224V_Lecture_16_>_Chapter_Summaries_>_Chemical_Science_Lecture", "document_type": "chapter summary", "lecture_number": 16, "start_ms": 4308391, "end_ms": 5002535}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 257845 ms - 370727 ms", "content": "Title: CS224V Lecture 16 > Transcript > 257845 ms - 370727 ms\n\nContent: Make all words react. Genie, red and blue. It's good. Okay, perfect. Yeah. So Monica told me, she said some conference, I assume EMLP right now. So I'm teaching a guest lecture about multimodal applications. So I'm Jackie, Jackie Young and I read Monica's name. She helped me work through a lot of the slides here. A little bit about myself. I am Jackie young. I'm a PhD but dissertation, haven't submitted my dissertation yet with Professor Monica and Professor James Landy. My thesis is about multimodal AI augmented user interface development architecture. So this entire architecture about how do we help people do better multimodal interactions. Right now I'm working on a startup called Skywalk and I'm VP of Software and ML. And then I do always available private voice interfaces, which is also kind of relevant. If you're interested, we can talk afterwards. So this lecture I want to talk about a few things about why do we need multimodal interactions? What is multimodal interactions? What", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_257845_ms_-_370727_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 257845, "end_ms": 370727}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 356269 ms - 424459 ms", "content": "Title: CS224V Lecture 16 > Transcript > 356269 ms - 424459 ms\n\nContent: kind of relevant. If you're interested, we can talk afterwards. So this lecture I want to talk about a few things about why do we need multimodal interactions? What is multimodal interactions? What are the major problems about? Like why are you not seeing multimodal interactions on your phone right now? And I will talk about two of my work which I think a lot of the systems and the details will be very relevant to you folks when you're building virtual assistant stuff. One of them is called React genie, which is a multimodal app development framework. The second is called GENIE wizard, which is addressing what I think the biggest problem that haven't been solved yet in multimodal interaction, which is the feature discovery problem. So I'll start by talking about what is multimodal interactions. So today we have great GUI interfaces. Like one example for making slides is PowerPoint. In PowerPoint you can have very accurate and powerful interactions. You have all of these different", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_356269_ms_-_424459_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 356269, "end_ms": 424459}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 412215 ms - 485365 ms", "content": "Title: CS224V Lecture 16 > Transcript > 412215 ms - 485365 ms\n\nContent: So today we have great GUI interfaces. Like one example for making slides is PowerPoint. In PowerPoint you can have very accurate and powerful interactions. You have all of these different objects, you have text boxes, you have pictures, you have shapes, and you have fonts, colors and line styles. And you can adjust everything accurately via graphical user interfaces by just clicking on it. However, currently it's kind of slow for applying actions to multiple objects and for less common features such as all of the stuff in those text boxes, you have to go through multiple clicks and there's a lot of nested menus as I just mentioned, and then it takes a long time to master all of them to actually make it work. So here are some examples of. Well, how about we go back a little bit. So given this problem about like applying actions to a lot of objects, kind of hard and less common features have to win through a lot of clicks. What could be some good solutions, especially given all the", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_412215_ms_-_485365_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 412215, "end_ms": 485365}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 473197 ms - 540369 ms", "content": "Title: CS224V Lecture 16 > Transcript > 473197 ms - 540369 ms\n\nContent: this problem about like applying actions to a lot of objects, kind of hard and less common features have to win through a lot of clicks. What could be some good solutions, especially given all the virtual assistant stuff, you have been working on during the quarter. Do anybody want to answer this? Using your voice to kind of customize what you're designing. Exactly, exactly. But there is a slight problem, which is if you share voice, people have been building this. I've seen some system about, oh, I describe what I want and generates a PowerPoint for me. But one small issue is that a lot of times you kind of want to work interactively with the agent. Right? If we are working on a PowerPoint together, a lot of times, what I would say, oh, it's like this box is a little bit small and all of the titles and all of the slides, I wish it to be in another font and stuff like that. So those are kind of stuff. When people are communicating with each other, we are referring to stuff on screen.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_473197_ms_-_540369_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 473197, "end_ms": 540369}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 529881 ms - 596503 ms", "content": "Title: CS224V Lecture 16 > Transcript > 529881 ms - 596503 ms\n\nContent: the titles and all of the slides, I wish it to be in another font and stuff like that. So those are kind of stuff. When people are communicating with each other, we are referring to stuff on screen. We are communicating the goals and we are going back and forth with the agent. So this is kind of the goal. Some of the example of that includes, oh, I want to make the text box bold in the slide master because all of the text boxes on the title should be bold. Or make the border of this shape with dots so that it's clearer that this is different from another slide. Or stuff like make everything right align or make every shape on this slide above this part yellow because they mean different things. So here's an example of that. Make all words react, red and bold. Because I want to emphasize this word right? And instead of the low level actions where I explicitly select some word and make it red, I can just say, oh, make all the word react genie, red and bold. And instead of those low level", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_529881_ms_-_596503_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 529881, "end_ms": 596503}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 581727 ms - 668253 ms", "content": "Title: CS224V Lecture 16 > Transcript > 581727 ms - 668253 ms\n\nContent: word right? And instead of the low level actions where I explicitly select some word and make it red, I can just say, oh, make all the word react genie, red and bold. And instead of those low level actions I select, I change color, I make it bold three times. I just say this high level actions. In fact, this idea of oh, I can communicate what I want by using both gesture and voice is not new since 1982. Wait, I said 1982. But this video must be earlier. So I think the paper and the video is published at a different time. Put a magenta square there. This guy called Richard Bode makes a blue diamond bear revolutionary demo very early on. Move that there. Yeah. So this is the kind of things people visioned before. But right now, obviously nobody's actually talking to their phone and touching their phone at the same time. I think the. And then I'll talk about why this is not widely adopted. My personal take is the most important issue is development cost. So there are three set of very", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_581727_ms_-_668253_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 581727, "end_ms": 668253}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 652683 ms - 735099 ms", "content": "Title: CS224V Lecture 16 > Transcript > 652683 ms - 735099 ms\n\nContent: their phone at the same time. I think the. And then I'll talk about why this is not widely adopted. My personal take is the most important issue is development cost. So there are three set of very difficult stuff these days. If you want to implement multimodal interaction, you have to tackle all of them. There is compositionality of multimodal commands. We need to Expose diverse Actions and APIs inside a Already built very complex GUI app. And we want to allow interchangeable and simultaneous multimodal interaction similar to what has been demonstrated there. So let's recall the example earlier. Make this text box bold in the slide master and all of the examples here. I wonder, based on what you have learned in this course, how would you implement those commands? Especially these days we have LLMs, unlike 1982, where I think most of the commands is just using pattern matching. Anybody want to talk about any potential ideas on this? Okay, you can just parse the HTML and look for the", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_652683_ms_-_735099_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 652683, "end_ms": 735099}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 710989 ms - 784577 ms", "content": "Title: CS224V Lecture 16 > Transcript > 710989 ms - 784577 ms\n\nContent: have LLMs, unlike 1982, where I think most of the commands is just using pattern matching. Anybody want to talk about any potential ideas on this? Okay, you can just parse the HTML and look for the property and change it to whatever you want it to be. Yeah. And so you are saying you send the LLM the entire web page and you send LLM the commands and then the LLM directly manipulates the web page. I think that's a great idea. That's actually a fantastic starting point. There's one thing I really like about your solution is that it's no longer limited to a single command at a time. You send the entire thing to LLM. So it does not matter how complex your command is, LLM will likely going to be able to do something about it. There's a few issues there. First of all is that if you change the HTML, how do you save it? That's going to be an issue. If you say I'll make this text bold and then it changes the HTML of Google Slides to make this bold. When you hit save, nothing gets saved because", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_710989_ms_-_784577_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 710989, "end_ms": 784577}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 774753 ms - 834543 ms", "content": "Title: CS224V Lecture 16 > Transcript > 774753 ms - 834543 ms\n\nContent: how do you save it? That's going to be an issue. If you say I'll make this text bold and then it changes the HTML of Google Slides to make this bold. When you hit save, nothing gets saved because you changed the graphical interface. So there's a mapping problem which is very interesting. Then there's also maybe it's not accurate. Some of the stuff might not be able to represent it by HTML. So I think that's a great solution. One thing, in the old days when we started working on this LM is not popular. We are working with GPT3 like the old GPT3 where you need a lot of prompting. So one thing previously people have been doing is like the traditional virtual system where we use intent based parsing where you have a different function, you have a different function, every single thing user wants. Right. Um, this is actually worse than your solution. I think your solution actually already better than the space that I'm targeting here. Um, so if you say I'll make this text box bold, then", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_774753_ms_-_834543_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 774753, "end_ms": 834543}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 824855 ms - 883065 ms", "content": "Title: CS224V Lecture 16 > Transcript > 824855 ms - 883065 ms\n\nContent: wants. Right. Um, this is actually worse than your solution. I think your solution actually already better than the space that I'm targeting here. Um, so if you say I'll make this text box bold, then maybe you need one function to make text box bold. In a slide master, if you make the border of the shape with little dots, maybe you need another function with a parameter called changing the border shape to a specific kind of border type or make everything right align you need another function for that. Make every shape above this doing something, you need another function for that. So this is pretty typical, right? If you look up, oh, like how do I integrate LLM with my program? The recommended way is to do API calling, right? They call it API calling. But the problem with API calling is there's so many things once you look at a PowerPoint, there's so many things you can do. A lot of them involves a combination of different actions. And with the API interface you translate them into", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_824855_ms_-_883065_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 824855, "end_ms": 883065}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 872369 ms - 936819 ms", "content": "Title: CS224V Lecture 16 > Transcript > 872369 ms - 936819 ms\n\nContent: there's so many things once you look at a PowerPoint, there's so many things you can do. A lot of them involves a combination of different actions. And with the API interface you translate them into single API calls, which you need millions of APIs to address millions of user requests. So that's not really scalable with just typical function calling. So what we really want to do, you can already see the pattern here. The way people address those problem is breaking down this request into smaller actionable actions. So for example, rather than say, oh, make the text box bold as one function, I can first figure out what is the slide that the user is referring to. Then I figure out what is the slide master of this slide. And then I figure out what does this text box mean in the slide master. And then I change a property of the text box to the property that I want, which is make the font bold. And similarly, all of the previously mentioned actions can be translated in a similar way. By", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_872369_ms_-_936819_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 872369, "end_ms": 936819}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 920471 ms - 987571 ms", "content": "Title: CS224V Lecture 16 > Transcript > 920471 ms - 987571 ms\n\nContent: And then I change a property of the text box to the property that I want, which is make the font bold. And similarly, all of the previously mentioned actions can be translated in a similar way. By the way, this thing, if you have written object oriented programming, you'll probably write function this way. This is talk called method training. And then we heavily rely on method training. We ask LLMs to translate what the user wants into a bunch of actions using method training. And what I call this new language. It's like a new programming language I created for this. We created for this called nlpl, which is short for natural language programming language. And by compositioning multiple functions, imagine there's a million user requests, right? If you address each one of them, you have to write a million different functions. But if you decompose those million functions, every single one of them, to 10 different actions, then you probably only need 100 actions, 200 actions to address a", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_920471_ms_-_987571_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 920471, "end_ms": 987571}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 975323 ms - 1033103 ms", "content": "Title: CS224V Lecture 16 > Transcript > 975323 ms - 1033103 ms\n\nContent: write a million different functions. But if you decompose those million functions, every single one of them, to 10 different actions, then you probably only need 100 actions, 200 actions to address a million different user requests. Um, so, um, when I designed this lpl, I mentioned a little bit about function, uh, method training. But in addition to that we also experimented with a few different formats. Uh, this is kind of also interesting. Um, if you are working on some virtual system problem with LLMs, um, designing your own language and ask LLM to parse is actually a great solution for that. So a few things we noticed, we tried different languages, we tried Python, we tried TypeScript, we tried something like Swift. The problem with Python is that it doesn't have Strong typing. So it's not 100% sure which function you should call, so it results in more errors. Another thing, I don't know if you are going to design your own language, pay attention to that at least when I tried it.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_975323_ms_-_1033103_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 975323, "end_ms": 1033103}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1021783 ms - 1091107 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1021783 ms - 1091107 ms\n\nContent: not 100% sure which function you should call, so it results in more errors. Another thing, I don't know if you are going to design your own language, pay attention to that at least when I tried it. When it's like very early on, if your function call does not explicitly say parameters. I noticed a lot of times the GPT will call the right function, but the parameters is in slightly wrong order. So I make each one of the parameter explicit, which is kind of like how Swift designed their languages. One more thing is that we notice a lot of the user queries are kind of like SQL where there's a lot of filtering, there's a lot of execution action for a bunch of items at the same time. So we kind of incorporate all of those features into this NLP language. Um, and to talk into more language features on nlpr, here's example of a sentence. Uh, change background color for all yellow shapes to orange. Um, as typical English sentence, all almost all English sentence at least have a verb and which", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1021783_ms_-_1091107_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1021783, "end_ms": 1091107}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1078011 ms - 1149597 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1078011 ms - 1149597 ms\n\nContent: on nlpr, here's example of a sentence. Uh, change background color for all yellow shapes to orange. Um, as typical English sentence, all almost all English sentence at least have a verb and which is change a background color for object which is a shape. Um, and in this case we slightly modify the verb. We say a change color to orange, the specific target that I want. And I also filter the shape by say oh, I want only the yellow shapes. So as I mentioned before, we use something like object oriented programming which is you can call function of an object. This is to solve a typical language structure where we have a singular object plus a verb. A lot of times, as I mentioned, you want like a SQL kind of sentence where you are applying one action to multiple objects. So the NLPR supports automatically distribute action to a list of objects. So if you say oh like I want I like all the food I ordered in my last order, then you can simply find out order, list all the foods and then call a", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1078011_ms_-_1149597_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1078011, "end_ms": 1149597}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1136493 ms - 1221365 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1136493 ms - 1221365 ms\n\nContent: distribute action to a list of objects. So if you say oh like I want I like all the food I ordered in my last order, then you can simply find out order, list all the foods and then call a single function. The function will get distributed into every single object in that list. There's also a bunch of other language features. Feel free to ask me about it or just browse the React Genie source code which is open source online. All of the examples I've shown before about PowerPoint is actually real, so here's a demo for that. Make all words React Genie red and blue. This is the first example I've shown before. Much better. Let's increase the font size a little bit. By the way, there's a lot of voice recognition error which is why I. Created the size of all text in this box to 24. It looks much better. Okay, on the second page we have the framework of React Genie, but The font is not clear enough, so let's change it to white. I can choose all boxes and make all text in this box white.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1136493_ms_-_1221365_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1136493, "end_ms": 1221365}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1198403 ms - 1300947 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1198403 ms - 1300947 ms\n\nContent: much better. Okay, on the second page we have the framework of React Genie, but The font is not clear enough, so let's change it to white. I can choose all boxes and make all text in this box white. Great. And I also want the arrow to be white. Make all arrows like this one white. Great. As we can see in the upper and lower sections, we describe the state code and UI separately. So let's make some difference. I can choose this one and make all the boxes of the same size orange. Great. Oh, but it looks like we need to change the font color back to black. So I will choose these two boxes. Change the text in these two boxes to black. Great. Yeah, so that's. To save time, I'll skip the rest of the examples. So we built this Genie PowerPoint app. It has 110 APIs involved, and then those 110 APIs can then be composed together to solve a lot of different user requests. However, Genie PowerPoint is built on top of existing PowerPoint. They provide an API pack that allows me to access those", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1198403_ms_-_1300947_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1198403, "end_ms": 1300947}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1287713 ms - 1356393 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1287713 ms - 1356393 ms\n\nContent: APIs can then be composed together to solve a lot of different user requests. However, Genie PowerPoint is built on top of existing PowerPoint. They provide an API pack that allows me to access those APIs in JavaScript, but it doesn't support a lot of the common APIs you would need. Making a PowerPoint like animation, changing font color and et cetera. The reason behind that is building a GUI is already a lot of work. To expose those API, even for a company as big as Microsoft, is a very, very hard task. Basically we are asking developers to create a human interface which millions of their developers already using, and in some sense the AI interface or programming interface, which currently not a lot of people are using. So they don't have enough incentive to build another entire interface. So we want to think about in React genie, how do we help developer expose those API with very, very little cost? So we start with what are people writing these days? In a typical web program, for", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1287713_ms_-_1356393_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1287713, "end_ms": 1356393}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1339695 ms - 1414003 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1339695 ms - 1414003 ms\n\nContent: So we want to think about in React genie, how do we help developer expose those API with very, very little cost? So we start with what are people writing these days? In a typical web program, for example, this is React. And typically in React you write some logic code which is the code that is actually paring all the state changes, and you have some UI code which is rendering those state changes. What we really want to do is rather than say, oh, you have to write Another type of APIs explicitly true, allow people to do that. We want to make it so that you write the minimal amount of code. So we add some thing called annotations. Who here have write any annotations in their code? This is not comments, this is a thing called annotations. What have you used annotations for? Just like values. Okay, Annotators for both Python and front end logic data classes. Yeah. One really interesting benefits of annotations is that from the compiler interpreter side, you can do stuff with annotations.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1339695_ms_-_1414003_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1339695, "end_ms": 1414003}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1397571 ms - 1462623 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1397571 ms - 1462623 ms\n\nContent: Okay, Annotators for both Python and front end logic data classes. Yeah. One really interesting benefits of annotations is that from the compiler interpreter side, you can do stuff with annotations. In fact, some of the language will translate the annotations into a bigger sized code in the compiler. Some of them is in the interpreter. When interpreter interprets them, they can see how this specific tree structure in the code has been annotated with a specific tag. Then I could do something different with it. So this is basically the easiest thing we can think about to allow developers to expose something to the two AI models without changing their code structure too much. They simply annotate, oh, this class, I think AI might want it, this function, the AI max wants it and that's it. So this is a very easy way to expose those actions. When using a React Genie app, another thing will happen is kind of similar to I don't know your name, but which is that like oh, when I say something I", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1397571_ms_-_1462623_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1397571, "end_ms": 1462623}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1452029 ms - 1509277 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1452029 ms - 1509277 ms\n\nContent: is a very easy way to expose those actions. When using a React Genie app, another thing will happen is kind of similar to I don't know your name, but which is that like oh, when I say something I change, I directly change the HTML, right? But if we take a different approach where if you say something we call some API to change states, then the problem becomes how do we know what's your reference? How do we reflect the changes? First problem is oh, if I say oh, like when is the last time I ordered food from this restaurant? How do I know what this restaurant is by just if I click on it? The other problem is oh, if I say oh, order one more hamburger, right? And then it did change the state. But what if the UI didn't update? It's kind of the reverse problem of your example, where if you say oh, order one more hamburger, I only change the HTML to update it by one. But when I click order, it doesn't actually place the order. The other way is I'll actually change the back end state. But", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1452029_ms_-_1509277_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1452029, "end_ms": 1509277}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1499229 ms - 1568013 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1499229 ms - 1568013 ms\n\nContent: oh, order one more hamburger, I only change the HTML to update it by one. But when I click order, it doesn't actually place the order. The other way is I'll actually change the back end state. But maybe the UI wasn't updated. How do the user know that their one burger order has been understood? Yeah, where's my burger? I didn't see that. And the other example is oh, if I say add one shake and one burger to the cart, right? And then we do it in the background, but the user didn't see anything. How does the user know that? We understood and we have created that cart. So the user needs to know what has happened in the background. So what we did is we leveraged a concept called UI mapping. So we want to establish a mapping between what's happening on the UI and what's happening in the backend. When the user says oh, like order something from this restaurant, we need to understand what that tag point represents in all of the objects in memory. Similarly, when the user says order a burger,", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1499229_ms_-_1568013_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1499229, "end_ms": 1568013}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1554205 ms - 1621363 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1554205 ms - 1621363 ms\n\nContent: When the user says oh, like order something from this restaurant, we need to understand what that tag point represents in all of the objects in memory. Similarly, when the user says order a burger, we need to update the counter on the ui. And when the user says create a new order, we need to create that order and show that order, render that order in the GUI so that user knows something did happen. So basically on the UI input mapping side, I need to know what object the user is tapping. On the output side, we need to know when there's some updates. I need to update the UI when there's a new object that is not currently represented on the ui, we need to navigate a user to somewhere that the user can see the changes. So that's the three problems and the very rough solution that I just presented. And now I'll go into the details of how I have implemented the React GENIE framework using all of the solutions I presented to solve all the problems I just presented. So React Genie tried to", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1554205_ms_-_1621363_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1554205, "end_ms": 1621363}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1606235 ms - 1701295 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1606235 ms - 1701295 ms\n\nContent: And now I'll go into the details of how I have implemented the React GENIE framework using all of the solutions I presented to solve all the problems I just presented. So React Genie tried to solve two problems. One is the ease of development. I want to make development very, very easy by heavily relying on annotations on top of existing React code. And the second part is I want to have very rich multimodal functionality. I want user to be able to say a very complex command. I want to have they can do combination of voice GUI actions and we want to show them the result accordingly. Here's a quick demo. Jack saw analysis of attributing React gene and he feels this photo is very familiar. He must have seen it somewhere like that recently. Show me posts from John. So it's not John. What about posts from Mark? Wow, Mark has too many photos. Well, can you show me posts from Mark that I have liked before? So it's also not Mark. That's not it. What about post from yesterday? Oh, that's it.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1606235_ms_-_1701295_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1606235, "end_ms": 1701295}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1668541 ms - 1758267 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1668541 ms - 1758267 ms\n\nContent: posts from Mark? Wow, Mark has too many photos. Well, can you show me posts from Mark that I have liked before? So it's also not Mark. That's not it. What about post from yesterday? Oh, that's it. It's Mary actually. Well, share this post with Emma. Yeah, so we kind of see the point where we build an app. The other app we build is about NDA management. Actually we need but basically we build an app that and by building the app in React Genie all of the functions expose invoice. So you can have this very natural conversation with your app as if it's your friend. You can have back and forth conversation, you can mention to stuff, it can ask you to do stuff and then it can work all the time. So quick crash course. I mentioned slightly before, all the GUI apps have state code, which is the logic behind it, and components code which is how you display the UI for the users. And we basically ask developers to add some annotation in the state code to declare which class, which property, which", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1668541_ms_-_1758267_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1668541, "end_ms": 1758267}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1745267 ms - 1817235 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1745267 ms - 1817235 ms\n\nContent: behind it, and components code which is how you display the UI for the users. And we basically ask developers to add some annotation in the state code to declare which class, which property, which function needs to be accessible through the multimodal interaction interface. And the components annotation, they annotate this component represent that object in the state. So we have this very explicit mapping in both directions when you tap on something, I know what it is. When we need to render something, I know what class I need to use to render the results. I think the last course, if I check the cross schedule correctly, you folks should understand, should learn about agent architecture on how to have a conversation with the user, right? Is the last course the agent? Yeah. Okay, perfect. In React, typically what happens is when you tap on something, the UI responses, something got changed and then the state got changed and the UI automatically gets updated and being shown to display.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1745267_ms_-_1817235_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1745267, "end_ms": 1817235}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1803595 ms - 1882891 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1803595 ms - 1882891 ms\n\nContent: In React, typically what happens is when you tap on something, the UI responses, something got changed and then the state got changed and the UI automatically gets updated and being shown to display. So reaction kind of combines the two. Um, the input can be text, it can also be touch or actually this one should be uh, UI input, but the UI input is mapped to the. To. Both of them is mapped to the same execution state which is um, we use this react to manage the program state and the result is both rendered as tags through a response generator generator and through UI updates. So that when needed I need to update the UI to let the user know. So I'll first talk about the response generator, sorry, the semantic parser and the response generator of React genie. So I've just shown this NLPL earlier, right? And now the question is, given the stuff on the left, how do I generate the program on the right? Any ideas you have learned Semantic parser? Last course, last lecture, right? So any", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1803595_ms_-_1882891_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1803595, "end_ms": 1882891}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1863135 ms - 1971161 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1863135 ms - 1971161 ms\n\nContent: earlier, right? And now the question is, given the stuff on the left, how do I generate the program on the right? Any ideas you have learned Semantic parser? Last course, last lecture, right? So any ideas? How do we do this? Anybody want to give it a try? How do we given the left generated the right? Any idea is fine. Any idea is better than no idea. Do you want to give it a try? Sorry, I don't know her name. Sure, yeah. So I guess you would first feed the text to a semantic parser for. Yeah, there's by the way a hint. Semantic parser is backed by LLM. So how do you make LLM? How do you prompt LLM? We didn't even fine tune it. How do you prompt LLM to generate the thing on the right? Give you like potential options like functions. That's actually very close. So first of all, like how do I write the program? Right? If you need to write a program, you would look at, look at. What are all the functions you would look like? Look at. Oh, these are. Oh, sorry. Do you have any ideas? You", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1863135_ms_-_1971161_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1863135, "end_ms": 1971161}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 1959873 ms - 2024813 ms", "content": "Title: CS224V Lecture 16 > Transcript > 1959873 ms - 2024813 ms\n\nContent: I write the program? Right? If you need to write a program, you would look at, look at. What are all the functions you would look like? Look at. Oh, these are. Oh, sorry. Do you have any ideas? You just do like information retrieval on all the documentation and then shove that in the LLM and then ask it to. That's actually an interesting idea because at this, the version we build, we didn't actually include that. But I think there are some potential situations where information ratio will be super crucial. For example, if I say something like, oh, search for posts around about Lake Tahoe and then I do need to do a little retrieval of what are all the post content and which one matches Lake Tahoe? The documentation for whatever. Oh, like the code documentation. If the documentation is too big and it might overflow the context. Yeah, exactly. It's very close. But what we observe is that for example even for PowerPoint there's only like about 100 or so API, although that's not complete.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_1959873_ms_-_2024813_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 1959873, "end_ms": 2024813}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2012173 ms - 2072073 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2012173 ms - 2072073 ms\n\nContent: big and it might overflow the context. Yeah, exactly. It's very close. But what we observe is that for example even for PowerPoint there's only like about 100 or so API, although that's not complete. I think the complete API will be around 200 but that's actually totally feedable within the context of the LLM. So what we did in fact is we compile actually I think the next slide have there. So first of all we compile the code, we recompile the code so that we extract only the function declarations. Kind of like documentation where we actually have a spot in the annotation where developer can include some documentation if they want to. Those documentation we passed through the LLM. So yeah, so basically we show the documentation to the LLMs. And another thing we need to do, which is a new thing with React genie, is because we have a new programming language. So if you just say some, give it some skeleton and then say I'll generate a program, it will generate, I think for our testing it", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2012173_ms_-_2072073_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2012173, "end_ms": 2072073}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2060633 ms - 2122843 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2060633 ms - 2122843 ms\n\nContent: with React genie, is because we have a new programming language. So if you just say some, give it some skeleton and then say I'll generate a program, it will generate, I think for our testing it will be sometimes Python, sometimes JavaScript. It seems like those are LM's favorite languages. Um, so we, we want to generate our specific syntax. So we'll give it some few shot prompts and we'll give it whatever user said and then we ask to generate the code that which we called parsed result. So this is the result of the semantic parser? Um, yeah. So pretty, pretty easy. And the for the response generator we do similar, similar things. We have the declaration, we have the instructions, we have the current interaction and we also put the results. So for example, if you say I'll order the food and then I will, I will return all the actual order has been placed and it contains these amount of food and then LM will say oh like your order with the hamburger and fries has been placed. Yeah, so", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2060633_ms_-_2122843_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2060633, "end_ms": 2122843}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2110635 ms - 2184389 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2110635 ms - 2184389 ms\n\nContent: and then I will, I will return all the actual order has been placed and it contains these amount of food and then LM will say oh like your order with the hamburger and fries has been placed. Yeah, so that's the semantic parser and the response generator. Next up is UI mapping. So basically in the uimap the reaction has a little runtime that when the app is running is keeping a mapping between all these are all the programming logic objects and these are all the pixels on screen and then which pixel maps to which object. And if user click on the screen says reorder this food then what we'll do is we'll first use the semantic parser to parse it into our NLPL language. And here there is a special function we inserted for every single class called current. Current is whatever object the user is referring to. In this case I need to figure out what is the food item the user is referring to by the tap point. Then I look at the UI mapping because I recorded which pixel range represent which", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2110635_ms_-_2184389_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2110635, "end_ms": 2184389}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2171863 ms - 2251743 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2171863 ms - 2251743 ms\n\nContent: is referring to. In this case I need to figure out what is the food item the user is referring to by the tap point. Then I look at the UI mapping because I recorded which pixel range represent which object. Then I can resolve this, know that this is a food item, which is the crunch wrap from Taco Bell and I can continue execution using that object as food current. So that's the UI mapping set. And there's a second part which is how do we represent the changes to the user? So if the objects are already on the ui, have anybody here react or any kind of reactive UI frameworks before. Okay, so you would know that if specific objects specific value in the state is being updated, then your UI automatically reflects that changes. In this case, because we have the UI mapping module, we know a specific value, is it represented on a UI or not? If it's already representing the ui because the UI is written in a way where basically the UI is a mapping from state to HTML. So if I know that a", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2171863_ms_-_2251743_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2171863, "end_ms": 2251743}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2236855 ms - 2309811 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2236855 ms - 2309811 ms\n\nContent: a specific value, is it represented on a UI or not? If it's already representing the ui because the UI is written in a way where basically the UI is a mapping from state to HTML. So if I know that a specific thing is already being updated on the ui, then I do nothing. I do nothing. But react will automatically handle it if the object is not on the ui. This is the case where this is like a recipe app. If you say oh, show me creamy potato recipe, then I know the object I want is a recipe called creamy potato, but it's not currently on screen. So what I will do is because I know the mapping back as well. I know the mapping from pixel to to state objects. I also have another mapping from state objects to a specific UI interface that can render this state object. So I would show that user interface I can render this object. So I get a response back to the user so they know what has happened. And finally is the execution. The execution is basically interpreter that we build for nlpl. So one", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2236855_ms_-_2309811_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2236855, "end_ms": 2309811}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2294003 ms - 2364205 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2294003 ms - 2364205 ms\n\nContent: I can render this object. So I get a response back to the user so they know what has happened. And finally is the execution. The execution is basically interpreter that we build for nlpl. So one example is if I say oh, make everything above this yellow, first thing I will do is using the semantic parser to parse it to something like this. Then I will start executing from the smallest component which is slides current, which we get from the user's context user's tag point. Next step is we iteratively parse all the. We first find out what are all the shapes. We then find out what is the point that I've been clicking. And then I filter out all the shapes that is above this and then I make all of the shapes above this yellow, which is a typical interpreter. How do you interpret? Do something with interpreter. Um, from the developers perspective, they simply write annotations on the GUI and on the state code. And we use this, the state code stuff to generate semantic parser, generate", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2294003_ms_-_2364205_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2294003, "end_ms": 2364205}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2350533 ms - 2431523 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2350533 ms - 2431523 ms\n\nContent: with interpreter. Um, from the developers perspective, they simply write annotations on the GUI and on the state code. And we use this, the state code stuff to generate semantic parser, generate execution unit to generate the response generator. And we use the UI part to generate UI mapping and UI update modules. So here is a wrap of what I just described. While the program is running, we have all the objects and all the UIs saved in memory. This is both provided by developers and we know explicitly what are the mapping between the objects and the ui. And developer have those state code with annotations. And when the user says something like reorder my last meal from this restaurant and then click on a restaurant, we first parse it to our domain specific language and then we executed iteratively. We first found out what is the tap point which is resolved to the restaurant. In this case it's Taco Bell. And we then figure out what are the order mapping to what are the order that I have", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2350533_ms_-_2431523_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2350533, "end_ms": 2431523}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2415323 ms - 2499965 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2415323 ms - 2499965 ms\n\nContent: We first found out what is the tap point which is resolved to the restaurant. In this case it's Taco Bell. And we then figure out what are the order mapping to what are the order that I have ordered last time, which is this order I created in March 3rd. And then we figure out what are the food in that order, create a new order and then we render the result back to the user, which is a page that renders an order and the user can finally place that order. Yep. So we tested React GENIE pretty extensively. We try to access for developers. Is it expressive? Does it save development time? Is it easy to learn and easy to use? We built some demo apps, three different demo apps. And we noticed that only 5% of the code needs to be added for multimodal interactions, which is way less than for example PowerPoint. They have 100 APIs, but there's tons of code to actually make that work. We also build a smaller demo app so that it can fit in the lab study. We can ask developer to test it. In fact,", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2415323_ms_-_2499965_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2415323, "end_ms": 2499965}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2482333 ms - 2554689 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2482333 ms - 2554689 ms\n\nContent: PowerPoint. They have 100 APIs, but there's tons of code to actually make that work. We also build a smaller demo app so that it can fit in the lab study. We can ask developer to test it. In fact, the expert developer is me because I just created this project and then I try to write this either in react Genie or GPT3 function calling. It takes way less time to write it in React Genie because I simply have to add some annotations. And GPT function calling is another whole, another level of complexity. And even then, because as I mentioned, I can only do one function call at a time with GPT3 function calling. So it doesn't even match the same utility that I can provide with React Genie. So with React Genie, people spend less time, less code and implement more features. We asked 12 developers to change this Timer app from only GUI version of the Timer app. Then we know that they have high prong comprehension of the framework afterwards and then they have very fast Completion time only a", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2482333_ms_-_2554689_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2482333, "end_ms": 2554689}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2539833 ms - 2610883 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2539833 ms - 2610883 ms\n\nContent: to change this Timer app from only GUI version of the Timer app. Then we know that they have high prong comprehension of the framework afterwards and then they have very fast Completion time only a little bit more than an hour. And a lot of people ask oh, if they can use this in their real life applications. We also tested if the generated apps actually helpful for the users. We first tested the language parser performance. We Notice that on 111 supported commands, the parser accuracy is very high, 91%. Note that most of those commands are multiple functions composed together, so it's actually a very hard task. And then one thing we do notice is that for 71% of the unsupported commands, meaning that those commands are not actually currently supported by the app, we can actually generate sensible NLP on, meaning if it generates a function and the function exists, it would have solved the user's issue. So I'll actually mention more on that later. But another thing you want to pay", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2539833_ms_-_2610883_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2539833, "end_ms": 2610883}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2597961 ms - 2668647 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2597961 ms - 2668647 ms\n\nContent: sensible NLP on, meaning if it generates a function and the function exists, it would have solved the user's issue. So I'll actually mention more on that later. But another thing you want to pay attention to is actually within the app that we wrote, there's actually a lot of the commands that are not supported and I'll talk about that later. Not supported, meaning the feature just does not exist in the app. So if the user asks that there's nothing that React Genie can do to help, that the developer has to provide new functions. We also asked 12 users to try an app built with React Genie versus an app that is GUI only. We find React Genie can save 40% of the time, lower cognitive load and have higher usability. So the next project I wanna talk about is called Genie wizard, which is how to address this basically not enough feature problem about multimodal interactions. So as I mentioned previously before, um, we have very good accuracy on the commands that are supporting, meaning if the", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2597961_ms_-_2668647_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2597961, "end_ms": 2668647}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2653553 ms - 2720511 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2653553 ms - 2720511 ms\n\nContent: this basically not enough feature problem about multimodal interactions. So as I mentioned previously before, um, we have very good accuracy on the commands that are supporting, meaning if the app can do it, we can do it very well. But if the app cannot do it, then obviously there's no way reaction you can do. But we notice a very interesting thing which is if it is a pure GUI app, then unsupported commands, meaning that you want to do something that is not supported by the app, is kind of rare because if you click on a button, then you know you want that button to happen. Whereas if I give you an app and say you can do whatever you want with it, then people start talking about stuff they think is relevant, but might not be actually currently supported by the app. One example of that is I think in the food ordering app we build, we have reviews for a restaurant, but we do not have reviews for food item. So people may say, oh, like order the most recommended food in a restaurant. It", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2653553_ms_-_2720511_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2653553, "end_ms": 2720511}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2705453 ms - 2773235 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2705453 ms - 2773235 ms\n\nContent: I think in the food ordering app we build, we have reviews for a restaurant, but we do not have reviews for food item. So people may say, oh, like order the most recommended food in a restaurant. It totally makes sense. If you go to a restaurant, you would ask a waiter, waitress, oh, I want something that you would recommend. But then when you do it with an app, it becomes very problematic. The problem is how do we help developers solve. How do we help developers know what commands need to be supported before their app is fully built? But an interesting thing is we observe that even for those unsupported commands, our parser can generate sensible nlpr, meaning that if those function exists, it would be correct parsers 53% of the time. In some sense this is like hallucination of LLMs. It hallucinates a function that doesn't exist, but if the function exists, it would have solved the user's problem. So basically the point of React GENIE wizard is trying to leverage this LLM", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2705453_ms_-_2773235_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2705453, "end_ms": 2773235}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2761433 ms - 2835189 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2761433 ms - 2835189 ms\n\nContent: LLMs. It hallucinates a function that doesn't exist, but if the function exists, it would have solved the user's problem. So basically the point of React GENIE wizard is trying to leverage this LLM hallucination to solve our problem, which is tell developers what are the functions they need to implement. GENIE wizard is ide. So IDE has been around for a long time. IDE helps GUI developers save time, they help them follow GUI best practices and they help them visualize GUI designs. In our case, we have built this IDE that basically helps developer when they're working on their code. We will give them guidance while they're working on it. And the guidance is in the format of suggested features. We'll say, oh, we think for your app these features might be needed for users. In this little box we will have a list of unique features and every feature. If you expand on them, we will show you examples of. Because user want to customize their pizza. So you might want to create a new function", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2761433_ms_-_2835189_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2761433, "end_ms": 2835189}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2821083 ms - 2895035 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2821083 ms - 2895035 ms\n\nContent: we will have a list of unique features and every feature. If you expand on them, we will show you examples of. Because user want to customize their pizza. So you might want to create a new function called a fooditem customize. And then if you hover on specific item, you will show you what are the generated user utterances. While the user is working on their project, for example, they just added a new function. We will track very quickly, like refreshing like half a second or something. Tell you what you've just done there. Supported another, let's say in this case 20% of commands that wasn't supported before. So the way we make it work is I kind of eluded to a part of it, which is the second part actually, which is how do I speculate on what functions are actually missing? And before that we have to first simulate what users might want to use in your app. And finally we want to create some feature suggestions for you so that you understand, so that you know you have some actionable", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2821083_ms_-_2895035_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2821083, "end_ms": 2895035}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2882563 ms - 2951185 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2882563 ms - 2951185 ms\n\nContent: we have to first simulate what users might want to use in your app. And finally we want to create some feature suggestions for you so that you understand, so that you know you have some actionable item when you're working on a multimodal app. So the way we simulate user commands. Wait, I think I want to pause a little bit here. Anybody have any thoughts on like we have an app or kind of halfly build, how can we simulate what user might want to do with the App with LLMs, obviously. Any thoughts? I have an app and then I want to know what user might want to do with it. As simple as possible. Go ahead, create a bunch of like profiles and tell the LLM they're that person and then have them interact with the app. Exactly. Very close. And one special thing when you think about is how do you represent the app. So basically we have the code signature. This is similar to React Genie where we extract the co skeleton so that we can send it to LLM in a more compact format. We ask LLM to first", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2882563_ms_-_2951185_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2882563, "end_ms": 2951185}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2940553 ms - 2999649 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2940553 ms - 2999649 ms\n\nContent: represent the app. So basically we have the code signature. This is similar to React Genie where we extract the co skeleton so that we can send it to LLM in a more compact format. We ask LLM to first generate the app description. For example, this app is a digital menu app designed for dining in Italian restaurant. This can be changed by the app designer developers if they want to target specific demographics where they have specific intention of their app. We send this app description and then we have a Persona generator. In fact we use some UI census data to generate what we think is kind of representative of populations. But developers can change, can shift the demographics in whatever way they want. And based on the food app it will come up with scenarios of this person is this eight year old. And this is their job. They want to use this app because something. And from that we generate interactive dialogue. So the user will say something and the app will respond in plain text.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2940553_ms_-_2999649_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2940553, "end_ms": 2999649}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 2986507 ms - 3054317 ms", "content": "Title: CS224V Lecture 16 > Transcript > 2986507 ms - 3054317 ms\n\nContent: year old. And this is their job. They want to use this app because something. And from that we generate interactive dialogue. So the user will say something and the app will respond in plain text. This is so that we can continue the conversation, but what we really want is only what the user said. And because this is a natural conversation similar to human to human conversation, a lot of times they will have references to stuff which is great for multimodal apps. The next part is to speculate on the missing features, which is similar to what I just mentioned before. You have interactive dialogue. We parse interactive dialogue into the dsl. And because so as I mentioned, when the feature is supported, our parse accuracy is very high. So we can know that oh, if a parsed result uh, can be resolved within the existing functions, this very likely is correct. And when the um, when the function is not supported then it will uh, 50% of time you will hallucinate the right function. So we", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_2986507_ms_-_3054317_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 2986507, "end_ms": 3054317}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3040525 ms - 3109637 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3040525 ms - 3109637 ms\n\nContent: be resolved within the existing functions, this very likely is correct. And when the um, when the function is not supported then it will uh, 50% of time you will hallucinate the right function. So we figure out what are the functions that are missing in those hallucinations. Uh, so that those are the trial result by the way, among the 50% that are not correct, the, the hallucinative result are not correct. Most of them actually directly generates not parsable react G dsl. We sometimes notice the LLMs tries to generate the implementation in within the response. So we just simply discard them. So we got pretty highly accurate what we call driver result. This is using abstract interpretation, meaning that we interpret the reaction in DSL when there's a missing function, we write down what are the missing functions. From the missing functions we run a clustering algorithm called a grammatory clustering. On the embedding of those missing functions we actually first ask them to say oh like", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3040525_ms_-_3109637_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3040525, "end_ms": 3109637}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3096109 ms - 3165465 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3096109 ms - 3165465 ms\n\nContent: the missing functions. From the missing functions we run a clustering algorithm called a grammatory clustering. On the embedding of those missing functions we actually first ask them to say oh like I know this is users command, this function doesn't exist. Can you tell me what is this function for? So the LM specs generates something like this which explains a little bit of what this function is for. And then we cluster on top of that. The reason behind that is sometimes LLMs may generate two functions with the same signature. For example, persons ID could be a unique ID of the person or it could mean their actual ID number, like their passport number and stuff. So based on the simulated user testing we will be able to from the code skeleton we generate those commands and from those commands we generate actual functions developers need to implement. So that's a quick overview of the pipeline again and from now we render all of those results on the Interface. We built two demo apps for", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3096109_ms_-_3165465_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3096109, "end_ms": 3165465}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3147285 ms - 3222545 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3147285 ms - 3222545 ms\n\nContent: we generate actual functions developers need to implement. So that's a quick overview of the pipeline again and from now we render all of those results on the Interface. We built two demo apps for testing Genie wizard and we asked 12 developers to. It's a within subject study meaning every developer build one app with React Genie and the other app without the guidance of React Genie. We simply tell them oh like try to add more functions to this app. Because user think about what users might want to use in this app and just try to implement more functions we ask them to improve the two test app one is a food menu app, one is a hotel booking app and one of the tools is Genie wizard and one of them is just typical VS code without the help of Genie Wizard. We actually had turned on Copilot for both of them. So that part is the same. There have been reviewers asking oh like what if you turn on Copilot, does it help? It doesn't really help. Um, so we noticed that the, for both of the apps,", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3147285_ms_-_3222545_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3147285, "end_ms": 3222545}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3209129 ms - 3279901 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3209129 ms - 3279901 ms\n\nContent: both of them. So that part is the same. There have been reviewers asking oh like what if you turn on Copilot, does it help? It doesn't really help. Um, so we noticed that the, for both of the apps, the, the blue one is the baseline and the orange one is Genie Wizard. Uh, the participant with Genie wizard actually reduce the missing feature by more than 40%. It's half of the missing feature just within I think within. I forgot if I have the, have the exact statistics here, but I think they did it within 30 minutes actually. So help them address 42% instead of only 10% of missing features within just a 30 minute session. So imagine how much you can do with this IDE if you are developing an actual fully flashed app where the development timeline is probably several months. Yeah, so that's basically it for all the projects. A few things I want to just randomly have a discussion about one thing which I found Very interesting is that I think these projects alludes to a new kind of system", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3209129_ms_-_3279901_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3209129, "end_ms": 3279901}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3266805 ms - 3335835 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3266805 ms - 3335835 ms\n\nContent: basically it for all the projects. A few things I want to just randomly have a discussion about one thing which I found Very interesting is that I think these projects alludes to a new kind of system research in the era of old AI systems. A lot of the system research do people are familiar what are the computer systems research? Like what area is counted as computer systems research? Any example other than what I have already shown here, GPUs and stuff, I'm sorry, building TPUs and GPUs and like that stuff. Yeah, kind of like that. Yeah, exactly. So yeah, like a lot of the system research these days, especially related to AI are about how do we accelerate training, how do we accelerate serving. But I think one thing super interesting to me is how can we both in both of the program in today's talk. And a lot of the stuff, I don't know if this is just me thinking, I don't know if Monica already mentioned this, but a lot of the stuff in this course is kind of about using systems research", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3266805_ms_-_3335835_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3266805, "end_ms": 3335835}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3324855 ms - 3386359 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3324855 ms - 3386359 ms\n\nContent: talk. And a lot of the stuff, I don't know if this is just me thinking, I don't know if Monica already mentioned this, but a lot of the stuff in this course is kind of about using systems research for those AI HCI problems. I try to make multimodal interaction easier to build. And Monica probably talk about how do you retrieve documents better and how do you do stuff better. In React genie I use this interpreter which is like a compiler, programming language concept. I build UI frameworks which is like a software engineering stuff. In GENIE Wizard I use speculative parsing, which is a way of building interpreters and building program analysis. And I build IDE tools to help developers. This is also like a software engineering concept. So I found this very particularly interesting that in this new age of AI you can truly use those AI to build better human systems Using the techniques have been invented long time ago by computer system researchers. So I think this could elude into a new", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3324855_ms_-_3386359_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3324855, "end_ms": 3386359}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3375063 ms - 3431121 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3375063 ms - 3431121 ms\n\nContent: new age of AI you can truly use those AI to build better human systems Using the techniques have been invented long time ago by computer system researchers. So I think this could elude into a new way of building new computer systems projects. And I think without like LLMs plus these computer system concepts really makes this much more grounded and much more accurate. And you can do all sorts of different stuff. For example, I can analyze oh, what's wrong with the programs by using program analysis to know, oh, maybe there's some hallucination happening, maybe there's something wrong so I shouldn't execute it. Maybe there's a function that is critical so I need to show a specific UI to further conform that result before I just execute that. Like if you say oh, delete all my orders, right? And maybe I need to confirm with confirm that maybe I need to ask you to enter password and stuff. So by building this with all the computer system technique, you really have more control, more", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3375063_ms_-_3431121_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3375063, "end_ms": 3431121}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3421665 ms - 3492071 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3421665 ms - 3492071 ms\n\nContent: And maybe I need to confirm with confirm that maybe I need to ask you to enter password and stuff. So by building this with all the computer system technique, you really have more control, more grounding for the LLMs. Another thing, have people heard of multimodal, like just multimodal models like VLMs and stuff? Yeah. So what kind of multimodal models have you heard about before? So I guess the first one that really gained traction, I guess they built it up with GPT4 Vision and then GPT 4.0. Right. But all of this is essentially trying to. When you have words, you bat the words and then you feed that into that's also punching mechanism. And they're trying to do the same thing with images. And then sound is just waveforms which are just images, correct? Similar. Yeah. So basically as far as I know, because a lot of the multimodal models are not open source. I personally haven't trained one yet, but I read a lot of papers. I think most, if you're interested, have this course covered", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3421665_ms_-_3492071_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3421665, "end_ms": 3492071}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3480439 ms - 3539177 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3480439 ms - 3539177 ms\n\nContent: as I know, because a lot of the multimodal models are not open source. I personally haven't trained one yet, but I read a lot of papers. I think most, if you're interested, have this course covered other multimodal machine learning before maybe. I assume no, but yeah. So basically, typically when you train a multimodal machine learning model, you first train the encoder separately. So for example, in the image and audio sense you probably usually people. What people do is they first train an encoder and decoder and the goal of the encoder decoder is just to compress the image. So they train an encoder and a decoder for image. They create a bottleneck in the middle. So you compress for example the image or audio data to a smaller, to a smaller representation. There's also a lot of techniques to make sure that the representation is diverse enough and it's dense. There's a lot of interesting concept there. But basically the core concept is first you train a compression algorithm for that", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3480439_ms_-_3539177_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3480439, "end_ms": 3539177}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3528393 ms - 3592967 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3528393 ms - 3592967 ms\n\nContent: to make sure that the representation is diverse enough and it's dense. There's a lot of interesting concept there. But basically the core concept is first you train a compression algorithm for that specific modality, which is basically the encoder here. And after encoder you have some projection to project it to the same space as language. And there's a bunch of interesting training techniques. I think the one I've seen is you can for example, first only train the projection. After you train the projection a little bit, then you um, then you train both the projection and the model together to make it even better. So something like that. So um, in some sense multimodal and the. The same thing can work with both voice and images. So in some sense, if you think about it, this is basically saying, oh, the multimodal models convert other modalities into tokens. And we are like. All the transformer stuff basically tells us machine learning model works very efficiently with tokens. So we", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3528393_ms_-_3592967_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3528393, "end_ms": 3592967}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3580199 ms - 3642309 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3580199 ms - 3642309 ms\n\nContent: saying, oh, the multimodal models convert other modalities into tokens. And we are like. All the transformer stuff basically tells us machine learning model works very efficiently with tokens. So we just have to convert whatever modality even if they are not naturally tokenized. Language is pretty tokenized already, but voice and images are not. So they tokenize those other different modalities and put it in the language token streams. Um, I Think what I have just done here is similar, right? So you have a program user want to interact with it. Some of the user's interaction is in. Is in speech, which is great because I can just transcribe it and it's just a language. So LLMs can handle those natively. But if there's app, right. Um, and then the app, like how do you tokenize an app? It's kind of, kind of weird. Can you map Uber to a specific token? I think it's going to be hard. Even like long term wise, it's going to be hard to tokenize apps. So what we did here with React Genie and", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3580199_ms_-_3642309_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3580199, "end_ms": 3642309}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3629601 ms - 3696407 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3629601 ms - 3696407 ms\n\nContent: kind of, kind of weird. Can you map Uber to a specific token? I think it's going to be hard. Even like long term wise, it's going to be hard to tokenize apps. So what we did here with React Genie and Genie wizard is we represent the app through all of its functions and the user interactions. So the app scala, the app, the function of the app, which we call program skeletons. And the interactions are being tokenized, not as creative as the vision and audio yet, but into explicit strings of text or slash programs. So basically both of them are in some sense a tokenization problem. There are also a lot of research these days, also kind of tangential to multimodal, not necessarily the kind of multimodal that I'm working here. There are people who say, oh, like for accessibility reason you might want to be able to say the user says click on order button and then you directly click on a button. So they kind of tokenize the current screenshot into a VRM and then they fine tune the model so", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3629601_ms_-_3696407_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3629601, "end_ms": 3696407}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3683199 ms - 3746715 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3683199 ms - 3746715 ms\n\nContent: want to be able to say the user says click on order button and then you directly click on a button. So they kind of tokenize the current screenshot into a VRM and then they fine tune the model so that when you say click on confirm button, they know which button to click. This is also similar to, I don't know. Have people used Anthropic recently update their desktop app to enable computer use? Have people seen that before? Yeah, so that's also basically just VRM tokenizing the screenshot. My personal view for the future is there will be a combination of both. In fact, there's probably going to be three mode. They're going to do multimodal interaction with the app. So on the most basic level is I said click on a button. It click on a button and it works. But then a great thing about this is that it's like a fallback if anything fails and then you just cannot use your hand, for example, you're cooking or something, it will work and it will work across all apps. Great. But the next thing", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3683199_ms_-_3746715_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3683199, "end_ms": 3746715}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3733961 ms - 3797081 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3733961 ms - 3797081 ms\n\nContent: that it's like a fallback if anything fails and then you just cannot use your hand, for example, you're cooking or something, it will work and it will work across all apps. Great. But the next thing is if you want anything more complex, if I want to say, oh, like I remember last time I eat at this restaurant was pretty good. What main dish did I have? Then this becomes much more complex for like a pure just like click by click system. Because if I open an app, the first time, there's no way that I can in the first time get the correct way of interacting the app. Right? The way I can interact with the app correctly is because I have interacted with the app quite a few times and I build a mental model in my mind. So I actually, I have a map of how the apps works and then I'll be able to complete a task in the first try. So basically I imagine the old stuff I've been building reaction is kind of like that map, but provided explicitly by the developer. So basically what reaction will do", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3733961_ms_-_3797081_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3733961, "end_ms": 3797081}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3785371 ms - 3848671 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3785371 ms - 3848671 ms\n\nContent: a task in the first try. So basically I imagine the old stuff I've been building reaction is kind of like that map, but provided explicitly by the developer. So basically what reaction will do is kind of like the intermediary stage, where you still have something concrete you want to work on, but you don't currently have some concrete. But it's not something small where you just click off one or two buttons. So then the skeleton I provided with React Genie will be super helpful. Use as something I translated into a composition of different commands into the skeleton that I created and we execute that. Great. And on an even higher level, there's probably going to be some kind of like agenda system and stuff where you can understand what users want, decompose them into individual actions, and then execute them interactively in multiple steps. So that will be my vision for the future of multimodal machine learning and multimodal interactions. So it will be a combination of stuff on the", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3785371_ms_-_3848671_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3785371, "end_ms": 3848671}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3831983 ms - 3911945 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3831983 ms - 3911945 ms\n\nContent: and then execute them interactively in multiple steps. So that will be my vision for the future of multimodal machine learning and multimodal interactions. So it will be a combination of stuff on the left and stuff on the right. Um, so to recap, today I talk about multimodal interaction. The history is very long, but adoption is limited due to the implementation complexities. Um, compared to voice interfaces, multimodal ones are flexible, efficient, clearer and less error prone. To solve this problem, I have built React Genie which aims to foster multimodal interaction adoption. It merges modern app features with multimodal interfaces for flexibility and ensuring easy development. It utilize object oriented state abstraction and declarative UI for modality synchronization. So you can either use Pure UI or you can use Pure GUI or Pure Voice UI or a combination of both. It employs LLMs to expose the app's entire state rather than limiting it to individual APIs for voice interfaces. And", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3831983_ms_-_3911945_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3831983, "end_ms": 3911945}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3894089 ms - 3976867 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3894089 ms - 3976867 ms\n\nContent: use Pure UI or you can use Pure GUI or Pure Voice UI or a combination of both. It employs LLMs to expose the app's entire state rather than limiting it to individual APIs for voice interfaces. And finally, I talk about GENIE wizard, which helped developer build usable multimodal apps in their first iteration. We make it work by using LLMs to simulate user interactions and speculate missing features using LLM hallucination and abstract interpretation. We try to help developers so that they can help users better interact with the apps. So all the stuff is open source here. It's on GitHub. This is just a link with everything. I hope you can build stuff with it. If you break it help me fix bugs with it. Yeah, so that's it. I just write all the recaps here. Any questions? Oh, this is really cool. Do you have any like human shape mechanism beside the ux? If you want to think to make it into industry, do you want consider like multi human universities? Oh yeah, yeah. So I actually have", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3894089_ms_-_3976867_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3894089, "end_ms": 3976867}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 3961103 ms - 4026811 ms", "content": "Title: CS224V Lecture 16 > Transcript > 3961103 ms - 4026811 ms\n\nContent: cool. Do you have any like human shape mechanism beside the ux? If you want to think to make it into industry, do you want consider like multi human universities? Oh yeah, yeah. So I actually have something when I'm building all the apps that I mentioned before, sorry, all the user study app before I actually do something where it's like a super easy way so you can present a screenshot to a user and they can say oh, on this screen maybe I want to do this. They can wizard of awesome multiple interactions and stuff. I think those are very compatible with what Genie wizard provides. So as a designer you can easily say this is a command we collected from actual users inserted into the app. So we make it super separate. We make it either Genie wizard or we use either Genie wizard or no Genie wizard and we compare with actual humans just because the Genie Wizard. The stage that we have built is we noticed that if you think about it, if you release an app and then the first app you release", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_3961103_ms_-_4026811_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 3961103, "end_ms": 4026811}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 4012763 ms - 4066923 ms", "content": "Title: CS224V Lecture 16 > Transcript > 4012763 ms - 4066923 ms\n\nContent: wizard and we compare with actual humans just because the Genie Wizard. The stage that we have built is we noticed that if you think about it, if you release an app and then the first app you release on App Store, people tried it and then they found the multimodal interaction to be flaky, sometimes working, sometimes not. Then people are like people are just going to stop using it. Like, like Siri. Basically when the Siri just released, everybody try everything with it and they notice nothing works. They notice only thing works is setting up timers and asking the weather and that's the only thing people ever do ever after. So this is the thing we were trying to avoid with Genie Wizard. We want to say oh like if you build a multimodal app, we want it to work in the first version. So we don't need it to work 99.9% of the time. We want it to work 90% of the time. So you still want to keep trying different stuff with it and then ask people keep trying. In fact, one thing I think it will", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_4012763_ms_-_4066923_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 4012763, "end_ms": 4066923}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 4056055 ms - 4143023 ms", "content": "Title: CS224V Lecture 16 > Transcript > 4056055 ms - 4143023 ms\n\nContent: need it to work 99.9% of the time. We want it to work 90% of the time. So you still want to keep trying different stuff with it and then ask people keep trying. In fact, one thing I think it will be super interesting is that because react Genie, the react GENIE parser, when you set something wrong, it will either call function doesn't exist or it will just error, right? So we can record in a server, we can record every function that either refers to a function called exist or just error doc and we can easily just like dynamically start like I can territory imagine industrial setting where once you publish the app, the developer just got a stream of kind of like Google Analytics but for missing functions. It will tell you oh like 90% of your users currently want this function. So you probably want to Implement that. Yeah, so that could totally work. Cool. Any other stuff is this course until 420 I assume. Okay. Um, yeah, so that's all about the lecture. Um, any other questions? One", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_4056055_ms_-_4143023_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 4056055, "end_ms": 4143023}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 4103271 ms - 4200345 ms", "content": "Title: CS224V Lecture 16 > Transcript > 4103271 ms - 4200345 ms\n\nContent: want to Implement that. Yeah, so that could totally work. Cool. Any other stuff is this course until 420 I assume. Okay. Um, yeah, so that's all about the lecture. Um, any other questions? One thing I just want to briefly mention is I don't know how many of you are building voice interfaces rather than just language interfaces. Kind of. Yeah, so. So one thing I have been noticing with voice, especially with all of the user studies, is I didn't mention it here, also didn't mention any paper. It's kind of interesting. Maybe I mention it once or twice in the papers. So I notice at least about 20% of the users when they try to use a study, they say, oh, like I like your study, it's great, but I would never use voice interface in the real world. Then they mentioned two reasons typically. One reason is that the voice interfaces, they're sometimes inaccurate, especially in noisy environments. Another problem is voice is very interesting because when you start talking randomly in public,", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_4103271_ms_-_4200345_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 4103271, "end_ms": 4200345}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 4181557 ms - 4250453 ms", "content": "Title: CS224V Lecture 16 > Transcript > 4181557 ms - 4250453 ms\n\nContent: reason is that the voice interfaces, they're sometimes inaccurate, especially in noisy environments. Another problem is voice is very interesting because when you start talking randomly in public, people look at you, which is weird. Maybe you just want to communicate with your computers rather than communicate with peoples. So that is going to be a problem for voice interfaces going forward. Although I 100% believe voice interface will be the future. Right. Because voice is way faster. There's paper showing voice interfaces are four times faster than typing. Right. So I think one thing I'm particularly interested is also what, what I'm working on in the startup is how can we enable like private voice interactions and accurate interactions. How do you so that you can interact with your computer when super noisy around? And how can we make computers recognize your speech even if your volume is low enough so nobody around you can hear you? So imagine like if I'm sitting down there and", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_4181557_ms_-_4250453_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 4181557, "end_ms": 4250453}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 4237885 ms - 4295463 ms", "content": "Title: CS224V Lecture 16 > Transcript > 4237885 ms - 4295463 ms\n\nContent: when super noisy around? And how can we make computers recognize your speech even if your volume is low enough so nobody around you can hear you? So imagine like if I'm sitting down there and I'm listening to a lecture, I want to write some notes rather than typing, I just start whispering and nobody else can hear me. And that will be great. So that's one interesting thing you can think about in the future. I think I saw a project at MIT where they put electrodes on the muscles that you use to speak. Exactly. You just mouth the words and the computer will recognize it. Right? Yeah. So this is kind of similar to what we're wearing, but we kind of want to work with existing hardwares just so that, yeah, you don't have to wear electrode on your face. But yeah, So I think those will have a very good future, basically. Could you just skip voice and go straight to neuralink? Yeah, I do believe that in some sense that will be the future. So. But I mean it's still going to be many years away", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_4237885_ms_-_4295463_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 4237885, "end_ms": 4295463}}
{"document_title": "CS224V Lecture 16", "section_title": "CS224V Lecture 16 > Transcript > 4282549 ms - 5002535 ms", "content": "good future, basically. Could you just skip voice and go straight to neuralink? Yeah, I do believe that in some sense that will be the future. So. But I mean it's still going to be many years away and I want it now. You know, like I want to use voice every all the time. Basically. Like I really imagine like what if I don't have to use this but rather I just say I just switch for next slides. But yeah. Cool. All right. I guess that's it for this lecture. Thanks a lot for being here. It.", "block_metadata": {"id": "CS224V_Lecture_16_>_Transcript_>_4282549_ms_-_5002535_ms", "document_type": "transcript", "lecture_number": 16, "start_ms": 4282549, "end_ms": 5002535}}
