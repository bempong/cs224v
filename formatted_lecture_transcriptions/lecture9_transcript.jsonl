{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Projects for Kids", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Projects for Kids\n\nContent: All right, are you guys ready for another 15 projects? Yay. Can we get the class started?", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Projects_for_Kids", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 93255, "end_ms": 117009}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Comments on the Google Proposal", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Comments on the Google Proposal\n\nContent: We want everybody's comments. Just spread yourself out on those sheets and hopefully that will work. You can also put the comments separately on the file and then upload later. You will be getting a lot more feedback in the next round as we reach your proposals.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Comments_on_the_Google_Proposal", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 117177, "end_ms": 181637}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Applying AI to Job Search: Natural Language Feedback", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Applying AI to Job Search: Natural Language Feedback\n\nContent: Adaptive Query Refinement through Conversational Relevance Feedback. The goal is to design a conversational search tool for job postings where you can use natural language to give feedback. This feedback will be encoded into user preferences and these preferences will then be used to re rank the gross result into an adjusted result.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Applying_AI_to_Job_Search:_Natural_Language_Feedback", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 181781, "end_ms": 415825}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Conversational Agent for Financial Modeling", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Conversational Agent for Financial Modeling\n\nContent: Our project is conversational agent for financial modeling. The goal is to be able to do knowledge extraction and logical reasoning for financial models. Right now we have a preliminary implementation which is a user query and three different LLM calls.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Conversational_Agent_for_Financial_Modeling", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 444055, "end_ms": 744555}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Autonomous Tool Creation by Agents", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Autonomous Tool Creation by Agents\n\nContent: We're doing autonomous tool creation by agents. Currently developers have to define what API calls and what tools that agents can use. Can we design an agent that transcends these limitations? Our main focus would be web like benchmarks.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Autonomous_Tool_Creation_by_Agents", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 747455, "end_ms": 1117759}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > How to Write a Story With AI", "content": "Title: CS224V Lecture 9 > Chapter Summaries > How to Write a Story With AI\n\nContent: The overall goal of our project is to create kind of an AI writing assistant. We're trying to think about how we can use AI to really engage with writers. Use AI to empower writers and help with their creativity.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_How_to_Write_a_Story_With_AI", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 1117807, "end_ms": 1440615}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Agents for Personalized Language Learning", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Agents for Personalized Language Learning\n\nContent: Our project is agents for personalized language learning. What if language learning could meet you exactly where you're at? Could model the immersive experience we all desire when learning a language. How can we navigate the limited token context window of most LLMs?", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Agents_for_Personalized_Language_Learning", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 1440995, "end_ms": 1787015}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > AI Adoption Facilitation Agents", "content": "Title: CS224V Lecture 9 > Chapter Summaries > AI Adoption Facilitation Agents\n\nContent: Almost 40% of animals entering shelters each year in the US will not be adopted. The shelter adoption process can be long winded, confusing and difficult. Can a task oriented agent promote the adopt, don't shop responsible ownership philosophy?", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_AI_Adoption_Facilitation_Agents", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 1788835, "end_ms": 1981847}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > How to Build a single-step agent workflow with Python", "content": "Title: CS224V Lecture 9 > Chapter Summaries > How to Build a single-step agent workflow with Python\n\nContent: So first off, the user will submit a request to find a suitable dog breed. The agents will then query adoption search in order to find dog breeds within the user's area. Here is a brief timeline for our project.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_How_to_Build_a_single-step_agent_workflow_with_Python", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 1981951, "end_ms": 2150089}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Pet GBT: Expansion to Other Animals", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Pet GBT: Expansion to Other Animals\n\nContent: Have you considered any voice from the docs? I think definitely some future expansion points would be adding more possible pets. Right now we are planning on possibly showing images to the user of the dog, but I think having videos or voice recordings could also be super cool to include.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Pet_GBT:_Expansion_to_Other_Animals", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 2150177, "end_ms": 2247835}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > AI to Help Insurance Adjusters Spot Fraud", "content": "Title: CS224V Lecture 9 > Chapter Summaries > AI to Help Insurance Adjusters Spot Fraud\n\nContent: Every year $300 billion of money actually goes into insurance fraud. This results in $900 that you pay every year extra for your insurance premium. Could we develop a conversational agents that can prompt, help the adjusters ask better questions to detect fraud early on?", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_AI_to_Help_Insurance_Adjusters_Spot_Fraud", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 2259655, "end_ms": 2442965}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > How to better diagnose schizophrenia with NLP?", "content": "Title: CS224V Lecture 9 > Chapter Summaries > How to better diagnose schizophrenia with NLP?\n\nContent: Researchers are looking at linguistic investigation of speech patterns in schizophrenia. How can we use language patterns in multilingual speech data to identify psychiatric conditions like schizophrenia? They are evaluating traditional NLP methods and how LLMs can improve off them.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_How_to_better_diagnose_schizophrenia_with_NLP?", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 2470715, "end_ms": 2779615}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > A Research Assistant for Handwritten Lab Notes", "content": "Title: CS224V Lecture 9 > Chapter Summaries > A Research Assistant for Handwritten Lab Notes\n\nContent: A research assistant for handwritten lab notes. The project stems from science experiments in physical sciences. Can we bridge the gap between the physical medium and the digital medium to spur innovation for physical scientists?", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_A_Research_Assistant_for_Handwritten_Lab_Notes", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 2805765, "end_ms": 3127049}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Lumina: The AI Agent for College Application Essay brainstorm", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Lumina: The AI Agent for College Application Essay brainstorm\n\nContent: Lumina is an AI agent for college application essay brainstorming. The AI agent helps the user put together an outline where they'll ask a few questions. From there a summarizing agent generates this final output to provide these ideas to the student.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Lumina:_The_AI_Agent_for_College_Application_Essay_brainstorm", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 3127137, "end_ms": 3427459}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > How to build a travel advisor with OpenAI", "content": "Title: CS224V Lecture 9 > Chapter Summaries > How to build a travel advisor with OpenAI\n\nContent: Travel Advisor is an LLM based agent that can take information from sites like Reddit or FlyerTalk and offer people advice. The three main areas of challenge would be eliminating noise from the data set, making sure that the LLMs are giving people personalized advice.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_How_to_build_a_travel_advisor_with_OpenAI", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 3427507, "end_ms": 3697405}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Projects: Conversational LLM Agents for Optimizing Job Applications", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Projects: Conversational LLM Agents for Optimizing Job Applications\n\nContent: Yanis Canoe: Project on conversational LLMs assisting for optimizing job applications. He says job applications require resumes, cover letters, but also non traditional application materials like personal websites, portfolios, LinkedIn and GitHub profiles. Canoe hopes to have a first version ready by next week.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Projects:_Conversational_LLM_Agents_for_Optimizing_Job_Applications", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 3699745, "end_ms": 3957597}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > How to Build a Sidekick with LLM Agents", "content": "Title: CS224V Lecture 9 > Chapter Summaries > How to Build a Sidekick with LLM Agents\n\nContent: We're building out Sidekick and intelligent calendar automation with LLM agents. We already know what we look like. Oh, there. Oh no, not yet. Okay.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_How_to_Build_a_Sidekick_with_LLM_Agents", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 3957701, "end_ms": 3982439}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > How to add events to your Calendar with Google Calendar", "content": "Title: CS224V Lecture 9 > Chapter Summaries > How to add events to your Calendar with Google Calendar\n\nContent: The back end server is going to connect with Google Calendar. The agent will automatically populate your Google Calendar for you. This would hopefully save a lot of time and also make studying and like, like balancing out your schedule and your calendar and setting it up way easier.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_How_to_add_events_to_your_Calendar_with_Google_Calendar", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 3982487, "end_ms": 4167186}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > What's Your Plan for Your Calendar?", "content": "Title: CS224V Lecture 9 > Chapter Summaries > What's Your Plan for Your Calendar?\n\nContent: The apps are mostly targeted towards enterprise users. Motion is definitely catered towards professionals. The other thing we're going to focus on is ease of use is, number one, want it to be as seamless as possible.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_What's_Your_Plan_for_Your_Calendar?", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 4167250, "end_ms": 4240815}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Inhalation Learning for multistage language model programs", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Inhalation Learning for multistage language model programs\n\nContent: Our project is going to be on rule induction learning for multistage language model programs. We have two methods that we are going to experiment with and the first one is this decision tree style prompting. We will implement this as a module in dspy.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Inhalation_Learning_for_multistage_language_model_programs", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 4242275, "end_ms": 4515671}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Inference-based DSPY optimizers", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Inference-based DSPY optimizers\n\nContent: The core idea behind this is very similar to some of the DSPY optimizers. You take a base language input and then it runs through some high quality candidate trials to see like what is the most optimal instruction. Key questions are how can we design optimizers for language model programs that effectively induce rules and apply them to specific tasks.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Inference-based_DSPY_optimizers", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 4515823, "end_ms": 4627475}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Inferring LLMs and their performance", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Inferring LLMs and their performance\n\nContent: The first task we'll do is personal identifiable information reduction. A similar task is many way intent taxonomy where you have over 50 to 100 classes in your domains. Lastly, it's called Agent Adventures with alfworld where the concept is your agent has a wide action space of things to do.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Inferring_LLMs_and_their_performance", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 4628375, "end_ms": 4705215}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Projects: Attractive Conversational Language Learning Assistant", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Projects: Attractive Conversational Language Learning Assistant\n\nContent: Our project is essentially adaptive conversational Language Learning Assistant for English learners. The motivation for this project is there are 1.5 billion English language learners globally. And a big problem for them is often getting real world conversation practice and getting tailored feedback.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Projects:_Attractive_Conversational_Language_Learning_Assistant", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 4723405, "end_ms": 4908137}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Chapter Summaries > Comments for the Presentations", "content": "Title: CS224V Lecture 9 > Chapter Summaries > Comments for the Presentations\n\nContent: There are several videos and they post it. Go home and look at the videos. There's only a few minutes each in for these videos and put your comments in. Getting comments from the class is very helpful. I will see you on Wednesday and we will go we'll have more lectures.", "block_metadata": {"id": "CS224V_Lecture_9_>_Chapter_Summaries_>_Comments_for_the_Presentations", "document_type": "chapter summary", "lecture_number": 9, "start_ms": 4908321, "end_ms": 5296205}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 93255 ms - 171173 ms", "content": "Title: CS224V Lecture 9 > Transcript > 93255 ms - 171173 ms\n\nContent: All right, let's get started. Can we get the class started? All right, are you guys ready for another 15 projects? Yay. Exactly. Okay, so I think it has been really interesting to see what kind of ideas are, you know, that you guys are interested in. It is really exciting. So you know the drill. We got to have feedback sheets, and we know that sometimes there was. I mean, Google cannot handle so many people giving feedbacks, so you can do more sheets. So we're going to create parallelism. It is embarrassingly parallelizable. So pick one. Just spread yourself out on those sheets and hopefully that will work. You can also put the comments separately on the file and then upload later. Okay, so no excuses. We want everybody's comments. And last time I didn't put in, I wasn't able to put in some of my comments, but I have added them after the class. So for those of you who presented, please take a look at those comments. They're a little bit short because, you know, we're trying to make", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_93255_ms_-_171173_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 93255, "end_ms": 171173}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 161725 ms - 223915 ms", "content": "Title: CS224V Lecture 9 > Transcript > 161725 ms - 223915 ms\n\nContent: some of my comments, but I have added them after the class. So for those of you who presented, please take a look at those comments. They're a little bit short because, you know, we're trying to make sure everybody gets those few comments right on the spot. But you will be getting a lot more feedback in the next round as we reach your proposals. And then you'll be talking to all the CAs and your mentors. All right, let's get started. We have a long program. Hello and welcome to our presentation. Adaptive Query Refinement through Conversational Relevance Feedback. My name is Bjorn Engahl and I'm an SPD student from Sweden. And I am Matthias Heubi and I'm from Switzerland and SAPD student as well. So the problem is something you're all most likely familiar with. It's when you do search tasks, for example, a job search, you often find yourself filtering based on full text keyword matching. And this totally lacks the richness of contextual embeddings. So, for example, AI may appear in", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_161725_ms_-_223915_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 161725, "end_ms": 223915}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 209109 ms - 284421 ms", "content": "Title: CS224V Lecture 9 > Transcript > 209109 ms - 284421 ms\n\nContent: for example, a job search, you often find yourself filtering based on full text keyword matching. And this totally lacks the richness of contextual embeddings. So, for example, AI may appear in text having totally different meanings, such as ad interim, and then you are presented with these jobs as a potential match. What also surprises me is that most platforms don't even allow you to use an exclusionary term if there's something that you don't want. So in the end, you end up reviewing lots and lots of jobs postings that totally don't match your preference. Even the market leader seems to have its problem in finding matching ads. So our goal is to see if we can design a conversational search tool for job postings where you can use natural language to give feedback on the presented job postings that you get, and also so the system can learn the user's preferences over time and thereby presenting more and more relevant ads. The main research questions that we will look into are how we", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_209109_ms_-_284421_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 209109, "end_ms": 284421}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 267853 ms - 338923 ms", "content": "Title: CS224V Lecture 9 > Transcript > 267853 ms - 338923 ms\n\nContent: that you get, and also so the system can learn the user's preferences over time and thereby presenting more and more relevant ads. The main research questions that we will look into are how we can translate this natural language feedback into a concise representation of the user's preferences. Previous studies have shown that once you get a list that is too long of different things you want the LLM to do, they stop paying attention to all of the elements in the list. So we want to look into this and see if we can find a good way to concisely represent the feedback. Also we want to see if we can have the LLM dynamically find the dimensions the so called slots. If you remember the GENIE worksheet, you had to manually fill in the slots in the worksheet, the attributes and we want to see if we can have the LLM discovery what the proper attributes are for this domain. Here's a simplified architecture of what we're trying to do. We start out with the corpus of job postings and we use a", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_267853_ms_-_338923_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 267853, "end_ms": 338923}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 324079 ms - 403897 ms", "content": "Title: CS224V Lecture 9 > Transcript > 324079 ms - 403897 ms\n\nContent: if we can have the LLM discovery what the proper attributes are for this domain. Here's a simplified architecture of what we're trying to do. We start out with the corpus of job postings and we use a regular SQL query to retrieve a gross result maybe based on industry and location. And then the user can review individual job postings and provide feedback why a given job is a fit or is not a fit. In his opinion. This feedback will be encoded into user preferences and these preferences will then be used to re rank the gross result into an adjusted result, bubbling the better matching job postings to the top. And now the user can iterate on this and provide feedback on the adjusted result and so on until we have very fine grained and closely matching preferences. To evaluate the system. We will mostly do it by hand initially, basically looking at the scores that the system is providing for individual job postings and then you know, manually see whether we agree and to try to find out why", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_324079_ms_-_403897_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 324079, "end_ms": 403897}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 385561 ms - 488039 ms", "content": "Title: CS224V Lecture 9 > Transcript > 385561 ms - 488039 ms\n\nContent: mostly do it by hand initially, basically looking at the scores that the system is providing for individual job postings and then you know, manually see whether we agree and to try to find out why a score ended up the way it is. Yeah, that's it from our presentation. Thank you and feedback is welcome. Thank you. Hi everyone, my name is Ehsan Ghassemi, I'm an MBA student at Stanford Graduate School of Business and I'm Samantha Levenes and I'm an undergrad student and our project is conversational agent for financial modeling. Our primary advisor for this project is Julia Yao, who comes from investment banking and private equity from Goldman Sachs and Carlyle. We're going to go over the explanation behind the motivation of this project, provide some key questions, exemplify some of the current limitations with the tools that exist, and create display our current basic implementation that we currently have. The motivation for this project comes from my own experience of going from ML", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_385561_ms_-_488039_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 385561, "end_ms": 488039}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 475217 ms - 539369 ms", "content": "Title: CS224V Lecture 9 > Transcript > 475217 ms - 539369 ms\n\nContent: limitations with the tools that exist, and create display our current basic implementation that we currently have. The motivation for this project comes from my own experience of going from ML Engineering to mba, doing my Master of Business Administration at Graduate School of Business and recognizing the challenges that exist with building Financial models. By the show of hands, how many people actually use Excel on your day to day life or Google sheets? Yeah, this answer would have been very different in gsb. So we're currently also having conversation with the teaching and learning department at Stanford GSP in terms of incorporating AI into different departments, especially finance. And we're trying to figure out exactly how we could do this. And the key question we were trying to answer is can we create a conversational agent that is not only able to do knowledge extraction but able to do logical reasoning for financial models? I'm going to pass it to Samantha for use cases. So", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_475217_ms_-_539369_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 475217, "end_ms": 539369}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 527345 ms - 597481 ms", "content": "Title: CS224V Lecture 9 > Transcript > 527345 ms - 597481 ms\n\nContent: is can we create a conversational agent that is not only able to do knowledge extraction but able to do logical reasoning for financial models? I'm going to pass it to Samantha for use cases. So as we can see, we kind of were doing some testing with what GPT can and can't do. So this is a simple example of what we would like GPT to do which is passing a query, can you give me the income statement information in 2011 as well as kind of an Excel financial model? And we'd like GPT to basically give us back this 2011 chart. But in reality GPT just says oh yeah, here's the data from 2011 and then returns us the entire spreadsheet. Another example is we'd love for a GPT to be able to do some logical reasoning. For instance, we might ask it how are the cogs or cost of goods sold revenue being forecasted? And in the sheet that we gave it, basically this was being forecasted by the average percent revenue from 2019 to 2021. So we'd love for GPT to be able to kind of do that extraction, see the", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_527345_ms_-_597481_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 527345, "end_ms": 597481}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 584705 ms - 648825 ms", "content": "Title: CS224V Lecture 9 > Transcript > 584705 ms - 648825 ms\n\nContent: And in the sheet that we gave it, basically this was being forecasted by the average percent revenue from 2019 to 2021. So we'd love for GPT to be able to kind of do that extraction, see the formula and go from there. But in reality GPT gives a bunch of reasons, a bunch of ways that it's being kind of extracted, none of which are correct. So yeah, those are some examples. And then right now we have kind of a preliminary implementation which is we'll have a user query as well as the Excel sheet, that's the financial model. And we're going to do some formatting transforming on the Excel sheet as well as we're going to extract the formulas to give to GPT manually. Then we're going to give the user query to three different LLM calls and those are all going to aim to answer the question with some multi step reasoning as well as chain of thought prompting. Then we're basically going to do a consistency check. So we're going to see are all of these LLMs answering it in a similar way and if", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_584705_ms_-_648825_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 584705, "end_ms": 648825}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 637561 ms - 719461 ms", "content": "Title: CS224V Lecture 9 > Transcript > 637561 ms - 719461 ms\n\nContent: some multi step reasoning as well as chain of thought prompting. Then we're basically going to do a consistency check. So we're going to see are all of these LLMs answering it in a similar way and if so, then will give an output. Yeah. Any questions? Yes. Is the user able to put in example spreadsheets with similar formatting? For example If a company has to do this report every quarter and they have the previous quarterly reports, could they add those in so that the LLM kind of knows it has to follow this specific format or template. The goal is to be able to create a generalized tool that is understanding of different semi structured formatting. So hopefully it would be able to generalize one specific format. For sure. Yeah. And also generally what we're trying to do is it's less of creating the financial model and more of like a Q and A type thing. So if we give it a financial model, can it extract information from that? I was going to ask for your consistency check. Do you plan", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_637561_ms_-_719461_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 637561, "end_ms": 719461}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 701567 ms - 808565 ms", "content": "Title: CS224V Lecture 9 > Transcript > 701567 ms - 808565 ms\n\nContent: the financial model and more of like a Q and A type thing. So if we give it a financial model, can it extract information from that? I was going to ask for your consistency check. Do you plan for that to look like, are you going to show all the answers to an LLM and say, like, what is the most like, common answer? Or how are you doing that? Like aggregation? So I was doing some preliminary reading on consistency check. I haven't fully done it yet, but yeah, that kind of is. The plan is to have three different LLM calls and answer that query and then a separate LLM to evaluate those and see if those are similar. Yeah. Cool. Hello. Okay, so for a bit of background on our project, we're doing autonomous tool creation by agents. So the project overview is that currently developers have to define what API calls and what tools that agents can use. But there's very little research as to agents developing their own tools to solve tasks. There are research into script generation. However,", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_701567_ms_-_808565_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 701567, "end_ms": 808565}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 793067 ms - 855553 ms", "content": "Title: CS224V Lecture 9 > Transcript > 793067 ms - 855553 ms\n\nContent: define what API calls and what tools that agents can use. But there's very little research as to agents developing their own tools to solve tasks. There are research into script generation. However, these coding agents, typically, at least in the context of creating tools, can't create complex functions and can't do anything more than like two or three compositions into two or three functions, essentially. And so a key question is, can we design an agent that transcends these limitations? The reason we're doing this is like there's kind of two kind of world, like, why try this at all? There's kind of like two ways people are doing agents right now. At least in web agents, it's exploratory, where you have a few limited policies and the agent kind of learns a site map, or they learn through reinforcement learning, or they just kind of explore. And then the other one is you can actually plan. Instead of constantly calling the lm, you actually have a planning agent which plans through the", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_793067_ms_-_855553_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 793067, "end_ms": 855553}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 845665 ms - 908757 ms", "content": "Title: CS224V Lecture 9 > Transcript > 845665 ms - 908757 ms\n\nContent: reinforcement learning, or they just kind of explore. And then the other one is you can actually plan. Instead of constantly calling the lm, you actually have a planning agent which plans through the whole environment and thinks for a long time. And that can be pretty expensive and pretty slow. But if we can have the planning agent plan and then create a script, it doesn't then have to replan at every step and hopefully we can save some computational costs. And so our main focus would be web like benchmarks. Am I missing anything? There. Cool. So some literature review. Webarena is kind of the one of the most recent web benchmarks we'll be testing. Our agents on there large language models as tool makers is the most recent development in LLMs as tool makers, but they suffer the limitations that I previously described, even with the decomposition that they were doing with splitting into smaller subroutines. Are more LLM calls all you need? This isn't a particularly big paper, but", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_845665_ms_-_908757_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 845665, "end_ms": 908757}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 896453 ms - 961353 ms", "content": "Title: CS224V Lecture 9 > Transcript > 896453 ms - 961353 ms\n\nContent: that I previously described, even with the decomposition that they were doing with splitting into smaller subroutines. Are more LLM calls all you need? This isn't a particularly big paper, but there's a very counterintuitive result in that more LLM calls on hard tasks actually resulted in a decrease in performance. So you would expect some log linear increase in the performance scaling laws. But that's not actually the case in this paper. For hard tasks, the LM actually performs worse the more you call to it. So that's something we found very interesting. Agent Workflow Memory is one of the most recent papers in web navigation web tasks. They do this kind of decomposition of tasks and they put it into memory. And when they see a similar task that might require a similar workflow, for example clicking on Amazon and then clicking Shop, then they might like kind of reproduce that action. It's not exactly like that, but I'm not going to go through that. And then agent E is just, hey, you", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_896453_ms_-_961353_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 896453, "end_ms": 961353}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 947273 ms - 1006983 ms", "content": "Title: CS224V Lecture 9 > Transcript > 947273 ms - 1006983 ms\n\nContent: clicking on Amazon and then clicking Shop, then they might like kind of reproduce that action. It's not exactly like that, but I'm not going to go through that. And then agent E is just, hey, you have a supervisor agent and a child agent. It's very kind of straightforward. And then RL learning on web interfaces is kind of an example of one of the more exploratory agents. They suffer from typical RL drawbacks very. It collapses typically on the expert examples, but they do combat it in some interesting ways. You can read the paper if you want, but it's very old. Yeah. So how we plan to do this project is basically we want to base off our architecture off of what Agent Workflow Memory has done. And basically what they do is that this memory state, there's an LN backbone. And then basically based on the state, the agent would then observe the state and then after that integrate, take actions or obtain actions that they can take and then integrate it back into memory. And then what we are", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_947273_ms_-_1006983_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 947273, "end_ms": 1006983}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 997959 ms - 1052563 ms", "content": "Title: CS224V Lecture 9 > Transcript > 997959 ms - 1052563 ms\n\nContent: on the state, the agent would then observe the state and then after that integrate, take actions or obtain actions that they can take and then integrate it back into memory. And then what we are doing is we are basing it off this, but introducing the ability for the agent to write kind of scripts. So every time it writes a script and then after it goes back and then based on the action taken by its scripts or the tools that it creates, and then let's say one or two functions each time, and then it goes back and then evaluates how that affects the state of the environment and then that feeds back into the memory as well. And then using this, we hope to create basically this Higher level script and then that gives us our LLM that is able to then create tools that can a high level tools that can execute on all of these by breaking it out this way. On evaluating this, we want to evaluate on basically web benchmarks. One we will definitely use is WebArena. Then a few other benchmarks that", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_997959_ms_-_1052563_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 997959, "end_ms": 1052563}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1039419 ms - 1093857 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1039419 ms - 1093857 ms\n\nContent: can execute on all of these by breaking it out this way. On evaluating this, we want to evaluate on basically web benchmarks. One we will definitely use is WebArena. Then a few other benchmarks that we are considering right now. One is Web Canvas. Web Canvas. The difference between Web Canvas and WebArena is that web Canvas is a real Internet benchmark. So Web arena is kind of static websites. Web Canvas is just like serve the web and then they find and then. But that's great because it's the real Internet, but it's also less reproducible because websites on the Internet kind of change over time. So that might create problems when you want to evaluate something else in future. And then another one we will be using is Web2Mine, which is just another realistic web benchmark similar to Web Arena. Then for what we'll be benchmarking against, we'll be benchmarking against the current state of the art web agents like some of the literature review we talked about over there, as well as just", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1039419_ms_-_1093857_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1039419, "end_ms": 1093857}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1084385 ms - 1181787 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1084385 ms - 1181787 ms\n\nContent: Then for what we'll be benchmarking against, we'll be benchmarking against the current state of the art web agents like some of the literature review we talked about over there, as well as just a GPT 4.0 or 01 preview baseline. Here are the expected outcomes and deliverables for the project. This is what we want to do for every week moving forward, which is to slowly implement each part of the system and then benchmark it against just different benchmarks and see how we can optimize the performance. I think that's all. Thank you. All right. Oh, hi. All right. Hi. So iconic stories have flows, right? So if you strip away the armors and the vfx, what Iron man tells the story is. Is the story of redemptions. How the protagonist, AKA Tony Stark, confronts a challenge, learn his lessons and merges as a stronger character. Similarly with La La Land, if you strip away the dancing sequence and amazing cinematography, you have a story of boy meets girl and a tragedy, depending on how you look", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1084385_ms_-_1181787_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1084385, "end_ms": 1181787}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1169219 ms - 1241557 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1169219 ms - 1241557 ms\n\nContent: as a stronger character. Similarly with La La Land, if you strip away the dancing sequence and amazing cinematography, you have a story of boy meets girl and a tragedy, depending on how you look at the ending. What this means that for boy meets girl story, you initially have a protagonist that meets their love interest and then something happens. But eventually the protagonist learns the lesson, fixes whatever problem there is and emerges as a happy ending or a sad one. And similarly you have Cinderella. So a great story evokes emotion. But what usually happens is that there are certain paces and certain patterns of story that work especially well. So that's why we start to wonder, how do we use AI to help story creators bring in storymen realize their visions faster? What we mean by this is that usually when you're creating A story, you're going from beat A to beat B. And you have characters and you have a world, which means that you have settings, you have different characters who", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1169219_ms_-_1241557_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1169219, "end_ms": 1241557}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1228565 ms - 1294861 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1228565 ms - 1294861 ms\n\nContent: is that usually when you're creating A story, you're going from beat A to beat B. And you have characters and you have a world, which means that you have settings, you have different characters who have different histories, and you have prior interactions. For example, Morty and Rick did XYZ or Rick invented XYZ machines. So how can we use AI to help the creator brainstorm different options based on these prior and consistent in world knowledge? So the overall goal of our project is to create kind of an AI writing assistant. And in creating this project, we're being very cognizant of kind of public thoughts and the general discourse around having AI write stories. We all know that AI has written stories that haven't exactly won the New York Times bestseller on Amazon. So the goal here is not to have AI write full stories. We're trying to think about how we can use AI to really engage with writers and use AI to empower writers and help with their creativity. This is just an example of", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1228565_ms_-_1294861_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1228565, "end_ms": 1294861}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1283045 ms - 1339071 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1283045 ms - 1339071 ms\n\nContent: not to have AI write full stories. We're trying to think about how we can use AI to really engage with writers and use AI to empower writers and help with their creativity. This is just an example of what technically the state of the art would be if you were trying to have ChatGPT write a story for you. Let's say you said, like, help me write a horror story. It just like tries to spit out a story and it's not letting your own creativity flow. It's giving you like, oh, someone moves into like a spooky house and guess what, there's lore there. Wow, it's so, like so original. So our goal is to not do that. Our goal is to kind of think of how we can create a knowledge creation and also task agent that can help engage with agents kind of similar in a storm fashion, to give them the questions and the ideas that they need to move their story forward. So as a very simple project plan, we're still brainstorming exactly how we want to implement it, but we do see potential with a genie worksheet", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1283045_ms_-_1339071_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1283045, "end_ms": 1339071}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1328679 ms - 1379761 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1328679 ms - 1379761 ms\n\nContent: the ideas that they need to move their story forward. So as a very simple project plan, we're still brainstorming exactly how we want to implement it, but we do see potential with a genie worksheet approach, with that being a framework for task oriented agents. You have your task specific APIs, you have the task declarations as the different possible actions that you could ask the agent and then get support on. Let's say you were asking based on this character's background and their history and their character traits. I wanted to know the best way for them to interact forward in the story, to make it interesting, or I have written this part of the story so far. I want to know kind of in the best way objectively, what's the quality of it, how can it be changed, things like that. And then you have incorporated knowledge corpuses, like storing writing guides on what makes quote unquote, objectively good writing, story structures and Databases of actual stories, possibly search engine", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1328679_ms_-_1379761_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1328679, "end_ms": 1379761}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1368961 ms - 1423569 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1368961 ms - 1423569 ms\n\nContent: you have incorporated knowledge corpuses, like storing writing guides on what makes quote unquote, objectively good writing, story structures and Databases of actual stories, possibly search engine access, et cetera. So creating a kind of GENIE agent like this for evaluation. Again we're comparing against what's technically the state of the art, which is things like perplexity, Claude, GPT 4.0, et cetera, at generating stories and also helping you write stories. We're going to do automatic and human evaluation. Automatic being. We can compare outputs using kind of AI scoring, obviously using different AIs so there's no bias towards its own model outputs and score it in terms of things like how useful might it be to writers, the coherence of the output with the query, et cetera. And then human evaluation, just hopefully creating a UI for this project and then having a limited pool of people actually try it out and see how useful it is in their writing process. And also generate", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1368961_ms_-_1423569_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1368961, "end_ms": 1423569}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1412769 ms - 1473747 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1412769 ms - 1473747 ms\n\nContent: then human evaluation, just hopefully creating a UI for this project and then having a limited pool of people actually try it out and see how useful it is in their writing process. And also generate qualitative and quantitative feedback, having people compare our outputs to outputs by state of the art models. So that's generally it. We'll take any questions. Yes. Oh, do we have to go or. Okay, wonderful. Obviously like when writing stories there can be a lot of bias, especially if you look at the like previous romance stories in terms of how it portrays women and if it can like pass the Bechdel test or anything. So I was wondering if you had any ideas in terms of like limiting bias when it's helping to write these stories. I think that's a super interesting question and it's definitely a question of what do we want to include in our knowledge corpus and what do we want to include in kind of the GENIE action workflow and counteracting bias. So I think that's definitely just a matter of", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1412769_ms_-_1473747_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1412769, "end_ms": 1473747}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1463955 ms - 1552383 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1463955 ms - 1552383 ms\n\nContent: of what do we want to include in our knowledge corpus and what do we want to include in kind of the GENIE action workflow and counteracting bias. So I think that's definitely just a matter of implementation and making sure that you're being cognizant of any biases. I think to add on that one thing that we're trying to do is to look at all the flows of a story. So that's, that's the emphasis of the project, to look at the flows of story, what makes a story flow well, but the actual story remains and actual creativity still remained in the hands of the author. Thank you. Awesome. Our project is agents for personalized language learning. Every year, myself embark on a journey to learn or master a new language. And for most people this journey is actually often unsuccessful. So what if language learning could be made adaptive? What if language learning could meet you exactly where you're at? And what if language learning could model the immersive experience we all desire when learning a", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1463955_ms_-_1552383_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1463955, "end_ms": 1552383}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1540999 ms - 1597565 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1540999 ms - 1597565 ms\n\nContent: learning could be made adaptive? What if language learning could meet you exactly where you're at? And what if language learning could model the immersive experience we all desire when learning a language, living in a foreign country along with a personalized and friendly partner at your side. So Brendan and I Believe that the future of LLMs is personalization. Right now, by their nature, LLMs are incredibly general. So for every single person who puts in a certain prompt in ChatGPT, they pretty much get the same answer. But in the future, let's say you want to order a specific type of furniture for your office. You have very specific needs and for that reason an LLM when trained on your data and your information would be able to say this is their budget, this is their style, and this is the type of furniture that they should be receiving. And similarly, it would be able to message your boss and say and write the perfect message explaining why you're running late within your style of", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1540999_ms_-_1597565_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1540999, "end_ms": 1597565}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1586875 ms - 1652123 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1586875 ms - 1652123 ms\n\nContent: the type of furniture that they should be receiving. And similarly, it would be able to message your boss and say and write the perfect message explaining why you're running late within your style of messaging. So the thrust for this project is thinking about the architectures and paradigms we can use to move LLMs to the more personal rather than the more general. And we are starting with this very specific task that is attempted by billions of people every year and can have a huge effect on people's lives. So how can we build so as sub questions for this research problem is how can we build an all encompassing user profile? And how can we navigate the limited token context window of Most of most LLMs, shout out Gemini for their 2 million token context window to still provide the user with a completely personalized experience. And then how can we keep updating the LLM interaction, interaction by interaction, when sometimes the information that it's getting is conflicting? And how can", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1586875_ms_-_1652123_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1586875, "end_ms": 1652123}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1636607 ms - 1702151 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1636607 ms - 1702151 ms\n\nContent: a completely personalized experience. And then how can we keep updating the LLM interaction, interaction by interaction, when sometimes the information that it's getting is conflicting? And how can we do this while still incorporating the latest advancements of language learning science? So the key features we are using for the next five, six weeks in this project in this class is to first make sure we want to have an LLM that can mimic real life language learning environment or an idealized language learning environment, increasing the difficulty relatively to your current profile, incorporating your unique goals as well as the unique goals for why you're learning a language in particular usually that is speaking. And we want to do this using some mixture of agents architecture where the explanations would be powered by together AIs like most recent paper and using a similar architecture as them. But for the actual language conversation and the practice we would be using just a", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1636607_ms_-_1702151_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1636607, "end_ms": 1702151}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1687991 ms - 1752939 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1687991 ms - 1752939 ms\n\nContent: the explanations would be powered by together AIs like most recent paper and using a similar architecture as them. But for the actual language conversation and the practice we would be using just a single prompt, right? And we're trying to reimagine what is a good explanation for a question that is a language question that a student might ask. And so we're trying to bring in linguistic theory and think about instead of just a sort of basic what is this word? Is it a noun, is it a verb? What part, you know, parts of speech instead also including what words appear near that word in the same in similar sentences, how often that word appears. So we're focusing on the most common words in their most common settings so people can get that immersive experience and for common needs of language learners. And yeah, I kind of touched on this earlier, but it would be like one main paradigm for the actual language learning and the language explanation. And this would be each response and each", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1687991_ms_-_1752939_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1687991, "end_ms": 1752939}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1740735 ms - 1842741 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1740735 ms - 1842741 ms\n\nContent: learners. And yeah, I kind of touched on this earlier, but it would be like one main paradigm for the actual language learning and the language explanation. And this would be each response and each thing that would be generated would be generated with more like care and thought, whereas the actual conversation with the LLM would be something that would need to happen in real time. Awesome. And here's our architecture and it's sort of like you can think of that as like two. Well, I guess it would be two with lungs. I guess there's two sides of the lung and those are the two sides. One is the conversational query and the other is more explanatory in nature. And yeah, and then all of that would be constantly updating some user database to make both of those personalized to you. Thank you. That's it for questions. Hello, can you hear me? Okay. Hi, I'm Priyanka. I'm Catherine. And we are planning to work on a dog adoption facilitation agents. So almost 40% of animals entering shelters each", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1740735_ms_-_1842741_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1740735, "end_ms": 1842741}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1790179 ms - 1893139 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1790179 ms - 1893139 ms\n\nContent: it for questions. Hello, can you hear me? Okay. Hi, I'm Priyanka. I'm Catherine. And we are planning to work on a dog adoption facilitation agents. So almost 40% of animals entering shelters each year in the US will not be adopted that year and almost 15%, or almost 1 million of them will be euthanized. So while over 6 million animals enter pet shelters every year, so much fewer are actually adopted, but not for lack of demand. More than 44% of all U.S. households own a dog, and that number is increasing every year. A study conducted by animal welfare nonprofits actually show that if an additional 6% of Americans chose to adopt a pet from a shelter instead of purchasing from a breeder, then all US Shelters would be no kill. The shelter adoption process can be long winded, confusing and difficult, especially for the elderly or for individuals with disabilities. So this led us to our key question. Can a task oriented agent promote the adopt, don't shop responsible ownership philosophy", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1790179_ms_-_1893139_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1790179, "end_ms": 1893139}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1882183 ms - 1935167 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1882183 ms - 1935167 ms\n\nContent: especially for the elderly or for individuals with disabilities. So this led us to our key question. Can a task oriented agent promote the adopt, don't shop responsible ownership philosophy by improving ease and accessibility of the shelter adoption process for prospective pet owners? So we looked into some existing work because we felt that this was a pretty intuitive use case for AI. But surprisingly, there was not a lot of work that actually aimed to make the process more accessible on the pet owner side. So a more famous example is Rescue Writer by Pets, which is a machine learning driven tool that's used to create custom pet profiles for shelters to help them advertise available pets. We also looked into studies that use artificial intelligence to help predict personality types in dogs for service work as well as AI. Based pet adoption systems that are more geared towards the shelter side to help them find clients. So we are aiming to kind of use this existing work, but make it", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1882183_ms_-_1935167_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1882183, "end_ms": 1935167}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1925295 ms - 1977127 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1925295 ms - 1977127 ms\n\nContent: for service work as well as AI. Based pet adoption systems that are more geared towards the shelter side to help them find clients. So we are aiming to kind of use this existing work, but make it more helpful for prospective pet owners. We identified some pain points, including difficulty determining good fit for a lifestyle. Oftentimes people will try and search for a dog or a pet that kind of meets their needs, but they'll give up halfway through because of difficulty determining best fit for what they need, difficulty identifying local shelters, and difficulty initiating this correspondence process. So there's often a lot of friction when it comes to the shelter adoption process. So we aim to build an agent that will prompt users for their ideal dog type and make it simple for them to share all of their lifestyle needs. Our agent will accept a variety of pet owner preferences and identify shelters in the local area with pets matching that criteria, and then instruct and support", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1925295_ms_-_1977127_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1925295, "end_ms": 1977127}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 1966459 ms - 2026909 ms", "content": "Title: CS224V Lecture 9 > Transcript > 1966459 ms - 2026909 ms\n\nContent: to share all of their lifestyle needs. Our agent will accept a variety of pet owner preferences and identify shelters in the local area with pets matching that criteria, and then instruct and support users to contact shelters of interest. Awesome. So let's go ahead and talk a little bit about the agent workflow. So over here is a diagram showcasing our overall agent workflow. So first off, the user will submit a request to find a suitable dog breed. And so this will, this can take in a number of parameters, including their ideal size for a dog, whether the ideal active level of the dog, whether the dog is hypoallergenic, things like that. And once the agent is able to gather all of that user input, then the agent will then query what we call our dog information subtask. And this is going to call the dog API, which provides a number of information on several different dog breeds. And then from there the agent will then inform and confirm with the user. So, so it'll report to the user", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_1966459_ms_-_2026909_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 1966459, "end_ms": 2026909}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2015381 ms - 2082361 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2015381 ms - 2082361 ms\n\nContent: to call the dog API, which provides a number of information on several different dog breeds. And then from there the agent will then inform and confirm with the user. So, so it'll report to the user what dog breeds match their ideal preferences and then confirm whether the user wants to then search for dogs that are adoptable within their area. And in order to do that, it'll call the Adopt a Pet API, which finds adoptable dogs and pets given a zip code along with dog breeds, sex, things like that. And then once the user says yes to that confirmation, the agents will then query adoption search in order to find dog breeds within the user's area. And from there it'll inform the user with a list of different adoption postings for dogs that could be their perfect match. So over here, over here is just a little bit more detail into the different sub tasks for our agent, just like we talked about. And we plan to make calls to a number of different, different API endpoints in order to", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2015381_ms_-_2082361_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2015381, "end_ms": 2082361}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2072773 ms - 2128887 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2072773 ms - 2128887 ms\n\nContent: here is just a little bit more detail into the different sub tasks for our agent, just like we talked about. And we plan to make calls to a number of different, different API endpoints in order to retrieve the information. And over here is a little bit of a breakdown into our technical details. So right now we're still planning on right now we're still debating on whether we want to use genie worksheet or lane chain but both of those are wonderful resources in order to build multi step agentic workflows. And then we also plan to use Python in order to actually make the calls to different API endpoints and build our different functions. Um, and then over here is a brief timeline for our project. So week five we're going to spend implementing basic get functionality and making testing out different calls to different API endpoints. For example, building functionality so that the agents can get information given a dog breed. And then week six we're going to expand and polish the", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2072773_ms_-_2128887_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2072773, "end_ms": 2128887}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2118111 ms - 2183617 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2118111 ms - 2183617 ms\n\nContent: out different calls to different API endpoints. For example, building functionality so that the agents can get information given a dog breed. And then week six we're going to expand and polish the functionality for each of the different subtasks. Weeks 7 through 8 we're going to work on prompt engineering the agents so that it understands understands what each of the functions does and can overall like build the overall agentic workflow. And then weeks nine and ten we plan to test and evaluate our agent and ready it for the demo. But yeah, thank you. Any questions? I have two questions. So first of all for you guys, query, have you considered any voice from the docs? So this is one thing and the secondary now, Now I know one of my friends, he's a CEO for it's called training and also pet GBT and his wife is one of the PL of Stanford. You can reach out to him. So they create a company doing this for all kind of animals including dogs and cats. But you know, I feel like you guys can", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2118111_ms_-_2183617_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2118111, "end_ms": 2183617}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2171457 ms - 2242487 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2171457 ms - 2242487 ms\n\nContent: GBT and his wife is one of the PL of Stanford. You can reach out to him. So they create a company doing this for all kind of animals including dogs and cats. But you know, I feel like you guys can also open to you know, cats or other dogs, you know, other animals you can see because they try to mimic how you know, these animals speak. Yeah, yeah, I think definitely some future expansion points would be adding more possible pets. So like the Adopt a Pet API, actually that website has adoption postings for cats, birds, like all types of pets. So I think total like expanding this agent to include other pets would be really great. Sorry, what was your first question? First question, the behavior, all kinds, maybe the video type, how this dog walk. I think that would definitely be a really cool expansion point. Right now we are planning on possibly showing images to the user of the dog, but I think having videos or like voice recordings of the dog could also be super cool to include in the", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2171457_ms_-_2242487_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2171457, "end_ms": 2242487}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2229853 ms - 2308527 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2229853 ms - 2308527 ms\n\nContent: expansion point. Right now we are planning on possibly showing images to the user of the dog, but I think having videos or like voice recordings of the dog could also be super cool to include in the agents. I think we can just move on to the next one and take questions later. Okay. Hi, I'm Johnny and the students here. So the problem that we're solving here is every year $300 billion of money actually goes into insurance fraud and that's actually 1.5% of the US GDP. Right. And this results in $900 that you pay every year extra for your insurance premium. And why is that the case? So this insurance fraud is increasing right now? Well, the adjusters who you call when you crash your car, right. They are bombarded with all the messages and emails, but they have to detect whether there's fraud potentially going on in their claims. And so the motivation that we see here is that first, the huge volume of claims these adjusters have to handle, at the same time, they are expected to process", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2229853_ms_-_2308527_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2229853, "end_ms": 2308527}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2297487 ms - 2362535 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2297487 ms - 2362535 ms\n\nContent: potentially going on in their claims. And so the motivation that we see here is that first, the huge volume of claims these adjusters have to handle, at the same time, they are expected to process these claims as fast as possible. And at the same time, these new technology, like deep fake generation, text generation, you can create police report or medical record very, very quickly. Right. And so we see a huge increase in frauds. So the question that we are answering here with our project is could we develop a conversational agents that can prompt, help the adjusters ask better questions to detect fraud early on? And so the idea is that we will have an AI copilot that will be listening in your conversation with the client and the adjuster and suggest questions that you should be asking to the, to the client. Right. So this could be questions that maybe the adjusters didn't think about digging deeper into the case and then at the same time at the end it would detect the fraud signal.", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2297487_ms_-_2362535_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2297487, "end_ms": 2362535}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2352035 ms - 2413263 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2352035 ms - 2413263 ms\n\nContent: the, to the client. Right. So this could be questions that maybe the adjusters didn't think about digging deeper into the case and then at the same time at the end it would detect the fraud signal. So system that we are designing here is we're going to have a text, a voice to text model that'll be transcribed and we will have a model language model that'll be doing chain of thoughts and real time chain of thoughts conversational trunking as well to process. Maybe there are some inconsistency in the response from the client. At the same time we have some databases that we can extract from to retrieve the fraud that's more relevant to this current situation. And at the end display the inconsistency and suggest questions to the users in real time to the adjusters in real time. So for example, there are datasets that we're using and then we'll be comparing our method without the AI copilot to see, okay, how many more frauds could we potentially detect in the same case? Right. Or maybe are", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2352035_ms_-_2413263_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2352035, "end_ms": 2413263}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2399631 ms - 2490169 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2399631 ms - 2490169 ms\n\nContent: there are datasets that we're using and then we'll be comparing our method without the AI copilot to see, okay, how many more frauds could we potentially detect in the same case? Right. Or maybe are there certain questions that are asked by the AI that are accepted by the gesture? And so this is kind of like what we're imagining. You know, this is a transcript and on the right side there will be more of the questions that should be suggested and some of the potential issues could be flagged. And at the end, hopefully we can have like a kind of a summary of the fraud cases and then the summary of the information from clients in general. Yep, that's it. Thank you. Hi there. So our project comes from a research project we're doing with Professor Yang in the CS department and Professor Stupicar who's in the psych department. And what they're looking at is across linguistic investigation of speech patterns in schizophrenia. So the research question more specifically that we're looking at", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2399631_ms_-_2490169_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2399631, "end_ms": 2490169}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2477953 ms - 2538415 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2477953 ms - 2538415 ms\n\nContent: who's in the psych department. And what they're looking at is across linguistic investigation of speech patterns in schizophrenia. So the research question more specifically that we're looking at is how can we use language patterns in multilingual speech data to identify psychiatric conditions like schizophrenia? So schizophrenia is a pretty big psychiatric disorder that affects 1% of the global population. But despite this, there's no clear or standard diagnostic procedure that researchers can turn to when making a diagnosis. So one avenue of diagnosis that researchers have looked into is language disturbances and language patterns. So there's been some preliminary research that shows that language disturbances, such as patterns in speech data or patterns in the way, or it's more so patterns in the way that you think that are represented through speech, but these disturbances can be early indicators for schizophrenia diagnosis. But these are rarely used. And with the, I guess, advent", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2477953_ms_-_2538415_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2477953, "end_ms": 2538415}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2527299 ms - 2587513 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2527299 ms - 2587513 ms\n\nContent: in the way that you think that are represented through speech, but these disturbances can be early indicators for schizophrenia diagnosis. But these are rarely used. And with the, I guess, advent of large language models in recent years, it's shown promise in extracting signal from raw textual data. So we were curious as to how LLMs could improve diagnostic accuracy, especially at an early stage for schizophrenia induced language disturbances. So our methodology is we're using LLMs in two different ways. So first we're working on getting access from for real doctor patient interview transcripts from Discourse in Psychosis, which is a large mental health research institution. But for the time being, we're also evaluating LLMs ability to generate realistic synthetic data, kind of playing in this like patient doctor role where we have two LLMs converse with each other with different system prompts. We're also looking at known feature extraction methods from previous research studies that", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2527299_ms_-_2587513_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2527299, "end_ms": 2587513}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2576129 ms - 2634887 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2576129 ms - 2634887 ms\n\nContent: this like patient doctor role where we have two LLMs converse with each other with different system prompts. We're also looking at known feature extraction methods from previous research studies that have used NLP methods for schizophrenia diagnosis. So we're going to use these as features to evaluate some baseline models. So we're going to train some baseline NLP models, like just regular classifiers and bert, and evaluate them against our LLM based approaches for both English and Spanish interview transcripts. So our proposed experiments, we have a few different experiments, like Justin just said. The first one's evaluating traditional NLP methods and how LLMs can improve off them. The second, like you're saying, was looking at LLM's performance on being able to extract signals from the raw speech transcripts versus the feature engineered datasets. And then the third one, which I think is the most important and the main reason that Professor Subicar and Professor Yang are looking", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2576129_ms_-_2634887_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2576129, "end_ms": 2634887}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2624591 ms - 2694241 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2624591 ms - 2694241 ms\n\nContent: raw speech transcripts versus the feature engineered datasets. And then the third one, which I think is the most important and the main reason that Professor Subicar and Professor Yang are looking into this, is identifying the gaps between English versus Spanish patient transcripts. So to do this, we're looking into using some multilingual LLMs to see how different ways that you can translate or take Spanish transcripts and predict and detect schizophrenia from those, too. A few different ways that they mentioned were first you translate it to English and then use the traditional English methods, because those have been more popularized right now. But now with the rise of LLMs, kind of seeing how these multilingual LLMs can go directly from the Spanish transcripts to a prediction. So this is kind of just our expected timeline, but it's kind of a lot of what we went through, so. Thank you. A pattern of it. Yeah. So an example could be, like, a disorganized thought. So as you're", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2624591_ms_-_2694241_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2624591, "end_ms": 2694241}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2674465 ms - 2736357 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2674465 ms - 2736357 ms\n\nContent: So this is kind of just our expected timeline, but it's kind of a lot of what we went through, so. Thank you. A pattern of it. Yeah. So an example could be, like, a disorganized thought. So as you're speaking, like, you're, like, you start talking about one. Like, you start going kind of like I'm doing right now, honestly, you start going down one route, and then you, like, just randomly translate to, like, something else. If I'm talking about, like, this project right now, and then I start talking about, like, some random sports game just now, like, that's like disorganized thought and being able to detect, like, kind of how you're progressing through. Because that's a symptom for many other disasters, diseases as well. Yeah, yeah, I think it's there. There's like a. They. The two professors gave us a list of a lot of different models that can. We can use as features, and I think that's what we're using as a baseline. But then that's also the point of using LLMs to see if they can", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2674465_ms_-_2736357_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2674465, "end_ms": 2736357}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2728533 ms - 2810453 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2728533 ms - 2810453 ms\n\nContent: gave us a list of a lot of different models that can. We can use as features, and I think that's what we're using as a baseline. But then that's also the point of using LLMs to see if they can take the raw speech transcript and, like, find, like, any, like, new. Newer ones. Do the doctors know? Do the doctors. Yeah, I mean, if you talk to an experienced doctor. Yeah. So right now they can detect it later on, super late. So if someone comes in with schizophrenia, they can detect it pretty later in the stage. And so right now I think they're trying to see if LLMs can help with more early detection and kind of push up the timeline and how early they can detect it, because they're trying to see if the LLMs can do better than right now people can. But they're trying to see if LLMs can find these patterns since it can do it more like mathematically find these patterns earlier to detect them. All right. Hey, everyone. I'm Praneet and this is my partner Fred. And we're going to be talking", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2728533_ms_-_2810453_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2728533, "end_ms": 2810453}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2774785 ms - 2853923 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2774785 ms - 2853923 ms\n\nContent: these patterns since it can do it more like mathematically find these patterns earlier to detect them. All right. Hey, everyone. I'm Praneet and this is my partner Fred. And we're going to be talking about a research assistant for handwritten lab notes. So basically, the motivation for this project, it kind of stems from, like, science experiments in physical sciences, primarily like we're focusing on chemistry, but they use handwritten notes when they're performing these experiments in like clean labs and stuff. And like a lot of critical information is recorded in these notes. But then given that it's handwritten, a lot of it is kind of lost to just file cabinets and all that stuff when people don't have the time to go search through like handwritten stuff and instead focus on digital notes. So what we're trying to do here is basically kind of allow people to start interacting more with handwritten notes that they have from these experiments and gain more insight from them. So", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2774785_ms_-_2853923_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2774785, "end_ms": 2853923}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2843113 ms - 2905829 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2843113 ms - 2905829 ms\n\nContent: notes. So what we're trying to do here is basically kind of allow people to start interacting more with handwritten notes that they have from these experiments and gain more insight from them. So there's a platform called Researchable AI which basically enables people to upload handwritten notes and it effectively digitizes them. Like it produces like a digital text string output from your handwritten notes. But then there's now more demand for basically being able to have more interactions with that digitized corpus of notes. In particular, people want to be able to kind of get new experiment ideas or just like search through what they've done previously for whatever experiments that they're working on. So with the just like standard LLM or large, yeah, large language models, today you can see an interaction we have with GPT4. And essentially what you see in the beginning, the left image, is that it kind of struggles to first even just parse the notes from these handwritten", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2843113_ms_-_2905829_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2843113, "end_ms": 2905829}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2893645 ms - 2950163 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2893645 ms - 2950163 ms\n\nContent: you can see an interaction we have with GPT4. And essentially what you see in the beginning, the left image, is that it kind of struggles to first even just parse the notes from these handwritten notebooks. So in this case it's missing the reactants, I think, in the, in the notes that are provided. And then as a consequence of this, when you try to ask it to generate new ideas for your experiments, like extensions and all that stuff, it kind of focuses on the reactants or just basically input that it's missing. It can't have context on that. And so in this case it's focusing on reagents. But we wanted to be able to actually talk about the end product, which is the critical part of this experiment. So given that information, the key questions that we have are basically, can we bridge the gap between the physical medium, like pen and paper and the digital medium to spur innovation for physical scientists? And then also can we leverage like an existing knowledge corpus such as ARXIV or", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2893645_ms_-_2950163_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2893645, "end_ms": 2950163}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2939259 ms - 2990983 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2939259 ms - 2990983 ms\n\nContent: gap between the physical medium, like pen and paper and the digital medium to spur innovation for physical scientists? And then also can we leverage like an existing knowledge corpus such as ARXIV or chem archive, to basically take this next step of saying, you know, given this, these experiments that you've done suggest some new extensions to that experiment for researcher efficiency. So this is kind of a rundown of the key pieces that we want to implement for our project. And the starting point is going to be the digitized lab notes. We're not going to start with the handwritten side because that's kind of what Researchable is already supporting right now. So we'll start with these digitized lab notes that are uploaded and then basically start a conversation with the user, ask for any additional experiment details that the user thinks are critical for this particular experiment. And then together with the notes, the digitized notes and the input that the user provides, we kind of", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2939259_ms_-_2990983_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2939259, "end_ms": 2990983}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 2980047 ms - 3035447 ms", "content": "Title: CS224V Lecture 9 > Transcript > 2980047 ms - 3035447 ms\n\nContent: additional experiment details that the user thinks are critical for this particular experiment. And then together with the notes, the digitized notes and the input that the user provides, we kind of digitize, we kind of parse all this stuff, text formulas, and if we can, we'll try to parse images as well. And essentially we want to generate like a structured experiment summary of what the user has been working on. Probably going to be using ChemDFM, which is like a large language model that has been fine tuned on chemistry textbooks and chemistry knowledge in general. And then following that, we're going to take this like summary and do kind of a section wise comparison against these pre processed corpus of notes that we're going to pull from Chem Archive and use that comparison to basically refine the reference set that we're pulling these generated experiment suggestions from. Following that, we're going to break down the references that we filter down to. We're going to break them", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_2980047_ms_-_3035447_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 2980047, "end_ms": 3035447}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3026447 ms - 3079775 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3026447 ms - 3079775 ms\n\nContent: refine the reference set that we're pulling these generated experiment suggestions from. Following that, we're going to break down the references that we filter down to. We're going to break them into a set of a list of methodologies and kind of similar to how the fact checking that we saw in class earlier kind of goes element wise through each claim. Similarly, we're going to go element wise through each of the methods that are provided and look for key similarities or differences between what's taken from the references and what's provided from the user's experiment notes. And then suggest from that. From that we're going to be able to suggest experiment extensions to the user and ask them to provide any additional input for refinement so that the next iteration that we run through will be able to get stuff that's more relevant to the user for their experiment. And then last piece is just the evaluation step. So we kind of do a little bit of both automated evaluation and then more", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3026447_ms_-_3079775_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3026447, "end_ms": 3079775}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3066221 ms - 3122581 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3066221 ms - 3122581 ms\n\nContent: be able to get stuff that's more relevant to the user for their experiment. And then last piece is just the evaluation step. So we kind of do a little bit of both automated evaluation and then more focus on manual evaluation. So for automated stuff we're going to be doing, one aspect is going to be comparing like the embedding similarity between the output experiment methods and the input experiment data as a measure of like the relevance of the suggested experiments to the provided user input. And then the second piece is essential prediction experiment to see if we can use another LLM as a judge of like the output relevance to the user's experiment too. And then the second part, which is going to be probably more of our focus is the manual evaluation. We're working with 10 like chemistry PhD students and graduates for essentially a qualitative evaluation of what the system provides to see if it's actually relevant to what the users, the experiment that the user is actually working", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3066221_ms_-_3122581_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3066221, "end_ms": 3122581}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3112660 ms - 3193805 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3112660 ms - 3193805 ms\n\nContent: PhD students and graduates for essentially a qualitative evaluation of what the system provides to see if it's actually relevant to what the users, the experiment that the user is actually working with. And I think that's, yeah, that's, that's it for us. Any questions? Hey everyone. Good morning, Paul and Elijah. And today we'll be presenting our project called Lumina, which is an AI agent for college application essay brainstorming. Yeah, for most students, brainstorming their essays is the hardest part of the college application, hardest part of the college application process. It's something that they have no experience in. It's entirely different than any essays they've been trained to write in their English classes. And most students have, are really lost on what colleges are looking for and even where to begin. I know everyone here in this room has, knows the pain of this process to at least some degree. And also it's one where there's a really big guidance gap where well", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3112660_ms_-_3193805_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3112660, "end_ms": 3193805}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3185565 ms - 3240113 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3185565 ms - 3240113 ms\n\nContent: for and even where to begin. I know everyone here in this room has, knows the pain of this process to at least some degree. And also it's one where there's a really big guidance gap where well resourced students have in some cases tons of like tens of thousands of dollars in private counselors thrown at them while other students have just no support at all. So with this motivation in mind, some of the key questions that we really want to address for our agent are how can AI agents help students brainstorm these college application essay topics that they otherwise might not think of naturally? And also how can we leverage interactions between AI agents and students to sort of generate these ideas and get them out of students brains and onto paper. And thirdly, we want to address how can we design an interface that sufficiently allows for student introspection without rushing this idea generation process. So what we came up with was an approach that's both conversational but dynamic and", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3185565_ms_-_3240113_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3185565, "end_ms": 3240113}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3227055 ms - 3282327 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3227055 ms - 3282327 ms\n\nContent: design an interface that sufficiently allows for student introspection without rushing this idea generation process. So what we came up with was an approach that's both conversational but dynamic and non linear, where the AI agent helps the user put together an outline where they'll ask a few questions that's in the darker maroon color there. And the student can respond to the questions in outline format in any order. They can just respond to the questions that they want to answer. And then Lumina will ask follow up questions and sub bullets to ask them to elaborate more and to provide further details or to steer them in a, in a brainstorming direction that'll be most applicable for, for college application essays. And as for our proposed workflow, we would first start off by having some initial questions that would prompt a user to sort of reflect on some of their previous experiences to sort of generate an initial conversation. And from there we would produce that outline that", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3227055_ms_-_3282327_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3227055, "end_ms": 3282327}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3272975 ms - 3319061 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3272975 ms - 3319061 ms\n\nContent: some initial questions that would prompt a user to sort of reflect on some of their previous experiences to sort of generate an initial conversation. And from there we would produce that outline that Elijah mentioned earlier in which students have the opportunity to fill in these questions concurrently that help them explore these different experiences. Experiences and meaningful topics that they probably experienced during high school. And from there, we would be using RAG and our AI agent to sort of generate additional questions that are more specific to the themes that students have reflected in their answers. And from there, this process would basically continue in a conversational manner up until the user prompts a system to sort of stop answering questions once they feel that they've answered enough. And upon that, we would initiate a final analysis in which we would have two agents, which one is an admissions officer and the other is an essay consultant to look through these", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3272975_ms_-_3319061_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3272975, "end_ms": 3319061}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3309793 ms - 3370207 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3309793 ms - 3370207 ms\n\nContent: answered enough. And upon that, we would initiate a final analysis in which we would have two agents, which one is an admissions officer and the other is an essay consultant to look through these themes, look through these responses, and come up with potential essay topics that a student could generally write about that would be successful in the college admissions process. And from there we would have a summarizing agent that generates this final output to provide these ideas and topics to the student. As for data sets, this is a place where there's a lot of really great free resources out there. There's lots of awesome lists of brainstorming questions and guides out there on the Internet that we were going to use as inspiration and to help put together a RAG database that the model can draw on, for example, helpful questions to ask the user. Additionally, there's good large public datasets on information about colleges, such as their size, cost, average SAT ACT scores, and that's", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3309793_ms_-_3370207_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3309793, "end_ms": 3370207}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3356289 ms - 3409097 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3356289 ms - 3409097 ms\n\nContent: draw on, for example, helpful questions to ask the user. Additionally, there's good large public datasets on information about colleges, such as their size, cost, average SAT ACT scores, and that's something that we're thinking about incorporating as well if the user has a list of specific schools that they're that they're most interested in. In addition to this, some of the metrics that we really want to keep in mind when we're evaluating our model are things like response relevance. So seeing how our questions prompt meaningful interactions and how these interactions really add to the context that we develop for each of our students as this conversation continues. In addition to that, we really want to take a further look into user engagement. So taking a further look into the willingness to continue these conversations and seeing how users respond to a number of questions. And in addition to this, we also want to take a look to time to insight. So generally measuring how quickly", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3356289_ms_-_3409097_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3356289, "end_ms": 3409097}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3398985 ms - 3484307 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3398985 ms - 3484307 ms\n\nContent: to continue these conversations and seeing how users respond to a number of questions. And in addition to this, we also want to take a look to time to insight. So generally measuring how quickly users arrive at meaningful topics and meaningful ideas for their application essays and as well as idea satisfaction. So taking a look at how satisfied students are with the topics that we generated for them for them to explore in their writing for college admissions process. And with that, that's all we have prepared. Thank you so much, everyone. Thanks. Hi, I'm Daniel. Hi, I'm Alan. So we're doing Travel Advisor. So the idea for this comes from the fact that a lot of times people are making travel plans or planning trips and they want to get information that they can't get from just going to a hotel website or like an airline. So the best source of that type of information is from other people and the discussions and insights that other people who have been there have had. So some of the", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3398985_ms_-_3484307_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3398985, "end_ms": 3484307}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3474077 ms - 3544289 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3474077 ms - 3544289 ms\n\nContent: a hotel website or like an airline. So the best source of that type of information is from other people and the discussions and insights that other people who have been there have had. So some of the common ones you guys might be familiar with would be like Reddit or FlyerTalk or TripAdvisor. And so we want to create an LLM based agent that can take information from these sites and offer people advice. Yeah, so for our system overview, we have four main steps that we want to follow. So first is the data collection process. Like we're thinking of scraping hotel related content from platforms like Reddit and flyertalk and potentially like individual hotel websites for the data processing part, like cleaning and structuring the data using NLP techniques. Finally, like using LLMs to provide more personalized recommendations based on the data that we have. And finally for the user interaction part, building a front end UI where the user can ask questions through a conversation format and", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3474077_ms_-_3544289_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3474077, "end_ms": 3544289}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3528909 ms - 3599661 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3528909 ms - 3599661 ms\n\nContent: more personalized recommendations based on the data that we have. And finally for the user interaction part, building a front end UI where the user can ask questions through a conversation format and receive personalized answers. Our plan for this project is to divide it into a few different parts. The first thing we're going to do is try to map anytime a user asks for some information, map it to a part of our database that's specific to that hotel or that destination. And then secondly, we want to make sure that the LLMs are giving people personalized advice. So we're going to store and remember the user's past conversations specifically for that destination so that they can continue the conversation from there. We're going to experiment with different prompts. So we're currently planning to use the OpenAI API for the LLM. And so we're going to experiment with different prompts to make sure that it's giving specific and accurate feedback from our database. And then some other", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3528909_ms_-_3599661_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3528909, "end_ms": 3599661}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3586893 ms - 3660969 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3586893 ms - 3660969 ms\n\nContent: planning to use the OpenAI API for the LLM. And so we're going to experiment with different prompts to make sure that it's giving specific and accurate feedback from our database. And then some other challenges that we anticipate. One is data cleanliness. So a lot of these forums will have a lot of people going back and forth arguing or talking about sort of unrelated things that we don't want coming up in our TripAdvisor. So filtering for that and figuring out what we actually want the end user to see is going to be a challenge. Yeah, so this is an example of what the UI could look like. It's more like a messaging interface where you can kind of chat with the agent directly in almost like a message messages kind of platform. And then yes, you can see here the responses is very detailed and it covers a wide range of different areas of consideration like price or the surrounding environments, et cetera. And yeah, you can ask follow ups to receive more detailed explanations for a", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3586893_ms_-_3660969_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3586893, "end_ms": 3660969}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3645225 ms - 3750919 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3645225 ms - 3750919 ms\n\nContent: and it covers a wide range of different areas of consideration like price or the surrounding environments, et cetera. And yeah, you can ask follow ups to receive more detailed explanations for a specific area that you're interested in. Yeah, yeah. So kind of went over this earlier, but the three main areas of challenge we expect would be eliminating noise from the data set, making sure that we're talking to the users based on their past experiences so that they can get exactly what they're looking for. And then also we didn't previously mention, but trying to reduce hallucination, which is another thing that hopefully prompting the LLM correctly can help eliminate. Thanks. Hi, I'm Canoe. I'm Yanis and we're going to present our project on conversational LLMs assisting for optimizing job applications. So the motivation of this project is that job applications are pretty overwhelming. They require resumes, cover letters, but also non traditional application materials like personal", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3645225_ms_-_3750919_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3645225, "end_ms": 3750919}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3736899 ms - 3804815 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3736899 ms - 3804815 ms\n\nContent: job applications. So the motivation of this project is that job applications are pretty overwhelming. They require resumes, cover letters, but also non traditional application materials like personal websites, portfolios, LinkedIn and GitHub profiles. And in today's competitive job market, creating these profiles is quite intensive and difficult for some people. And existing tools in AI and LLM agents don't fully address the needs for personalization integration of like the materials as far as like job applications go, and neither do they offer sufficient guidance. So the key questions that we wanted to answer are what are the limitations of current job application tools and how can we use LLMs to address those gaps through automation and customization? The second point that we wanted to talk about was how can conversational LLM agents be leveraged to create personalized portfolio websites, GitHub profiles, et cetera in order to enhance one's application? We did some research into", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3736899_ms_-_3804815_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3736899, "end_ms": 3804815}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3790281 ms - 3855597 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3790281 ms - 3855597 ms\n\nContent: to talk about was how can conversational LLM agents be leveraged to create personalized portfolio websites, GitHub profiles, et cetera in order to enhance one's application? We did some research into what prior work existed in this domain. We found there was a paper called Resume Flow that kind of, that was published this year that uses an agentic workflow to automate the resume and cover letter process creation. But those again aren't really extendable towards like, like the non traditional job application materials like websites, GitHub profiles, et cetera. There's also a few different startups that work in this space, but again they're not uniquely personal, nor are they, you know, can they be applied to a wide range of like application materials outside of the traditional resume and cover letter. There's also a few different APIs and services that we found online that we want to, you know, make use of in our project. So yeah. So our solution is to build an LLM powered assistant", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3790281_ms_-_3855597_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3790281, "end_ms": 3855597}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3841869 ms - 3897215 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3841869 ms - 3897215 ms\n\nContent: cover letter. There's also a few different APIs and services that we found online that we want to, you know, make use of in our project. So yeah. So our solution is to build an LLM powered assistant that will guide through different step of building your non traditional application materials. The first one is the personal website creation to be able from your LinkedIn or your CV that you input to be able to create automatically an original personal website to Also optimize your LinkedIn profile with some recommendations of keywords that you should use and also the same thing for your GitHub profile. We'll use several evaluation metrics. The first one is job alignment. You can for example, input a job that you want and it will adapt to that job given the keywords that are on that job description. And we'll give you personalized recommendation. Also the content preservation, because we want to preserve the content that you have on your Resume, on your LinkedIn and not be hallucinating", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3841869_ms_-_3897215_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3841869, "end_ms": 3897215}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3886967 ms - 3985063 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3886967 ms - 3985063 ms\n\nContent: And we'll give you personalized recommendation. Also the content preservation, because we want to preserve the content that you have on your Resume, on your LinkedIn and not be hallucinating on this. And we also evaluate it with human evaluation. So that's our timeline in the next few weeks. We're already week five. It's going to go very fast, but we want to have a first version ready by next week, by the end of next week and then evaluate this first version. If this is successful, we'll continue adding some more basic features like the one mentioned before in the paper before to have our agent be as complete as possible. And then we'll evaluate it again. Thank you for your feedback in advance. It. You betcha. Yeah. Hello everyone. So my name is Stephen and this is Kevin and we're building out Sidekick and intelligent calendar automation with LLM agents. Oh, we can move that one. Yeah. Oh, there. Oh no, not yet. We already know what we look like. Okay. Anyways, so show fans who uses", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3886967_ms_-_3985063_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3886967, "end_ms": 3985063}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 3961653 ms - 4037411 ms", "content": "Title: CS224V Lecture 9 > Transcript > 3961653 ms - 4037411 ms\n\nContent: out Sidekick and intelligent calendar automation with LLM agents. Oh, we can move that one. Yeah. Oh, there. Oh no, not yet. We already know what we look like. Okay. Anyways, so show fans who uses calendar systems. Wow. Everyone's lives are basically on the calendars. However, I think one of the biggest issues that a lot of people get into getting into the calendars is the initial barrier of setting everything up. And so something that I wish I could do is just have it so I could find times to put all of my study sessions, like to have a big like 5 hour long study session and break them up into several hour long chunks and add them into my calendar because it's pretty, pretty busy at the moment. Yeah. And so. And so. Yeah. And so for this, for this app we'd basically have a few different features. One is an automatic event calendar creation which you saw in like the previous gif where it would just take up a task and then it'd be able to break it up based on like, like which when you", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_3961653_ms_-_4037411_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 3961653, "end_ms": 4037411}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4026921 ms - 4081839 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4026921 ms - 4081839 ms\n\nContent: features. One is an automatic event calendar creation which you saw in like the previous gif where it would just take up a task and then it'd be able to break it up based on like, like which when you would need to do that task. And then you could also just converse with the agent as well in order to give it more details and anything of that sort and it would just automatically populate your Google Calendar for you. And so this would hopefully save a lot of time and also just make studying and like, like balancing out your schedule and your calendar and setting it up way easier as well. So next is the technical details. Yeah, one of the things I really hope to do this. Calendar applications is just being able to take a picture of my schedule at the beginning of the year in Access and then it automatically will add everything as reoccurring events in my calendar. So the back end server we're planning on writing in Python with fastAPI and that's going to connect with Google Calendar.", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4026921_ms_-_4081839_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4026921, "end_ms": 4081839}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4071911 ms - 4132639 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4071911 ms - 4132639 ms\n\nContent: it automatically will add everything as reoccurring events in my calendar. So the back end server we're planning on writing in Python with fastAPI and that's going to connect with Google Calendar. Google Tasks, the front end, we're not sure yet either. If we want it to be more of a graphical approach we will make our own. But we could also have this as a tool that like potentially like a custom GPT could use and that it could use that as a tool calling agent. And then for the Agentic framework, we're honestly not sure which is the best one to use because there's so many on the market right now. But OpenAI just released one last week called Swarm. Langgraph is the most technically advanced one and then this crew AI and Autogen is also out there too. So for our timeline we already set up the integration for the backend to we're already able to write tasks to gcal and that was a whole a lot of like authentication with person's calendar. So we're hopefully going to create a REST API and", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4071911_ms_-_4132639_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4071911, "end_ms": 4132639}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4119823 ms - 4189973 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4119823 ms - 4189973 ms\n\nContent: integration for the backend to we're already able to write tasks to gcal and that was a whole a lot of like authentication with person's calendar. So we're hopefully going to create a REST API and sort of be able to end to end create events like be able to post LLM post events and then in the upcoming weeks we'll set up the agent. We also want it to be able to handle ambiguity so if we don't give it enough information it will ask rather than just assuming that it has the information and messing with our calendars. And then we want to be able to have the agent be able to find some free time on our calendars and be able to schedule events accordingly and then just refinement and testing bug fixes and our demo, that's that timeline. And so yeah, thank you for listening. Anyone have any thoughts or questions or suggestions for what you'd like? Well, yeah, that is us. I feel like there's a couple projects out there like the Notion Calendar that does similar stuff like is there a way for", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4119823_ms_-_4189973_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4119823, "end_ms": 4189973}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4168018 ms - 4261093 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4168018 ms - 4261093 ms\n\nContent: or questions or suggestions for what you'd like? Well, yeah, that is us. I feel like there's a couple projects out there like the Notion Calendar that does similar stuff like is there a way for you to narrow it down or apply it to a specific case? Yeah, so this is one of the issues that I was facing was that I really want something like I didn't want to make this because I wish I could just find something. I'm lazy. I want to find something that works. But the problem is like a notion to do is Pomadon, Microsoft Tasks and all these apps are mostly targeted towards enterprise users. And so I feel like the feature set is either way too overwhelming or also not catered towards individual college students, especially when you got to the ones that sort of plan everything yourself. Motion is definitely catered towards professionals. So just, I feel like the other thing we're going to focus on is ease of use is, number one, want it to be as seamless as possible. Anything else? All right.", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4168018_ms_-_4261093_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4168018, "end_ms": 4261093}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4227993 ms - 4307967 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4227993 ms - 4307967 ms\n\nContent: definitely catered towards professionals. So just, I feel like the other thing we're going to focus on is ease of use is, number one, want it to be as seamless as possible. Anything else? All right. Hello. Hello. Okay. Yeah. So our project is going to be on rule induction learning for multistage language model programs. So essentially, instead of picking a single task, we have a couple tasks we're going to optimize, and we have a method that we want to pitch to you all. So we're going to present this example sort of mo Monica motivated this example in class where let's say you have some sort of LLM agent that's trying to do PhD applications selection, and it learns. Yeah, you can click from these applications some rules about what makes it possible to accept or reject an application. And this isn't actually what we're going to be doing. This is just an example of something that you potentially want to learn rules for. So there might be some rules, like if the applicant has no", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4227993_ms_-_4307967_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4227993, "end_ms": 4307967}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4298087 ms - 4355401 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4298087 ms - 4355401 ms\n\nContent: And this isn't actually what we're going to be doing. This is just an example of something that you potentially want to learn rules for. So there might be some rules, like if the applicant has no publications and they did their undergrad in the United States, then it learns that the person who's doing this process would normally want to reject. You can click to the next slide. So many of these complex tasks will have simplified underlying structures and heuristics that can be learned to simplify the task over time as you interact with more data. And current paradigms for interacting with LLMs don't really support this. Well. So, for example, if we wanted to do few shot examples where we put like an application and the decision in context just to show those two rules that we wrote before, that would take at least four demos. And this scales in O of 2 to the nth power. So just putting all of these as few shot demos doesn't really make a lot of sense. And then handwritten prompts, you", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4298087_ms_-_4355401_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4298087, "end_ms": 4355401}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4343929 ms - 4396645 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4343929 ms - 4396645 ms\n\nContent: that would take at least four demos. And this scales in O of 2 to the nth power. So just putting all of these as few shot demos doesn't really make a lot of sense. And then handwritten prompts, you could put all of these in, but you yourself might not know all of the heuristic rules that your task needs. So for some personalized things, you may have some ideas, but for some tasks, it would be best if you could actually learn what these underlying rules are. So we can go to the next slide and here's an example of, you know, like how this data would look like. You would see, for example, some natural language input of, you know, like an applicant saying, you know, what their motivations are, where they come from. And then this is what an LLM would say. We can see here, like, in some cases, the LLM matches the label. Some cases it doesn't. And that's what we're trying to hear, correct. And what our proposal is, you know, how can we infer some of these decision tree features that allow", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4343929_ms_-_4396645_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4343929, "end_ms": 4396645}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4386143 ms - 4443711 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4386143 ms - 4443711 ms\n\nContent: the LLM matches the label. Some cases it doesn't. And that's what we're trying to hear, correct. And what our proposal is, you know, how can we infer some of these decision tree features that allow people to come to the right decisions? And right now it's a very manual process. You'd have to figure out what are the right data points, what are the right relevant features. We would try to use an induction optimizer here. And then once the LLM has given features, it'll now fill in those features with the relevant information it gets from the input and then that's just how the LLM will learn from it and then it will apply that knowledge and eventually get to the label. So how will we actually learn from these features that the LLM proposes? So we have two methods that we are going to experiment with and the first one is this decision tree style prompting. So essentially if we have a classification task such as this binary class of classification task of accept and reject, or we have an N", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4386143_ms_-_4443711_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4386143, "end_ms": 4443711}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4431455 ms - 4487437 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4431455 ms - 4487437 ms\n\nContent: with and the first one is this decision tree style prompting. So essentially if we have a classification task such as this binary class of classification task of accept and reject, or we have an N way classification task with something like we're trying to decide between five options to classify something, what we can do is propose a lot of these features and then actually learn a decision tree on the LLM proposed features such that we can figure out which questions need to be asked to derive the final answer. So essentially these first two steps are what we showed. Propose the features and label the features with an LLM and then we'll learn which features are predictive using a decision tree and then convert that decision tree into different prompts that can be given to the language model so that it can navigate to the correct answer. And so this is like an inference time solution and we will implement this as a module in dspy, like that decision tree or something like that, which is", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4431455_ms_-_4487437_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4431455, "end_ms": 4487437}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4475863 ms - 4527415 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4475863 ms - 4527415 ms\n\nContent: so that it can navigate to the correct answer. And so this is like an inference time solution and we will implement this as a module in dspy, like that decision tree or something like that, which is actually something that we sort of independently thought of, but also found this paper that does something quite similar. So if you go to the next slide, it comes up with these different questions to ask for specifically for classification tasks. And this is like a tweet emotion classifying thing, but it comes up with these questions and builds this decision tree. So this particular thing isn't entirely novel. What we would be doing is extending it to multistage programs and benchmarking against our other method which ARNAV will now present. Yep. And the next method is now revising the set of rule based instructions we provide. So 1 through 4 is the similar steps that Michael already described. The fifth step is where we find the optimal instructions. And the core idea behind this is very", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4475863_ms_-_4527415_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4475863, "end_ms": 4527415}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4518631 ms - 4565433 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4518631 ms - 4565433 ms\n\nContent: rule based instructions we provide. So 1 through 4 is the similar steps that Michael already described. The fifth step is where we find the optimal instructions. And the core idea behind this is very similar to some of the DSPY optimizers from before essentially you take a base language input where your language model has some instruction on what to do and then it runs through some high quality candidate trials to see like what is the most optimal instruction. And all of this happens by some kind of proposal generation, instruction optimization. And then you end up with this thing we call DSPY rio, which is an optimized version of these rules that were induced from your input. And here's an example of what that kind of looks like. You start off with the very baseline input of given the student's application. Think step by step to produce a decision to accept or reject the student very baseline input to the language model. Once it has gone through the process of learning the rules and", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4518631_ms_-_4565433_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4518631, "end_ms": 4565433}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4556735 ms - 4609093 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4556735 ms - 4609093 ms\n\nContent: application. Think step by step to produce a decision to accept or reject the student very baseline input to the language model. Once it has gone through the process of learning the rules and it's now optimized that instruction, you now get a more high quality set of inputs to give to the language model. And all we wanted to say here is that we're showing this example which is a single stage thing of given an application, you know, decide accept or reject. But in reality these can be multistage systems that have dozens of language model calls. And what we'll do to actually optimize those is bootstrap output labels. Basically if it succeeds at the whole task, we'll say that the intermediate successes must have been good too. And then we can induce these rules on those. And the key questions we want to answer are how can we design optimizers for language model programs that effectively induce rules and learn how to apply them to specific tasks? What strategies can we use to propose and", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4556735_ms_-_4609093_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4556735, "end_ms": 4609093}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4598813 ms - 4651431 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4598813 ms - 4651431 ms\n\nContent: we want to answer are how can we design optimizers for language model programs that effectively induce rules and learn how to apply them to specific tasks? What strategies can we use to propose and test these hypotheses for optimizing the language model programs? What type of tasks will benefit from these induced rules and when Is it better to use this inference time algorithm for rule following the decision tree method versus learning better instructions at compile time where we will just put them into your prompt. Now get into some of the tasks. So the first task we'll do is personal identifiable information reduction. So this is just, you know, in healthcare data you have a lot of components that you like to leave private. So we would use this rule based induction optimizer to identify that relevant information and then have it removed from its original input. A similar task is many way intent taxonomy where you have over 50 to 100 classes in your domains and how you can build high", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4598813_ms_-_4651431_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4598813, "end_ms": 4651431}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4641015 ms - 4691367 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4641015 ms - 4691367 ms\n\nContent: that relevant information and then have it removed from its original input. A similar task is many way intent taxonomy where you have over 50 to 100 classes in your domains and how you can build high quality classification tasks solved with language models with the pipelines of these rule induction optimizers. Lastly, this one's a fun one. It's called Agent Adventures with alfworld where the concept is your agent has a wide action space of things to do. And we would use a language Model pipeline to induce the right things to do essentially, and that would be through a set of rules and guidelines induced by the pipeline. Evaluation here would be starting off with very baseline evaluations. You know, how would a basic language model perform with, you know, no sets of rules, just in general the basic inputs and outputs. From there we would do the optimization techniques, a few shot learning, instruction optimization, maybe even some fine tuning and really taking the rule based optimizer", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4641015_ms_-_4691367_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4641015, "end_ms": 4691367}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4682055 ms - 4759471 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4682055 ms - 4759471 ms\n\nContent: the basic inputs and outputs. From there we would do the optimization techniques, a few shot learning, instruction optimization, maybe even some fine tuning and really taking the rule based optimizer to the next level in terms of, you know, how those rules are defined and if we can get higher quality sets of rules. And lastly we'll do some scale performance analysis to see which LLMs perform better than others. Yeah, that's everything. Hi everyone, I'm Akshana. I'll try to keep this quick. Our project is essentially adaptive conversational Language Learning Assistant for English learners. So the TLDR is we're trying to make a personalized English tutor. The motivation for this project is there are 1.5 billion English language learners globally and a big problem for them is often getting real world conversation practice and getting tailored feedback. A lot of existing solutions like you might think of Duolingo, are like vocab and grammar first and they aren't conversational. So you", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4682055_ms_-_4759471_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4682055, "end_ms": 4759471}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4748063 ms - 4811319 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4748063 ms - 4811319 ms\n\nContent: real world conversation practice and getting tailored feedback. A lot of existing solutions like you might think of Duolingo, are like vocab and grammar first and they aren't conversational. So you have people that kind of know a lot of like words in English but aren't great at speaking in like a real world conversation. And so, you know, there's kind of existing solutions with like ChatGPT, but some problems that they have is they tend to be robotic. You know, the way people speak is not often grammatically correct. Like if you ask someone how they're doing, they'll say I'm doing good instead of I'm doing well. And if you've like talked to ChatGPT, you know that that's not how most people talk. So our project is to design an application that simulates a personal English tutor. And so we kind of view this in a few stages. So one is giving users a personalized weekly lesson plan which each day kind of has like a short conversation. This lesson plan will be defined on user inputted", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4748063_ms_-_4811319_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4748063, "end_ms": 4811319}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4797269 ms - 4861885 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4797269 ms - 4861885 ms\n\nContent: we kind of view this in a few stages. So one is giving users a personalized weekly lesson plan which each day kind of has like a short conversation. This lesson plan will be defined on user inputted goals, their interests, their motivations, as well as their kind of prior conversations. So we can scale the like intensity of the language that the assistant will use. Then there'll also be a real time conversation component where we'll use like speech to text and text to speech to simulate a voice conversation. And lastly we'll kind of give some post conversation feedback so that users can improve on their conversations. In terms of prior work, there's been a bunch of analyses in terms of using LLMs for educational purposes and the benefits that that can have especially on language acquisition. And we found a paper by AL Safari et al. Which developed a teaching assistant for a college course. This was a little bit different than our use case because a teaching assistant is generalized", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4797269_ms_-_4861885_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4797269, "end_ms": 4861885}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4849845 ms - 4952505 ms", "content": "Title: CS224V Lecture 9 > Transcript > 4849845 ms - 4952505 ms\n\nContent: And we found a paper by AL Safari et al. Which developed a teaching assistant for a college course. This was a little bit different than our use case because a teaching assistant is generalized whereas we're trying to do more of a personal thing. But the architecture can kind of follow. We plan to implement like develop a custom corpus of lessons use RAG as well as a custom memory module to determine lesson generations. For conversations, we'll use speech to text in text to speech with a prompt engineered LLM. And finally we'll use like LLMs as judges to give feedback based off of like phrasing fluency and pronunciation. And for evals we plan to use human evaluators by finding some ESL learners with ChatGPT as a baseline. And our deliverable at the end of the quarter will be a working application encompassing all of this. Thank you. So we I want to finish on time. There are several videos and they post it. So I like to make this homework for everybody. Go home and look at the videos.", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4849845_ms_-_4952505_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4849845, "end_ms": 4952505}}
{"document_title": "CS224V Lecture 9", "section_title": "CS224V Lecture 9 > Transcript > 4906393 ms - 5296205 ms", "content": "encompassing all of this. Thank you. So we I want to finish on time. There are several videos and they post it. So I like to make this homework for everybody. Go home and look at the videos. There's only a few minutes each in for these videos and put your comments in. And I think that have getting comments from the class is very helpful and also I like to have the class be exposed to all the ideas being that are being worked on. Okay, so that wraps up the presentations and I will see you on Wednesday and we will go we'll have more lectures. I think that some of these lectures will be very helpful to a lot of the people here. Okay, perfect time it it it.", "block_metadata": {"id": "CS224V_Lecture_9_>_Transcript_>_4906393_ms_-_5296205_ms", "document_type": "transcript", "lecture_number": 9, "start_ms": 4906393, "end_ms": 5296205}}
