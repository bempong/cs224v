{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Tutorial", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Tutorial\n\nContent: 15 presentations. And 15 next Monday. There are people who did not manage to sign up. We will probably have to get them to send us videos. So we have a very exciting day.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Tutorial", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 98495, "end_ms": 121845}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > How to fact-check misinformation on YouTube", "content": "Title: CS224V Lecture 8 > Chapter Summaries > How to fact-check misinformation on YouTube\n\nContent: The Truth Sleuth AI aims to tackle the misinformation on YouTube. The agent will analyze the video, audio and comments for a holistic understanding. The second part will be extracting and verifying the key claims made in the video. The third aspect will be generating a balanced discussion to encourage critical thinking.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_How_to_fact-check_misinformation_on_YouTube", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 122345, "end_ms": 337045}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Cognitive Agents for Kids", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Cognitive Agents for Kids\n\nContent: And we're designing cognitive agents for kids. Key question is how can we build cognitive architecture that enables agents to effectively learn about and also engage with kids. Future applications could use such a cognitive architecture to develop interactive learning games.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Cognitive_Agents_for_Kids", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 371605, "end_ms": 736845}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > How To Train a Chatbot to Solve Difficult Conversations", "content": "Title: CS224V Lecture 8 > Chapter Summaries > How To Train a Chatbot to Solve Difficult Conversations\n\nContent: Loki is going to mimic human conversations and emotional responses to help users practice difficult conversations. Users will get a feedback report on what they did well and what they could improve on when they have this real life conversation. Our end goal is to help people with having difficult conversations in the real world by simulating them through a chatbot in a safe and low cost environment.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_How_To_Train_a_Chatbot_to_Solve_Difficult_Conversations", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 756865, "end_ms": 990199}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > A policy agent with econometrics", "content": "Title: CS224V Lecture 8 > Chapter Summaries > A policy agent with econometrics\n\nContent: We want to help policymakers create better policies by pegging into the evidence of previous empirical research on what works in policy. The solution we propose is a policy agent that is able to execute three main capabilities. And finally it's able to generate some advice.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_A_policy_agent_with_econometrics", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 990327, "end_ms": 1227239}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Quantum Economics for Policy Makers", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Quantum Economics for Policy Makers\n\nContent: For lots of economic policy, they don't have any optimal solution. What we want to do is to give a nuanced perspective to the policymaker. We want to point out what were the factors that were behind the success of certain policy in a country.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Quantum_Economics_for_Policy_Makers", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 1227367, "end_ms": 1334395}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Can we reliably Monitor the Skill Acquisitions of AI Agents?", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Can we reliably Monitor the Skill Acquisitions of AI Agents?\n\nContent: Instead of building AI agents, we are going to be focusing on whether or not we can reliably monitor the skill acquisitions of these agents as they get stronger and stronger. The question we want to ask is that can we reliably detect and monitor the capability and skill acquisition of potentially superhuman AI intelligence?", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Can_we_reliably_Monitor_the_Skill_Acquisitions_of_AI_Agents?", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 1360985, "end_ms": 1523749}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Acquired capabilities in the agent negotiation", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Acquired capabilities in the agent negotiation\n\nContent: We ground our experiment in the agent negotiation task, specifically the multi issue bargaining task. To simulate the acquired capabilities, we allow a few tool or function callings to be used for the agent. These acquired capabilities include non cognitive, non cognitive ones and cognitive ones. To test whether a weaker model could monitor such emerging capabilities.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Acquired_capabilities_in_the_agent_negotiation", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 1523917, "end_ms": 1808035}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Beyond Fine Dime: A Platform to Navigate Diets", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Beyond Fine Dime: A Platform to Navigate Diets\n\nContent: In the US over 60% of households have a dietary restrictions. Fine Dime will gather user's preferences and dietary restrictions and then go through restaurant and dish information to find compatible restaurants. If anybody has dietary restrictions, shoot us a text.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Beyond_Fine_Dime:_A_Platform_to_Navigate_Diets", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 1845025, "end_ms": 2021535}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Easily Legal: A Translation Tool for Spanish Speaking Lawyers", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Easily Legal: A Translation Tool for Spanish Speaking Lawyers\n\nContent: Our project is called Easily Legal. Our target audience is, you know, Spanish speakers predominantly. Currently, only 13.7% of the US population speak Spanish. Our objective is to develop a practical, deployable tool that enables clear comprehension of legal documents for Spanish speakers.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Easily_Legal:_A_Translation_Tool_for_Spanish_Speaking_Lawyers", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 2022555, "end_ms": 2243045}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > VLM: The Virtual Art Docent", "content": "Title: CS224V Lecture 8 > Chapter Summaries > VLM: The Virtual Art Docent\n\nContent: Ollie: Our project idea is a multimodal virtual art docent. In recent years a bunch of art museums have been struggling with retaining docents. The goal is to build an app where someone can take a picture of an artwork and the model will output specific facts.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_VLM:_The_Virtual_Art_Docent", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 2248465, "end_ms": 2548595}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > How to Generate Personalized Educational Stories for Kids", "content": "Title: CS224V Lecture 8 > Chapter Summaries > How to Generate Personalized Educational Stories for Kids\n\nContent: Goodnight GPT project is around generating personalized educational stories for children. Will collect preference data through a series of short interactions and then work towards a children's data set.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_How_to_Generate_Personalized_Educational_Stories_for_Kids", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 2563345, "end_ms": 2765685}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Personalized LLM Room Designer", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Personalized LLM Room Designer\n\nContent: Personalized LLM Room Designer. Can we build such an agent using the power of LLM? And second, can we let the user to actively participate in this design process so that they can provide feedbacks at any time.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Personalized_LLM_Room_Designer", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 2768505, "end_ms": 2994735}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Project Idea: Drug Price Negotiation", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Project Idea: Drug Price Negotiation\n\nContent: One in three U.S. adults on prescription drugs have unable to take this medication just because of how expensive it is. Medicare and Medicaid services recently passed a law that allows them to negotiate drug prices to lower costs. The project aims to streamline the process of identifying and analyzing candidate drugs for figuring out which ones Medicare should negotiate.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Project_Idea:_Drug_Price_Negotiation", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 3024685, "end_ms": 3247695}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > How to Build a Career Advice Chatbot", "content": "Title: CS224V Lecture 8 > Chapter Summaries > How to Build a Career Advice Chatbot\n\nContent: FindYourPath AI aims to build a chatbot that can recommend personalized career paths by utilizing unstructured structured data. The system would also utilize reinforcement learning to adapt recommendations based on how students feel the chatbot is performing.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_How_to_Build_a_Career_Advice_Chatbot", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 3288095, "end_ms": 3578465}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > System Logs", "content": "Title: CS224V Lecture 8 > Chapter Summaries > System Logs\n\nContent: The day to day monitoring and maintenance will likely take up the majority of the software lifecycle. Oftentimes the only thing available to the developers is a huge bulk of log messages sent out by different services. Our idea is to build an information retriever to parse and extract information from logs and a conversational agent to help the developer with their questions and request.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_System_Logs", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 3578545, "end_ms": 3858155}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Presentations. 1", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Presentations. 1\n\nContent: Presentations. Do we have Mateo and Nick here? Because I think the slides are not there but their name is up there. So maybe we need to squeeze a few more next the next class. Are there any questions about the topics that have been presented?", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Presentations._1", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 3871415, "end_ms": 4274975}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > Storm and GENIE Worksheet", "content": "Title: CS224V Lecture 8 > Chapter Summaries > Storm and GENIE Worksheet\n\nContent: Another comment is that we have presented two infrastructures. There is Storm and then there is GENIE worksheet. We are actually creating another framework that helps you analyze information. That's the kind of discussion that we will have in the next week or so.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_Storm_and_GENIE_Worksheet", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 4276075, "end_ms": 4350775}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Chapter Summaries > A mentor for the class", "content": "Title: CS224V Lecture 8 > Chapter Summaries > A mentor for the class\n\nContent: For people who are proposing the projects, we will give you a mentor. It will be one of the TAs from the class, and we will look at it and we'll do a match. Every week you will be writing down the weekly. You will have a meeting with a mentor every week.", "block_metadata": {"id": "CS224V_Lecture_8_>_Chapter_Summaries_>_A_mentor_for_the_class", "document_type": "chapter summary", "lecture_number": 8, "start_ms": 4351075, "end_ms": 4987695}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 98495 ms - 187845 ms", "content": "Title: CS224V Lecture 8 > Transcript > 98495 ms - 187845 ms\n\nContent: All right, let's get started. It's 3:00. I know people have to come from other classes. So we have a very exciting day. 15 presentations. Okay. And 15 next Monday. And then there are people who did not manage to sign up. We will probably have to get them to send us videos. So let's start with the first one. We are going to keep you guys on time. Everybody has four minutes, three minutes for speaking, one minute for question. And Adam will keep you honest. Two and a half minutes, you get warning. Okay, so let's start. The first one is a video, right? So we just play because there are. We called it a Truth Sleuth AI. So the problem we are trying to tackle is the misinformation on YouTube and everyone knows there's a lot of false and misleading content online. It's really easy to create a deceptive video than to actually verify it. And fact checking is not really easy. It takes time, skills and access to the reliable sources. And the studies have shown a better approach is to actually", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_98495_ms_-_187845_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 98495, "end_ms": 187845}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 172385 ms - 251937 ms", "content": "Title: CS224V Lecture 8 > Transcript > 172385 ms - 251937 ms\n\nContent: deceptive video than to actually verify it. And fact checking is not really easy. It takes time, skills and access to the reliable sources. And the studies have shown a better approach is to actually proactively fact checking the content instead of just removing it. And an approach like that would actually also foster balanced discussion while making sure freedom of expression is maintained. So what we would like to present is Tooth Sleuth AI, a sort of a multimodal debunker agent. So it will have four parts. The first part, the agent will analyze the video, audio and comments for a holistic understanding of the video. And the second part will be extracting and verifying the key claims made in the video against reliable sources or perhaps even a user defined data corpus. The third aspect will be generating a balanced discussion to encourage critical thinking. And finally is present all the information and the discussion in a user friendly interface, perhaps as some sort of a text or a", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_172385_ms_-_251937_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 172385, "end_ms": 251937}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 238215 ms - 315281 ms", "content": "Title: CS224V Lecture 8 > Transcript > 238215 ms - 315281 ms\n\nContent: be generating a balanced discussion to encourage critical thinking. And finally is present all the information and the discussion in a user friendly interface, perhaps as some sort of a text or a report or audio or a video. So the next part, how does it work? So we think we can do it in four different modules. The first module will be video processing which will leverage the YouTube API and the speech to text model from Google. The second part will be extracting the claims. So we plan on using the LLM with the prompt engineering. The third part will be fact checking the claims extracted in this step number two and we were looking, we are thinking of using the Data Commons and Google Fact Check API and perhaps some sort of feedback loop using LLM to cross validate the results. And the final part, essentially we would like to generate a healthy discussion with the framework similar to Storm and just generate the output, the discussion or audio or video in a format that's desirable for", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_238215_ms_-_315281_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 238215, "end_ms": 315281}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 296169 ms - 421511 ms", "content": "Title: CS224V Lecture 8 > Transcript > 296169 ms - 421511 ms\n\nContent: part, essentially we would like to generate a healthy discussion with the framework similar to Storm and just generate the output, the discussion or audio or video in a format that's desirable for the user and also perhaps even some plugin that will just show everything on the side. So that's all I have. Thanks for everyone's time. And we would love to get some sort of feedback on our approach and about the idea originality of the idea. Thank you. And we're designing cognitive agents for kids. So we're going to walk you through existing work that is. Oh, is this. Hello. Hello. Okay, so we're going to walk you through existing work, existing research, and then the challenges that come with the existing research and then talk through our approach and also some future work that we're considering. So we've been looking at programmed agents that can facilitate learning and creativity and self expression in kids. There are things like robots that interact with kids and researchers have", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_296169_ms_-_421511_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 296169, "end_ms": 421511}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 407015 ms - 494211 ms", "content": "Title: CS224V Lecture 8 > Transcript > 407015 ms - 494211 ms\n\nContent: So we've been looking at programmed agents that can facilitate learning and creativity and self expression in kids. There are things like robots that interact with kids and researchers have observed these interactions. And then there's also these agent based programming interfaces that teach kids computational thinking. There's also voice based systems that teach kids computational thinking as well. We found that there are some key areas of improvements from these agents and one is tone. We found that we could improve the tone at which the agents talked and interact with kids to make it more friendly. It could also promote communication for kids that have less expressive characteristics and they can do this by probing questions. There's also an area to increase engagement through resources before these interaction resources after to enhance like learning activities and enhance like the whole process. Um, so we want to aim to build this by not just an LLM input and output and also not", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_407015_ms_-_494211_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 407015, "end_ms": 494211}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 475599 ms - 554501 ms", "content": "Title: CS224V Lecture 8 > Transcript > 475599 ms - 554501 ms\n\nContent: before these interaction resources after to enhance like learning activities and enhance like the whole process. Um, so we want to aim to build this by not just an LLM input and output and also not just a language agent, but a cognitive language agent. That which is inspired by the Quala research that involves a memory architecture for retrieval and learning of like experiences and episodic memories that the agents have with kids and then going through this kind of like reasoning process to provide like hypothesis about characteristics about these kids and interact better with individual kids in their environment. So yeah, our key question is how can we build cognitive architecture that enables agents to effectively learn about and also engage with kids? Yeah. And briefly, as briefly mentioned, we aim to tackle this question by drawing a lot of inspiration from this cognitive architecture framework presented by a group of researchers at Princeton. And I think they do a good job at", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_475599_ms_-_554501_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 475599, "end_ms": 554501}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 543437 ms - 609161 ms", "content": "Title: CS224V Lecture 8 > Transcript > 543437 ms - 609161 ms\n\nContent: we aim to tackle this question by drawing a lot of inspiration from this cognitive architecture framework presented by a group of researchers at Princeton. And I think they do a good job at neatly capturing a lot of the existing ideas about agent design out there. And as you can see, there are lots of models and components like memory, reasoning, learning. And what these authors encourage us to do is to, yeah, view agent design in a more modular way and depending on the application and domain we want our agents to be in, we can tweak and design each component accordingly. And yeah, for us we want to design such a architecture for kids. So to zoom in a bit more, we hope to be implementing memory architecture such as the episodic memory to store history of interactions. And in the semantic memory space, we want to dedicate space specifically for the kids profile about like interests, personality traits, learning styles, et cetera. And for cognitive processes, we hope to design internal", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_543437_ms_-_609161_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 543437, "end_ms": 609161}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 595905 ms - 656389 ms", "content": "Title: CS224V Lecture 8 > Transcript > 595905 ms - 656389 ms\n\nContent: memory space, we want to dedicate space specifically for the kids profile about like interests, personality traits, learning styles, et cetera. And for cognitive processes, we hope to design internal actions like retrieval and learning where we infer new understandings about the kid and use this to reason about external actions and how the agent will actually interact with the world and kid. And we foresee a lot of downstream applications, designers and developers could use such a cognitive architecture to develop interactive learning games, agent based programming interface, collaborative family games or interfaces to guide reflection, go setting, storytelling, et cetera. And yeah, our expected demo would be an agent driven conversational system powered by our cognitive architecture designed to learn about a child's personality and interests. And for evaluation. We draw a lot of inspiration from Jun Sung Park's very famous generative agents paper where we will run an ablation study", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_595905_ms_-_656389_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 595905, "end_ms": 656389}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 645109 ms - 718903 ms", "content": "Title: CS224V Lecture 8 > Transcript > 645109 ms - 718903 ms\n\nContent: to learn about a child's personality and interests. And for evaluation. We draw a lot of inspiration from Jun Sung Park's very famous generative agents paper where we will run an ablation study to compare the performances between the agent with the full architecture or the agent without the kid profile or without the learning process. And finally a human author as like a baseline or point of reference as well. And yeah, we'll just interview these different agents and configurations to see which one performs best, ask human evaluators to rank the quality and calculate like a true skill rating for these. Yeah, so as for future work, we plan to look at everything else that is not semantic and episodic memory related. This includes also unintended consequences in the more like external actions area where people will be using this architecture to create interfaces that actually interact with kids. So these interactions definitely with generative. Yeah, okay, thank you. I don't think we", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_645109_ms_-_718903_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 645109, "end_ms": 718903}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 697531 ms - 788987 ms", "content": "Title: CS224V Lecture 8 > Transcript > 697531 ms - 788987 ms\n\nContent: area where people will be using this architecture to create interfaces that actually interact with kids. So these interactions definitely with generative. Yeah, okay, thank you. I don't think we have time for questions for this presentation but just for like future presenters, I'll just give like a 3, 0 just in terms of like you have 30 seconds left, so just make sure to look at me. If you're running low, I'll make a sound. Yeah, I'm just like screaming I think. Okay, great. Hello, my name is Elizabeth Holm. My partner Dylan De has a last minute interview so he couldn't make it, but we are going to be working on Loki, which simulates difficult conversations. The key questions that we have are how can we provide reliable, accessible information that helps people build and maintain healthy relationships on a bigger scale or sorry, on a smaller scale, can an emotionally responsive AI chatbot simulate tough conversations to promote emotional resilience and relationship building? Now our", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_697531_ms_-_788987_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 697531, "end_ms": 788987}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 777355 ms - 839319 ms", "content": "Title: CS224V Lecture 8 > Transcript > 777355 ms - 839319 ms\n\nContent: relationships on a bigger scale or sorry, on a smaller scale, can an emotionally responsive AI chatbot simulate tough conversations to promote emotional resilience and relationship building? Now our motivation for this is that a lot of people struggle with initiating and carrying through difficult interpersonal conversations which includes resolving conflict, setting boundaries, or even giving feedback at work. And access to therapy is limited, especially from marginalized communities. So we are aiming to develop a chatbot that simulates these conversations with different emotional tones and allows the users to practice in a safe and affordable space before taking those skills into the real world. Our goal is to help users improve their confidence and interpret personal skills. As a quick overview, Loki is going to mimic human conversations and emotional responses to help users practice difficult conversations such as setting boundaries and conflict resolution. As an overview on how", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_777355_ms_-_839319_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 777355, "end_ms": 839319}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 826375 ms - 887861 ms", "content": "Title: CS224V Lecture 8 > Transcript > 826375 ms - 887861 ms\n\nContent: overview, Loki is going to mimic human conversations and emotional responses to help users practice difficult conversations such as setting boundaries and conflict resolution. As an overview on how the users will experience this, first they will specify a tone for the bot, such as empathetic, assertive, neutral or defensive. Or also a random tone which can help them prepare for anything. And then they will have a simulated conversation going back and forth. And at the end they will get a feedback report on what they did well and what they could improve on when they have this real life conversation. For a little bit of a technical overview. For the tone simulation, we're going to be using prompt engineering for the LLM to simulate specific tones, such as empathetic or assertive ones I mentioned before. For the user inputs, we're going to limit things down to preset options so that we can test those and make sure that that's it's a seamless user experience with a more controlled setting", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_826375_ms_-_887861_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 826375, "end_ms": 887861}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 874231 ms - 938497 ms", "content": "Title: CS224V Lecture 8 > Transcript > 874231 ms - 938497 ms\n\nContent: before. For the user inputs, we're going to limit things down to preset options so that we can test those and make sure that that's it's a seamless user experience with a more controlled setting there. And then the chatbot will hopefully engage in realistic dialogue based off of those preferences that they set. On the genie worksheet side of things, the LLM is going to evaluate user responses, like each utterance that they say, based off of a few metrics that we're going to define. A few that we have in mind are safe space, emotional charge, and like the acknowledgement of what the bot is saying. And then it'll be scored from one to five on those various categories. And at the end, it'll summarize all of the scores and give suggestions based off of that scoring system. And the way that we're going to test our bot is weekly user testing in a more casual sense, just seeing how they're interacting with it and making it a more seamless use. And then, yeah, comparing the feedback that", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_874231_ms_-_938497_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 874231, "end_ms": 938497}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 924481 ms - 986447 ms", "content": "Title: CS224V Lecture 8 > Transcript > 924481 ms - 986447 ms\n\nContent: we're going to test our bot is weekly user testing in a more casual sense, just seeing how they're interacting with it and making it a more seamless use. And then, yeah, comparing the feedback that we're, that the chatbot is getting with professional therapy advice for accuracy. And then more towards the end, we will evaluate the chatbot performance with user ratings. Yeah, based off of a few different, essentially a survey that we'll send them. How natural was the tone, how good was the feedback, et cetera. And here we have a quick timeline of how we're going to do it. This week we're Going to finalize a few of the implementation details. Week five, we want to implement the initial conversation flows. Week six, implement the end of conversation report. And then weeks seven through 10 have user testing and iterate on our, on our design and if time incorporate some sort of ui. Our end goal is to help users with having difficult conversations in the real world by simulating them through", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_924481_ms_-_986447_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 924481, "end_ms": 986447}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 972631 ms - 1061184 ms", "content": "Title: CS224V Lecture 8 > Transcript > 972631 ms - 1061184 ms\n\nContent: user testing and iterate on our, on our design and if time incorporate some sort of ui. Our end goal is to help users with having difficult conversations in the real world by simulating them through a chatbot in a safe and low cost environment. Thank you. Hello everybody. We want to help policymakers create better policies by pegging into the evidence of previous empirical research on what works in policy. So bad policies are created by these causes, right? We have corrupt politicians, in effect, institutions, but something that we can do, minimum we can do, is helping policy makers overcome cognitive biases, in other words, knowing what works and what doesn't. And if we can provide that to policy makers in a structured way, I think we believe that policies can be more effective at solving global problems. This is not something new. This has already four decades of people here in Stanford and other universities studying estimators, statistical estimators, and doing research on what", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_972631_ms_-_1061184_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 972631, "end_ms": 1061184}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1047856 ms - 1112023 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1047856 ms - 1112023 ms\n\nContent: global problems. This is not something new. This has already four decades of people here in Stanford and other universities studying estimators, statistical estimators, and doing research on what works and what doesn't in policy. And what we want to do is essentially like our two research questions, that and light. Our solution is, first of all we want an agent that is able to interpret econometrics, that is able to analyze tables. Econometrics, for those who don't know anything about it, is the combination between economics and statistics. And it's super helpful to analyze when a policy, a social policy is useful or not. More importantly, we want to know how to scale the success of a policy that work in a country that has data. And we want to tell the policymaker, hey, you can learn from this case and this is the key things that you need to have into account. Okay, so what motivated these research questions is that the process of grounding evidence based policies is very complex,", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1047856_ms_-_1112023_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1047856, "end_ms": 1112023}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1099111 ms - 1161293 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1099111 ms - 1161293 ms\n\nContent: this case and this is the key things that you need to have into account. Okay, so what motivated these research questions is that the process of grounding evidence based policies is very complex, it's time consuming and it's sometimes done by governments that don't have like enough resources, time, et cetera. So the solution we propose is a policy agent that is able to execute three main capabilities. The first one is that, as David was saying, interpret economic results which are like complex by nature. The second one is identify and highlight what are the conditions. Because all of this is in the context of experiments. So the results are very tied to the concession to the conditions of these experiments. And it's of key importance that the agent is able to identify that relationship. And finally it's able to generate some advice. And the advice is in the form of how can we scale successful policies, how can we identify the risks in these policies and how can we suggest some", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1099111_ms_-_1161293_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1099111, "end_ms": 1161293}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1146953 ms - 1206977 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1146953 ms - 1206977 ms\n\nContent: And finally it's able to generate some advice. And the advice is in the form of how can we scale successful policies, how can we identify the risks in these policies and how can we suggest some partners from previous policies. So if we jump into a more concrete expected output of how the agent will work, the first interaction would be how like the policymaker says, I want to do a policy for my county that foster inclusion. So the agent here will start like prompting the user to get more details as we were doing for example in homework two with like the hotel booking reservation. And it's like, okay, in which field do you have any target population, et cetera. And once we have like all the detailed information, the agent will go and retrieve relevant papers for this policy. After that we want to emulate Cold Storm in the sense of like the user participating in a discussion with experts. And finally, the output will not be like a Wikipedia like article, but it will be a memo which is", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1146953_ms_-_1206977_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1146953, "end_ms": 1206977}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1197369 ms - 1271221 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1197369 ms - 1271221 ms\n\nContent: want to emulate Cold Storm in the sense of like the user participating in a discussion with experts. And finally, the output will not be like a Wikipedia like article, but it will be a memo which is like a pretty standard document in this context. So yeah, that's the expected output. And yeah, thank you so much. If you have any questions. Thank you. Since I used to be an econometrician, theoretically, so I have a couple of questions. First of all, I think this is a very great idea for, you know, developing this kind of, you know, agent for policy maker. However, as we know, right in the real world, lots of policy maker, they don't encounter any specific economic situation. And also for lots of economic policy, they don't have any optimal solution. Let's say for the monetary policy, in some sense they can help the growth of economy. However, you will pay for the cost of hyperinflation to some extent. How will you guys mitigate or make a trade off between this kind of policy? Because", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1197369_ms_-_1271221_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1197369, "end_ms": 1271221}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1256911 ms - 1322543 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1256911 ms - 1322543 ms\n\nContent: sense they can help the growth of economy. However, you will pay for the cost of hyperinflation to some extent. How will you guys mitigate or make a trade off between this kind of policy? Because there's not always have a better solution. Secondly, for those like counterfactual economic situation, as you mentioned earlier, you can do some quasi experiment. However, for some countries, their policy is not always applied. For the other country, because different country, they have different situation. Have you solved all that? Yeah, yeah. Both great points. Definitely in evidence there is no evidence contradicts itself. Right. So what we want to do is to give a nuanced perspective to the policymaker. This is what it must. For example, this is what most of the research tells you that this policy works or not. That's something that we think is useful to the policymaker and also tell to the policymaker, hey, in these cases out of 10 studies, policy didn't work. Another case, you are right,", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1256911_ms_-_1322543_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1256911, "end_ms": 1322543}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1311089 ms - 1397061 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1311089 ms - 1397061 ms\n\nContent: works or not. That's something that we think is useful to the policymaker and also tell to the policymaker, hey, in these cases out of 10 studies, policy didn't work. Another case, you are right, scaling policy is super challenging. Right. But we want to point out to the policymaker what were the factors that were behind the success of certain policy in a country. Thank you so much. So hello Everyone, our project is going to be a little bit different. So instead of building AI agents, we are going to be focusing on whether or not studying whether or not we can reliably monitor the skill acquisitions of these agents as they get stronger and stronger. So A while ago, OpenAI published this paper Week to strong Generalization which asked the questions, can we reliably supervise these AI intelligence as they become stronger and potentially eventually passing human levels of intelligence? And so in today's world, the human experts are the teachers and the AI models are the students. They", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1311089_ms_-_1397061_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1311089, "end_ms": 1397061}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1386749 ms - 1447119 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1386749 ms - 1447119 ms\n\nContent: as they become stronger and potentially eventually passing human levels of intelligence? And so in today's world, the human experts are the teachers and the AI models are the students. They can in some tasks outperform an average level human being, but they can, they're still not reaching the levels of human experts. However, in the fir in the, in the future, they may get like so much more powerful such that we now face the problem that can we still supervise these agents as they become, as they are getting beyond our level of intelligence? So in their analogy, we are using kind of weaker models to supervise stronger models to stimulate and study this behavior of superalignment. And so in our vision, you know, the current world, like all the skills, domain knowledge and cognitive capabilities are explicitly taught and elicited by human beings through say, fine tuning, prompting, grounding, tool callings, et cetera. But in the possible futures, these agents may learn to autonomously", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1386749_ms_-_1447119_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1386749, "end_ms": 1447119}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1436199 ms - 1497915 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1436199 ms - 1497915 ms\n\nContent: are explicitly taught and elicited by human beings through say, fine tuning, prompting, grounding, tool callings, et cetera. But in the possible futures, these agents may learn to autonomously acquire new skills and capabilities through interacting with the environment themselves. And the question we want to ask is that can we reliably detect and monitor the capability and skill acquisitions of potentially superhuman AI intelligence? And it is important not only because we want to better understand and interpret these AI models, but more so, we would like to detect early warning signs of potential harmful behaviors and capabilities. So formally, we frame our research problem as follows. Can weaker AI models effectively detect and characterize the acquisitions of new capabilities by stronger AI agents in a dynamic interaction setting? And specifically we would like to target can weaker models identify that stronger agents have acquired new capabilities? Can weaker models determine when", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1436199_ms_-_1497915_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1436199, "end_ms": 1497915}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1484323 ms - 1560173 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1484323 ms - 1560173 ms\n\nContent: AI agents in a dynamic interaction setting? And specifically we would like to target can weaker models identify that stronger agents have acquired new capabilities? Can weaker models determine when during the interaction does these capabilities occur? Are these capabilities acquired? And finally, given varying levels of prior information, to what extent can weaker models accurately characterize the newly acquired capabilities? And for our experimental, preliminary experimental fairy more Claire is going to describe. Yeah, to answer those questions, we ground our experiment in the agent negotiation task, specifically the multi issue bargaining task. In this task settings, two agents are going to negotiate a split of items between them. Each agent have different valuation for the items and the goal for them is to maximize their own utility. To simulate the acquired capabilities, we allow a few tool or function callings to be used for the agent. Um, these acquired capabilities include", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1484323_ms_-_1560173_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1484323, "end_ms": 1560173}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1543947 ms - 1628437 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1543947 ms - 1628437 ms\n\nContent: the goal for them is to maximize their own utility. To simulate the acquired capabilities, we allow a few tool or function callings to be used for the agent. Um, these acquired capabilities include Non cognitive, non cognitive ones and cognitive ones, which we will discuss more in detail later. But for non cognitive ones it refers to the ability to interact with the meta negotiation environment and the cognitive capabilities refers to knowledge acquisition, reasoning, memory, et cetera. Um, to get a better idea of the like a more concrete scenario we will have a few items on the table. For example orange, apple and banana. For a vanilla agent it might think that oh he oh it values the apple the most, so it's going to get the apple first and then split the rest. However, for a skill augmented agent, it might realize that there is a function call called add item which can add more apples to the table. So it will take advantage of this function and then proceed with like 11 apples on the", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1543947_ms_-_1628437_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1543947, "end_ms": 1628437}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1613149 ms - 1685511 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1613149 ms - 1685511 ms\n\nContent: agent, it might realize that there is a function call called add item which can add more apples to the table. So it will take advantage of this function and then proceed with like 11 apples on the table. So just to clarify, the function calling, the capabilities we're simulating here is by manipulating the game environment by changing the available resources on the table. So in this case we only have one apple on the table and the agents are usually, you know, just negotiating on oranges and bananas. But if the agent has the capability to alter the game environment and actually add more apples to it, it can hugely like maximize its own goal without like having, without like having the previous kind of quandaries that the agents were having. So we also have a list of relevant capabilities for this multi issue bargaining, negotiation. For cognitive skills, Sorry, for non cognitive skills it includes adding and removing items, bundling items, transform each agent's values and imposing a", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1613149_ms_-_1685511_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1613149, "end_ms": 1685511}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1669231 ms - 1753159 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1669231 ms - 1753159 ms\n\nContent: for this multi issue bargaining, negotiation. For cognitive skills, Sorry, for non cognitive skills it includes adding and removing items, bundling items, transform each agent's values and imposing a time constraint on like its opposing agent. And for cognitive ones it it usually includes reasoning, which is whether the agent can draw parallels from other knowledge domains like empathy simulation, followed by a strategic planning based on the other based on the other agents perception of the items, social behaviors and other types of meta learning. And to test whether a weaker model could monitor such emerging capabilities, we have two major experimental setups. The first one is that we will give the monitor model two negotiation logs. The first one will be between two base agents and the second one will include an agent with these acquired new capabilities. And then we will ask the monitor model two type of questions. The first one is that we will. Give the previous list of", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1669231_ms_-_1753159_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1669231, "end_ms": 1753159}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1738093 ms - 1854689 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1738093 ms - 1854689 ms\n\nContent: and the second one will include an agent with these acquired new capabilities. And then we will ask the monitor model two type of questions. The first one is that we will. Give the previous list of capabilities to. The model and ask whether the model could recognize which agent is a strong agent and among the list of capabilities, which capability the stronger agent is acquiring. And the second one is that without the list, can the model accurately recognize these capabilities and the stronger agent? The second experiment setup is that if. The capability emerges in the middle of. The negotiation process, can the weaker model identify at which point of time the agent has emerged new capabilities so we've already experimented. Very preliminary experience. Yeah, okay. Yeah, sorry. So we can give you feedback offline. Okay. Okay, cool. Thank you. Cool. So we're planning to build Fine Dime, which is basically a platform to help people navigate dietary restrictions. So this is a problem", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1738093_ms_-_1854689_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1738093, "end_ms": 1854689}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1804791 ms - 1906711 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1804791 ms - 1906711 ms\n\nContent: give you feedback offline. Okay. Okay, cool. Thank you. Cool. So we're planning to build Fine Dime, which is basically a platform to help people navigate dietary restrictions. So this is a problem that's growing. In the US over 60% of households have a dietary restrictions. And this has gone up every year since 2007. And despite the fact that this is a growing issue that's gotten a lot more complex, there's still kind of a really big lack of resources to help navigate them. And so as someone with a lot of dietary restrictions, this is a problem that I encounter a lot. But oftentimes the current process is basically just like a lot of online searches to look through different restaurants, like, go through each of their menus to find compatible options. And then on top of that, oftentimes, like, calling restaurants to ask about, like, cross contamination or, like, detailed questions it might be hard to find on the menu. And I guess this was an area where we thought we saw a lot of", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1804791_ms_-_1906711_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1804791, "end_ms": 1906711}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1894115 ms - 1953473 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1894115 ms - 1953473 ms\n\nContent: like, calling restaurants to ask about, like, cross contamination or, like, detailed questions it might be hard to find on the menu. And I guess this was an area where we thought we saw a lot of potential improvement in kind of automation. And so this is all specifically when you're going out to eat to find a restaurant. But yeah, our plan implementation is a conversational interface, kind of like the what we've already seen in class to gather the user's preferences and dietary restrictions and then go through restaurant and dish information to find compatible restaurants and specific dishes from those restaurants that work with the user's needs. And then something that happens a lot is, especially if you have restrictive dietary needs, you have to call in the restaurant to make sure that it's prepared a certain way, or there's a dish you can have if they can make some kind of substitution. And so it's a very quick phone call, but it's annoying having to make six of them to figure out", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1894115_ms_-_1953473_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1894115, "end_ms": 1953473}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1944481 ms - 2002809 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1944481 ms - 2002809 ms\n\nContent: it's prepared a certain way, or there's a dish you can have if they can make some kind of substitution. And so it's a very quick phone call, but it's annoying having to make six of them to figure out who can do it. And so that's something that we're hoping to do to gather what the user needs and then automate a phone call to make sure that that's something that can or can't be satisfied at the restaurant. So the way we were planning on doing that is for gathering the restaurant and dish information, either gathering it ahead of time by, like, scraping or using Yelp and Google Maps to gather it on the fly for things like which restaurants are near you. And then for automating the phone call, we were planning on using the Real Time API. Yeah. And then the last thing we want to ask is, if anybody has dietary restrictions, shoot us a text, because we'd love to see if it's useful for you and how you do it. Thank you. I think there are a lot of people typing into the document and it is not", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1944481_ms_-_2002809_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1944481, "end_ms": 2002809}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 1986409 ms - 2074973 ms", "content": "Title: CS224V Lecture 8 > Transcript > 1986409 ms - 2074973 ms\n\nContent: has dietary restrictions, shoot us a text, because we'd love to see if it's useful for you and how you do it. Thank you. I think there are a lot of people typing into the document and it is not possible sometimes to put it in. I recommend that you put it in a different file and then we can add it at the end. If you're having problems with the WI. Fi, I think we can call it Questions, Questions. No, we can take it. Yeah, there's one question. Got a question? Hello. Our project is called Easily Legal. I'm Peter and this is Jordan. So our key question is that, well, we're looking at like existing models like ChatGPT, Claude and Gemini. They often pose challenge challenges for like non native speakers, especially like Spanish speakers, like immigrant type demographics, especially in like legal contexts like contracts, small claims court cases, stuff like that. We try to use ChatGPT to like, I mean, we've used ChatGPT multiple times to sort of help translate docs that are in Spanish, like", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_1986409_ms_-_2074973_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 1986409, "end_ms": 2074973}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2062877 ms - 2122767 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2062877 ms - 2122767 ms\n\nContent: contexts like contracts, small claims court cases, stuff like that. We try to use ChatGPT to like, I mean, we've used ChatGPT multiple times to sort of help translate docs that are in Spanish, like legal docs in Spanish to English. And a problem we keep like, that keeps occurring for these current models is that they maintain a lot of the jargon and they sort of, even when we ask to simplify to sort of like colloquial terms, it doesn't speak like an actual person. It's like the translation that we often find is not like no one really speaks like that. And it's super confusing even for like native Spanish speakers. So our question is, can we create a large language model tailored for legal doc readability and understanding? So our target audience is, you know, Spanish speakers predominantly. Yeah, immigrants who often, I mean, struggle to understand these legal documents. Currently, only 13.7% of the US population speak Spanish and creates like a significant barrier for finding help", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2062877_ms_-_2122767_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2062877, "end_ms": 2122767}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2110235 ms - 2172803 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2110235 ms - 2172803 ms\n\nContent: Yeah, immigrants who often, I mean, struggle to understand these legal documents. Currently, only 13.7% of the US population speak Spanish and creates like a significant barrier for finding help when it comes to legal matters, especially for us, like our background too. Our parents would often ask us to like, even when we were kids to translate legal docs. And we try our best, but, you know, we're not pros and even now they still ask us to and we can do a better job. But I mean, our parents shouldn't have to rely on constantly texting us, shooting us emails to translate documents. So our objective is to develop a practical, deployable tool that enables clear comprehension of legal documents for Spanish speakers of all levels. The data sources we're going to be using are just like legal doc databases, both in English and Spanish, like the Mexican Congress's like, Legal Doc database, also for Peru, Bloomberg Law and the Glenn Foundation. Just for a bunch of legal docs, we're going to be", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2110235_ms_-_2172803_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2110235, "end_ms": 2172803}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2160249 ms - 2228265 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2160249 ms - 2228265 ms\n\nContent: databases, both in English and Spanish, like the Mexican Congress's like, Legal Doc database, also for Peru, Bloomberg Law and the Glenn Foundation. Just for a bunch of legal docs, we're going to be using like current Spanish language translations of English documents and Community Driven Text so our output could be more, you know, real world colloquial, like short form Spanish and yeah, as far as like kind of a technical approach. We plan to use a lot of translation APIs as well as like pre trained models and try and focus on like fine tuning them as well as gathering more. Just a common focus were loans, house loans and immigrations and for metrics, just some basic ones a lot of people know just bleu scores to see how accurate it is at translation and rogue in order to see how well it is administration as well as the flesh Kincaid in order to see how well it's able to summarize and simplify certain legal matters. And finally our deployment would be we'd want to create like an app,", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2160249_ms_-_2228265_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2160249, "end_ms": 2228265}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2216537 ms - 2315141 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2216537 ms - 2315141 ms\n\nContent: is administration as well as the flesh Kincaid in order to see how well it's able to summarize and simplify certain legal matters. And finally our deployment would be we'd want to create like an app, sort of like a chatbot, but that is scannable and where you can upload legal docs and get like real time translations and like you can ask questions, you know, when you need like comprehensive understanding of a legal document in real time. Question. Hey, I'm Ben. I'm Ollie and we're presenting on our project idea, which is a multimodal virtual art docent. So as part of the motivation for this project, we believe that docents are like a super essential part of the art museum exhibit experience because as indicated in this image, it really allows you to actively engage with the work, have both your physical connection to the art in person, but also a connection with a person that you can ask questions so you can get the narrative of the art historical context and then also dive into the", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2216537_ms_-_2315141_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2216537, "end_ms": 2315141}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2301717 ms - 2365043 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2301717 ms - 2365043 ms\n\nContent: your physical connection to the art in person, but also a connection with a person that you can ask questions so you can get the narrative of the art historical context and then also dive into the actual visual, physical and aesthetic elements. But in recent years a bunch of art museums have really been struggling with retaining docents and volunteer workforces in general, especially with COVID And a bunch of major museums have even canceled their art docent programs which were long essential aspects of their overall program. And that includes like the Chicago Art Institute for example, which is like one of the biggest museums in the world. There are also only 9% of US high schools that have AP art history programs and a lot of those are concentrated in affluent areas. But a lot of people still have access to museums, but not people that they can interact with at those museums to get this personal connection to the art. We've also seen throughout different psych literature that visits", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2301717_ms_-_2365043_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2301717, "end_ms": 2365043}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2352761 ms - 2412347 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2352761 ms - 2412347 ms\n\nContent: still have access to museums, but not people that they can interact with at those museums to get this personal connection to the art. We've also seen throughout different psych literature that visits to art museums demonstrably improve psychological well being for visitors. So we wanted to see if there was a way for us to leverage this LLM technology to promote a really accessible interactive experience with artwork in person. So we originally just wanted to see what ChatGPT and Gemini and Google Lens could do. About this problem, but we ran into all sorts of different problems with each of these systems. ChatGPT was just hallucinating about basic facts, constantly had really surface level analysis, although with really common works it was actually pretty good at identifying things via the vlm. And it often would include kind of long tail irrelevant. Oh, that's supposed to be irrelevant, not relevant irrelevant information. While Gemini would kind of take the other approach where it'd", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2352761_ms_-_2412347_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2352761, "end_ms": 2412347}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2400971 ms - 2457387 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2400971 ms - 2457387 ms\n\nContent: vlm. And it often would include kind of long tail irrelevant. Oh, that's supposed to be irrelevant, not relevant irrelevant information. While Gemini would kind of take the other approach where it'd be really conservative about not wanting to hallucinate. So it just would like basically include no useful information in its response and just ask you to provide it yourself. Then Google Lens basically just works as pure image retrieval. So there's not really this interactive and analytic component we really want to instill. So yeah, just as a super quick example, this is kind of like canonical with what we found with ChatGPT, but it's super confident about what the name of this painting is, who it's by, when it's from, and the art style it's in. And literally all four of these are wrong. But all of this data is in this cool corpus we found, by the way, that's the artist that it thought it was. Looks totally different from this. We found a great corpus called WikiArt that this is an", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2400971_ms_-_2457387_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2400971, "end_ms": 2457387}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2446595 ms - 2506157 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2446595 ms - 2506157 ms\n\nContent: But all of this data is in this cool corpus we found, by the way, that's the artist that it thought it was. Looks totally different from this. We found a great corpus called WikiArt that this is an example from that has all this data on different art pieces, including the actual facts. So we want to start by grounding our responses in that. Yeah, so essentially the goal or vision for our project is to build an app where someone can take a picture of an artwork or paint painting, and the model, output specific facts, quick facts about the model, or sorry about the painting. And what we don't want is for the model to just output a ton of long information that is not relevant. And so we want to do a ton of prompt optimization. And we also want to explore how well the agentic framework we've been learning about in this class can generalize to multimodal input. So there's this really cool recent paper, Coli, I highly recommend checking it out that basically uses VLMs for retrieval. And so", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2446595_ms_-_2506157_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2446595, "end_ms": 2506157}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2495039 ms - 2584753 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2495039 ms - 2584753 ms\n\nContent: been learning about in this class can generalize to multimodal input. So there's this really cool recent paper, Coli, I highly recommend checking it out that basically uses VLMs for retrieval. And so what we want to do is kind of apply that framework in this setting. And here's kind of our proposed potential workflow where we do kind of a multi step information retrieval before doing the multimodal analysis. And we really want to focus on grounding our work and also using alignment with LLMs and vlms as a judge to make sure that our response is refined and grounded in the actual visual information that the user gave as input. So yeah. Any questions? Cool. Thank you. Hello everyone, I'm Aditri and this is AdVit and our project is called Goodnight GPT. Around generating personalized educational stories for children. So main questions that motivated us were can we generate engaging stories for children, collect their preference data through a series of a few short interactions and then", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2495039_ms_-_2584753_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2495039, "end_ms": 2584753}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2571649 ms - 2626897 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2571649 ms - 2626897 ms\n\nContent: stories for children. So main questions that motivated us were can we generate engaging stories for children, collect their preference data through a series of a few short interactions and then work towards a children's data set? Because we haven't really been able to find a children's preference dataset so far. So as a motivating example, a lot of the conversational virtual assistants we look at today are text based, but there's so much more you can do with text. So here's an example from John Mo Stilton. It's the exact same text, but if you add things like fonts, different colors, bold italics, and just small images here and there, it makes the text a lot more engaging, especially for younger audiences and especially want the content to be more educational. So as an additional example, I think this is Cat in the Hat. Very simple thing where it's just adding an image and then just playing around with where the text is placed can make the whole experience a lot more engaging for us.", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2571649_ms_-_2626897_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2571649, "end_ms": 2626897}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2617473 ms - 2667931 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2617473 ms - 2667931 ms\n\nContent: I think this is Cat in the Hat. Very simple thing where it's just adding an image and then just playing around with where the text is placed can make the whole experience a lot more engaging for us. It may not matter as much because we're a lot older, but for younger audiences it can make a play a key role. And some of our motivations behind doing this are just the increase in homeschooling, where in the past six years, partly due to the pandemic, it's increased by a lot. And because of that there's no personalized systems for learning about topics meant for younger audiences. So we're trying to build something. We looked at one example and it wasn't that good. Additionally, immersion, where currently all the virtual assistants are just text based or some have images, but there's so much more you can do to make the experience a lot better. And then finally just preferences where there's no child preference data set because most of these volunteers for RHF or 18, so maybe building", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2617473_ms_-_2667931_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2617473, "end_ms": 2667931}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2656915 ms - 2714819 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2656915 ms - 2714819 ms\n\nContent: more you can do to make the experience a lot better. And then finally just preferences where there's no child preference data set because most of these volunteers for RHF or 18, so maybe building towards that as well. Just to talk a little bit about the methods, we're planning to gather a few preferences through a chat based interface. So we'll enable the ability to provide a topic that the user wants to learn about, a few reference samples and styles that might be engaging to them. So we could ask for instance, do like text in this style or another style like Roald Doll versus like Cat in the Hat. And then we'll use textile unbundling to try to understand the key features of that text and then apply it to generate our story. Then once we have the story, we can apply those processing for the format. We can use that to identify where the Most impactful places to put images, generate consistent images and finally put it all together nicely into a click through animation UI along with", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2656915_ms_-_2714819_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2656915, "end_ms": 2714819}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2705179 ms - 2763761 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2705179 ms - 2763761 ms\n\nContent: the format. We can use that to identify where the Most impactful places to put images, generate consistent images and finally put it all together nicely into a click through animation UI along with potential voice narration, letting parents or users put their own voices. Then we'll breeze through the methods real quick. So we looked at some works on style to cell transfer. So we'll do that. We'll identify key things from the references and inject that into our generated context. Yeah, we can just go through and then Image consistency is pretty challenging, but there have been a lot of recent works that try to use only prompting to generate long form consistent images across 8 to 10 across a set of 8 to 10 images. So we'll play around with that prompt engineering piece too. And then for evaluation, we'll collect pairwise preferences through our UI and also try to evaluate image consistency using LLM as a judge and trying to see if the key entities represented in the story are", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2705179_ms_-_2763761_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2705179, "end_ms": 2763761}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2749553 ms - 2850955 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2749553 ms - 2850955 ms\n\nContent: for evaluation, we'll collect pairwise preferences through our UI and also try to evaluate image consistency using LLM as a judge and trying to see if the key entities represented in the story are represented in the images as well. Thank you. So hi everyone, my name is Jen and this is Jason and our project is called Personalized LLM Room Designer. So the motivation. So people nowadays need to hire professionals to design their house or their room, which takes a lot of time and money. And some people, they just prefer not to leave their home design entirely to some other people or they actually have some general idea but just don't know how to execute it. So this means that people really want personalized agent to help them with such design. And thanks to LLM we can now build such agent. So the key questions first, can we build such an agent using the power of LLM? And second, can we let the user to actively participate in this design process so that they can provide feedbacks at any", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2749553_ms_-_2850955_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2749553, "end_ms": 2850955}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2835331 ms - 2911585 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2835331 ms - 2911585 ms\n\nContent: So the key questions first, can we build such an agent using the power of LLM? And second, can we let the user to actively participate in this design process so that they can provide feedbacks at any time. So a simple example, so the user might say okay, now I want to design a bedroom. And the agent will say okay, here's the bedroom. And the user can provide some feedback like oh, I don't want a desk or a chair in the bedroom or I want it to be more old fashioned. So the agent will adjust their output according to the feedback and in the end generate some design that the user are satisfied with. Yeah, so we reviewed some of the related works and these are some of the common issues of those works. So a lot of papers, they cannot directly revise the design from user feedback. So you ask for a design and the model will generate something and then it's pretty complicated if you want to move the window to the left a little bit or something like that. And then other feature, other issues", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2835331_ms_-_2911585_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2835331, "end_ms": 2911585}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2899699 ms - 2976121 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2899699 ms - 2976121 ms\n\nContent: a design and the model will generate something and then it's pretty complicated if you want to move the window to the left a little bit or something like that. And then other feature, other issues are a lot of papers only feature one agent which is just the designer. So we think that it has very limited capabilities when designing the room. And then some earlier works have furniture overlaps and out of bound when trying to place the furniture in the room. So our current plan is to use this existing framework which builds the room from an empty room and then you place all the basic equipments and then you select all the furnitures and then place the furnitures. Then we try to add these new features to allow multi agent interaction. So we have multiple agents with different roles that will come up with a design together. And then we will add user feedback for each step so that users have more flexibility to control what rooms they will get. And then for evaluation we probably will", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2899699_ms_-_2976121_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2899699, "end_ms": 2976121}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 2957591 ms - 3061711 ms", "content": "Title: CS224V Lecture 8 > Transcript > 2957591 ms - 3061711 ms\n\nContent: will come up with a design together. And then we will add user feedback for each step so that users have more flexibility to control what rooms they will get. And then for evaluation we probably will mainly get GPT4 evaluation and very limited human evaluation for quantitative evaluation. We have this out of boundary rates which evaluates how likely is the furniture to get out of the room. Yep. And that's all. Hi everyone, I'm Arjun and this is my partner Ethan. And today we're going to talk to you about our project idea which is drug price negotiation. Bot so to give some context, you know, one in three U.S. adults on prescription drugs have unable to take this medication just because of how expensive it is. And this has been an issue for a really long time. And recently with the inflation reduction act, the Medicare and Medicaid services under they're known as CMS recently passed this law that allows them to basically negotiate drug prices to lower costs. But a big sort of issue", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_2957591_ms_-_3061711_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 2957591, "end_ms": 3061711}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3048999 ms - 3111259 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3048999 ms - 3111259 ms\n\nContent: reduction act, the Medicare and Medicaid services under they're known as CMS recently passed this law that allows them to basically negotiate drug prices to lower costs. But a big sort of issue with this process is very sort of labor intensive and time intensive to actually identify which drugs to negotiate for because there's a lot of sort of extensive criteria in terms of how much Medicare is spent on it in terms of the impact on public health in the US and so we want to basically leverage LLMs to see can we streamline this process of identifying and analyzing candidate drugs for figuring out which ones Medicare should negotiate? Yeah. And for a high level criteria, what Medicare and Medicaid look for when it comes to potential drugs for negotiation are high spending, where Medicare and Medicaid spending a lot per state where there's a usage high for those candidate drugs where spending is high, usage is high and prices have potentially been increased, is there market competition", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3048999_ms_-_3111259_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3048999, "end_ms": 3111259}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3100259 ms - 3153935 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3100259 ms - 3153935 ms\n\nContent: Medicaid spending a lot per state where there's a usage high for those candidate drugs where spending is high, usage is high and prices have potentially been increased, is there market competition that could actually lower the prices? Naturally. And then once you figured that out, how long have they been in market? Because you still want to maintain incentives for pharmaceutical producers to kind of bring these drugs into market. And then finally are these conditions like very serious, relevant and kind of standard with health trends in the US and we have corresponding data for each of these areas and we think that this structured approach lends itself to kind of a GENIE worksheet based kind of task agent that helps walk a policymaker through identifying what could be a good drug to negotiate prices on. Yeah. And so to give you some more insight into what the process looks like, you know, as Ethan mentioned, there's sort of these different sort of criteria that goes into determining", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3100259_ms_-_3153935_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3100259, "end_ms": 3153935}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3143389 ms - 3198211 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3143389 ms - 3198211 ms\n\nContent: prices on. Yeah. And so to give you some more insight into what the process looks like, you know, as Ethan mentioned, there's sort of these different sort of criteria that goes into determining which drugs should be used to be negotiated on. So for example, looking at spending, so Medicare, there's historical data on Medicare spending and how much they've spent on these drugs historically. And so looking at that, also looking at sort of trends in the price as well, is there sort of drugs that have significantly increased in price over the last couple years? And looking into that as well. And then also a big sort of specification is Medicare can't negotiate drugs price on drugs that have sort of generics or have existing competition out there. And so looking at those drugs as well, and then finally looking at sort of like health trend insights and whether sort of what is the, what health outcomes will this drug have if we're able to sort of reduce its price and what conditions does it", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3143389_ms_-_3198211_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3143389, "end_ms": 3198211}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3186683 ms - 3242347 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3186683 ms - 3242347 ms\n\nContent: finally looking at sort of like health trend insights and whether sort of what is the, what health outcomes will this drug have if we're able to sort of reduce its price and what conditions does it treat? Yeah. And for a brief walkthrough on methodology, this workflow is pretty simplified and we're going to build it out. But you can imagine a GENIE worksheet that kind of constrains the agent to draw from specific databases when queried, when queried with specific information regarding prices or spending, which have two different associated data sets. The GENIE worksheet will also trigger Python code that allows us to analyze and kind of bring up like spending charts and graphs. And then one of the interesting components of this application we think is the ability to take in PDF documents like SEC filings so you can build counter arguments from a manufacturer's perspective and then kind of go back and forth and simulate a negotiation from Medicaid and Medicare side and then the private", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3186683_ms_-_3242347_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3186683, "end_ms": 3242347}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3232661 ms - 3332659 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3232661 ms - 3332659 ms\n\nContent: SEC filings so you can build counter arguments from a manufacturer's perspective and then kind of go back and forth and simulate a negotiation from Medicaid and Medicare side and then the private company side to build out a policy brief in that regard. So yeah, thank you. Hello. How's it going everybody? My name is Carter, this is Jonathan, and we're building FindYourPath AI. So just a little bit about the motivation of this project. Specifically, career exploration during high school is often a critical motor for improving socioeconomic mobility. But the access to opportunities to do so remains unequally distributed. We pull some statistics on high school students around the US where 40% report feeling unconfident about their chosen career Post High School, 72% report feeling rarely often exposed to different career options, and 64% report having five or fewer conversations about careers with teachers or counselors. And so on top of that, traditional career guidance tools often fail", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3232661_ms_-_3332659_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3232661, "end_ms": 3332659}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3320579 ms - 3375001 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3320579 ms - 3375001 ms\n\nContent: exposed to different career options, and 64% report having five or fewer conversations about careers with teachers or counselors. And so on top of that, traditional career guidance tools often fail to adapt to students evolving interests, which further limits their ability to discover fulfilling paths. And personal mentors often drive solutions and actually offer significant improvements in career discovery, but unfortunately don't scale. A lot of students don't have access to one on one mentorship or specialized career counselors or college counselors to give them the advice to actually pursue career paths. And so it kind of begs the key question of our project. What if every high school student in the US had a personal career discovery mentor? And how do we align recommendations with a student's evolving interests, skills and life achievements over time? So our idea is to build a chatbot that can recommend personalized career paths by utilizing unstructured structured data. So", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3320579_ms_-_3375001_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3320579, "end_ms": 3375001}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3362741 ms - 3422357 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3362741 ms - 3422357 ms\n\nContent: a student's evolving interests, skills and life achievements over time? So our idea is to build a chatbot that can recommend personalized career paths by utilizing unstructured structured data. So structured data would be kind of datasets we find online with job postings, survey data and unstructured BE student LinkedIn profiles, user input on the kinds of careers that they want to explore and the skills that they want to learn, et cetera. So the idea is to use a rag model with a Siamese neural network to enable effective comparisons between student preferences and generating relevant career recommendations for them. And then as Carter said, one key thing that we want to implement in the system is the ability for it to be adaptive and change according to user preferences because career, especially for students, career interests change so much over time. And also the career landscape changes quite a bit over time as well. So the idea is I touched a little bit on data sources so I'll", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3362741_ms_-_3422357_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3362741, "end_ms": 3422357}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3411317 ms - 3469025 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3411317 ms - 3469025 ms\n\nContent: especially for students, career interests change so much over time. And also the career landscape changes quite a bit over time as well. So the idea is I touched a little bit on data sources so I'll skip over this. After some basic pre processing, the system would basically be a rag model. So it searches for relevant career paths, matching student profiles and generate suggestions. The stymies neural network would be used to map these student profiles to the most relevant paths according to sorry. By comparing and generating similarities between their preferences and career data. And then we would also utilize reinforcement learning to adapt recommendations based on how students feel the chatbot is performing and also if their preferences change longer term. So this is an example of like a basic kaggle data set that we found on LinkedIn job postings with descriptions, assigned skill sets. This would be an example of some of the unstructured data that we would use. So asking questions", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3411317_ms_-_3469025_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3411317, "end_ms": 3469025}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3456349 ms - 3517393 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3456349 ms - 3517393 ms\n\nContent: a basic kaggle data set that we found on LinkedIn job postings with descriptions, assigned skill sets. This would be an example of some of the unstructured data that we would use. So asking questions like the kind of skills that you want to develop at work, the kind of work culture that you would excel in, and also like the core values that you seek to find. This is like a very high level, just first stab idea of the architecture. So due to different types of data and knowledge base, combining it in the shared latent space of the Siamese neural network and then adding that to the RAG model and then the reinforcement learning from the user after the initial recommendation to sort of refine the system I guess really quickly going more into our system. So we derived this method from a paper talking about semantic textual similarity. So specifically Siamese neural networks would be mapping sort of the career preferences with the grounded truth in the data on job postings and what skills", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3456349_ms_-_3517393_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3456349, "end_ms": 3517393}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3505361 ms - 3558885 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3505361 ms - 3558885 ms\n\nContent: talking about semantic textual similarity. So specifically Siamese neural networks would be mapping sort of the career preferences with the grounded truth in the data on job postings and what skills are required for those careers. So we look to inject similar strategy and then in terms of the RL of our system we will be using a feedback loop where users can provide their preferences in the suggested career paths, basically rejecting or modifying the suggested options and then rating the relevance of the recommendations and in terms of sort of the result of that feedback. So the positive feedback would update the reward system affecting the likelihood of similar recommendations for users with similar preferences and then the negative feedback would update the penalty system. So re weighting certain features like deemphasizing skills that the user doesn't like. And then lastly your policy optimization so updating how it selects career paths or new queries. And then lastly we have a bit", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3505361_ms_-_3558885_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3505361, "end_ms": 3558885}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3547749 ms - 3654929 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3547749 ms - 3654929 ms\n\nContent: certain features like deemphasizing skills that the user doesn't like. And then lastly your policy optimization so updating how it selects career paths or new queries. And then lastly we have a bit naive approach with evaluation but expressing feedback so determining precision on true positives, false positives. Similarly with recall, measuring how many relevant career paths there were and then lastly an F1 score to determine the balance in precision and recall. So that's essentially our system. That's all our slides. Yeah. Any questions? It once maintain forever. It means that the system design and implementation is only the beginning. The day to day monitoring and maintenance will likely take up the majority of the software lifecycle. While trying to keep a software system healthy, a developer may care about the following. First they might want to identify the cause of software failures. A common question to ask is why did my service fail? And eventually they want to find out which", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3547749_ms_-_3654929_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3547749, "end_ms": 3654929}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3640947 ms - 3718491 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3640947 ms - 3718491 ms\n\nContent: may care about the following. First they might want to identify the cause of software failures. A common question to ask is why did my service fail? And eventually they want to find out which stage in the workflow has thrown what kind of exception that has caused the failure. And maybe also get a link to the breakpoint if possible. A lot of events such as a new deployment could happen automatically or overnight. And such events could easily be buried in the pile of log files to further debug the system. They might also want to retrieve the runtime history and get to know what happened during a specific period of time. Finally, to fix the issue, the developer may want to request an action. A very common one would be to revert the software to an older version. Now we know the developer's task, but what do they have to work with? The answer is logs. Oftentimes the only thing available to the developers is a huge bulk of log messages sent out by different services and sub modules in the", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3640947_ms_-_3718491_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3640947, "end_ms": 3718491}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3703331 ms - 3781495 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3703331 ms - 3781495 ms\n\nContent: but what do they have to work with? The answer is logs. Oftentimes the only thing available to the developers is a huge bulk of log messages sent out by different services and sub modules in the system. Unlike the errors from individual programs such as the Python script, which usually contain helpful trace backs to let you know which line, which file errored out and why, the system logs can be much more complicated. One service may report an error or warning and affects another service which throws another message, so on and so forth until something fails. Also, the system log could span thousands and thousands of lines, which is not something you can simply copy and paste to ask ChatGPT about. Even for professionals with years of experience, sifting through these log files is still a pain. As software engineers ourselves, we would like to build something that makes monitoring and debugging software systems much easier. Luckily, the logs have some nice properties that make our hope", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3703331_ms_-_3781495_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3703331, "end_ms": 3781495}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3766505 ms - 3850107 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3766505 ms - 3850107 ms\n\nContent: As software engineers ourselves, we would like to build something that makes monitoring and debugging software systems much easier. Luckily, the logs have some nice properties that make our hope quite accessible. First, the logs are usually in a standard format defined by the platform, meaning that it's unlikely to improvise something that's confusing and unexpected. Second, the logs are usually verbose and informative enough for basic triading tasks. Last but not least, the log texts are in natural language, which means that we can leverage the power of LLMs to handle them precisely. Our idea is to build an information retriever to parse and extract information from logs and a conversational agent to help the developer with their questions and request. Here is a system overview. A GENIE worksheet will be created for the interaction between the user and the LLM based conversational agent for information retrieval. We will experiment with three different solutions including direct", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3766505_ms_-_3850107_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3766505, "end_ms": 3850107}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3836419 ms - 3955119 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3836419 ms - 3955119 ms\n\nContent: worksheet will be created for the interaction between the user and the LLM based conversational agent for information retrieval. We will experiment with three different solutions including direct prompting, rag and text to SQL. That's all for our presentation. Thank you. Presentations. Do we have Mateo and Nick here? Because I think the slides are not there but their name is up there. On Monday. Anybody? No? So maybe we need to squeeze a few more next the next class. So are there any questions or discussions about the topics that have been presented? Because we wanted to speak to the schedule and we are. We actually have a few minutes now. No. Yes, I guess. But yeah I think applies to multiple things but I feel like one concern that I have about a lot of those is that it's targeting like or like I'm not sure how like evaluations would happen in the sense that like it's the ideal user would be like I imagine like either kind of hard to get hold of in like a user study or with or like", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3836419_ms_-_3955119_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3836419, "end_ms": 3955119}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3942383 ms - 4007401 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3942383 ms - 4007401 ms\n\nContent: or like I'm not sure how like evaluations would happen in the sense that like it's the ideal user would be like I imagine like either kind of hard to get hold of in like a user study or with or like for example like I've noticed like I think there are multiple products that were like about like hurting like children or like helping children. I feel like maybe sometimes the preferences of children are like not exactly good for them. So I wonder if there was there's like A little bit of consideration in terms of evaluation we had where there's like some other metric other than just which help. I think that's a very, I think that's a very good concern. And I want to point out that for a lot of the projects that we were talking about, there is related work. And we don't, you know, we are, we have a seven week schedule. And so we definitely want to encourage everybody to do the readings to find out what people have learned. And we will speculate to some extent. I mean, we will probably", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3942383_ms_-_4007401_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3942383, "end_ms": 4007401}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 3992673 ms - 4067341 ms", "content": "Title: CS224V Lecture 8 > Transcript > 3992673 ms - 4067341 ms\n\nContent: we have a seven week schedule. And so we definitely want to encourage everybody to do the readings to find out what people have learned. And we will speculate to some extent. I mean, we will probably make some wrong choices as well because we will probably have to learn more as we go. So. But definitely try to do the research for the grounding of the material. I see that some of the talks have very good grounding. You know, here are the papers that have been presented. So I think that that's the main point. So the thing I want to emphasize, and I put, I have comments, but I couldn't put it into the outline sheet because of the traffic. I think it is not, you know, you can't put them in, but we can all put them in at the end of the class. And what I think happens, what we heard today is that there are a lot of very, very well motivated topics and they are only doable just right now with the LLMs that we have. So I like the fact that they are very ambitious. They are kind of pushing the", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_3992673_ms_-_4067341_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 3992673, "end_ms": 4067341}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4049389 ms - 4127721 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4049389 ms - 4127721 ms\n\nContent: there are a lot of very, very well motivated topics and they are only doable just right now with the LLMs that we have. So I like the fact that they are very ambitious. They are kind of pushing the limit of what we can do. But I want to focus on the fact that you do have about seven weeks. You cannot finish a big project in seven weeks. But you can put up a prototype of the key things that you want to test. You are saying that these are the things that we need to do. But here, for example, we heard about the room design. Then one of the topics is the furniture overlapping. So you have a choice. You can say, I'm going to ignore it for this paper or for this project. Or you can say, I just want to work on that piece. Okay, I'm much happier if you take a problem and you identify something that you have enough time for and try to dig into what is difficult about that topic rather than put up something that kind of barely works for many, many different, different use cases or different", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4049389_ms_-_4127721_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4049389, "end_ms": 4127721}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4114405 ms - 4181488 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4114405 ms - 4181488 ms\n\nContent: that you have enough time for and try to dig into what is difficult about that topic rather than put up something that kind of barely works for many, many different, different use cases or different case studies. I think that there are. So that's what I would like to emphasize. I care more about the difficulties that you find out in the projects rather than say, oh look, if I script my input properly, it will come to a good. It will work well. The minute you move away from your script, it doesn't work anymore. So that is not as interesting as something is. Like, I try to do this and I discovered three problems. Now that is something that we want to hear. And then for each one, you just say, I have picked this one and I have gotten this far. Then one of the things that we want in the project report at the end is future work. What does it say about the future? Don't do this. I have tried it. Don't do this. Or here is really what we have discovered, okay? So keep in mind the whole", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4114405_ms_-_4181488_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4114405, "end_ms": 4181488}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4169795 ms - 4230723 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4169795 ms - 4230723 ms\n\nContent: project report at the end is future work. What does it say about the future? Don't do this. I have tried it. Don't do this. Or here is really what we have discovered, okay? So keep in mind the whole problem is to understand the problem, okay? And bring it down. And the first thing you want to do when you start your project is to just identify the easiest case and go into it. Rather than saying, oh, this is all the things I have to build. And you build horizontally all the pieces. And I say, now I'm ready to do the next step. It's like, don't do that. Okay? I have seen so many projects in the past in other projects, and not in this class necessarily, but where they say, oh, I need all these places pieces and they keep building all the infrastructure. And I say, oh, I run out of time. And then by the time you actually, if you didn't run, if you don't run out of time, you start working on it. Oh, that infrastructure is not what I need. I can't use it anyway because I'm stuck here. So in", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4169795_ms_-_4230723_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4169795, "end_ms": 4230723}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4220683 ms - 4284415 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4220683 ms - 4284415 ms\n\nContent: by the time you actually, if you didn't run, if you don't run out of time, you start working on it. Oh, that infrastructure is not what I need. I can't use it anyway because I'm stuck here. So in research, you want to go depth first. You really want to stick. You know, it's like, I think the biggest problem is this. Go work on it first. Okay? You, you create your harness. Get to the point. It says, suppose I have finished doing this. This is the part that I'm worried about. Try it out. That's what you. That's what you need to do, okay? And then you can, I think some of these projects that I have heard, you will be simplifying the project. You'll be doing a section of that project, okay? And so this is a standard. So we will help you through this process. But make sure that you try something out first, because with LLMs, you can quickly get to one round. You know, something to say, oh, this is not. This is not helpful. Another comment is that we have presented two infrastructures.", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4220683_ms_-_4284415_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4220683, "end_ms": 4284415}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4265387 ms - 4349091 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4265387 ms - 4349091 ms\n\nContent: out first, because with LLMs, you can quickly get to one round. You know, something to say, oh, this is not. This is not helpful. Another comment is that we have presented two infrastructures. There is Storm and then there is GENIE worksheet. What I heard today there are a couple of projects that are actually neither here nor there. They're kind of a mixture of these things. We are actually creating another framework that helps you analyze information. In your study, for example, Storm has all the. Is just doing search on free text. It actually doesn't bring in data from tables, which you need. And so some people were talking about using the worksheet framework because we have databases there, but we actually are putting together. So you can actually do the same thing more at the investigation level with Storm bringing those two pieces in. So we will try to make that available to. To, you know, that's the kind of discussion that we will have in the next week or so about how you go", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4265387_ms_-_4349091_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4265387, "end_ms": 4349091}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4335165 ms - 4412341 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4335165 ms - 4412341 ms\n\nContent: level with Storm bringing those two pieces in. So we will try to make that available to. To, you know, that's the kind of discussion that we will have in the next week or so about how you go about doing these things. Okay. Any other things that we can discuss, we should discuss here? Yes. Logistical level, how do we find a mentor? And. Yeah, should we just reach out or. Is there a good question? So we are only seeing a fraction of the projects because a lot of projects were derived or part of the projects that have been presented. For people who are proposing the projects, we will give you a mentor. Okay. It will be one of the TAs from the class, and we will look at it and we'll do a match. And so everybody will have a mentor for sure. Okay. Regardless of whether it is something we propose or something that you propose. Okay. And the. What we're going to do, we will explain this in more details, but every week you will be writing down the weekly. You will have a weekly report on what", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4335165_ms_-_4412341_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4335165, "end_ms": 4412341}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4396989 ms - 4459873 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4396989 ms - 4459873 ms\n\nContent: or something that you propose. Okay. And the. What we're going to do, we will explain this in more details, but every week you will be writing down the weekly. You will have a weekly report on what happens last week, and you will have a meeting with a mentor every week. So they will be helping you with whether you have questions or whether they see that you are stuck or whether they see what you're doing and say, maybe we should try something else. So you will have somebody to talk to. So the way, roughly the way it's going to work is that you get to talk to the TAs, and I'll talk to all the TAs, and I will be actually monitoring all the projects through the TAs because there are just too many projects. I cannot talk to everyone personally, but we will be doing the discussion. So I'll be aware of what everybody is doing also, and we will try to give you the help as you go. And the hardest part really is formulating the problem, by the way. Okay. Because we are not trying to build a", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4396989_ms_-_4459873_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4396989, "end_ms": 4459873}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4447599 ms - 4516897 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4447599 ms - 4516897 ms\n\nContent: aware of what everybody is doing also, and we will try to give you the help as you go. And the hardest part really is formulating the problem, by the way. Okay. Because we are not trying to build a product in seven weeks. And the projects are not. Are not exactly like fully. It's not all complete for sure. So we have to kind of navigate to make the most out of that time. Out of the time you have. What is really neat that we have found is that because you have a time limit, a lot of times you actually get to the issues faster because you are building the first prototype, you can get by without doing all the pieces that you will need in the product. But by doing so getting a first prototype up, you can actually learn faster. Okay. So this is kind. So don't feel bad to say, oh, I want to do this, but I can't. The whole point really is to put a time limit on it. You do the exploration and you will probably learn the most for the, you know, in that period. And then at some point you say,", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4447599_ms_-_4516897_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4447599, "end_ms": 4516897}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4504161 ms - 4577391 ms", "content": "Title: CS224V Lecture 8 > Transcript > 4504161 ms - 4577391 ms\n\nContent: this, but I can't. The whole point really is to put a time limit on it. You do the exploration and you will probably learn the most for the, you know, in that period. And then at some point you say, okay, this is what I want to do, and you can take the time to build it out. Okay. Any other questions? Nope. So the. If that is the case, then we will just meet everybody on Monday. And you know, I see that there is a maybe 50% of the class here and I just want to restate what we said at the beginning is that we really expect people to come to class and we will make a post about that. And clearly, you know, we know who are, who are coming to class. And we have already discussed the fact that there is a percentage of the grade that is allocated to the class participation. And clearly we can see who is giving the feedback to the class because we all benefit from trying to come up with the feedback and also listening to other people's feedback. And that is the kind of participation that we", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4504161_ms_-_4577391_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4504161, "end_ms": 4577391}}
{"document_title": "CS224V Lecture 8", "section_title": "CS224V Lecture 8 > Transcript > 4568343 ms - 4987695 ms", "content": "who is giving the feedback to the class because we all benefit from trying to come up with the feedback and also listening to other people's feedback. And that is the kind of participation that we have been expecting. Okay. All right, I'll see you on Monday. It. It.", "block_metadata": {"id": "CS224V_Lecture_8_>_Transcript_>_4568343_ms_-_4987695_ms", "document_type": "transcript", "lecture_number": 8, "start_ms": 4568343, "end_ms": 4987695}}
