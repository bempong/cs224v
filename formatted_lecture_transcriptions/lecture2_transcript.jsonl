{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries >  Paralympics 2018", "content": "Title: CS224V Lecture 2 > Chapter Summaries >  Paralympics 2018\n\nContent: Thank you. With 30 seconds to spare. All right, we're ready to start.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>__Paralympics_2018", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 35215, "end_ms": 92155}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Cognitive Skills in the 224V lecture", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Cognitive Skills in the 224V lecture\n\nContent: This is the second lecture of the CS224V. The goal is to create systems that have cognitive skills using using LLM as a natural language processing subroutine. Today we're going to talk about the next level of the cognitive skills. What is after reading? Writing.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Cognitive_Skills_in_the_224V_lecture", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 98895, "end_ms": 236595}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Writing 101, Homework", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Writing 101, Homework\n\nContent: You will see how we put a lot of LLM faces together to create this, this process of something that humans learn how to do and that is writing. The homework is released on the website and it is due in one week's time. There are only two homeworks. You get one late day.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Writing_101,_Homework", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 237335, "end_ms": 325575}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Guest Lecture", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Guest Lecture\n\nContent: Yijia is a PhD student at Stanford NLP Group. He is giving a guest lecture on knowledge curation together with his collaborator Yu Chung and Professor Lam. They plan to talk about how we could potentially go beyond just receiving information to knowledge curating. Feel free to interrupt me if you have any questions.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Guest_Lecture", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 336915, "end_ms": 439683}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Information Retrieval in Natural Language Processing", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Information Retrieval in Natural Language Processing\n\nContent: Natural language processing Technology has greatly evolved in the past decades. One major goal of NLP still exists is we try to fulfill humans information need. The goal of retrieval system is very simple. It wants to get a ranked set of documents from the document collection.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Information_Retrieval_in_Natural_Language_Processing", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 439859, "end_ms": 540135}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > GPT4: The Problem of Knowledge Transfer", "content": "Title: CS224V Lecture 2 > Chapter Summaries > GPT4: The Problem of Knowledge Transfer\n\nContent: A major issue of when people trying to use these models for knowledge task is these models they do hallucinate. One popular paradigm is this type of retrieval augmented generation. This type of technology is already widely applied in question answering task. Are people's information needs really satisfied?", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_GPT4:_The_Problem_of_Knowledge_Transfer", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 541475, "end_ms": 1032541}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > The problem of knowledge curation", "content": "Title: CS224V Lecture 2 > Chapter Summaries > The problem of knowledge curation\n\nContent: This is a problem I met when last year when I doing the pitch. There is actually a project that is very successful. They try to do this type of knowledge curation. But even like for now, even like we have very powerful technology, this task is still pretty challenging.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_The_problem_of_knowledge_curation", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 1032733, "end_ms": 1744971}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Creating a Wikipedia-like system with Storm", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Creating a Wikipedia-like system with Storm\n\nContent: Storm is the first language model empowered system that can generate a Wikipedia like articles from scratch. You can also browse like what people are interested in learning using our tool. Storm has aroused interest across various communities. Are we sharing on Zoom?", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Creating_a_Wikipedia-like_system_with_Storm", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 1745163, "end_ms": 1967395}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > How to teach AI to generate articles with deep brevity", "content": "Title: CS224V Lecture 2 > Chapter Summaries > How to teach AI to generate articles with deep brevity\n\nContent: How we can potentially build this kind of language model in power system to generate articles with good breadth and depth. One major challenge of building this type of system is how we can decouple a task into different small steps or craft a pipeline that the language model could potentially execute.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_How_to_teach_AI_to_generate_articles_with_deep_brevity", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 1969335, "end_ms": 2550245}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Storm Live Demo", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Storm Live Demo\n\nContent: We are happy to give a live demo. Do anyone has any topic you want to give it a try? How do LLMs input images like embed images? You guys can just try it out offline. Hopefully it's not too parallel to make our server crash.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Storm_Live_Demo", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 2551825, "end_ms": 2918049}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Storm system: The whole system", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Storm system: The whole system\n\nContent: We're building this system generating the long form like articles, like Wikipedia articles and how we can actually evaluate them quantitatively. You can compare your language model generated results directly with the Wikipedia articles. And besides the final article content, what else we can evaluate in order to assess what's the breadth and depth of the article.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Storm_system:_The_whole_system", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 2918097, "end_ms": 3500577}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > How does the system grade Wikipedia articles?", "content": "Title: CS224V Lecture 2 > Chapter Summaries > How does the system grade Wikipedia articles?\n\nContent: We compare the actual content of the article with the human written Wikipedias. We use another a separate language model besides the one that generates the actual article. Using these metrics and storm consistently outperform all the baselines here just as a proxy.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_How_does_the_system_grade_Wikipedia_articles?", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 3500721, "end_ms": 4239949}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > The Storm System", "content": "Title: CS224V Lecture 2 > Chapter Summaries > The Storm System\n\nContent: There are like 75,000 people using the Storm system. Besides the automatic and human evaluation, we also conduct the in the wild evaluation. In the last part of the lecture we are going to talk about how to bring the human in the loop in the knowledge curation process.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_The_Storm_System", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 4240117, "end_ms": 5040255}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Chapter Summaries > Storm and Cold Storm", "content": "Title: CS224V Lecture 2 > Chapter Summaries > Storm and Cold Storm\n\nContent: Storm and Cold Storm will be deployed in the wild, asking thousands of people to evaluate and provide their feedback. For the assignments generally it divides into three sections. How can you analyze the result and how do they inspire you to develop next level better language model powers knowledge curation system.", "block_metadata": {"id": "CS224V_Lecture_2_>_Chapter_Summaries_>_Storm_and_Cold_Storm", "document_type": "chapter summary", "lecture_number": 2, "start_ms": 5042435, "end_ms": 5140365}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 35215 ms - 167961 ms", "content": "Title: CS224V Lecture 2 > Transcript > 35215 ms - 167961 ms\n\nContent: Thank you. Wow. With 30 seconds to spare. Thank you very much. All right, we're ready to start. All right, everybody. So this is the second lecture of the CS224V. And in the first lecture we talked about the fact that our goal here is to create systems that have cognitive skills using using LLM as a natural language processing subroutine. And in the very first class, we use the example of reading. You know, how do I read the material? And we talked about the wiki chat. So today we're going to talk about the next level of the cognitive skills. And what do you think it is? What is after reading? Writing. Thank you. That's exactly what we're going to do. There are a lot of work on question answering, conversational agent and stuff like that, but there is not a whole lot of work on writing. And we worked on that project at the beginning of the school year last year as part of this project. And today we have two special guests to give this lecture and they're the ones who actually did the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_35215_ms_-_167961_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 35215, "end_ms": 167961}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 156481 ms - 234383 ms", "content": "Title: CS224V Lecture 2 > Transcript > 156481 ms - 234383 ms\n\nContent: we worked on that project at the beginning of the school year last year as part of this project. And today we have two special guests to give this lecture and they're the ones who actually did the work. There is Yijia and Yu Cheng Zhang and they were both first year students last year at this time and they did this project for the 224V class. The first paper was on Storm was submitted at the end of the quarter and it was presented in the summer at the NACO conference in Mexico in June. And after that they continued to work on the collaborative version of Storm and it was just accepted into emnlp, which will be presented in November. So that worked out really well. And by the way, besides being papers that people can read, the software has been downloaded by a lot of people and it is online. And I was wondering why it was called Storm. Yija and friends came up with this idea and. But it seems like it is kind of taking over by Storm, taking over the Internet by storm. There's a lot of", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_156481_ms_-_234383_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 156481, "end_ms": 234383}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 221163 ms - 293063 ms", "content": "Title: CS224V Lecture 2 > Transcript > 221163 ms - 293063 ms\n\nContent: And I was wondering why it was called Storm. Yija and friends came up with this idea and. But it seems like it is kind of taking over by Storm, taking over the Internet by storm. There's a lot of people using it and it was pretty exciting. So that's what you're going to hear Today we move on to the writing we are hearing from the people who actually did the work as part of this project. And there's another reason why we have it in this lecture and that is that this is the material for the homework for the first homework. So pay attention because the homework will be based on that. You will see how we put a lot of LLM faces together to create this, this process of something that humans learn how to do and that is writing. And so with this homework you get a chance to see how you can put more sophisticated processes together using LLM. And you will Be doing this for the homework. The homework is released on the website and it is due in one week's time. And there's a question like can we", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_221163_ms_-_293063_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 221163, "end_ms": 293063}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 275235 ms - 357669 ms", "content": "Title: CS224V Lecture 2 > Transcript > 275235 ms - 357669 ms\n\nContent: sophisticated processes together using LLM. And you will Be doing this for the homework. The homework is released on the website and it is due in one week's time. And there's a question like can we get extensions? And so forth. So the policy for this class is that there are only two homeworks. You get one late day. You can apply it to the first homework or you can apply it to the second homework. You don't have to ask us any questions about that because sometimes things happen. But I do want to encourage you guys to start the project only you want to use that extra day on the second homework is what I want to get is what I guess. Okay. All right, let's get started. Let me turn this over to Yijia and thank you. Hello everyone, my name is Yijia. I'm currently a PhD student at Stanford NLP Group. Today I'm really glad to get a chance to give a guest lecture on knowledge curation together with my amazing collaborator Yu Chung and Professor Lam. So in today's lecture, I guess after. So I", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_275235_ms_-_357669_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 275235, "end_ms": 357669}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 341819 ms - 403403 ms", "content": "Title: CS224V Lecture 2 > Transcript > 341819 ms - 403403 ms\n\nContent: Group. Today I'm really glad to get a chance to give a guest lecture on knowledge curation together with my amazing collaborator Yu Chung and Professor Lam. So in today's lecture, I guess after. So I guess after the first lecture on Monday, you guys potentially already roughly get a sense like with the current advancements of large language models or foundations models in general, we can do things more beyond just optimizing the model parameter to get a better output for the single input. Instead, we could potentially use these models to build more sophisticated system and even do something more creative. So for today's lecture, we plan at the beginning to give a very quick introduction of information retrieval and retrieval augmented generation together with language model. They would be the building block of what we'll discuss later. And the major part, we plan to talk about how we could potentially go beyond just receiving information to knowledge curation and how we can craft a", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_341819_ms_-_403403_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 341819, "end_ms": 403403}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 392091 ms - 447633 ms", "content": "Title: CS224V Lecture 2 > Transcript > 392091 ms - 447633 ms\n\nContent: be the building block of what we'll discuss later. And the major part, we plan to talk about how we could potentially go beyond just receiving information to knowledge curation and how we can craft a language model empowered system to do so. For the second half of the lecture, I'll hand it over to my amazing collaborator, Yu Cheng. He will talk about how we can evaluate this type of system and some lessons we learn when working on this project in the last year. And more excitingly, he will share our recent progress on trying to bring human in the loop to really allow human user to collaborate with this type of language modeling power system or what some people call this language model agent nowadays. So during the lecture, feel free to interrupt me if you have any questions because very likely other people in the room or on Zoom, they have similar question. Okay, let's get started. So why do we want to talk about information retrieval at the beginning? This is not like a very new", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_392091_ms_-_447633_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 392091, "end_ms": 447633}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 435811 ms - 495123 ms", "content": "Title: CS224V Lecture 2 > Transcript > 435811 ms - 495123 ms\n\nContent: likely other people in the room or on Zoom, they have similar question. Okay, let's get started. So why do we want to talk about information retrieval at the beginning? This is not like a very new topic. People have been studying IR since last century. So the problem is like, even though the natural language processing Technology has greatly evolved in the past decades and even just in the past one or two years. I think one major goal of NLP still exists is we try to fulfill humans information need because there are just too much information existing in text form. You can imagine there are just so many web pages on the Internet, so many books in the library or just even in your personal computer. In the file system there are just too many files. How many times you try to find a file but just cannot find where it is. So the goal of retrieval system is very simple. So they assume the user come with a concrete query and it wants to get a ranked set of documents from the document", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_435811_ms_-_495123_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 435811, "end_ms": 495123}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 482323 ms - 543851 ms", "content": "Title: CS224V Lecture 2 > Transcript > 482323 ms - 543851 ms\n\nContent: but just cannot find where it is. So the goal of retrieval system is very simple. So they assume the user come with a concrete query and it wants to get a ranked set of documents from the document collection. So the ranking should be based on how much each document is relevant to the query or not. So you can imagine the document collection could be of any granularity. It definitely could be just the files in the file system of your computer. But it can also be something really large like the whole Internet. If it's the whole Internet, then the retriever is just the search engine which I guess everyone is familiar with and use it every day. So for example, if I give a Query say Storm GitHub, if I go to use search engine, I don't want to see web pages talking about hurricane or just some climate phenomenon. But apparently like Google search did a very good job here. Like the first ranked website is exactly the GitHub repository of our open source project. So since our topic today is", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_482323_ms_-_543851_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 482323, "end_ms": 543851}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 529731 ms - 594717 ms", "content": "Title: CS224V Lecture 2 > Transcript > 529731 ms - 594717 ms\n\nContent: climate phenomenon. But apparently like Google search did a very good job here. Like the first ranked website is exactly the GitHub repository of our open source project. So since our topic today is still about language model or building language model assistance system, why retrieval system is relevant here. So when preparing this lecture, I really want to find a failure case of GPT4. Since people always talk about these models are good at even doing every task. So I ask it a very simple question is where did I obtain my bachelor's degree? So very quickly it generates an answer and the tone is very confident. It say like okay, is obtain her bachelor degree from National Taiwan University. This unfortunately is not true. So this is like a very trivial example. Just like some trivial question about my personal experience. But it kind of review a major issue of when people trying to use these models for knowledge task is these models they do hallucinate. So a hallucination is a response", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_529731_ms_-_594717_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 529731, "end_ms": 594717}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 580263 ms - 637687 ms", "content": "Title: CS224V Lecture 2 > Transcript > 580263 ms - 637687 ms\n\nContent: about my personal experience. But it kind of review a major issue of when people trying to use these models for knowledge task is these models they do hallucinate. So a hallucination is a response that is not faithful to the facts of the world. But sometimes I really think about if we really require the model to do not hallucinate at all. It's pretty strict to the model because there are just too much long tail information existing on the world. For example, I just started my career as a PhD student. So where I did my bachelor degree is not very important. And kind of fall into the long tail distribution of the world information. And also since these models they are very very large, it's just impossible to do continual post training every day to keep them updated. However, there are new stuffs coming up daily. So there will be knowledge cutoffs of these models. And finally you can imagine there are always some private data, they are not included in the training source. Then if the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_580263_ms_-_637687_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 580263, "end_ms": 637687}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 626703 ms - 688383 ms", "content": "Title: CS224V Lecture 2 > Transcript > 626703 ms - 688383 ms\n\nContent: stuffs coming up daily. So there will be knowledge cutoffs of these models. And finally you can imagine there are always some private data, they are not included in the training source. Then if the model never seen them in training, there is no way for you to expect okay, they can accurately answer everything related to the question. So in order to mitigate this type of problem, one very popular paradigm I think in the past one or two years is this type of retrieval augmented generation. We try to combine the power of retriever system which can return you with a set of rank documents and the power of language model which can do really good generation. This type of technology is already widely applied in question answering task. For the question I just asked, you can imagine if I use the RAG based paradigm, then instead of directly use it to ask the model to generate, you should first do a round of search. Then since I have my personal website, then it's very easy for it to just", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_626703_ms_-_688383_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 626703, "end_ms": 688383}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 676143 ms - 736627 ms", "content": "Title: CS224V Lecture 2 > Transcript > 676143 ms - 736627 ms\n\nContent: the RAG based paradigm, then instead of directly use it to ask the model to generate, you should first do a round of search. Then since I have my personal website, then it's very easy for it to just retrieve a piece of information about my personal background. Then if you combine this background with the initial query given current model, they are very good at reasoning, it's pretty easy for it to obtain the final answer. So to correct the answer saying okay is obtaining her bachelor degree from Peck University, which is correct. So since we already have this kind of rag paradigm, it's pretty lucky. Like we are in the time where we are indeed having better embedding models and infrastructure for information retrieval. So you can imagine if for query you want to get a ranked set of documents, then things usually like this type of similarity, you cannot just do keyword matching, you want to have semantic similarity. So having better embedding models will help you do that. And also we", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_676143_ms_-_736627_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 676143, "end_ms": 736627}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 725875 ms - 786569 ms", "content": "Title: CS224V Lecture 2 > Transcript > 725875 ms - 786569 ms\n\nContent: then things usually like this type of similarity, you cannot just do keyword matching, you want to have semantic similarity. So having better embedding models will help you do that. And also we are seeing better and better infrastructure which allows us to not just obtain better performance but also achieve low latency at the same time. So this advancement is good since if we want to use this rag paradigm, we hope to have better retrieval system. And on the other side, I think you don't need to wait me to tell you is in the last couple of years we are seeing stronger and stronger language models coming up every couple of months. So they are giving us better power in generation and reasoning. However, in today's lecture we don't really plan to go deep into all different sorts of retrieval systems and all the history of of developing the whole family of language models. Instead we Want to think about this original matter question we ask are people's information needs really satisfied?", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_725875_ms_-_786569_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 725875, "end_ms": 786569}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 772789 ms - 834655 ms", "content": "Title: CS224V Lecture 2 > Transcript > 772789 ms - 834655 ms\n\nContent: systems and all the history of of developing the whole family of language models. Instead we Want to think about this original matter question we ask are people's information needs really satisfied? Even though we have better retrieval system, better language model and we are kind of trying to combine them to build rag for question answering. I think sometimes when I think about humans information need especially I'm doing this type of research work, I need to search a lot of time to find related work for my own research project. I feel we are not so different from our ancestors who tried to pick up some fruits in a forest. Like you can imagine a long time ago, people need to pick up fruit in the forest just to make sure they got enough energy to survive. You can imagine if we don't have any technology then if we want to look for information, the best thing we can do is maybe just wander on the Internet to do all sorts of browsing or maybe go to library to randomly pick up a book book", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_772789_ms_-_834655_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 772789, "end_ms": 834655}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 823323 ms - 883145 ms", "content": "Title: CS224V Lecture 2 > Transcript > 823323 ms - 883145 ms\n\nContent: technology then if we want to look for information, the best thing we can do is maybe just wander on the Internet to do all sorts of browsing or maybe go to library to randomly pick up a book book to see if it's something useful for my work. It's kind of similar like those people, they are just wandering in the large forest to try to see if they are lucky enough to find some edible fruits. But with this type of current retrieval system or question answering system, it's kind of we already have the map of the whole forest. So suppose I have a concrete question then instead of I need to go everywhere try to sports this thing to see whether I'm lucky enough to find the stuff. I can just use the system to quickly get the answer. So this is good. Since it saves us a lot of energy and save us time, we can quickly find what we want. However, even though we already have this type technology, the problem is the real forest may look like this. So instead it's like okay, this is part we can", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_823323_ms_-_883145_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 823323, "end_ms": 883145}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 869313 ms - 933403 ms", "content": "Title: CS224V Lecture 2 > Transcript > 869313 ms - 933403 ms\n\nContent: us time, we can quickly find what we want. However, even though we already have this type technology, the problem is the real forest may look like this. So instead it's like okay, this is part we can query. We have a map about there is a larger part which we don't even know existence. So if you think about the formulation of the retrieval system, it always assume like you start with a concrete query. And also how the retriever is evaluated is they usually check whether the top K documents returned they are relevant to your query or not. So the problem is if I don't know, okay, there are some parts I don't exactly know. I cannot come up with query to ask about them. There is no way for me to find the information about them even though those information may be very useful for my work. So when thinking about this I kind of want to share is like how I think about the whole whole landscape of knowledge processing. I think there exists there kind of a pyramid. So at the very top is what we", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_869313_ms_-_933403_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 869313, "end_ms": 933403}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 919155 ms - 980837 ms", "content": "Title: CS224V Lecture 2 > Transcript > 919155 ms - 980837 ms\n\nContent: thinking about this I kind of want to share is like how I think about the whole whole landscape of knowledge processing. I think there exists there kind of a pyramid. So at the very top is what we call like trying to access the Information is basically what information retrieval system and question answering system try to do is basically suppose we already have a concrete query. This taste system hopes to quickly help us to find what we want. But one level on top of that is what we call like knowledge curation. In what we want to discuss in today's lecture, if you go to search in Google very likely you will see some definition like this. It means to find or organize and present relevant and valuable content. So going back to this bear picking analogy is some sort like okay, there is a magical way like it can know what you want, what you prefer and just automatically collect all the relevant fruits in the forest that to your taste. And it further process them maybe into juice or jam so", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_919155_ms_-_980837_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 919155, "end_ms": 980837}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 970693 ms - 1029685 ms", "content": "Title: CS224V Lecture 2 > Transcript > 970693 ms - 1029685 ms\n\nContent: way like it can know what you want, what you prefer and just automatically collect all the relevant fruits in the forest that to your taste. And it further process them maybe into juice or jam so it's easier for people to digest. So in today's we actually want to talk about okay, how we can use the technology in the bottom layer to really build a system that can help us to curate the information and for completeness. Even though we want to touch this part, I want to point out by saying knowledge curation we still assume all the information they already exist someplace. So you can imagine at the very top there is another level called knowledge discovery. It basically tries to discover previously non existent information so no matter how hard you search, it just doesn't exist. But today we won't touch this very top. I guess you guys may get a chance to learn about that in the later part of the CS224V course. And today we'll still focus on the middle layer where we try to see okay, how", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_970693_ms_-_1029685_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 970693, "end_ms": 1029685}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1017813 ms - 1079481 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1017813 ms - 1079481 ms\n\nContent: won't touch this very top. I guess you guys may get a chance to learn about that in the later part of the CS224V course. And today we'll still focus on the middle layer where we try to see okay, how to do this kind of knowledge curation. But if you just look at this definition, it's still pretty abstract. What do you mean by just finding, organizing and presenting content? This is actually a problem I met when last year when I doing the pitch. I actually this like the analogy and this few slides. I already proposed them last year when I just started this project. But it's just like it's very hard to directly approach this problem since it's pretty abstract. And if you look at a definition, you don't know how you can turn it into some problem like a machine learning problem. So I just want to give a hint here. There is actually a project that is very successful. It's kind of a lot of human. They try to do this type of knowledge curation, try to curate good content for. For the mankind.", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1017813_ms_-_1079481_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1017813, "end_ms": 1079481}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1067101 ms - 1138401 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1067101 ms - 1138401 ms\n\nContent: to give a hint here. There is actually a project that is very successful. It's kind of a lot of human. They try to do this type of knowledge curation, try to curate good content for. For the mankind. It's a kind of very successful project. I guess everyone must heard of it. Do anyone want to have a guess? Yeah, exactly. I think this may be not very hard given the slides is already Online but exactly, but exactly. I mean, for example, here is a query I also search. It's different from the previous query is a storm GitHub which is very concrete for this query. If you want to learn something about sustainability, it's a very large topic. Stanford even have a new school just to study this problem. Then very likely all the documents as long as it's related to sustainability. If you want to evaluate the retriever, you can say retriever is doing a good job since the top ranked articles they are indeed about this topic. However, very likely when you go to Google search, you still expect the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1067101_ms_-_1138401_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1067101, "end_ms": 1138401}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1126353 ms - 1186795 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1126353 ms - 1186795 ms\n\nContent: the retriever, you can say retriever is doing a good job since the top ranked articles they are indeed about this topic. However, very likely when you go to Google search, you still expect the first entry is not an advertisement but like a good Wikipedia article because this is like a very quick way for you to get an overview of this topic since it's pretty comprehensive and also organized and things like every content come with this kind of inline citations. So it's also pretty reliable and easy to verify. So the reason why I think Wikipedia is amazing project is a lot of people even view it as a miracle because when it's just launched, no one really believe a bunch of crowdsourcing efforts can really lead to high quality content that can curate information for the human being. But even though for such a successful project, you can imagine there are new topics coming out every day and there are people interested in different type of things. So there's no way like you expect. For", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1126353_ms_-_1186795_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1126353, "end_ms": 1186795}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1176155 ms - 1239411 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1176155 ms - 1239411 ms\n\nContent: though for such a successful project, you can imagine there are new topics coming out every day and there are people interested in different type of things. So there's no way like you expect. For every topic there is a person who can quickly help you write a Wikipedia article so you can directly read it. So as a computer scientist, it's very natural to ask, okay, if the human labor is a bottleneck here, can we just design an algorithm or use machine to do the job? Actually this is not the problem like we first proposed at the very beginning of the century. Researchers at mit, they already tried to study this problem, but at that time, since the technology is not there yet, what they can do is basically crafting some rule based method and use some template based method to write articles for very restricted topic. But even like for now, even like we have very powerful technology, this task is still pretty challenging. For example, I think a couple of days ago when OpenAI just released", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1176155_ms_-_1239411_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1176155, "end_ms": 1239411}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1225411 ms - 1289063 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1225411 ms - 1289063 ms\n\nContent: very restricted topic. But even like for now, even like we have very powerful technology, this task is still pretty challenging. For example, I think a couple of days ago when OpenAI just released its Oi model, since it's reported to have superpower or superhuman ability encoding benchmark and mathematical benchmark. I just think about okay, can I still secure a job after I graduate from Stanford? Because I still need a few years to graduate and at that time maybe the model are even better than now. So I asked O1 to say okay, can you generate a Wikipedia article about the future of work? So after a couple of seconds it gives me a pretty long article, I just show a part of it. But one thing you can directly notice is since its model is not grounded on external source, it won't give you text with citation. So you can't really directly go to verify whether this information are true or not. Or if you are interested in part of it. There is no way for you to resort to external source to", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1225411_ms_-_1289063_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1225411, "end_ms": 1289063}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1277759 ms - 1337563 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1277759 ms - 1337563 ms\n\nContent: text with citation. So you can't really directly go to verify whether this information are true or not. Or if you are interested in part of it. There is no way for you to resort to external source to learn more. But another thing I think is more severe is maybe you don't have enough time to read what it produces. But when I read the article directly generated by the model, I found it actually don't have a lot of details. So a lot of content is mentioned are just those big terms. It's kind of natural because if you think about how those language models are trained, it has a tendency to give you the tokens that happen to live in the regime with high probability. But this is not exactly what we want. When we really want to learn something in depth, we hope to know more about details and concrete examples. So how can we potentially build an algorithm or design a system that can do the task pretty well? I think researchers have been thinking about this problem since a long time. So even", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1277759_ms_-_1337563_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1277759, "end_ms": 1337563}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1323763 ms - 1387267 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1323763 ms - 1387267 ms\n\nContent: examples. So how can we potentially build an algorithm or design a system that can do the task pretty well? I think researchers have been thinking about this problem since a long time. So even so, since the MNT researchers, they proposed this task back in like five or six years ago, when the transformer model just come, there are another group of researchers, I think it's from Google, they try to revisit this task using the transformer model. So how they formulate this task is as follow. So they assume there is a given ordered paragraph and the first step they do is try to encode, concatenate and truncate them into a single sequence. So you can imagine after you have a single sequence, it's pretty direct, you can directly feed them into a transformer architecture. And then the second step for them is they collect a bunch of this type of human written Wikipedia article and try to fine tune the transformer model so that it can learn to write an article based on the given information. So", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1323763_ms_-_1387267_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1323763, "end_ms": 1387267}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1375123 ms - 1439027 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1375123 ms - 1439027 ms\n\nContent: for them is they collect a bunch of this type of human written Wikipedia article and try to fine tune the transformer model so that it can learn to write an article based on the given information. So this is actually pretty standard. I think now it's because it's a work done like six years ago. Now maybe you already have tried this on some other AI class. And also now given we have stronger language model, we can have more powerful model to replace the model W here, which can do the job pretty well, but doesn't mean the problem itself is already solved. So if you look at this task definition, even though everything looks makes a lot of sense, but if you take a look at assumption, it actually assume the references are given, like everything starts with this ordered paragraphs so but if you have ever wrote a survey paper or related work section for your own research paper yourself, you probably know like even collecting good references requires literature research which is non trivial", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1375123_ms_-_1439027_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1375123, "end_ms": 1439027}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1426947 ms - 1489303 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1426947 ms - 1489303 ms\n\nContent: have ever wrote a survey paper or related work section for your own research paper yourself, you probably know like even collecting good references requires literature research which is non trivial and it's pretty hard to expect the user they can come up with already good quality ordered paragraphs which you can use to summarize into the final article. So I think last year there's another relevant work trying to do this and they try to release this restriction by assuming okay, can we use the RAG paradigm also to do this task? Since we have already seen success that tried to use retrieval augmented generation to build QA system. So what they do is they directly apply the RAG paradigm by the first step is not going to generate. They use the query to search the general Internet to obtain the references and then by combining the references with the original topic, they can just instruct the language model to generate text with citations similar to the format of Wikipedia article. So this", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1426947_ms_-_1489303_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1426947, "end_ms": 1489303}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1476815 ms - 1535213 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1476815 ms - 1535213 ms\n\nContent: references and then by combining the references with the original topic, they can just instruct the language model to generate text with citations similar to the format of Wikipedia article. So this is pretty good because now we lose our assumption. We don't expect the user when they come. They also need to provide us with a set of collected references. But when I look at their experiments, especially the features in their paper, one thing I really notice is they don't really ground on a very diverse set of articles. If you really read a human written Wikipedia article it's usually with like hundreds of different sources. But in their paper the farest end in the X assets is like five documents. And it's actually not really hard to figure out the reason because if you use Google search yourself then for a single query, even though maybe at the very bottom the Google search give you like 20 pages. But maybe you are only interested in the first page because you probably know like the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1476815_ms_-_1535213_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1476815, "end_ms": 1535213}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1524655 ms - 1584851 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1524655 ms - 1584851 ms\n\nContent: yourself then for a single query, even though maybe at the very bottom the Google search give you like 20 pages. But maybe you are only interested in the first page because you probably know like the other the content later is usually either not relevant or they are just of low quality. So if you just do a single round of search then you just end up with a very limited set of references. So there is no way for you to write an article that has come that has the comprehensiveness which is on par with a human written article. So the two methods I just introduced here, I think they represent some critical milestones in this line of work where human try to design algorithm or build system to do the knowledge curation. Um, since the first work was done shortly after the transformer model was invented, it showcased like the possibility of using neural nets to curate information into a dense article if already provided um, the grounded sources and the second work here try to directly apply", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1524655_ms_-_1584851_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1524655, "end_ms": 1584851}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1571907 ms - 1640495 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1571907 ms - 1640495 ms\n\nContent: invented, it showcased like the possibility of using neural nets to curate information into a dense article if already provided um, the grounded sources and the second work here try to directly apply the RAC paradigm to do the task. So it kind of loses the assumption and it closely uh, is More is closer to the real use case. Before introducing our proposed method, I want to give a pause here and reflect on the exciting point I think we've reached so far. So I think we are witnessing not only a new milestone which can maybe help us design a better knowledge curation system, but also a broader paradigm shift in how we can potentially approach task solving using machine learning or new AI models. So I think prior to the recent advancements of large language models or foundation models, suppose you are a machine learning engineer and one day your boss assign a task and a handful of specific input. Very likely what you can do will just converge into the two paradigms. One is maybe you go", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1571907_ms_-_1640495_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1571907, "end_ms": 1640495}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1627327 ms - 1688755 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1627327 ms - 1688755 ms\n\nContent: you are a machine learning engineer and one day your boss assign a task and a handful of specific input. Very likely what you can do will just converge into the two paradigms. One is maybe you go to directly optimize the model parameters. Like you can do some continue pre training post training which I do when I was an undergrad and also you can maybe just directly do downstream task fine tuning to expect the model can learn what you want the output to look like or you can do something what people call as prompt engineering which you try to craft the input so that the model can better understand your task and give you the desired output. This is probably what you can do to solve the task. However, if you think about this type of language model not just as a single neural net with parameters but as a building blocks, I think it actually unlock a larger design space since there is no one restricting like you can only call the model once like maybe the boss. You both only care about", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1627327_ms_-_1688755_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1627327, "end_ms": 1688755}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1678219 ms - 1737363 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1678219 ms - 1737363 ms\n\nContent: but as a building blocks, I think it actually unlock a larger design space since there is no one restricting like you can only call the model once like maybe the boss. You both only care about whether you can do the task, they don't care about what happened in the black books. So you can actually have the freedom to call the model multiple time if that can lead to the better outcome. And you can even let the language model to interact with other softwares like the retriever we just introduced at the beginning. In contrast to optimizing the model parameters which I think we kind of have mature technology like there are back propagation and we also have some standard and well defined loss function to help you to do that. We are not very clear like say if we want to craft this type of language model empowered system, how we should break down the task into different small step for the model to execute or how to optimize the pipeline effectively when we already have the initial stage of", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1678219_ms_-_1737363_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1678219, "end_ms": 1737363}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1725793 ms - 1787111 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1725793 ms - 1787111 ms\n\nContent: model empowered system, how we should break down the task into different small step for the model to execute or how to optimize the pipeline effectively when we already have the initial stage of the system. So I think this is a pretty active research area and there are a lot of interesting opportunities waiting to be unlocked. So actually our recent paper published in Naco this year, it introduces Storm, which is exactly the first language model empowered system that can generate a Wikipedia like articles from scratch assuming the user just give you the topic as the input. So we don't expect they already come with the references and we also don't expect them to participate in the middle to instruct the model what the next step to take. So actually for example for the same query I asked 01 before like if I want our system to write an article about the future of work, it will give us a pretty comprehensive article. Actually I can show it. So basically something like this. So it's a very", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1725793_ms_-_1787111_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1725793, "end_ms": 1787111}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1773955 ms - 1838633 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1773955 ms - 1838633 ms\n\nContent: before like if I want our system to write an article about the future of work, it will give us a pretty comprehensive article. Actually I can show it. So basically something like this. So it's a very pretty long one like, but also it's kind of well organized into using this kind of hierarchical outline. So if you are interested maybe how this would related to remote work, you can just click so it will direct you to the corresponding content. And also as I say it's kind of grounded in the external source so it's pretty easy for you to verify. You can check like where this information is coming from and if you are interested you can just click it to learn more. So actually after we published the paper and released the open source code base, my collaborator Yu Chong spends a lot of efforts trying to make sure we can keep hosting this service. And so far we have a lot of people really try to try Storm. Instead of downloading the open source code, it directly tries to our services. So we", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1773955_ms_-_1838633_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1773955, "end_ms": 1838633}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1825913 ms - 1937505 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1825913 ms - 1937505 ms\n\nContent: trying to make sure we can keep hosting this service. And so far we have a lot of people really try to try Storm. Instead of downloading the open source code, it directly tries to our services. So we can also keep track what topic people are interested in. There are actually a lot of very interesting topics so you can imagine there is no way like you try to. Oh, sorry, how to do this? Let me take a look. Are we sharing on Zoom? But are you connecting on Zoom? Maybe, I think it can be. Anyway, I think we can leave this live demo later if it doesn't work out. It's like if you have already download our slides, I think you can later just click like we have a link here so you can take a look at what's the final article. You can also browse like what people are interested in learning using our tool. And you can also you are also encouraged to try it out on your own. Basically I add a tiny link here. So if you click this outline to direct you to our online demo. So I think and also I'm", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1825913_ms_-_1937505_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1825913, "end_ms": 1937505}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1917897 ms - 1990181 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1917897 ms - 1990181 ms\n\nContent: our tool. And you can also you are also encouraged to try it out on your own. Basically I add a tiny link here. So if you click this outline to direct you to our online demo. So I think and also I'm pretty glad to share is like after we release this paper and also open source, our code base Storm has aroused interest across various communities. Like there are open source developers who is interested in using our code base, but they are also just general people. They learn about our work through watching YouTube videos. Also, even though our paper is just published in Naco this summer, our paper has already led to multiple follow up works which I think are pretty exciting. Now I think I want to delve a little bit Deeper into the technical detail about how we do it. So how we can potentially build this kind of language model in power system to generate articles with good breadth and depth. One thing I think I mentioned before is one major challenge of building this type of system is how", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1917897_ms_-_1990181_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1917897, "end_ms": 1990181}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 1977325 ms - 2044911 ms", "content": "Title: CS224V Lecture 2 > Transcript > 1977325 ms - 2044911 ms\n\nContent: build this kind of language model in power system to generate articles with good breadth and depth. One thing I think I mentioned before is one major challenge of building this type of system is how we can decouple a task into different small steps or craft a pipeline that the language model could potentially execute. And one thing we learn while I'm working on this project, it's kind of worthwhile to observe how human do a task or like the common human workflow for completing a complicated task. So you can imagine suppose you now have this problem like you are assigned to write a grounded and long article or report for some sort of purpose. You probably didn't directly go into the actual writing. So actually in the last century before people are interested in how to teach AI to generate or write articles, there are a lot of research trying to teaching young children or just teaching like college students how to write better. So there is like a very. This is a very impactful paper", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_1977325_ms_-_2044911_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 1977325, "end_ms": 2044911}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2032455 ms - 2092783 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2032455 ms - 2092783 ms\n\nContent: or write articles, there are a lot of research trying to teaching young children or just teaching like college students how to write better. So there is like a very. This is a very impactful paper which has been cited thousand times. So what it proposed is saying okay, for us human, when we go to writing, we didn't go straight forward to kind of generates a bunch of tokens. Instead the first step we do is actually focusing on the prewriting stage where we kind of refine our idea and discover more information about the topic we care about. And also there is work in like teaching people how to do research they mention is like for human, in order to get the information you need, it's pretty important for you to ask good questions. This is actually pretty straightforward because now you can imagine with good IR system or QA system, if you can have the good query at the very beginning, it's pretty easy for you to just use the system to get what you want. Uh, actually I highly recommend", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2032455_ms_-_2092783_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2032455, "end_ms": 2092783}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2080263 ms - 2143257 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2080263 ms - 2143257 ms\n\nContent: imagine with good IR system or QA system, if you can have the good query at the very beginning, it's pretty easy for you to just use the system to get what you want. Uh, actually I highly recommend reading this book not just for doing this task. If you're just interested in research, I think it shares some very interesting insights on how you can potentially do the research. So inspired by how human decoupled your workflow when designing the system, we also try to break down this very complicated task into two stage. So instead of directly use the language model to predict the next token to do the actual generation, we focus more on the pre writing stage where we formulate the challenge as given the topic T. We hope the system can automatically find a set of references R and then create an outline O which is defined as a list of multilevel section headings to organize R. You can imagine. Suppose I already turned a topic into the list of references and the hierarchical outline, the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2080263_ms_-_2143257_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2080263, "end_ms": 2143257}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2131105 ms - 2192963 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2131105 ms - 2192963 ms\n\nContent: an outline O which is defined as a list of multilevel section headings to organize R. You can imagine. Suppose I already turned a topic into the list of references and the hierarchical outline, the actual writing stage is not that hard. Given the strong generation ability of current language model. You can just use the technology similar to those people use back in 2018. You can just include all the references and the outline section to prompt the model to generate actual content with citation. However, solving the pre writing task itself is pretty challenged because as you can imagine, if you just do a single round of search using the topic, then the references it can collect is usually not very diverse. So how to design a system that can automatically do high quality multi hop search is an open question. And another thing is like after you search all the documents, how you can bring out the most useful information and organize them into this kind of hierarchical structure. This also", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2131105_ms_-_2192963_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2131105, "end_ms": 2192963}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2181069 ms - 2241647 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2181069 ms - 2241647 ms\n\nContent: an open question. And another thing is like after you search all the documents, how you can bring out the most useful information and organize them into this kind of hierarchical structure. This also requires the model to have the ability to induce concept from the set of references you already collect. So when we think about so when we try to approach this problem, since we are inspired by how humans do the literature research, we are now trying to not just teaching the language model to answer the question, instead we focus on how we can potentially instruct them to ask good question. You can imagine if we can use a way to let the model ask good question, then combined with the current rag based QA system, we can directly get what we want, which is pretty useful to collect the references we need in the pre writing stage. But unfortunately if you directly try to prompt the language model to ask question, the results are pretty suboptimal. Because like theoretically since these models", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2181069_ms_-_2241647_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2181069, "end_ms": 2241647}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2229775 ms - 2290833 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2229775 ms - 2290833 ms\n\nContent: we need in the pre writing stage. But unfortunately if you directly try to prompt the language model to ask question, the results are pretty suboptimal. Because like theoretically since these models are trained through the instruction tuning, you can instruct it to do any task you want. You can maybe just say okay, now I want to talk about 2022 Winter Olympics opening ceremony. Can you ask 30 questions to learn about the topic? But unfortunately all the top questions it usually give you are only about those kind of what, when, how question which is not very interesting. They only cover like very basic facts about the topic. Another thing is I think recently there is a very hyped or popular term. It's called like inference time scaling where people say okay, maybe if you just quote language model once, the result is not good, I may just use my compute in the reference time. Like try to quit multiple times with different temperature. Then maybe I can hope, okay, one of the results is", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2229775_ms_-_2290833_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2229775, "end_ms": 2290833}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2279425 ms - 2340411 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2279425 ms - 2340411 ms\n\nContent: model once, the result is not good, I may just use my compute in the reference time. Like try to quit multiple times with different temperature. Then maybe I can hope, okay, one of the results is satisfying what I want. However, this kind of brute force is not very effective. And a major problem is like there has been research saying for this type of open ended generation, the language model has a problem of not producing very diverse outputs. So maybe even though you ask it to answer ask like 30 questions, all the questions, they are very similar. So it's not very useful to collect a very comprehensive list of references. So what we try to do is instead of directly use the topic you want to write as the input query, Storm uses perspective as a latent variable to control the breadth of the search. So what do we mean by the perspective? So you can imagine if you are a student who is assigned to write a report about sustainability, then instead of directly use it as a query to search", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2279425_ms_-_2340411_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2279425, "end_ms": 2340411}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2327827 ms - 2387111 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2327827 ms - 2387111 ms\n\nContent: the search. So what do we mean by the perspective? So you can imagine if you are a student who is assigned to write a report about sustainability, then instead of directly use it as a query to search Google to obtain the references you want, you may want to first check out how people write articles about a similar topic. Maybe they are not existing Wikipedia article talking about sustainability, but there may be some relevant articles talking about sustainable development or just corporate social responsibility, similar type of things. By reading them you kind of get a sense when I need to write this topic what aspects or what perspectives I should focus on. So similarly, for the first step of our system we do the same thing like requiring the system to do the survey of the relevant articles and to come up with this type of perspective. So you can imagine one of the perspective it come up maybe saying okay, now I am a social scientist, I'm interested in bringing a social perspective", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2327827_ms_-_2387111_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2327827, "end_ms": 2387111}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2376607 ms - 2435281 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2376607 ms - 2435281 ms\n\nContent: and to come up with this type of perspective. So you can imagine one of the perspective it come up maybe saying okay, now I am a social scientist, I'm interested in bringing a social perspective focus on topics such as social sustainability, blah blah blah. So with this type of additional information it kind of serve as the latent variable when you when previously people train model to do condition the text generation. So it's actually we provide the model with more information and additional knowledge. So with this additional information it's more likely to give us more concrete questions and can help us and suppose we can run different perspectives in parallel. Then with very efficiently we can collect a bunch of information with good coverage. While this type of design I think is effectively improved the breadth in order to improve the depth as well. We also leverage the multi turn conversation ability of language models to allow follow up questions. Actually you can imagine", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2376607_ms_-_2435281_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2376607, "end_ms": 2435281}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2423659 ms - 2483653 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2423659 ms - 2483653 ms\n\nContent: effectively improved the breadth in order to improve the depth as well. We also leverage the multi turn conversation ability of language models to allow follow up questions. Actually you can imagine sometimes it's pretty hard for you to ask those very in depth question at the beginning. Suppose you don't really understand the field very well and some in depth questions by nature they can arise only after reading the information gathered in previous rounds. For example, if we still talk about like the 2022 Winter Olympic ceremony, maybe at a certain term the model sees some content saying okay, this time the athletes from over 19th century they will enter the stadium in a specific order. After looking at this information then maybe you will realize oh, maybe the specific order is something interesting or special to this specific event. So with some probability the model may keep asking okay, how is the order determined? And going through this direction you can find more in depth", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2423659_ms_-_2483653_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2423659, "end_ms": 2483653}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2472205 ms - 2534139 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2472205 ms - 2534139 ms\n\nContent: interesting or special to this specific event. So with some probability the model may keep asking okay, how is the order determined? And going through this direction you can find more in depth information. And also since I think current models they are post trained to align to be helpful conversation assistant. So they are pretty natural in just keep this conversation flowing. So by using this way to simulate this conversation between a Wikipedia writer who ask question and the expert which is actually just a rag based question answering system we can keep the conversation growing. And since the conversation involves multiple turn we actually solve the problem. Like how we can instruct the language model to do multi hop search. It's actually we are not directly asked to come up with multiple search query but use this kind of conversational setup to use it to kind of find information that may need that are like multi hop related to the original topic previously. We plan to do a live", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2472205_ms_-_2534139_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2472205, "end_ms": 2534139}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2519955 ms - 2683429 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2519955 ms - 2683429 ms\n\nContent: search query but use this kind of conversational setup to use it to kind of find information that may need that are like multi hop related to the original topic previously. We plan to do a live demo but since there's no way to open that website I guess we'll just leave you guys to explore it afterwards and think anyway I think we can just continue. I think I already get out of the PowerPoint but doesn't work. So normally when you play it. Maybe I think we can just drop it. Why don't they just go to Storm directly. But it's not shown on this. But it's not on showed on this. Unplugging it. Oh, this is because you are doing. Mirroring. On that other page. So here is what I usually do. I don't do. I guess maybe just let them use it. Oh yeah, it's working. Oh yeah. I think it's okay. It's finally figured out this screen issue. Yeah, it's basically. I think we are also happy to give it. And then your computer stuck. No, it doesn't. It doesn't work. I think your computer has some problem. I", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2519955_ms_-_2683429_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2519955, "end_ms": 2683429}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2658023 ms - 2786483 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2658023 ms - 2786483 ms\n\nContent: figured out this screen issue. Yeah, it's basically. I think we are also happy to give it. And then your computer stuck. No, it doesn't. It doesn't work. I think your computer has some problem. I think it's. I guess we can skip it. What do you think? Oh, this is so. Okay. It's finally seems to be working. We are happy to give a live demo. Do anyone has any topic you want to give a try? Yeah, you can. How do LLMs input images like embed images? I think your computer just happened. I think anyway I think we just. You can. You guys can just try it out offline. There's very weird is I never made this computer like the MacBook just starting this way. I think we can just let you choose to keep giving the second half of the lecture. I tried it offline. It works very well. Yeah. It's not the system is down. It's basically the computer is down. Yeah. So this laptop is down. Yeah. I think you can just try it on your own. Yeah yeah. Hopefully it's not too parallel to make our server crash. Does", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2658023_ms_-_2786483_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2658023, "end_ms": 2786483}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2763737 ms - 2844035 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2763737 ms - 2844035 ms\n\nContent: is down. It's basically the computer is down. Yeah. So this laptop is down. Yeah. I think you can just try it on your own. Yeah yeah. Hopefully it's not too parallel to make our server crash. Does this model if I ask stuff that has occurred like in the last year will have knowledge of that. Yeah, because it's like grounded on the Internet. So it's more like suppose the information is existing in somewhere in the Internet, then very likely it can't find it. But suppose you're talking about some topic which does not exist, like there is no information talking about it anyway. There is no way for it to come up with information which I think is like you need to go into like how you can discover new stuff. It's like how you can verify a few hypothesis like a different problem we are solving. But suppose there are information only. I think we can just keep moving since this computer has so much. I don't understand. Okay, so yeah, question. I had a question like since the LLM is like relying", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2763737_ms_-_2844035_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2763737, "end_ms": 2844035}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2820511 ms - 2899013 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2820511 ms - 2899013 ms\n\nContent: suppose there are information only. I think we can just keep moving since this computer has so much. I don't understand. Okay, so yeah, question. I had a question like since the LLM is like relying on online information that's continuously updated rather than it's pre training, are you able to get away with using like models that have left that you spent less time or compute pre training for that reason? You mean like replacing the language model using the system with smaller cheaper models. 7 billion lava model or something? You can buy that since in the storm repository we developed the system with highly modular way. So you can replace with many other language models, either commercial models or your local running models through some client. And also you can replace with the search engine. So in the online demo we use the binsearch and you can replace with any other search engine you want. And we support about seven or eight sources and like DuckDuckGo, Google and Yo.com your local", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2820511_ms_-_2899013_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2820511, "end_ms": 2899013}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2885451 ms - 2962455 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2885451 ms - 2962455 ms\n\nContent: So in the online demo we use the binsearch and you can replace with any other search engine you want. And we support about seven or eight sources and like DuckDuckGo, Google and Yo.com your local vector database, anything you want. And for language model it supports like GPT models, Gemini models, cloud models and llama models and many of them. And you can free to explore them in the GitHub and download to play with them of a small amount. Yeah. Question for the latent variable, are you. Actually like in practice introducing the perspective? Like are you prompting it to act as for example social scientist in training on data from different fields like social science etc. I'm just wondering what that actually looks like. So you're asking how we find the perspective for what are those actual perspectives on? Yeah. So given the user input topic, we first search in the Wikipedia to find the most relevant existing Wikipedia articles online and analyze their written articles about potential", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2885451_ms_-_2962455_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2885451, "end_ms": 2962455}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2940657 ms - 3011513 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2940657 ms - 3011513 ms\n\nContent: perspectives on? Yeah. So given the user input topic, we first search in the Wikipedia to find the most relevant existing Wikipedia articles online and analyze their written articles about potential like perspectives in their articles. And then we prompt the language model with different perspectives. Say some of them are like social scientists, some of them are computer scientists and others are like policymakers. All of them different perspectives and give them the role of the perspective and the detailed description and prompt the language model. You are taking this Perspective and you are going to search the topic on the Internet about something and it will generate the search queries and search differently generate different search queries. Does that answer your question? Yep. If there's no more question, we can dive into the fourth part of the lecture, which is we're building this system generating the long form like articles, like Wikipedia articles and how we can actually", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2940657_ms_-_3011513_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2940657, "end_ms": 3011513}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 2998921 ms - 3071223 ms", "content": "Title: CS224V Lecture 2 > Transcript > 2998921 ms - 3071223 ms\n\nContent: there's no more question, we can dive into the fourth part of the lecture, which is we're building this system generating the long form like articles, like Wikipedia articles and how we can actually evaluate them quantitatively. So the question is how can we evaluate and do we have any like golden answers ground truth we can compare with? You may think like, not really. Since like human written articles are not perfect as well. And since like Wikipedia is a special type of articles online where it is collaborative edit by hundreds or thousands of people across like using hundreds of references. And the timeline may expand for years and it may not be the actual the ground truth with the golden answer. And you can compare your language model generated or the system generated results directly with the Wikipedia articles. And besides the final article content, what else we can evaluate in order to assess what's the breadth and depth of the article? So the first approach we adopt is", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_2998921_ms_-_3071223_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 2998921, "end_ms": 3071223}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3053855 ms - 3124055 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3053855 ms - 3124055 ms\n\nContent: with the Wikipedia articles. And besides the final article content, what else we can evaluate in order to assess what's the breadth and depth of the article? So the first approach we adopt is evaluate the outline of the article when you like, try the article in our demo where you just see in the slides. The Wikipedia articles usually are very long, like thousands of words, but they are divided or organized into a hierarchical structure with different sections subsections with their headings. The very first thing we compare is like can we compare the article structure with the human written article structure? Like saying, does it cover relatively the same or even better breadth of the topic? In order to assess the breadth of the article generated, we propose two metrics. The first one is compare the heading soft recall, which basically compare the embedding of the section names, section headings between the system generated article and the human written article. Basically compare their", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3053855_ms_-_3124055_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3053855, "end_ms": 3124055}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3113423 ms - 3175637 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3113423 ms - 3175637 ms\n\nContent: the heading soft recall, which basically compare the embedding of the section names, section headings between the system generated article and the human written article. Basically compare their semantic similarities. And the second one is we compare the named entities mentioned in their headings. To compare if they mentioned roughly the same or more or less entities in the article. And compare, we compare with multiple baselines. The first baseline is direct generate which we just skip the topic and instruct the language model to generate the full length article, the RAG we just introduced and the RAC expand. Basically we enhance the RAG models. We first give the topic and then instruct it to retrieve information. And then construct the outline. And based on the outline each section, we instruct the RAG model to do another round of retrieval for each section and then refine the outline again. And do this iteratively. And after that we compare the iteratively refined outline with the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3113423_ms_-_3175637_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3113423, "end_ms": 3175637}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3164141 ms - 3234137 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3164141 ms - 3234137 ms\n\nContent: instruct the RAG model to do another round of retrieval for each section and then refine the outline again. And do this iteratively. And after that we compare the iteratively refined outline with the storm generated outline. And STORM consistently does better job than all the baselines here. And another question you may come into mind is we have two design principles in the storm system. One is you generate multiple perspectives like language model agents taking different perspectives. And the second is that those agents who are going back and forth conversations and doing it in multiple rounds have in depth conversations in order to retrieve more information, cover breadth and depth. And does these two design principles contribute collaboratively well to the overall system? That's where the ablation studies may come to the place where you try to understand the different parts of the system, contribute to its overall performance and to check if any of the parts may unintentionally", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3164141_ms_-_3234137_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3164141, "end_ms": 3234137}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3221201 ms - 3296997 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3221201 ms - 3296997 ms\n\nContent: the ablation studies may come to the place where you try to understand the different parts of the system, contribute to its overall performance and to check if any of the parts may unintentionally harm the overall performance. So we compare with two, two ablations. One is without multiple perspective taken by language model agents and one is without back end for conversations, multiple rounds of conversations. And both ablation studies shows performance drop as compared to the full storm system. So in the ablation system. So the question is how does ablation system work as compared to the full system? So for without multiple perspective, it's just disabled like language model to taken in different perspective. Just each of them were taken in the same perspective with the same prompts when they generate the search query. So it is not multiprospective. And without multiple rounds of conversation they are not going back and forth. Ask in depth question. So in the full swarm system it's", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3221201_ms_-_3296997_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3221201, "end_ms": 3296997}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3284661 ms - 3350225 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3284661 ms - 3350225 ms\n\nContent: they generate the search query. So it is not multiprospective. And without multiple rounds of conversation they are not going back and forth. Ask in depth question. So in the full swarm system it's first ask a question, answer a question, then generate a follow up question and then answer again and then we disable that and try to compare any more questions. For the conversation of the expert. How many rounds of conversation or what's the depth of the conversation that you guys found is like the best tuning for like the model. Because there has to be a certain point at which we stop wearing the escrow between the generator like the wiki text generator and the escrow. Yeah, good question. The question is how many rounds of conversation we adopt in the system. So this is really a hyperparameter setting and we are not doing extensive search. Hyperparameter search in order to maximize the metrics given the consideration of the first is the context window, like how much information you", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3284661_ms_-_3350225_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3284661, "end_ms": 3350225}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3337741 ms - 3406955 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3337741 ms - 3406955 ms\n\nContent: setting and we are not doing extensive search. Hyperparameter search in order to maximize the metrics given the consideration of the first is the context window, like how much information you search and how much information you can incorporate into a generation. Back when we are doing experiments, we are using 3.5 and GPT4 and we choose I think it's three like five rounds of five rounds of QAs. And each round it will search for three queries and each query will return like top 10 results. And that is already a lot. And we do not do like hyperbrand research in order to maximize the metrics. The only reason I ask is because in the simulation we look at the performance without conversation. So it would be interesting to see what would be the performance at like different rounds of conversation. Yeah. Have you considered other evaluation metrics? Yeah, we're having more measures to later. How do you take care of conflict facts when you do like the different perspective? So the question is", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3337741_ms_-_3406955_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3337741, "end_ms": 3406955}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3388945 ms - 3461909 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3388945 ms - 3461909 ms\n\nContent: Yeah. Have you considered other evaluation metrics? Yeah, we're having more measures to later. How do you take care of conflict facts when you do like the different perspective? So the question is how do we take care of conflicting information during multiple perspective search? The answer is we do not take special care of comparing the fact check between multiple sources and decide which is better. So the information used in the article solely relies on what is available online and if it's incorrect online or some of them are conflicting with each other, we just feed them into the model and let the model figure out. So we don't do special process for that part. Yeah, some Wikipedia articles will have like links or references about like some more esoteric topic just so happens to be in some very like slight way related to what the actual topic is. Do you guys also have a way of sort of measuring that level? I know you talked about like the multi step reason but also most of just like", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3388945_ms_-_3461909_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3388945, "end_ms": 3461909}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3450307 ms - 3517083 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3450307 ms - 3517083 ms\n\nContent: very like slight way related to what the actual topic is. Do you guys also have a way of sort of measuring that level? I know you talked about like the multi step reason but also most of just like the opportunity for sort of random branches. If you can imagine the current Wikipedia generating as a node in a graph. So you're basically asking like if we are comparing with the real human written art Wikipedia articles with their references. How you guys went about the. How you guys went about designing a way for the actual current article that's been generated to have references that are very outside of the original query, which is so like a lot of Wikipedia articles have that as just because like so many things are just correlated. Good question. Let's leave it to the next slide. And since we are, another design choice is that we do not directly compare the system generated article references directly with the Wikipedia generated references. It's that many of the references written in", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3450307_ms_-_3517083_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3450307, "end_ms": 3517083}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3504483 ms - 3573585 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3504483 ms - 3573585 ms\n\nContent: we are, another design choice is that we do not directly compare the system generated article references directly with the Wikipedia generated references. It's that many of the references written in the Wikipedia article are either inaccessible or expired or in the other languages. So in the study we only taken English as the source. So due to these considerations and the accessibility of all the references in the Wikipedia article actually compare the actual content, not the direct match of the references. So besides we compare the overall outline with the structure of the article. We compare the actual content of the article with the human written Wikipedias. So we construct a dataset called Fresh Wikidataset which basically we curate a list of high quality Wikipedia articles that are written after the training code of the language model we use and its quality is of like B class B and above. So Wikipedia have the different class of the quality of each article to a B, C, D and", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3504483_ms_-_3573585_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3504483, "end_ms": 3573585}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3559433 ms - 3632707 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3559433 ms - 3632707 ms\n\nContent: are written after the training code of the language model we use and its quality is of like B class B and above. So Wikipedia have the different class of the quality of each article to a B, C, D and something else. And we adopt the articles above B quality and we ensure there are multiple layers of outlines in the article and we ensure it's in English and in not other languages. And we filter out all the sensitive topics and that may cause any trouble. So we compare those articles. The first metrics on the left side is we do a very naive and commonly adopted just compare the raw and compare the entity like recall between those two articles. You may wonder that this is not good enough since human articles are subjective for it is not golden enough or how can we access the actual content written by the system itself. So we adopt another evaluation method on the right side which is the rubric reading. So by rubric reading we mean we take the large language model itself as a judge and the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3559433_ms_-_3632707_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3559433, "end_ms": 3632707}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3618883 ms - 3687117 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3618883 ms - 3687117 ms\n\nContent: by the system itself. So we adopt another evaluation method on the right side which is the rubric reading. So by rubric reading we mean we take the large language model itself as a judge and the large language model will see the generated article along with the topic pairs and compare like if the generated article is of high quality in multiple dimensions. It's similar to like asking human annotators or evaluators to grade the generated article along different dimensions. But we use the large language model to scale the evaluation up and we indeed do the human annotations and compare the human grading with the language model gradient to see if they agree. So using the large language model as a judge, we provide it with the rubric along different dimensions like the interest level of the article, if it is well organized, if it is relevant to the original topic and what's the overall coverage of the topic it perceives. And each of them will assign to 1, 2, 5 score and each score will be", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3618883_ms_-_3687117_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3618883, "end_ms": 3687117}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3671725 ms - 3751907 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3671725 ms - 3751907 ms\n\nContent: if it is well organized, if it is relevant to the original topic and what's the overall coverage of the topic it perceives. And each of them will assign to 1, 2, 5 score and each score will be assigned with the meaning like which criteria will be meshed with that score. And instruct the language model to generate a score for each article along each dimensions. A key point here is that we use another a separate language model besides the one that generates the actual article. As there's a study that's the language model will prefer its own generated content. So we adopt the separate the second different language model to judge it and using these metrics and storm consistently outperform all the baselines here just as a proxy. Yeah. Do you have to pre train it or do something to it so that it can do this well? So we use another existing language model that is already like fine tuned specifically for the grading purposes for the LLM. As a judge, did you take a look at how it scores with", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3671725_ms_-_3751907_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3671725, "end_ms": 3751907}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3733769 ms - 3814105 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3733769 ms - 3814105 ms\n\nContent: it can do this well? So we use another existing language model that is already like fine tuned specifically for the grading purposes for the LLM. As a judge, did you take a look at how it scores with the corresponding human written articles? The scores of the human written articles. So there are some considerations as you said, like human written articles. If you take in like if you look at the Wikipedia generated, it is like Thousands of words 3,000 to 5,000 words. And that is way beyond the capability of the evaluation capability of the language model. And the second one is in order to ensure that like the fair comparison of all the method and baselines, we compare the language model generated instead of like judging the human one and the language model generative on which one is like making more sense as like we said, like the human written one is not golden as well. So we compare like based on each other in this dimension. So when we choose those articles so we do some processing", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3733769_ms_-_3814105_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3733769, "end_ms": 3814105}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3791191 ms - 3869787 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3791191 ms - 3869787 ms\n\nContent: more sense as like we said, like the human written one is not golden as well. So we compare like based on each other in this dimension. So when we choose those articles so we do some processing to truncate them in a meaningful way as much like as much as makes sense as possible when we do the evaluation. Yeah, question. So I think it's kind of difficult for. So I think what I'm thinking for is like accuracy and it's part of the article quality I believe. So I think it's really difficult to actually make sure that whether the article generates accurate or not. And so how do you kind of think about whether the generic content is actually tied to the cited source? Because when you're asking questions and generating answers, there could be hallucination. So you're saying that. So the question is how to check and evaluate the cited source. Actually is the source that support the claim it makes in the article? Is that the question? Yeah, we do some check. So given the sentence with the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3791191_ms_-_3869787_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3791191, "end_ms": 3869787}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3857699 ms - 3941351 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3857699 ms - 3941351 ms\n\nContent: is how to check and evaluate the cited source. Actually is the source that support the claim it makes in the article? Is that the question? Yeah, we do some check. So given the sentence with the citations this pair we ask the language model to say does does the source entail the claim it supports? And we run that experiment and report in the paper. Also in the next slides we do another round of check ask the human to actually to manually check those citations to eliminate the possibility of hallucination Here. What are the last three metrics measuring? So basically they compare like how similar they are and just as a very rough proxy to fast prototyping our method and do a sanity check if it's not like divid too much from the actual to the left are to the human read mark. For information sources. Do we do any redlining to make sure that like the information source is more credible or not? Like do we stack them like where we generate horrible information? Excellent question. The", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3857699_ms_-_3941351_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3857699, "end_ms": 3941351}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3930591 ms - 4000031 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3930591 ms - 4000031 ms\n\nContent: sources. Do we do any redlining to make sure that like the information source is more credible or not? Like do we stack them like where we generate horrible information? Excellent question. The question is how do we filter the like the source and what sources are trusted and what sources are not trusted. So in Wikipedia community they maintain a large table of trusted and untrusted sources. It's not like very strict but it filter out like commonly perceived untrusted sources and we use that table to do like real based filtering of what sources are generally unreliable and just disregard that. Okay, so the next step is all the automatic evaluation you see here are you can, you may not be fully convinced by its quality since it's judged by language model, it's judged by predefined those relatively naive metrics like entity recall. So we actually do the human evaluation, invite humans to actually to look at those articles and compare it with the human written Wikipedia articles and see", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3930591_ms_-_4000031_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3930591, "end_ms": 4000031}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 3988175 ms - 4054621 ms", "content": "Title: CS224V Lecture 2 > Transcript > 3988175 ms - 4054621 ms\n\nContent: naive metrics like entity recall. So we actually do the human evaluation, invite humans to actually to look at those articles and compare it with the human written Wikipedia articles and see how it does there. So we are writing Wikipedia like articles. So it does make sense to ask Wikipedia editors to look at it. So one of our collaborators spent a lot of time to find 20 Wikipedia editors with extensive editing experience in the past five years and invite them to conduct this study. So we invite them to look at the language model generated report and human written reports and ask them to grid on the same rubric at the language model as judge rubric reading just but in the 1, 2, 7 lacquer scale and more fine grained grading scale along each axis as Orac here means rag enhanced model which is iteratively search and refine the outline. That's baseline. That is the strongest baseline in our experiment. And we compare the storm with the stronger baseline and ask those editors to grade and", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_3988175_ms_-_4054621_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 3988175, "end_ms": 4054621}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4043789 ms - 4121905 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4043789 ms - 4121905 ms\n\nContent: is iteratively search and refine the outline. That's baseline. That is the strongest baseline in our experiment. And we compare the storm with the stronger baseline and ask those editors to grade and to rate which one is. Better question to like grade a say. A very good Wikipedia article by humans. Yeah, like have you asked the graders to grade like a well written Wikipedia article by human like existing ones? So we asked them first to compare these two and then we asked them to compare to say like in storm and in human written one and to give free response to say how do you perceive the storm performed when you are actually using this one? And there's another slice coming up 100% human level. This table would well like each actual opinion larger than four. Okay, so in this part you're asking human Wikipedia editors to compare storm generated and Orac generated articles. Right. I'm surprised there's not a human written article baseline in this. Oh yeah, that's so the rationale here.", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4043789_ms_-_4121905_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4043789, "end_ms": 4121905}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4106907 ms - 4180341 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4106907 ms - 4180341 ms\n\nContent: human Wikipedia editors to compare storm generated and Orac generated articles. Right. I'm surprised there's not a human written article baseline in this. Oh yeah, that's so the rationale here. We conduct this study is because we want to come to study how the breadth and depth of the storm can introduce as compared to the strongest baseline as in different dimensions. This is the research question one. And the research question two is does the Wikipedia editors really find the storm will be useful when they are writing Wikipedia articles and if they were willing to use storm to do their normal job. And that is a second step question today. How did you go about finding the rubric? Or is this the rubric for like Wikipedia themselves uses for judging articles in terms of interest level to verify? Good question. So we survey like Wikipedia has a guideline of what a good Wikipedia article looks like and they have a page describing what a good wiki article looks like and what are", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4106907_ms_-_4180341_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4106907, "end_ms": 4180341}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4164973 ms - 4239357 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4164973 ms - 4239357 ms\n\nContent: to verify? Good question. So we survey like Wikipedia has a guideline of what a good Wikipedia article looks like and they have a page describing what a good wiki article looks like and what are contribution guidelines by those editors. And we take inspiration from that page and fill out something that is irrelevant to the article generation problem have here and we filter them out and leave this five here and it's kind of inspired and stick to the Wikipedia rules. What's the criteria for the verifiability? Verifiability is how do they perceive the second source can be verified to the claim in the paper in the generated content and we provide the link with an actual cited snippet of like each claim. Right. And they will click into that link, see that paragraph and see if that paragraph can entail the claim in the article. What about claims that didn't have a source that's already. That's also the verifiable. If there is no citations then you cannot verify at all, right? Yeah, that's", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4164973_ms_-_4239357_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4164973, "end_ms": 4239357}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4224253 ms - 4288761 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4224253 ms - 4288761 ms\n\nContent: entail the claim in the article. What about claims that didn't have a source that's already. That's also the verifiable. If there is no citations then you cannot verify at all, right? Yeah, that's true. Out of the evaluation methods you showed, which ones are you using? Like as you're iterating on your own system to improve it and which ones are you doing at the very end to compare to your benchmark? Because this is like very expensive, you can't do it a lot of times. Yes, very good question. So in the automatic evaluation we run them frequently since it's like very fast and language model as such doesn't take that much process as well. And for all the human evaluation, we personally read a lot of like generated articles ourselves during prototyping, especially each before we have this nice UIs, we read them in command line and read them into text form and we compare them side by side and see what's the generated article and what's the offline and we go in depth and personally search", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4224253_ms_-_4288761_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4224253, "end_ms": 4288761}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4276265 ms - 4341277 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4276265 ms - 4341277 ms\n\nContent: UIs, we read them in command line and read them into text form and we compare them side by side and see what's the generated article and what's the offline and we go in depth and personally search for the topics and we have some knowledge about the topics we're experimenting with. Yeah, we have to fast forward this. So besides the automatic and human evaluation, we also conduct the in the wild evaluation since like human evaluation also like restricted in the Wikipedia community and only 20 people. And we want to see how the general public, the general Internet perceived the usefulness and effectiveness of the storm. So we host the demo you just used we just showed online and There are like 75,000 people using this and crushing our system multiple times. And happily we are seeing that many people leave feedbacks like we collect 20,000 feedbacks and they ask questions in multiple domains and we'll show that in the later slides and many of them will leave very nice comments and some of", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4276265_ms_-_4341277_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4276265, "end_ms": 4341277}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4330573 ms - 4399867 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4330573 ms - 4399867 ms\n\nContent: people leave feedbacks like we collect 20,000 feedbacks and they ask questions in multiple domains and we'll show that in the later slides and many of them will leave very nice comments and some of their comments inspire us to do the follow up work we present next. So in the wild evaluation. We ask people why do they use Storm? And people ask they want to do use Storm to do their own research, their own learning process, they want to conduct the market analysis, they want to do the literature search, all kinds of this and you can possibly find in common. They are doing all doing like complex information seeking which is no single like short answer can answer their question but requires in depth multi rungs of multiple hop search in order for them to come up with a good idea of what they are searching for. And there are many domains like they are asking for agriculture, computer science, biology, physics, geography and also gaming, food, music, laws, animals and just anything. So let's", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4330573_ms_-_4399867_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4330573, "end_ms": 4399867}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4382361 ms - 4457297 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4382361 ms - 4457297 ms\n\nContent: searching for. And there are many domains like they are asking for agriculture, computer science, biology, physics, geography and also gaming, food, music, laws, animals and just anything. So let's revisit our meta question Are people's information needs really satisfied with the Storm system here? So it's like when we collect the feedback, some journalists leave the comments that I'm writing an article in a specific perspective and the Storm is great, it covers a lot of content, very breathcom like directions, but it's not very tailored to my interest since I'm searching in a certain direction and I want to like switch the gear where the steer the information search focus in one line of work, right? And researchers like us if we want to use the Storm system for our own research, we may have inherent like latent research interest with a goal in mind and sometimes we want to see it freely explore and sometimes we have targeted search to want to switch the gear and guide all the", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4382361_ms_-_4457297_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4382361, "end_ms": 4457297}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4444839 ms - 4512181 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4444839 ms - 4512181 ms\n\nContent: may have inherent like latent research interest with a goal in mind and sometimes we want to see it freely explore and sometimes we have targeted search to want to switch the gear and guide all the language model agents to search in certain directions and many other users may like my thoughts kind of involve I changed my mind when the storm see some articles generated and when I see the Storm article I have more follow up questions come to my mind and there's a link that is very interesting. I want the language model agents with the Storm to talk more about it or like I know this relevant topic and you also include it in the discussion and in the final article. Those are all like questions. People may come when they use the Storm system and unfortunately Storm doesn't support this as like the only interaction is the human input and the language model will generate your full length article with like citations. So in the last part of the lecture we are going to talk about how to bring", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4444839_ms_-_4512181_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4444839, "end_ms": 4512181}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4500707 ms - 4573585 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4500707 ms - 4573585 ms\n\nContent: the only interaction is the human input and the language model will generate your full length article with like citations. So in the last part of the lecture we are going to talk about how to bring the human in the loop in the knowledge curation process. The first thing come to our mind is after the article generated using Storm we can ask questions, follow up questions and we can also edit the generated article in order to satisfy our own needs. Right? Another approach common approach is we chat with the ChatGPT or the language model chatbot and after chat we kind of ask it to summarize what we have chat. And that is another common approach. So on the left is the user initiative approach where users are actively driven, the information seeking process where users propose a question and the language model agent where the chat bot will answer your question. On the right is the system initiative. The storm will decide what perspectives the topics they're going to cover and users will", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4500707_ms_-_4573585_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4500707, "end_ms": 4573585}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4562169 ms - 4632397 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4562169 ms - 4632397 ms\n\nContent: the language model agent where the chat bot will answer your question. On the right is the system initiative. The storm will decide what perspectives the topics they're going to cover and users will only contrast the topic inputs. So can we have a mixed initiative system we're considering? Users may be uncertain about their own goals when they formulate their own like first search query. When we search for something very in depth, we may change our interest in the middle right and considering the status of the user's attention in the timing of the services, like using those kind of systems and how to allow efficient direct invocation and terminations of the system and the human. And how to how to effectively provide a mechanism for efficient agent user collaboration in order to have a good result. And how to maintain a working memory so that when the user input the system will know what users are talking about, it knows what it's searching for right now and how to plan the next step", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4562169_ms_-_4632397_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4562169, "end_ms": 4632397}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4619285 ms - 4692809 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4619285 ms - 4692809 ms\n\nContent: result. And how to maintain a working memory so that when the user input the system will know what users are talking about, it knows what it's searching for right now and how to plan the next step when the users engage in the system. So that's how we come up with the idea of collaborative storm, which we call cold storm. We compare with common paradigm of information search. On the left is we use the search engine where we have to manually search and read a lot of content and which is very cognitively heavy to read all of them. And in the middle is the storm where we generate a report, we read a report and manually revise it, we ask follow up questions and kind of revision in some way. And on the right is like the chatbot we just discussed and can we have like more interactive human in the loop way is on the bottom where like we simulate a situation where we we're talking about a topic in a round table discussion format where you are one participant and there are multiple other", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4619285_ms_-_4692809_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4619285, "end_ms": 4692809}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4678017 ms - 4747301 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4678017 ms - 4747301 ms\n\nContent: in the loop way is on the bottom where like we simulate a situation where we we're talking about a topic in a round table discussion format where you are one participant and there are multiple other language model participants. And in this roundtable discussion you can be an observer where you are silent and observing all those language model agents talking to each other and you are learning and catch up with the background. And when you have an interest shift where you want to propose something, you interrupt their speaking and say okay, I'm interested in this. Could you explain more about that? And I don't disagree your idea on this. Could you talk about more on what? So that is user actively engaging. One of the question we have is like when we are observing those language model agents talking to each other, how do they like enable us to explore the Domain of unknown unknowns. That's the area we are never thought of in the process. So the key idea is that we just proposed that we", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4678017_ms_-_4747301_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4678017, "end_ms": 4747301}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4731169 ms - 4808469 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4731169 ms - 4808469 ms\n\nContent: talking to each other, how do they like enable us to explore the Domain of unknown unknowns. That's the area we are never thought of in the process. So the key idea is that we just proposed that we let the users to take the observing job and sometimes actively engaging with the discussion in order when they are actually very interested in some discussion with the aspects. It kind of borrow from the actual human behavior when we are exploring those unknown topics. It's like the children or students are observing their parents or the professors experienced experts talking to each other. And you are in a room that's listening to them talking. And you might be very inspired by some of their discussions. And since the discussion may go in depth and very long, how to effectively track the discussion focus and how to reduce the user's cognitive load is another challenge. So we propose a collaborative discourse protocol which manage the whole mechanism of the roundtable discussion. Who will", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4731169_ms_-_4808469_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4731169, "end_ms": 4808469}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4790007 ms - 4858309 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4790007 ms - 4858309 ms\n\nContent: focus and how to reduce the user's cognitive load is another challenge. So we propose a collaborative discourse protocol which manage the whole mechanism of the roundtable discussion. Who will take the turn, what will happen next when the user engage, what language model will respond? And when the topics go to too boring or too niche, how to jump out of that to think out of the box. So due to the time limit, I will fast forward to the next slides. So the key challenge is that when we ask the language model agents to talking to each other, there are two intents they may take. Some of them will take the intent of question asking, like I will ask a new question, ask a follow up question. Some of them will answer the question like providing a potential answer to the question just proposed. Will elaborate on my previous answer about more details. But when we ask the language model to choose which intent they they will take, they almost always take the question answering, providing more", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4790007_ms_-_4858309_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4790007, "end_ms": 4858309}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4846187 ms - 4914829 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4846187 ms - 4914829 ms\n\nContent: Will elaborate on my previous answer about more details. But when we ask the language model to choose which intent they they will take, they almost always take the question answering, providing more details, causing the whole conversation go to a very niche, very boring or very repetitive behavior. So how do we effectively ask the question when we do the own search ourselves, when we're browsing or surfing the web, when we click to some link and we may accidentally find some interesting but not directly relevant thing and we want to click that link and then it will shift our searching focus. And inspired from this behavior, we kind of propose a new role, the moderator role in the roundtable discussion where the moderator will see the discourse history and re rank the unused information retrieved by other experts in the conversation history that is not directly relevant to its original query, but generally relevant to your topic and your own interests. And it will utilize those unused", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4846187_ms_-_4914829_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4846187, "end_ms": 4914829}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4902859 ms - 4974417 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4902859 ms - 4974417 ms\n\nContent: by other experts in the conversation history that is not directly relevant to its original query, but generally relevant to your topic and your own interests. And it will utilize those unused information and to think about how we can generate a thought provoking question and to shift the discussion focus and the question again, how to evaluate this? So we evaluate to compare with dual ablation studies like in our discussion we see like the roundtable takes three rows. One is language model agent or the experts who will do the QA with each other. The second one is the moderator we talk here and the third one is the user and we do the ablation study like control how many experts and how many moderators will make most sense and make the collaborative discourse most efficient. And we find that with a single moderator, like with like with just expert, one expert and one moderator will provide the most benefits because of the moderator will ask thought provoking questions. Here we also", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4902859_ms_-_4974417_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4902859, "end_ms": 4974417}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 4959223 ms - 5029047 ms", "content": "Title: CS224V Lecture 2 > Transcript > 4959223 ms - 5029047 ms\n\nContent: that with a single moderator, like with like with just expert, one expert and one moderator will provide the most benefits because of the moderator will ask thought provoking questions. Here we also conduct the automatic evaluation on the final report, the generated report based on the discourse history along also with the rubric rating along the relevant breadth, depth, novelty and information diversity along each axis. And Storm consistently outperform all the baselines. Here, along with the final article evaluation, we also evaluate the discourse itself. Does the discourse question answer in turn actually like be consistent and engaging with each other and thus like the question asking turn is thought provoking, that is automatic evaluation. Besides automatic evaluation, we conduct human evaluation where we invite 20 users in the wild to play with Cold Storm and to evaluate how does it compare to the search engine, how does it compare to the react chatbots, et cetera. And through", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_4959223_ms_-_5029047_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 4959223, "end_ms": 5029047}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 5015943 ms - 5087561 ms", "content": "Title: CS224V Lecture 2 > Transcript > 5015943 ms - 5087561 ms\n\nContent: evaluation where we invite 20 users in the wild to play with Cold Storm and to evaluate how does it compare to the search engine, how does it compare to the react chatbots, et cetera. And through the human evaluation, evaluators commonly agree that Cold Storm may bring up something that they never thought of and it is much less mentally taxing for the users to use. And this is the second to last slides. The key takeaways from this lecture is that we introduce how to generally build a language model powered systems that can that's inspired from the real human behaviors. Like in Storm, we borrow from how people do the writing by emphasis on the pre writing and Cold Storm we kind of emulate how people conduct the collaborative discourse like emulating the experience of students and child listening to their parents and professors. And we also emphasize in this lecture we talk a lot about the evaluations. Besides automatic evaluation, human evaluation really matters. In storm we invite 20", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_5015943_ms_-_5087561_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 5015943, "end_ms": 5087561}}
{"document_title": "CS224V Lecture 2", "section_title": "CS224V Lecture 2 > Transcript > 5073813 ms - 5140365 ms", "content": "to their parents and professors. And we also emphasize in this lecture we talk a lot about the evaluations. Besides automatic evaluation, human evaluation really matters. In storm we invite 20 experienced Wikipedia writers, right? In cold storm we invite 20 people in the Wild and both Storm and Cold Storm will be deployed in the wild, asking thousands of people to evaluate and provide their feedback. And for the assignments generally it divides into three sections. It will let all of you to use Storm and Cold Storm in the UI to perceive how it works. Then we'll dive deeper into the technical details of Storm and Cold Storm and play with actual coding there. It's not a lot of coding, but just to get you familiar with the overall language model system. And the last is how can you analyze the result of the Storm and Cold Storm and how do they inspire you to develop next level better language model powers knowledge curation system. Yeah. Thank you.", "block_metadata": {"id": "CS224V_Lecture_2_>_Transcript_>_5073813_ms_-_5140365_ms", "document_type": "transcript", "lecture_number": 2, "start_ms": 5073813, "end_ms": 5140365}}
