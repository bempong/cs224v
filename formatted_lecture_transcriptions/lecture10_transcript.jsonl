{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > A Day in the Life", "content": "Title: CS224V Lecture 10 > Chapter Summaries > A Day in the Life\n\nContent: Here. at 3:00, we talk about the most important things. Right at 3 o'clock when a lot of you are still not here. I hope you guys are watching on tv.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_A_Day_in_the_Life", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 51085, "end_ms": 113725}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Projects are due today", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Projects are due today\n\nContent: We're going to be doing projects and what we are going to do is that every week you have to put in a weekend report. TAs will look at them and work with you every week as you progress for the rest of the quarter. If you want to reach out to me, you are very well welcome to.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Projects_are_due_today", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 113845, "end_ms": 224965}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > How to improve and evaluate a chatbot in LLM", "content": "Title: CS224V Lecture 10 > Chapter Summaries > How to improve and evaluate a chatbot in LLM\n\nContent: With LLM the game has moved up. We are not just talking about improving some academic benchmarks. LLMs means that we can potentially deploy what we are talking about. What we're gonna talk about is how to improve and evaluate chatbots. And I hope this will be very useful for your project.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_How_to_improve_and_evaluate_a_chatbot_in_LLM", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 225665, "end_ms": 492031}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > ML and the LLM Planet", "content": "Title: CS224V Lecture 10 > Chapter Summaries > ML and the LLM Planet\n\nContent: People are struggling to get to a chatbot that actually have a good conversation. And you don't do an ML project without a dataset. If you want to get better results, you change the hyper parameters. And then fine tuning on LLMs.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_ML_and_the_LLM_Planet", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 492183, "end_ms": 626533}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > What does fine tuning do in the world of LLMs?", "content": "Title: CS224V Lecture 10 > Chapter Summaries > What does fine tuning do in the world of LLMs?\n\nContent: For some of the problems that we worked on, we discovered that GPT was better than the hand annotations. What does fine tuning do in the world of LLMs? The evaluation metrics. Everything has to be rethought for the problem you have at hand.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_What_does_fine_tuning_do_in_the_world_of_LLMs?", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 626589, "end_ms": 807065}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Fine-tuning the LMS", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Fine-tuning the LMS\n\nContent: The question is can I fine tune for specific tasks? Does it make sense to fine tune it on specific tasks. Our first experience of building something with LLM was The Autism project. It could honestly lead it to generalize worse across.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Fine-tuning_the_LMS", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 807185, "end_ms": 1118115}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Can we fine- tune a smaller model?", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Can we fine- tune a smaller model?\n\nContent: Some of the modern models are like mixtures of experts anyway, where they have like six or seven smaller models. Can we fine tune a smaller model? It's not just fine tuning, it's really distilling from the larger model to a smaller one.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Can_we_fine-_tune_a_smaller_model?", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 1118775, "end_ms": 1252745}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Small Learning Systems with LLM", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Small Learning Systems with LLM\n\nContent: When distilling, we do need a LLM teacher model. With more data created from the teacher, the smaller one gets better. Improving the training data doesn't get better with use on its own. What we recommend is that you take a two step approach.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Small_Learning_Systems_with_LLM", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 1253485, "end_ms": 1563009}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Don't evaluate LLM pipelines", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Don't evaluate LLM pipelines\n\nContent: First time we face the problem of assessing these LLM pipelines. The tendency is just collect the data set and evaluate and repeat. Don't think that you are ready for evaluation until you are happy with how it behaves. Once you know how GPT works, then you can guess what is gonna trip it.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Don't_evaluate_LLM_pipelines", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 1563137, "end_ms": 1693715}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > What Did It Take to Get to 98% Accuracy in Wiki Chat", "content": "Title: CS224V Lecture 10 > Chapter Summaries > What Did It Take to Get to 98% Accuracy in Wiki Chat\n\nContent: How do you get to 98% accuracy? We go through two routes. You do the search, summarize and filter from the Internet. Then you use LLM to generate sentences to generate response. But it takes a lot of steps.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_What_Did_It_Take_to_Get_to_98%_Accuracy_in_Wiki_Chat", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 1695375, "end_ms": 1733325}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > The Limits to the Process", "content": "Title: CS224V Lecture 10 > Chapter Summaries > The Limits to the Process\n\nContent: The Approach uses two pipelines, the reg and the generation. If the retrieved information is not good enough, it just makes it up. At the very beginning, don't worry about how many pipeline, how many stages you need. You can tune, optimize, distill, do whatever to improve efficiency.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_The_Limits_to_the_Process", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 1734105, "end_ms": 1913807}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > How to improve the LLM system", "content": "Title: CS224V Lecture 10 > Chapter Summaries > How to improve the LLM system\n\nContent: There are some very good open source retrieval models that we're just using. Once you retrieve, don't ask it to answer the question. No matter how good your retrieval is, there will be questions for which there are no answers. You have to be very careful with the static conversation data sets.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_How_to_improve_the_LLM_system", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 1913911, "end_ms": 2263005}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > So what can LLMs do and not do?", "content": "Title: CS224V Lecture 10 > Chapter Summaries > So what can LLMs do and not do?\n\nContent: How do you simulate conversations? In our case, we say the user is GPT4. The agent is the pipeline. Are LLM conversations similar to human conversations? We want to evaluate automatically if possible.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_So_what_can_LLMs_do_and_not_do?", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 2263045, "end_ms": 2473815}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Is it possible to automate the evaluation of conversationality?", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Is it possible to automate the evaluation of conversationality?\n\nContent: Is it possible to automate the evaluation of conversationality? You ask it. We don't know. We have guesses. Once humans can't scale, then you can use an LLM to judge and train it. Be careful.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Is_it_possible_to_automate_the_evaluation_of_conversationality?", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 2474115, "end_ms": 2574419}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Is GPT good enough for fact checking?", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Is GPT good enough for fact checking?\n\nContent: In experiments we showed that it was 83% in terms of checking for Factuality. But my concern is it doesn't identify all the claims. If my evidence retriever is wrong, then you cannot fact check it. We actually check manually for those ones.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Is_GPT_good_enough_for_fact_checking?", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 2574587, "end_ms": 3005229}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Wiki Chat vs. Atlas: Factuality and Conversationality", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Wiki Chat vs. Atlas: Factuality and Conversationality\n\nContent: Wiki chat combines IR and LLM. Everything is pretty high except for atlas. ATLAS loses on conversationality because they don't have the LLM generation pipeline. GPT4 is actually wrong more than 50% of the time and it's still got a lower rating than us.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Wiki_Chat_vs._Atlas:_Factuality_and_Conversationality", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 3005357, "end_ms": 3269061}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Wikipedia: Language barrier", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Wikipedia: Language barrier\n\nContent:  LLM can potentially remove the language barrier. Only 11% of the articles are in English on the Japanese Wikipedia. As you talk, you can discover things that you are interested in. It also expands your access to cultural knowledge. Do we need a chatbot?", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Wikipedia:_Language_barrier", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 3269173, "end_ms": 3437225}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Wikidata: Multilingual", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Wikidata: Multilingual\n\nContent: Is it easy to do multilingual? There are two building blocks that we use. LLM and retrievers both struggle with multilingual entities. The solution here is to tie them together using the Wikidata entity translations.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Wikidata:_Multilingual", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 3438005, "end_ms": 3812225}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Other multilingual use cases", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Other multilingual use cases\n\nContent: The opportunity here is actually to evaluate in the wild. And, and that is the next step that we have to do. There are a lot of very motivating examples for multilingual work. The two steps are improve and assess.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Other_multilingual_use_cases", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 3818205, "end_ms": 4009215}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Being persuasive in a chatbot", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Being persuasive in a chatbot\n\nContent: Can we create domain independent chatbots beyond providing facts such as persuasion? Persuasion is very useful for many things and here are just some of these examples. If you are the agent, as a human agent, would you be following a strict order? What would you do?", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Being_persuasive_in_a_chatbot", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 4010035, "end_ms": 4359939}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > Lemonade: A zero-shot agent", "content": "Title: CS224V Lecture 10 > Chapter Summaries > Lemonade: A zero-shot agent\n\nContent: Use LLM to generate responses to everyday persuasion tasks. Don't start with evaluation. Start with assessment. Here's the first example we tried. Can we just use Wiki Chat?", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_Lemonade:_A_zero-shot_agent", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 4359987, "end_ms": 4628007}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > The Intelligence of the Wiki Chat Pipeline", "content": "Title: CS224V Lecture 10 > Chapter Summaries > The Intelligence of the Wiki Chat Pipeline\n\nContent: The question is can we use simulated users and simulated evaluation. The first experiment showed that it works really well and perhaps too well. What do you have to think about when it is working too well?", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_The_Intelligence_of_the_Wiki_Chat_Pipeline", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 4628151, "end_ms": 5014265}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Chapter Summaries > The LLM vs. C#", "content": "Title: CS224V Lecture 10 > Chapter Summaries > The LLM vs. C#\n\nContent: The LLM based system development is different. You have to create the tough examples to assess and repeat. And you have to deploy and learn from the real world. We need to evaluate with users in the wild. For the persuasive act, the persuasive bot looks like we do have a zero shot bot.", "block_metadata": {"id": "CS224V_Lecture_10_>_Chapter_Summaries_>_The_LLM_vs._C#", "document_type": "chapter summary", "lecture_number": 10, "start_ms": 5014765, "end_ms": 5214865}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 51085 ms - 169009 ms", "content": "Title: CS224V Lecture 10 > Transcript > 51085 ms - 169009 ms\n\nContent: Here. All right, we're ready to start. 3:00, we talk about the most important things. Right at 3 o'clock when a lot of you are still not here. I hope you guys are watching on tv. Anyways, there's nothing due today because we have been busy working on the proposal and hearing all the presentations. And now the fun begins. We're going to be doing projects and what we are going to do is that every week you have to put in a weekend report and that means on Monday you're going to put in a report on what you have done the week before and then the TAs will look at them and work with you every week as you progress for the rest of the quarter. Okay? And everybody, you know, you have a group of two or three, everybody has to do their own thing. You say, who has done what that week? Okay, you have a question? There was two questions. We would do that on grade school, I suppose. We would do that on grade school, I suppose, I presume. Let me. I think that we actually want you to put it on the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_51085_ms_-_169009_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 51085, "end_ms": 169009}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 156407 ms - 227289 ms", "content": "Title: CS224V Lecture 10 > Transcript > 156407 ms - 227289 ms\n\nContent: a question? There was two questions. We would do that on grade school, I suppose. We would do that on grade school, I suppose, I presume. Let me. I think that we actually want you to put it on the Google Doc. I will give you the instructions for the weekend report soon in the specifics. And the other question I want to take us, that hasn't been released yet, right? We are working on it because the proposals just finished on Monday. So we will be working on that and you will find out who you get very, very soon. Okay. And I think that some people have reached out to me. I just spent an hour talking to a group and so as I said before, you know, you will have a TA for your project, a mentor for your project, and I will be talking to the mentors. So I know what's going on. And at the same time, if you want to reach out to me, you are very well welcome to. And I have my office hours on Monday after the class. Okay. And if that doesn't work, we can work out something. So we're going back to", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_156407_ms_-_227289_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 156407, "end_ms": 227289}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 213445 ms - 287875 ms", "content": "Title: CS224V Lecture 10 > Transcript > 213445 ms - 287875 ms\n\nContent: if you want to reach out to me, you are very well welcome to. And I have my office hours on Monday after the class. Okay. And if that doesn't work, we can work out something. So we're going back to the lectures and so today we're going to talk about how you're going to do your project. Basically people have started asking me, right, you use GPT and it seems to be working already, but it's not working completely now what do we do? So what we're going to talk about is how do we improve the chatbots and how do we evaluate it all this is very different now with LLMs. I mean everybody in the class has taken 124, 224 and you know how to work with machine learning, with machine learning projects. So what do we do here? So with LLM the game has moved up. We are not just talking about improving some academic benchmarks. LLMs means that we can now potentially deploy what we are talking about, what we are building. And if you want to deploy, it has to be close to what we can build with human", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_213445_ms_-_287875_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 213445, "end_ms": 287875}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 272071 ms - 343113 ms", "content": "Title: CS224V Lecture 10 > Transcript > 272071 ms - 343113 ms\n\nContent: some academic benchmarks. LLMs means that we can now potentially deploy what we are talking about, what we are building. And if you want to deploy, it has to be close to what we can build with human workers. Okay? So we're looking for the performance that is close to the human performance. It is not about getting the academic benchmarks. And we are improving from 40% to 50% and say state of the art, because 50% doesn't do you any good. 60% is not good enough either. 70%. If you're doing a chatbot at 70%, how many turns are you going to get? But everybody is now used to LLMs. You can actually have good conversations with GPT. And we are now going after the more like something that can be deployed. And what we have been talking about in the last few classes is that we're just gonna emulate what humans do. Don't take shortcuts, don't just get the answers from GPT, but if you do what humans do, break them down, we have a much better shot at getting the human performance. Okay? That's the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_272071_ms_-_343113_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 272071, "end_ms": 343113}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 330993 ms - 396903 ms", "content": "Title: CS224V Lecture 10 > Transcript > 330993 ms - 396903 ms\n\nContent: what humans do. Don't take shortcuts, don't just get the answers from GPT, but if you do what humans do, break them down, we have a much better shot at getting the human performance. Okay? That's the basics. It's chatbot in practice. So what we're gonna talk about is how to improve and evaluate chatbots. Okay? And a lot of papers that you read will just tell you, this is our system. And of course it works. Otherwise the paper is not published and then you see the final product. You don't actually hear how they get there, but everybody here has to do a project and you have to get there. And that's the focus for this class. Okay? What are the considerations in creating the product? And I hope that this will be very useful for your project. And so the way I'm going to do this class and the rest of the classes is that I'm going to focus on your understanding of the material. Okay? There's not a lot of facts to memorize from this class, but if you understand it, you can apply it to your", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_330993_ms_-_396903_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 330993, "end_ms": 396903}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 385599 ms - 446207 ms", "content": "Title: CS224V Lecture 10 > Transcript > 385599 ms - 446207 ms\n\nContent: rest of the classes is that I'm going to focus on your understanding of the material. Okay? There's not a lot of facts to memorize from this class, but if you understand it, you can apply it to your projects and future projects. So I will be pausing. I'm going to ask you questions. This is why I want you to come to class and we're going to do it together interactively, okay? So you have to think with me because the only thing you get out of it is what you think. It's not what I just said, okay? It's kind of meaningless. You can read papers. You don't have to come to lectures. Okay? So that's what we want. If you brought your paper, please put it up. And for the rest of the time we will be doing lectures and we will try to make it interactive. Okay. Some. If you don't bring the paper, we'll fix it next time. Okay. Or maybe I have learned your name. That would be even better. So we're going to be using wiki chat as an example. It was the first time we had to deal with this problem and", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_385599_ms_-_446207_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 385599, "end_ms": 446207}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 433575 ms - 502103 ms", "content": "Title: CS224V Lecture 10 > Transcript > 433575 ms - 502103 ms\n\nContent: fix it next time. Okay. Or maybe I have learned your name. That would be even better. So we're going to be using wiki chat as an example. It was the first time we had to deal with this problem and we had to do a lot of. We made up a lot of things. And since then we have built two more chats along the same lines. One is the multilingual chat. I hear various people are looking at multilingual systems, so I'm going to talk about that. And another one is that let's go beyond facts. Let's talk about getting, getting something, changing people's mind and what does it mean to create a persuasive bot? Okay, so this is all beyond the wiki chat, but they're very similar, as I will show you. And what I will do is that after we describe how we improve and evaluate chatbots, we'll take the same lessons and apply it to the next two, the two topics here. Okay, so that's the plan for the course. Okay, good. Yeah. All right, so two weeks ago, when we were talking about how we build the wiki chat, this", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_433575_ms_-_502103_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 433575, "end_ms": 502103}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 484131 ms - 555005 ms", "content": "Title: CS224V Lecture 10 > Transcript > 484131 ms - 555005 ms\n\nContent: and apply it to the next two, the two topics here. Okay, so that's the plan for the course. Okay, good. Yeah. All right, so two weeks ago, when we were talking about how we build the wiki chat, this is Sina's term. Sina is my student who's been working on this very actively. And Sina called this the LLM planet. And I'd like to keep that, have you guys keep that in mind. It's not the same as before. So let's take a look at LLP without LLMs. This is what we normally, you know, this is, I think, what you hear in 224, right? And so they're usually solving more, you know, they're solving the easier problems, classification being one of the easiest. But then there's semantic parsing and so forth. But all in all, you know, people are just struggling to get to a chatbot that actually have a good conversation. But they're more the easier problems. And you don't do an ML project without a dataset. That has always been the rule. The first thing is, which dataset do you have? Because I have to", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_484131_ms_-_555005_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 484131, "end_ms": 555005}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 541816 ms - 615557 ms", "content": "Title: CS224V Lecture 10 > Transcript > 541816 ms - 615557 ms\n\nContent: conversation. But they're more the easier problems. And you don't do an ML project without a dataset. That has always been the rule. The first thing is, which dataset do you have? Because I have to train, validate and evaluate and test, right? Def. I mean train, def and test. And if you want to get better results, you change the hyper parameters, you get more data, you do fine tuning on LLMs. So that's the process. So you start with the data and you start training. And then you see the numbers improve and then, and then you say success. Okay, my numbers are the best I can do and better than the previous sets of numbers. Is that enough? Is that enough? No, because huh, it's not useful. It doesn't mean that it is useful, even if it is state of the art. Okay, and then the evaluation metrics. For simple things like classification, you can talk about the result and then you do the F1 precision and recall on the gold answer. And if it is more pros, you will have the blue score and the rouge", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_541816_ms_-_615557_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 541816, "end_ms": 615557}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 601003 ms - 670431 ms", "content": "Title: CS224V Lecture 10 > Transcript > 601003 ms - 670431 ms\n\nContent: For simple things like classification, you can talk about the result and then you do the F1 precision and recall on the gold answer. And if it is more pros, you will have the blue score and the rouge score. And those are the metrics that we used before. We're not going to be using it. So I'm not going to even revise, I'm not going to talk more about it and let's just move on. Right, so now with nlp, with LLMs, we're talking about harder problems. We are writing full essays and we are doing copilot, we're changing code, we're doing a lot of things. So start with a dataset was like how to create a dataset. What is the dataset? That's one of the hardest questions. To get better results, what do we do? We change the prompts, we change the pipeline. We saw wiki chats, how we have done that. And then you say fine tune with more data. What does fine tuning do? Okay, so we will return to this question. What does fine tuning do in the world of LLMs? The evaluation metrics. What evaluation", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_601003_ms_-_670431_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 601003, "end_ms": 670431}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 655437 ms - 726749 ms", "content": "Title: CS224V Lecture 10 > Transcript > 655437 ms - 726749 ms\n\nContent: And then you say fine tune with more data. What does fine tuning do? Okay, so we will return to this question. What does fine tuning do in the world of LLMs? The evaluation metrics. What evaluation metrics can we do? F1 on goal data? It turns out that for some of the problems that we worked on, we discovered that GPT was better than the hand annotations. I was doing a semantic parsing project. The annotations were done by my PhD students. These are not just people that you can hire easily off the street. When we get the results from GPT, we did the comparison and say, oh my goodness, GPT results are different and as a matter of fact better because humans are lazy. Even the students say this is good enough. This is what you would expect roughly. But GPT actually went beyond that at the beginning in order to use the dataset, we kept saying, ok, let's tell GPT the rules for the dataset. And after we do that a few rounds and we say, why on earth are we doing that? It's already better and", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_655437_ms_-_726749_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 655437, "end_ms": 726749}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 715213 ms - 772437 ms", "content": "Title: CS224V Lecture 10 > Transcript > 715213 ms - 772437 ms\n\nContent: in order to use the dataset, we kept saying, ok, let's tell GPT the rules for the dataset. And after we do that a few rounds and we say, why on earth are we doing that? It's already better and you're trying to dumb it down. It's like, don't do that. Okay, I don't know. There's a couple of papers that we wrote that says, yeah, if you look at the old dataset, it looks like we are worse. But don't because there were papers written to say LLMs are not good. Enough. But if you. Because that is because people actually don't look at the results. I kept telling my students, you have to look at the results. But the minute you look at the results you realize that oh yeah, the performance seems bad, but it is actually better than the manual annotations. So this is very, very important. Anybody working on the academic data sets, you get some numbers, you do the error analysis and then you decide who is better and then you continue. Don't waste your time trying to tune for the gold answers that", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_715213_ms_-_772437_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 715213, "end_ms": 772437}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 760701 ms - 828023 ms", "content": "Title: CS224V Lecture 10 > Transcript > 760701 ms - 828023 ms\n\nContent: on the academic data sets, you get some numbers, you do the error analysis and then you decide who is better and then you continue. Don't waste your time trying to tune for the gold answers that were given in the data set. Academic data sets. Okay, so keep, really keep that in mind. And the other, these other scores are totally meaningless. GPT is so much better. You know, finding if they are using the same words in, you know, the common words and stuff like that. It just doesn't make sense. So now we are off, we don't have a data set, we are changing the problems and pipeline. We're not, you know. And then what are the evaluation metrics? Everything has to be rethought for the problem you have at hand. Okay, let's talk about fine tuning. So fine tuning, there are people who fine tune the largest models and that is very, very useful. There's fine tuning to get alignment, fine tuning to improve its instruction. Following these are all general skills and fine tuning for that has turned", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_760701_ms_-_828023_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 760701, "end_ms": 828023}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 814329 ms - 893903 ms", "content": "Title: CS224V Lecture 10 > Transcript > 814329 ms - 893903 ms\n\nContent: largest models and that is very, very useful. There's fine tuning to get alignment, fine tuning to improve its instruction. Following these are all general skills and fine tuning for that has turned out to be really important. Fine tuning to understand what code looks like, what general medicine terminology looks like, that is all fine. The question is can I fine tune for specific tasks? Okay, can I, should I fine tune for classification task, for semantic tasks, semantic parsing task and so forth? What do you guys think? Does it make sense to fine tune it on specific tasks? Because that's what we do, right in nlp. What do you guys think? Should we, when should we? And when should we not? Any question? Anybody? Yes, please. I don't think this is the answer you're looking for, but maybe the answer I'll give is yes, if these tests are not embedded into the LM already. But maybe in the future we would hope that all these evaluates like the LMS are capable of zero shot or something like", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_814329_ms_-_893903_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 814329, "end_ms": 893903}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 878511 ms - 957947 ms", "content": "Title: CS224V Lecture 10 > Transcript > 878511 ms - 957947 ms\n\nContent: answer I'll give is yes, if these tests are not embedded into the LM already. But maybe in the future we would hope that all these evaluates like the LMS are capable of zero shot or something like that. So lots of things that it can do. Zero shot, right? Is that what you mean? Or few shot? You don't want to if you don't need to. The question is the performance is not good enough. For example, should we do fine tuning? Well, I think that that's an interesting point. If I don't need to, why should I? I want to tell you our first experience of building something with LLM. It was The Autism project. We were asking GPT to respond to an autistic person's response to evaluate whether it is empathetic or not. We did this experiment. Actually, this was done by a high school student working with me as an intern. This high school student is very exceptional. He already knows how to do machine learning. He's basically got the skills of 224N. I said, why don't you do this? Let's give it a try.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_878511_ms_-_957947_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 878511, "end_ms": 957947}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 944659 ms - 1011059 ms", "content": "Title: CS224V Lecture 10 > Transcript > 944659 ms - 1011059 ms\n\nContent: me as an intern. This high school student is very exceptional. He already knows how to do machine learning. He's basically got the skills of 224N. I said, why don't you do this? Let's give it a try. Then he was changing the prompt and eventually he says, I got it. It is now giving a good response. I'm ready to fine tune. And I said, what are you fine tuning it for? GPT is already giving you the right answers. You are using its result to fine tune itself. What is that for? But this is the mindset a lot of people did 224n equivalent, including this high school student. They said, I need to have training data I generated because I can't really do it by hand if the problem is too hard. I got the training data. Let's find you. And I said, what does it mean for you to fine tune? The data is generated by GPT4 itself. The minute you fine tune, what does that mean? Okay, what do you think will happen if you fine tune with that data on GPT4? First of all, it's very expensive. GPT4 is a big", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_944659_ms_-_1011059_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 944659, "end_ms": 1011059}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 997083 ms - 1058870 ms", "content": "Title: CS224V Lecture 10 > Transcript > 997083 ms - 1058870 ms\n\nContent: generated by GPT4 itself. The minute you fine tune, what does that mean? Okay, what do you think will happen if you fine tune with that data on GPT4? First of all, it's very expensive. GPT4 is a big model. Okay, you fine tune with it. Suppose you can do that. What do you expect to get out of doing that? Yeah. Is it possible that what you're trying to solve for is the fact that the student in this example had to sort of like manually search and iterate on the prompts until they found a good prompt. So he did. He found a good prompt. And then like fine tuning would mean like any other model can just use these examples and we don't need to find another prompt. Ah, I'm talking about improving that model. Oh, you prove it. That one. Okay, let's start with that. I'm just talking about fine tuning that model. Does it make sense to fine tune that model? I'll say no. I really have. You say no. And what do you think happens if you do fine tune with that model? Yeah, it might improve your", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_997083_ms_-_1058870_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 997083, "end_ms": 1058870}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1047101 ms - 1112063 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1047101 ms - 1112063 ms\n\nContent: tuning that model. Does it make sense to fine tune that model? I'll say no. I really have. You say no. And what do you think happens if you do fine tune with that model? Yeah, it might improve your performance on that like specific set. It might improve your performance for that specific set. But it could honestly lead it to generalize worse across, like all other tasks. Bingo. Okay. You are giving it random problems and it is already solving it. Now you say, I take a sample of this and I want to replicate that. How good is your sample and why is it going to Improve anything because it is already doing a good job. You're taking itself to fine tune itself. Most likely your sample is not general enough for whatever sample it is and it is most likely to just make it worse. Okay, so be careful. It's like this is a novice mistake. I would say we don't do that. You have to understand why you are fine tuning, that is fine tuning on the large model. Because this is at the beginning, we don't", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1047101_ms_-_1112063_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1047101, "end_ms": 1112063}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1097749 ms - 1157675 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1097749 ms - 1157675 ms\n\nContent: It's like this is a novice mistake. I would say we don't do that. You have to understand why you are fine tuning, that is fine tuning on the large model. Because this is at the beginning, we don't know what we're doing. And that was a reasonable suggestion, but we said no, no, no, don't do that. It's a waste of cycles. So the second thing is, can we fine tune a smaller model? That was brought up. So when we fine tune a smaller model, we talked about it already in the last class and that is that we take the large model and potentially a pipeline of large models and we generate a lot of inputs and outputs and then you take that and you fine tune a smaller model. And that's called distilling. It's not just fine tuning, it's really distilling from the larger model to a smaller model, from a teacher model to a student model. Yes. Some of the modern models are like mixtures of experts anyway, where they have like six or seven smaller models. Would it ever make sense to like if those models", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1097749_ms_-_1157675_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1097749, "end_ms": 1157675}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1145755 ms - 1214619 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1145755 ms - 1214619 ms\n\nContent: model to a student model. Yes. Some of the modern models are like mixtures of experts anyway, where they have like six or seven smaller models. Would it ever make sense to like if those models are specified to some skill or some type of question, would it make sense to fine tune like just one of the expert models within the larger model? So what is interesting about the mixture of models is that you're fine tuning on the whole task and it figures out what those specialized models are. I don't know how to fine tune each of these models and whether you're going to make it work better or worse, it is most likely not better. The reason, one way of thinking about what mixture of models, what mixture of experts do, is that you know that in these large models we are doing these dense matrix multiplies. But in practice the neural network is not so dense. It's not like all connected. And when you have a mixture of model, you are encouraging it to kind of create dense sub models when together", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1145755_ms_-_1214619_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1145755, "end_ms": 1214619}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1199735 ms - 1272551 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1199735 ms - 1272551 ms\n\nContent: But in practice the neural network is not so dense. It's not like all connected. And when you have a mixture of model, you are encouraging it to kind of create dense sub models when together create kind of like a sparse model. Okay, but you wanted to go figure out what the sparsity is. Where the sparsity is based on training, not based on you pushing the data to each of the subsets because you don't know what these mixture of models are doing. That is the beauty of the concept of expert of models is because you are really relying on deep training, deep learning to design on the architecture. Anytime we go and design that you are not based, it's not data driven. So these are two different concepts. A good question. Now we're talking about distilling. So what happens to distilling? So first of all, when we do that, we do need a LLM teacher model because it is like if you don't have a teacher model, you just have a hard time generating enough data for the kind of problems we are trying", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1199735_ms_-_1272551_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1199735, "end_ms": 1272551}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1259181 ms - 1326165 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1259181 ms - 1326165 ms\n\nContent: of all, when we do that, we do need a LLM teacher model because it is like if you don't have a teacher model, you just have a hard time generating enough data for the kind of problems we are trying to solve. So it's not like you can skip straight to the smaller model to begin with. Okay, so keep that in mind. And then it is faster, cheaper and of course, if you run it on your own machine, it is more privacy. What about the performance? What do you expect of the performance? Is it better than. Well, it's probably better than the original small model. Is it better than the big model? Do you expect it to be better than the big model? Not at the beginning. Not at the beginning. I would say the usage, it might get better. How does it get better with usage? Well, with more data created from the teacher, the smaller one gets better. But then you have to compare it with so is it still better than the teacher? So if the teacher sees the same data, if it is, I mean, if you're adding training", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1259181_ms_-_1326165_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1259181, "end_ms": 1326165}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1311683 ms - 1376347 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1311683 ms - 1376347 ms\n\nContent: teacher, the smaller one gets better. But then you have to compare it with so is it still better than the teacher? So if the teacher sees the same data, if it is, I mean, if you're adding training data to it, and I mean, you got an interesting point because sometimes you just cannot afford the large model to run at scale. So you're using the smaller model and as a result, if you're able to get user feedback or something so that you can improve on the training data, then you can improve it, but you've not Improving the training data doesn't get better with use on its own. The feedback is what makes it. Well, if it doesn't feedback, it's the same model as before. Okay, so the answer without that concept of doing better data, is it better or worse than the big model? Yes. Is it better, worse just because it's smaller? Yes, it is usually just worse. I mean, you're just learning from the teacher and the teacher is a sample of what the teacher can do. It is not the full teacher's strength,", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1311683_ms_-_1376347_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1311683, "end_ms": 1376347}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1366651 ms - 1433027 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1366651 ms - 1433027 ms\n\nContent: just because it's smaller? Yes, it is usually just worse. I mean, you're just learning from the teacher and the teacher is a sample of what the teacher can do. It is not the full teacher's strength, but it makes a huge difference. It sees a big improvement, but it is usually still worse than the teacher model. When you don't think that your system is working well, don't just go, fine, tune it. That's not a typical solution. Maybe there are places where it is true, but in general this is what we are expecting. So now what do you do? I cannot just get more. First of all, I don't know how to get more data without the teacher model. Then after that, if the teacher is not doing well. I mean, it's just going to be all worse. So how do we improve an LLM based system? That's the question we have to face. So the old idea is that you collect data, you train, you tune in hyperparameters, gone, throw it out of the window. So what do we do? So what we see is that what we recommend is that you take", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1366651_ms_-_1433027_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1366651, "end_ms": 1433027}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1416913 ms - 1487587 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1416913 ms - 1487587 ms\n\nContent: face. So the old idea is that you collect data, you train, you tune in hyperparameters, gone, throw it out of the window. So what do we do? So what we see is that what we recommend is that you take a two step approach. The first one is that you have to improve the system by using tough examples. Okay? Don't just say here's a random sample and get it to and try it and say, oh, it looks pretty good. It's easy to make it look very good. It's hard to make it not look good. Your work is to make it not look good because otherwise you're not going to improve it. So you have to come up with the tough examples. It's kind of strange. It's a little bit like the good old software engineering basically. Okay, you have to prepare the edge cases, the hard cases to test out your software model. And it's very manual. And that doesn't sound like a really good idea. But what is surprising here is that LLM is such a, we kind of learned how to use work with LLM. It is so well rounded, okay. In a sense it", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1416913_ms_-_1487587_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1416913, "end_ms": 1487587}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1469573 ms - 1541681 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1469573 ms - 1541681 ms\n\nContent: very manual. And that doesn't sound like a really good idea. But what is surprising here is that LLM is such a, we kind of learned how to use work with LLM. It is so well rounded, okay. In a sense it doesn't have these weird cliffs when it fails. It's pretty, you know, you can, you can almost see what you need to do to fix it or why it is not working. And if you can, you know, handle the things that you observe, you can actually get generality. The generality of LLM is, makes it possible for you to just try enough cases. And after you get the cases, the tough cases done well and you know, and fix what you can observe with a handful of these examples, it actually has a pretty good general behavior after that. Okay. Because it has a generality baked into the system. But you have to be good at coming up with the tough cases. Okay, so this is what we are doing right now. But of course you can tune it a little bit by doing some exponential search on the combination of problems and stuff", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1469573_ms_-_1541681_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1469573, "end_ms": 1541681}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1529395 ms - 1590925 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1529395 ms - 1590925 ms\n\nContent: at coming up with the tough cases. Okay, so this is what we are doing right now. But of course you can tune it a little bit by doing some exponential search on the combination of problems and stuff like that. But this is by and large a, you know, it's like an, you know, you're learning, you're getting the insights of what LLMs are good and bad at. That's the first step. And after that you have to do evaluation because the problem is so hard. We're going to use LLMs to help you do the evaluation. But you have to be very careful how you use LLMs. Okay, so this is the Key idea we're going to discuss in this class. So we're going to use Wiki Chat as a case study. It's really the first time we face the problem of assessing these LLM pipelines. The tendency is just collect the data set and evaluate and repeat. My team, my research team, not the high school student, they would keep asking me, it's like, okay, can we go and evaluate now? Can we go evaluate now? And I said, look, I already", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1529395_ms_-_1590925_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1529395, "end_ms": 1590925}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1577845 ms - 1639809 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1577845 ms - 1639809 ms\n\nContent: and repeat. My team, my research team, not the high school student, they would keep asking me, it's like, okay, can we go and evaluate now? Can we go evaluate now? And I said, look, I already know that it has a problem. I can find you one example where it does really badly. Just one example. But I know I can generate many more examples just like that. If you're not fixing that problem, why evaluate? Okay? It's not about whether that question has a high distribution in the real world or not because you know it is going to fail whenever you see that problem in practice. Because we are going for something that can be used. So don't think that you are ready for evaluation until you are happy and satisfied with how it behaves on the things that you can try, okay? If when you try it, it doesn't work, don't think about evaluation, okay? This is kind of like a tendency for people who have been studying ML and they just say, well, let's just evaluate what percentage it is. It's a matter of", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1577845_ms_-_1639809_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1577845, "end_ms": 1639809}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1628545 ms - 1696775 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1628545 ms - 1696775 ms\n\nContent: work, don't think about evaluation, okay? This is kind of like a tendency for people who have been studying ML and they just say, well, let's just evaluate what percentage it is. It's a matter of your distribution. All right? So don't do that. It's like you fix the things that you can, you see are difficult because there are other cases that you don't even see. All right? So this is just something that keep that in mind. When you finish your project, you show it to me and I put in a well designed tough case and it fails. Then it fails, okay? It's not about the percentages. All right? So as a matter of fact, this is what happens to our project. My group will come back and say, oh, it works. And I say, how about this? And it breaks. And then they would go back and they come back and I say, how about this? You kind of have to. Once you know how GPT works, then you can guess what is gonna trip it. The more experience you have, the better it is that you are at it. So we're gonna talk about", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1628545_ms_-_1696775_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1628545, "end_ms": 1696775}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1680989 ms - 1747671 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1680989 ms - 1747671 ms\n\nContent: about this? You kind of have to. Once you know how GPT works, then you can guess what is gonna trip it. The more experience you have, the better it is that you are at it. So we're gonna talk about Wiki Chat. I just wanted to remind everybody. What did it take to get to the 98% accuracy? We go through two routes. You do the search, summarize and filter from the Internet and then you use LLM to generate sentences to generate response because we want the conversationality. And then we filter out the bad claims and then we put them back together. We tell the LLM Put it together, okay? Don't try to answer the question. That's the most important thing we learned. And then you refine it and you get a good answer. But it takes a lot of steps. So I want to tell you a little bit about the process itself. I don't want, you know, this is why we have a lecture, right? Tell you what happened rather than the end result. So what happened is that you have the high level design. It was done in the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1680989_ms_-_1747671_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1680989, "end_ms": 1747671}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1736353 ms - 1807165 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1736353 ms - 1807165 ms\n\nContent: itself. I don't want, you know, this is why we have a lecture, right? Tell you what happened rather than the end result. So what happened is that you have the high level design. It was done in the class. And the first thing we tried was to do generation first only generation. You say, let's generate and just fact check and eliminate claims. And within a couple of trials of different examples, we say, oh, there is no way you can solve the. You cannot answer any new questions like Queen Elizabeth just died. There is nothing you can do to get LLM to generate the answer that you can do elimination on. You have to do reg. So we said, okay, we need two pipelines, the reg and the generation. You have to do generation because otherwise the REG is very, very boring. And sometimes it just misses the connections. And then. So that's a high level. So the next level is that, okay, we got this pipeline, let's start using it. And we discovered that, man, if you just use rag, it just hallucinates", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1736353_ms_-_1807165_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1736353, "end_ms": 1807165}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1792293 ms - 1864587 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1792293 ms - 1864587 ms\n\nContent: the connections. And then. So that's a high level. So the next level is that, okay, we got this pipeline, let's start using it. And we discovered that, man, if you just use rag, it just hallucinates like crazy because that's what happened to a lot of commercial products. Here is the retrieved information. You ask for the answer. If the retrieved information is not good enough, it just makes it up. We have to kind of kill that. Okay? So that's what we observe. And we say, okay, let's do that and make it say, I don't know. And then it gets to the next level. And then I would come up with these hard cases and I would talk about recent sports and things like that. That has history and it just messes up the events in time. Okay? It confuses the two Super Bowls and stuff like that. And time is so critical. So we fixed that problem. And what was funny was that I still remember we were just doing it every week. It's like, go back, go home, go back, try something else. And then one time they", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1792293_ms_-_1864587_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1792293, "end_ms": 1864587}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1851035 ms - 1920207 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1851035 ms - 1920207 ms\n\nContent: so critical. So we fixed that problem. And what was funny was that I still remember we were just doing it every week. It's like, go back, go home, go back, try something else. And then one time they went back and they took two weeks. They came back and they said, okay, let's try this one. They really worked on each of these pipelines and it is well designed. And it just started to work. And then it just works. It's not like, wow, it works. But then we still have a lot of issues. It just started to work on all kinds of things and we tried it. So this makes me feel like optimistic that this process actually can finish with this Approach. And one thing that I learned from this is that at the very beginning, don't worry about how many pipeline, how many stages you need, okay? You get it to work first and then you can tune, optimize, distill, do whatever to bring the performance to improve the efficiency. Yeah, question. I was just curious, what specific small, well designed steps did you", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1851035_ms_-_1920207_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1851035, "end_ms": 1920207}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1906199 ms - 1973475 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1906199 ms - 1973475 ms\n\nContent: first and then you can tune, optimize, distill, do whatever to bring the performance to improve the efficiency. Yeah, question. I was just curious, what specific small, well designed steps did you guys improve upon? For example, with rag, was it the embeddings that were the issues or the chunking or like the querying that was causing each. How do you improve retrieval? Or yeah, like which specific steps are possible? We have not time to improve retrieval. We just say that once you retrieve, don't ask it to answer the question. Because no matter how good your retrieval is, there will be questions for which there are no answers and it will hallucinate. So you. So we can talk about how to improve retrieval later. Okay, so let's just continue with what we can do without changing the guts of retrieval. Okay then. And today there are some very good open source retrieval models that we're just using. There are ways to improve it. We can discuss that later. Okay, so it's not just. The point", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1906199_ms_-_1973475_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1906199, "end_ms": 1973475}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 1958123 ms - 2024369 ms", "content": "Title: CS224V Lecture 10 > Transcript > 1958123 ms - 2024369 ms\n\nContent: retrieval. Okay then. And today there are some very good open source retrieval models that we're just using. There are ways to improve it. We can discuss that later. Okay, so it's not just. The point here is that there are just questions for which there are no answers. You still have to be not hallucinating. All right, so that was the process. And this doesn't count the evaluation. This is to getting it to work. The point here is that after we, we are happy with it, the evaluation was good. It is not like I go through the evaluation and keep tweaking my system. Okay, you, this is what you hope for is that the evaluation is, I mean the assessment, the improvement is done and then it's good enough. And then you just evaluate and then you just report as opposed to this iterative evaluation and proof process. All right, but it takes time. And you know, this is the scientific method. Even though you think it is good, you have to do the evaluation to convince yourself and others. The 8, 0", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_1958123_ms_-_2024369_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 1958123, "end_ms": 2024369}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2011633 ms - 2081971 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2011633 ms - 2081971 ms\n\nContent: and proof process. All right, but it takes time. And you know, this is the scientific method. Even though you think it is good, you have to do the evaluation to convince yourself and others. The 8, 0 datasets do not work. A lot of times we expected these numeric benchmarks is like F1s and so forth and blue score and stuff like that. It doesn't work. So let's look at the wiki chat as an example. You have to look at the details before you can understand the general principles. There were chatbots in the past, I think I showed you some example before of other chatbots. We try it, it doesn't work, but they reported that it works. The reason is that a lot of times they just crowdsource the conversations. Any time you crowdsource a conversation, anytime you crowdsource the conversation. They are something that GPT knows very well. So we said that one of the biggest problems with GPT is that they don't know how to handle tail or expert topics. But I cannot find crowdsourced workers that can", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2011633_ms_-_2081971_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2011633, "end_ms": 2081971}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2068339 ms - 2137555 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2068339 ms - 2137555 ms\n\nContent: something that GPT knows very well. So we said that one of the biggest problems with GPT is that they don't know how to handle tail or expert topics. But I cannot find crowdsourced workers that can talk about tail and expert topics. Okay, you just can't find them. So how do you evaluate that? If you cannot evaluate that, then you are just saying, oh, it's as good as GPT. And that would be just totally the wrong conclusion. So that's the first problem is creating a data set. The second one is that evaluation conversations in the past, they were static. You always wonder about whether the data set has been used in training. And a lot of datasets have been used in training. And it's just nasty. They get outdated very quickly. If you want to talk about a new topic in a couple of years, that dataset is no longer talking about new topic. Right? And so, for example, wizard of Wikipedia is a very well used data set. It is in the pre training data. And so whatever new benchmarks will be used", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2068339_ms_-_2137555_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2068339, "end_ms": 2137555}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2118891 ms - 2196729 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2118891 ms - 2196729 ms\n\nContent: dataset is no longer talking about new topic. Right? And so, for example, wizard of Wikipedia is a very well used data set. It is in the pre training data. And so whatever new benchmarks will be used in the future. So you have to be very careful with the static conversation data sets. So what do we do? How do you evaluate tail questions? Humans don't know it. How do you evaluate LLM conversations? So in the past we would just say, look, here's a conversation up to this point and what is the next step that you're supposed to take? And then you do it for step 1, 2, 3, 4, 5, right? Turning them into each individual tests. But in the LLM world, you have real conversations, each conversation for each model. They will talk about different things. All the dialogue cannot be compared turn by turn because they just go totally differently in different ways. It really doesn't make sense to say let's standardize on the first N conversations to test the n + one term, no such thing. Then you cannot", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2118891_ms_-_2196729_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2118891, "end_ms": 2196729}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2182265 ms - 2250585 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2182265 ms - 2250585 ms\n\nContent: because they just go totally differently in different ways. It really doesn't make sense to say let's standardize on the first N conversations to test the n + one term, no such thing. Then you cannot just get a numeric result to measure the success. And then in the end you still have to wonder, how do you minimize human effort, cost and potential errors? One of the things that we have learned is that a lot of the dialogue data sets, they were full of errors. So there is this very, very famous data set called multiwas. It was annotated four or five times. It includes one round with Google, one round of Amazon, and it's still full of errors. Because it is really hard to annotate something like a dialogue. Okay? So it is not only expensive to annotate, it is actually hard to get an accurate enough dialogue data set. So all this is just hugely difficult. So we're going to leverage LLM as much as we can. Okay? Because the problem is now complicated. You are doing what LLMs can do, but as", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2182265_ms_-_2250585_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2182265, "end_ms": 2250585}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2234657 ms - 2309291 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2234657 ms - 2309291 ms\n\nContent: enough dialogue data set. So all this is just hugely difficult. So we're going to leverage LLM as much as we can. Okay? Because the problem is now complicated. You are doing what LLMs can do, but as it goes up, we will try to use LLMs to evaluate as much as you can and help do the experiment as much as you can. All right? But you have to be careful. All right? So what can LLMs do and not do? So one thing we can do is to pretend that they are users. How do you can simulate the conversations? So in our case, we say the user is GPT4. The agent is the pipeline. We try to evaluate, and we want to be able to evaluate the popular pages, the less popular pages, and the recent events. So we can pick the topic. And GPT is good at. If you give it a topic on tail, it's actually good at following a conversation on the tail, okay? And even on new, they can follow it and they can talk, ask reasonable questions and so forth. Okay? So now it is better than your crowdsourced workers. Okay, so then the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2234657_ms_-_2309291_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2234657, "end_ms": 2309291}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2294243 ms - 2367281 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2294243 ms - 2367281 ms\n\nContent: on the tail, okay? And even on new, they can follow it and they can talk, ask reasonable questions and so forth. Okay? So now it is better than your crowdsourced workers. Okay, so then the question is, are LLM conversations similar to human conversations? So now we have to go through this process. And again, when we do this work, remember at the beginning you are just trying to improve the system before you worry about evaluation. And the way for us to get to the LLM conversations is that if you just ask GPT, it's like generate end conversations. Before long, it would just be doing the same thing. So what we do is that we say, look, create a user profile. It's actually easy for GPT to generate different user profiles. And once you generate different user profiles, you take each one and say GPT make a conversation with these profiles, then the conversations will diverge because otherwise you just get almost the same conversations. Because that's what it does. You're just changing the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2294243_ms_-_2367281_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2294243, "end_ms": 2367281}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2352703 ms - 2424817 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2352703 ms - 2424817 ms\n\nContent: GPT make a conversation with these profiles, then the conversations will diverge because otherwise you just get almost the same conversations. Because that's what it does. You're just changing the parameters in a sense. So you generate that and you eyeball the result. That's the first round. Then once you get enough, good enough results, then we say, okay, let's prove by doing a real evaluation. The concept here is that at some point you have to do a user study. So here is how we do a simulation prompt. And you say you're talking to an intelligent AI assistant named wikichat. The current year is current year. Time. Time is very critical. If you simulate it, who knows where it thinks it is, what it is simulating. Okay, maybe it is something in the future, something in the past. We have to ground. We have to decide. It's like this is current and today's date is this, and you would like to talk about the title you know a bit about the title and so forth. Okay, so this we give them some", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2352703_ms_-_2424817_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2352703, "end_ms": 2424817}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2412769 ms - 2482833 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2412769 ms - 2482833 ms\n\nContent: ground. We have to decide. It's like this is current and today's date is this, and you would like to talk about the title you know a bit about the title and so forth. Okay, so this we give them some starting point and then you say ask interesting follow up questions when needed. It is not that surprising. This is roughly what we do. And here is just a way of giving putting into the parameters and specifying the format and so forth. So you've seen the prompt in the wiki chat pipeline, but here is the prompt for the simulation. Okay, so going back to what can LLMs do and not do? We want to evaluate automatically if possible. One of the question is, are the LLM evaluations similar to human evaluations? The only way you can tell is that you let it do it automatically. You do some amount by hand and then you do the correlation and then you say it matches. If you don't do this step, it's not grounded. All right, so when we make wikichat we have two interests. One is to make it factual and", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2412769_ms_-_2482833_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2412769, "end_ms": 2482833}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2466611 ms - 2547941 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2466611 ms - 2547941 ms\n\nContent: and then you do the correlation and then you say it matches. If you don't do this step, it's not grounded. All right, so when we make wikichat we have two interests. One is to make it factual and the other one is for it to be conversational. So let me ask you, what do you think? Is it possible to automate the evaluation of conversationality? You ask it. I mean, I show you the conversations. Like is it, does it sound like a good conversation? That would be the question. Who thinks that it is good for conversationality? Who thinks it is not? Who thinks Nothing. You have to raise your hand. It's just a guess. You don't know necessarily. The question is how do you know? So we have different guesses. How do we know in the end whether we can use it or not? Yes. And then technically you can have humans evaluated and then once humans can't scale, then you can use an LLM to judge and train it or something like that. Train. We go to train again. Be careful. Train. And with LLMs they don't", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2466611_ms_-_2547941_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2466611, "end_ms": 2547941}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2533971 ms - 2606751 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2533971 ms - 2606751 ms\n\nContent: have humans evaluated and then once humans can't scale, then you can use an LLM to judge and train it or something like that. Train. We go to train again. Be careful. Train. And with LLMs they don't really go well together. They learn the human criteria. Well, you are way ahead. So the basic thing is that you evaluate first automatically. You do some hand automatic evaluation. If they match, they match. If they don't, you can do all these fancy things or you go straight to human evaluation. Okay, so you get the idea. You have to be careful. We don't know. We have guesses. What about factuality? Can we use GPT for the factuality? How many think people think? Yeah, you have to have. There are only two choices. I need a hands up on one or the other. Who thinks they can check factuality automatically? Who thinks that you can't? Okay, actually I gave you the answer in the last lecture, but that was a little while ago. In experiments we showed that it was 83% in terms of checking for", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2533971_ms_-_2606751_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2533971, "end_ms": 2606751}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2589083 ms - 2669825 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2589083 ms - 2669825 ms\n\nContent: automatically? Who thinks that you can't? Okay, actually I gave you the answer in the last lecture, but that was a little while ago. In experiments we showed that it was 83% in terms of checking for Factuality. So you're asking it if it is factual. And of course, by the time it says yes, then you ask it again. Did I get it right? Yeah, but we know that it is at 83%. For the experiment, I would say no. If I use automatic checking, it would say 100%. And that is just wrong by a lot. Meaning that if it is something that we already have evaluated by hand, we know that you cannot use this for this purpose. So we cannot do fact checking automatically. So here is how we're going to do the summary, how we do the evaluation. This is a summary. So we take the head tail and the recent topics, different experiments, different users, we simulate it. And then when we talk about. And then we have the simulated user talk to our chatbot. So this is our chatbot, and this is just a simulation of a user.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2589083_ms_-_2669825_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2589083, "end_ms": 2669825}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2655071 ms - 2740269 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2655071 ms - 2740269 ms\n\nContent: experiments, different users, we simulate it. And then when we talk about. And then we have the simulated user talk to our chatbot. So this is our chatbot, and this is just a simulation of a user. And the evaluation goes like this. You take the sentences from here for the factuality, we do the claim identifier, evidence retriever, and we give it to the crowd worker and say, is this correct? This is all done manually. Then for conversationality, we go through the rater, a LLM rater, and is that good enough? Is that good enough for fact checking? Is that fact checking good enough? Yes. My concern is it doesn't identify all the claims. So you're only checking the claims. It identifies. You identify all the claims. So that's one problem. What else? Yes, if it's a really expert topic, you may not be able to validate based on the claim and the evidence if it's correct or not. Yes. These two steps are automatic. Okay, the claim identifier is automatic. The second one is that the evidence", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2655071_ms_-_2740269_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2655071, "end_ms": 2740269}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2729109 ms - 2800915 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2729109 ms - 2800915 ms\n\nContent: not be able to validate based on the claim and the evidence if it's correct or not. Yes. These two steps are automatic. Okay, the claim identifier is automatic. The second one is that the evidence retriever is automatic. If my evidence retriever is wrong, then you cannot fact check it because we may not be getting the right answer. So we don't know. So what happened is that for fact checking, we took the extra step when the claim. If the claim check, if the crowd sources, this is fine, we let it go, okay? We may miss some claim, we may miss some claims because the claim identifier may not have pulled out everything. We missed some there. And the problem is that for the ir, it may not retrieve the relevant articles. So we actually go and check manually for those ones where the crowdsource workers say this does not support it, we go check those. But the good news is that it is rare. Okay, so this is how we actually have to do fact checking. Okay? So remember it. When you are doing your", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2729109_ms_-_2800915_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2729109, "end_ms": 2800915}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2785121 ms - 2861501 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2785121 ms - 2861501 ms\n\nContent: workers say this does not support it, we go check those. But the good news is that it is rare. Okay, so this is how we actually have to do fact checking. Okay? So remember it. When you are doing your project, you have to justify how you do your evaluation. So this is the implementation of WikiChat we are using the Colbert V2 at that time for the IR it is fast enough. It is trained on Ms. Marco, the Wikipedia corpus. We have to tell people the date because if they look at the data set, they can check. You want to give people information. So it is reproducible. And we tell people exactly which model we are using. We are using that for the chat and we're using a combination. I mean, the details are in the paper. So the baseline model, I mean the experiment is that we have to do comparisons. It's not just about you, but all how we compare. So we took some the best of the state of the art, which is like Atlas is a state of the art on knowledge grounded tasks. We have to compare with", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2785121_ms_-_2861501_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2785121, "end_ms": 2861501}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2848409 ms - 2931003 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2848409 ms - 2931003 ms\n\nContent: It's not just about you, but all how we compare. So we took some the best of the state of the art, which is like Atlas is a state of the art on knowledge grounded tasks. We have to compare with straight up LLMs, of course. And then we evaluated on 600 dialogue turns. So this is the simulated evaluation. And the blue is straight up GPT4. It does very well on head. This is what people experience when they say oh, GPT works. This is because of the head. When you get to tail 56% accuracy, what is it? Accuracy factual recent 47% it is not zero. Because when you talk about a recent event, you can talk about the player or the winner or whatever, they have a history and you ask questions about that, they that's all good. So it comes to about 47%. And this is just on the whole, on average, not too meaningful. But the separate results are more interesting. So if you take wiki chat using GPT4, we improve the performance. It made the accuracy up to the 99% on head. It is now all above 90s for the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2848409_ms_-_2931003_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2848409, "end_ms": 2931003}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2913555 ms - 3005621 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2913555 ms - 3005621 ms\n\nContent: meaningful. But the separate results are more interesting. So if you take wiki chat using GPT4, we improve the performance. It made the accuracy up to the 99% on head. It is now all above 90s for the head tail and for the reasons this is the, you know, this is factuality. So to see the difference between GPT4 and GPT 3.5, we put Wiki Chat on the GPT 3.5 pipeline. And you see a drop, you absolutely see the drop. Not so much for the head. And then it's. And then, and then it. And then we also did the fine tuning Wiki Chat llama. What is interesting is that the Wiki chat llama, as we said, is worse than GPT4, but it is actually kind of it is better than GPT 3.5. Okay, so the distillation from the GPT 4 to llama is better than the GPT 3.5 for this task. So there's hope, right? SGPT 4 becomes GPT 5 and so forth. Then we can match the performance of GPT4, for example, and that would be pretty good. Then finally we also check against Atlas. ATLAS is the State of the art at that time. And", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2913555_ms_-_3005621_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2913555, "end_ms": 3005621}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 2990657 ms - 3074307 ms", "content": "Title: CS224V Lecture 10 > Transcript > 2990657 ms - 3074307 ms\n\nContent: GPT 5 and so forth. Then we can match the performance of GPT4, for example, and that would be pretty good. Then finally we also check against Atlas. ATLAS is the State of the art at that time. And factuality of ATLAS was pretty decent. It is a fine tuned model. So it's like what we are doing in factuality a little bit worse, but not significantly worse, which is what we expected actually. Now let's talk about conversationality. It turns out that everything is pretty high except for atlas. We saw that ATLAS is the same, but what it loses on is conversationality because they don't have the LLM generation pipeline. They just retrieve and then it's just boring as hell. Okay. And that's the difference that you can see from the conversationality. You have a hand up here? Yeah. Okay, great. Thank you. So this is the evaluation and what else do we do? We say that what is interesting about Wiki chat is that it actually combines IR and LLM. So if you look at the results, we take these sentences", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_2990657_ms_-_3074307_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 2990657, "end_ms": 3074307}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3057921 ms - 3134181 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3057921 ms - 3134181 ms\n\nContent: you. So this is the evaluation and what else do we do? We say that what is interesting about Wiki chat is that it actually combines IR and LLM. So if you look at the results, we take these sentences and we see that some the yellow sentences are based on LLM. The blue is based on the information retrieval and you see that it has a good mix of using both pipelines, showing that it is useful. Okay. You know, it could be all one or the other. Right? And this is why you have to do the data analysis. When you design your experiment, you say this part is new, go evaluate it and when. So we analyze the design. Yes, we need to do two pipelines and yes, we need to factor LLM output and so forth. So these are the details. So are we done analyzing wiki chat? Are you happy? I gave you a lot of results, Are you happy now? Who's happy? Nobody. Okay, for all of you, what is, what do you want? What else do you want? Yes, you need like human review, the actual performance and bingo. Thank you. We said", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3057921_ms_-_3134181_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3057921, "end_ms": 3134181}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3115561 ms - 3197717 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3115561 ms - 3197717 ms\n\nContent: Are you happy now? Who's happy? Nobody. Okay, for all of you, what is, what do you want? What else do you want? Yes, you need like human review, the actual performance and bingo. Thank you. We said this is you want to have real user evaluation and this is the most important metric. Okay. And for this we actually only did it for the this we use the new topic for the analysis. We don't want to do a tail because the real user is not familiar. But new topics is also hard for GPT. So we use new topic and here is the result. We did this experiment where we say here is the first sentence of a very new Wikipedia page. Not wikichat has, I mean GPT has not been trained on it and we asked them to rate the conversation and they got 3.4 and we got 3.8. We're very happy about that because we did not ruin the conversationality. The fact checking is done ourselves. It is not the user fact checking because they actually didn't know what is factual. And when they gave GPT 4.3.4 it's not because it", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3115561_ms_-_3197717_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3115561, "end_ms": 3197717}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3184117 ms - 3251449 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3184117 ms - 3251449 ms\n\nContent: ruin the conversationality. The fact checking is done ourselves. It is not the user fact checking because they actually didn't know what is factual. And when they gave GPT 4.3.4 it's not because it knows it is. It's not because they know it is not factual. As a matter of fact, the users are just not aware that the GPT4 is actually wrong more than 50% of the time and it's still got a lower rating than us. Okay, so this is the whole point is like how do I retain the conversationality while being factual? Which ATLAS could not do. Okay, yes. When you did humans involved in fact checking, were they involved in the claim identification or the information retrieval or only like. It's just the same as the pipeline I discovered in like saying whether or not the claims were verified by the evidence. If the crowdsourced it, the crowdsource workers said that it's not verified. Then we did a manual test. That's what we discussed. It's the same pipeline at that point, but the conversation, you", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3184117_ms_-_3251449_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3184117, "end_ms": 3251449}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3239625 ms - 3318603 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3239625 ms - 3318603 ms\n\nContent: If the crowdsourced it, the crowdsource workers said that it's not verified. Then we did a manual test. That's what we discussed. It's the same pipeline at that point, but the conversation, you know, the rating is obviously human rating here. The factuality is done by hand with two stages. Just not just the crowdsource, but also us. Okay, so that wraps up the first part and let's use it for other chats. Okay, so the first thing we want to do is to do something that is really an amazing contribution of LLM is that you can potentially remove the language barrier. How many languages do we know here? How many know more than two languages here? Two. More than two. Not bad. Two languages. All right, so but it's not a lot more than two or three. And the question here is LLMs can do all of them all at once. Okay, so here's an example. I've heard about Yoshio Toyoshima. What shall I read from him? And then if you look at Wikipedia, there's nothing there. But if you look at the Japanese", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3239625_ms_-_3318603_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3239625, "end_ms": 3318603}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3303619 ms - 3383005 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3303619 ms - 3383005 ms\n\nContent: them all at once. Okay, so here's an example. I've heard about Yoshio Toyoshima. What shall I read from him? And then if you look at Wikipedia, there's nothing there. But if you look at the Japanese Wikipedia, of course the information is there. And really only 11% of the articles are in English. So you're missing like 90% of the world's knowledge that are written down in Wikipedia. How sad. Here's another example. Murakami is very famous. He is on English Wikipedia and he says, how did he get started as a writer? But if you can read Japanese, it actually has an amazing story about how he decided to write by watching a baseball game. Okay, now that's fascinating, but you have to know Japanese, right? And so the question here is that it is now on the web, you can go try it yourself, asking anything that you want and you will find a lot of unexpected results. So for example, what was funny was that Sino is doing this, was talking about this Persian physician at Vichena, and he", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3303619_ms_-_3383005_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3303619, "end_ms": 3383005}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3367565 ms - 3441479 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3367565 ms - 3441479 ms\n\nContent: asking anything that you want and you will find a lot of unexpected results. So for example, what was funny was that Sino is doing this, was talking about this Persian physician at Vichena, and he discovered that it was the Fact that he treated hernia was in the Japanese Wikipedia. So this old Persian doctor was actually written up in Japanese. And you would never guess. The whole point here is that if you break down the barrier, the information can just come from anywhere, right? So that is cool because I ask you guys, everybody has, you know, is like, it is really hard to speak many languages. And it also expands your access to cultural knowledge and do we need a chatbot? And then it says yes, because as you talk, you can discover things that you are interested in because you may not just go start reading on a Wikipedia from other languages. So having a conversation is now you can drive it to the areas of interest of yours. Is it easy to do multilingual? There are two building", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3367565_ms_-_3441479_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3367565, "end_ms": 3441479}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3425821 ms - 3497295 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3425821 ms - 3497295 ms\n\nContent: just go start reading on a Wikipedia from other languages. So having a conversation is now you can drive it to the areas of interest of yours. Is it easy to do multilingual? There are two building blocks that we use. One is LLM. LLM actually can understand and speak a lot of languages. It is actually pretty good. It struggles sometimes with the low resources. That is something that can be improved independently of what we are doing. The other one is retriever. We mentioned in the earlier class that there are multilingual retrievers like BGE, M3 and recently GTE came out and they do multilingual. So in other words, you take the corpus, you run it through the, these, these embeddings, and now you can tick any language, tick any question in any language, and you can retrieve paragraphs from any language back to you. Okay, so these are the two building blocks that have already got multilingual. And so in the wiki chat, we really simply just tell the draft stage to respond in the same", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3425821_ms_-_3497295_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3425821, "end_ms": 3497295}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3483065 ms - 3559503 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3483065 ms - 3559503 ms\n\nContent: any language back to you. Okay, so these are the two building blocks that have already got multilingual. And so in the wiki chat, we really simply just tell the draft stage to respond in the same language as the user utterance. So you speak in Chinese, it comes back speaking in Chinese. If you don't say that, it might be speaking to you back in English. So that's relatively easy to do. Where's the difficulty? They're in the entities. It's all in the entities. And so LLMs and retrievers, they both struggle with multilingual entities. How do you transliterate names? If you take an entity like a song name. Okay, you probably a lot of people have experienced that the English song name and the native song name, they are not connected to each other. And so the solution here is to tie them together using the wikidata entity translations. Okay, so let's talk about the details. So here's an example. So this guy is called, I'm embarrassed to say that I've messed up his name because we usually", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3483065_ms_-_3559503_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3483065, "end_ms": 3559503}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3541265 ms - 3622107 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3541265 ms - 3622107 ms\n\nContent: using the wikidata entity translations. Okay, so let's talk about the details. So here's an example. So this guy is called, I'm embarrassed to say that I've messed up his name because we usually call him H.T. kong. He's my advisor, but we never say his full name. Okay, so I was told that Kong is actually a descendant of Confucius. Now that's really cool. If you know Chinese, Kong is Kong Ji. The Kong is the same name, same last name. So I tried it in English and Ricky Chet comes back and says, I'm not certain about Xiang Chong Kong's lineage. It's possible, however, if I say it in Chinese, I know his Chinese name is that Chinese name. It is basically Kong Xiangzong. Is he a Confucius of Confucius descendant? And it actually answers it. Not only is he just a descendant, we actually know that he's the 75th generation. Very specific, and it is absolutely true. Okay, so if I ask in Chinese, it works. Now how do I get multilingual Q and A? Now I have to tie these two things together,", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3541265_ms_-_3622107_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3541265, "end_ms": 3622107}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3607149 ms - 3683527 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3607149 ms - 3683527 ms\n\nContent: that he's the 75th generation. Very specific, and it is absolutely true. Okay, so if I ask in Chinese, it works. Now how do I get multilingual Q and A? Now I have to tie these two things together, right? So if I say is like this person is the same as that person, then you can say, oh, yeah, okay. It is that he is a descendant of Confucius. So this is the key concept is that you have to tie the name, you know, in one language with a name that has, you know, in the language for which the information is available. So if you can read Chinese, what it just says is that Kong, these are all the descendants, the 75th descendant of Confucius, and his name is up there in Chinese. But what happened here is that all the Wikipedia entities are linked. These are all links. If you click on it, it will open up the Wikipedia page. And then the page they have the info box. And the info box is what's in Wikidata. And you can see the English name and the Chinese name. All the entities are given unique", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3607149_ms_-_3683527_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3607149, "end_ms": 3683527}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3668765 ms - 3735907 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3668765 ms - 3735907 ms\n\nContent: open up the Wikipedia page. And then the page they have the info box. And the info box is what's in Wikidata. And you can see the English name and the Chinese name. All the entities are given unique identities across all different countries. This is really amazing. This is one of the most important things that you get in Wikidata. So once we get that by clicking on the link, trivial. It's not even AI here. And then you take the name and then you stick it next to the name of that entity. And now you get. And then you send it to the. To the indexer, right? The IR system that it now knows that that English name is tied to this page. And you would be able to answer that question. And this is absolutely key to link the entities. And Wikipedia made it really easy. And you will see the problem with entities over and over again when you're dealing with multilingual. I know that some people are interested in multilingual analysis. This is very, very important. Linking the entities, the names,", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3668765_ms_-_3735907_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3668765, "end_ms": 3735907}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3724865 ms - 3800987 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3724865 ms - 3800987 ms\n\nContent: entities over and over again when you're dealing with multilingual. I know that some people are interested in multilingual analysis. This is very, very important. Linking the entities, the names, the terms, the locations and so forth. Okay, so now we have built this. 10 Wikipedias in the top 10 languages that is most active. It is put in the wild. We have now got 35,000 questions. And what is the impact of this? Obviously it's a lot of knowledge for all of us. But what I am most excited about is that there are a lot of people who don't know English. And the English encyclopedia is large, it's 10%, but it is much larger than any other languages. And making that available to all the people that LLM can speak to is just amazing. Okay. Oh, it's now 30 languages. It was 10 languages just three days ago. It's now in 30 languages. And since this is a distribution of the people from different countries, English is still highest, but it is just 50% and then it goes down. We have a lot of", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3724865_ms_-_3800987_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3724865, "end_ms": 3800987}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3786176 ms - 3863793 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3786176 ms - 3863793 ms\n\nContent: three days ago. It's now in 30 languages. And since this is a distribution of the people from different countries, English is still highest, but it is just 50% and then it goes down. We have a lot of friends in Italy don't know Italian, Chinese, Korean and so forth. So you really open up the Wikipedia to a lot of people through this multilingual chat. Any suggestions, what are other multilingual use cases that you can think of that are really, you know, that pops into your head? Yeah. Teaching a Chinese or Spanish speaker English. So you speak in what language? Yeah, so they want you to speak Spanish because they don't understand English. And then if they want you to teach English, they need both language there. Okay. For teaching languages. Okay, what else? Yeah, I know some of the smaller languages. The Wikipedia articles are like much smaller, like stuff. So you, you could use like for well known topics. You could use what's in the English Wikipedia page to fill out the Wikipedia", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3786176_ms_-_3863793_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3786176, "end_ms": 3863793}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3852855 ms - 3915399 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3852855 ms - 3915399 ms\n\nContent: languages. The Wikipedia articles are like much smaller, like stuff. So you, you could use like for well known topics. You could use what's in the English Wikipedia page to fill out the Wikipedia page for. You can fill it up. People are already doing that with translations, but now this is an interactive version of retrieving data. What else? Do you have a hand there? Yes, I'm reading also a lot of tourism as well. Like. Oh my God. Yes. Oh, I cannot, I cannot agree more. I was in Japan and I tried Google search and I just cannot find anything that I really need because I don't know the terms in Japanese. Right. And the only way I got around is that I have a Japanese friend with me and he searches in Japanese. So again, the entities are very important. Okay, yes, in the last. Yeah, one more. Oh, I have one more news articles that we had talked about before where if you had like historical documents in different languages, you could perform more interesting analysis. If you could. A lot", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3852855_ms_-_3915399_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3852855, "end_ms": 3915399}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3905871 ms - 3982765 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3905871 ms - 3982765 ms\n\nContent: Oh, I have one more news articles that we had talked about before where if you had like historical documents in different languages, you could perform more interesting analysis. If you could. A lot of languages. Yes, absolutely. And we also heard last Monday and last Wednesday there were a lot of very motivating examples for multilingual work. So and in summary here is that we said that there are two steps. The improve and assess. And in this case we already know about the entity linking problem because we have written a few papers about multilingual Chatbots. So when we do the experiment, it is confirmed and we fix it pretty easily. So that step was pretty easy. And what about evaluation? Did I talk about evaluation? It's actually really difficult because you don't know the languages. You can only know any person knows a handful of questions languages. So what did we do in evaluation? The answer is nothing yet. Okay. However, we have 35,000 users, I mean 35 questions from the wild.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3905871_ms_-_3982765_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3905871, "end_ms": 3982765}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 3961529 ms - 4041483 ms", "content": "Title: CS224V Lecture 10 > Transcript > 3961529 ms - 4041483 ms\n\nContent: can only know any person knows a handful of questions languages. So what did we do in evaluation? The answer is nothing yet. Okay. However, we have 35,000 users, I mean 35 questions from the wild. The opportunity here is actually to evaluate in the wild. And, and that is the next step that we have to do. And we have a couple of projects where the evaluation actually has to be done in the wild. So keep that in mind. Okay. Because we have seen wiki chat in English, we don't feel like it is so important to do that. And it is also very hard. All right, let's talk about something very different in terms of the goal. And we want to say let's not just talk about Wikipedia and facts. Let's talk about being persuasive. Totally different in a sense. And I know some projects that people have proposed here require you to have better conversational skills than just spewing out facts. Okay, so how did we do? So the question is, can we create domain independent chatbots beyond providing facts such", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_3961529_ms_-_4041483_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 3961529, "end_ms": 4041483}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4027459 ms - 4096202 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4027459 ms - 4096202 ms\n\nContent: here require you to have better conversational skills than just spewing out facts. Okay, so how did we do? So the question is, can we create domain independent chatbots beyond providing facts such as persuasion? Persuasion is very useful for many things and here are just some of these examples. It is really to try to influence people. It's not just to give them more knowledge. Then you have all these goals from establishing mutual trust and credibility. You have to be strategic in presenting the persuasive appeals. You have to elicit emotional reactions and so forth. We're now talking about persuasion skills. We haven't talked about anything on the human side so far. We just facts, facts, facts. But we need persuasion skills and we also must provide factual answers. Nonetheless, the factuality is so basic. So we have have to start with Wiki Chan in our work. So the closest state of the art in this topic before our work is this paper. It was presented only in 2022 and they try to use", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4027459_ms_-_4096202_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4027459, "end_ms": 4096202}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4082273 ms - 4150829 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4082273 ms - 4150829 ms\n\nContent: is so basic. So we have have to start with Wiki Chan in our work. So the closest state of the art in this topic before our work is this paper. It was presented only in 2022 and they try to use LLMs in the latest use. So this work follows a very traditional line of work in conversational agent. They look at agents utterances and model them with a fixed set of dialogue acts. The word dialogue acts are very old. It is 1959. John Austin, who came up with that. Basically every time we make any statement, you can classify it according to the intent. I am providing information, that's the intent. You ask me a question, that's an intent. So there are only a small number of intents. That kind of Describe those kinds of things that we talk about. Not the content itself, but the intent has these high level dialogue acts. And for persuasion you can be a little bit more specific and you can say this is for emotional appeal, this is for logical appeal and so forth. So these are the dialogue acts.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4082273_ms_-_4150829_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4082273, "end_ms": 4150829}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4138045 ms - 4208832 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4138045 ms - 4208832 ms\n\nContent: level dialogue acts. And for persuasion you can be a little bit more specific and you can say this is for emotional appeal, this is for logical appeal and so forth. So these are the dialogue acts. So for this, in this paper they talk about the work called Save the Children. It's a website to ask people to donate money to Save the Children. It's a global website and they have identified the following dialogue acts, starting from the top greeting to closing at the end and anything in between the different kinds of appeals. Foot in the door is very interesting. You know, it's like, oh, can I get you to donate a little bit and stuff. So these are the things that I have identified. It's a small number of them, Frankly. Okay, so 11. Okay. Dialogue acts. And so they. This is, this paper is actually a series and in previous work they've done some experiment and then establish that the best agent that they found that they can make is. Follows a strict order. You start with opening and then you", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4138045_ms_-_4208832_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4138045, "end_ms": 4208832}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4194472 ms - 4270775 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4194472 ms - 4270775 ms\n\nContent: a series and in previous work they've done some experiment and then establish that the best agent that they found that they can make is. Follows a strict order. You start with opening and then you say something like, have you heard of the organization? And then you go down the different levels of appeal and then a personal story and propose donation. And it's a static. It's a static order. Are you surprised by that? What does it mean? Why is it the best? Is that what you would do? No, what would you do? I would try to create some sort of heuristic. I mean, as a human. As a human, what would you do? I don't know about the heuristics. As a human, you just. I would try to create an Irish case of a conversation. No, I want you to say, if you are the agent, as a human agent, would you be following a strict order? What would you do? I will live in what matters. Yeah. You have to respond, you have to react to how the user, what the user is saying. So why is it that they say this is the best", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4194472_ms_-_4270775_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4194472, "end_ms": 4270775}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4254667 ms - 4336717 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4254667 ms - 4336717 ms\n\nContent: a strict order? What would you do? I will live in what matters. Yeah. You have to respond, you have to react to how the user, what the user is saying. So why is it that they say this is the best in their experiment? Yeah, probably because it doesn't have like flexibility for different, like conversational persuasive options. No, that's good. That is number one. There's another good reason. What do you think of this? The second good reason is that. What did I say? When you see some result, you ask what the experiment is. If they get a crowdsourced worker, it's easy. They all want to finish the job. Okay. So you always have to think back. It's like, how did you get this result? Because it is anti intuitive. Okay. And a lot of this is in the experiment setting. What kind of conversations are you doing? What choices are you picking? So those are the two problems, frankly, really clear when I saw that, oh, it's a strict order. It's not possible. So the. And it's also very expensive. They", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4254667_ms_-_4336717_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4254667, "end_ms": 4336717}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4321091 ms - 4395443 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4321091 ms - 4395443 ms\n\nContent: you doing? What choices are you picking? So those are the two problems, frankly, really clear when I saw that, oh, it's a strict order. It's not possible. So the. And it's also very expensive. They are creating a strict order for every single kind of task. Right. If I'm talking about tourism recommending you to go visit Mexico or whatever it is, it's a fixed order. How come you, how do you come up with a fixed order? And what are the dialogue acts for each of the different kinds of application? I should have fixed it once and for all. But we're near the end. So here is their framework. They said that, look, I am going to use LLM. I'm going to use LLM. I take the input and I got a dispatch. If you have a factual question, I get an answer and I have. If it is, you know, I give you a social response and at the end I am using my strategy that is in fixed order to add to the conversation. So I answer your question and then I do what I have designed in my strict order. So that's that paper.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4321091_ms_-_4395443_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4321091, "end_ms": 4395443}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4378443 ms - 4462999 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4378443 ms - 4462999 ms\n\nContent: and at the end I am using my strategy that is in fixed order to add to the conversation. So I answer your question and then I do what I have designed in my strict order. So that's that paper. So we actually wrote a paper, this is with my team from KDDI in Japan and we just got. The paper was accepted and it will be in EMNLP in November. We'd be presenting that. And this paper is a zero shot agent. What does it mean to have a zero shot agent? Yeah, that means you didn't show conversations as an example on how to respond. Zero shot means. Oh, what it is. Actually that is an interesting question. The zero shot in this case means that it is the same prompt for all applications. I don't specialize it for any application. And that's amazing, right? If you get that done, it's so much more effective and efficient. So as I said, we follow the order, right? Assess and evaluate. Don't start with evaluation. Everybody wants to start with evaluation. Start with assessment. So here's the first", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4378443_ms_-_4462999_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4378443, "end_ms": 4462999}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4449475 ms - 4526621 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4449475 ms - 4526621 ms\n\nContent: and efficient. So as I said, we follow the order, right? Assess and evaluate. Don't start with evaluation. Everybody wants to start with evaluation. Start with assessment. So here's the first example we tried. I don't want to donate to Save the Children. The user said, very common. And then this is the response from GPT. Save the Children is a global organization that works in over 100 countries. One of the most inspiring stories is about a girl named Maria. She was living in extreme poverty and had to drop out of school to help her family. Thanks to Save the Children's education programs, she was able to go back to school. Your donation can help more children like Maria achieve their dreams. Is that a good answer? Yeah. No. Why not? You don't like it? Yeah, it's too long. It's too long. Well, it seems long because I have to read it to you. Remember, we're in persuasion mode. We're not answering quick factual questions. We're chatting. Okay. Keep that in mind. Is it better now if you", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4449475_ms_-_4526621_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4449475, "end_ms": 4526621}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4510095 ms - 4592559 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4510095 ms - 4592559 ms\n\nContent: Well, it seems long because I have to read it to you. Remember, we're in persuasion mode. We're not answering quick factual questions. We're chatting. Okay. Keep that in mind. Is it better now if you think about that in the context? Maybe, but it's still long. Any yes else? I feel like they might have just like made up Maria. Like maybe she's not a real person. I'm a little suspicious. She is not. She is. So what we see here is that it seems to generate decent response to everyday persuasion task like this. Why do you use agent dialogue acts? I don't know which act it is. I don't care because the LLM can already generate the answer. But the problem is Alison suspected Maria doesn't exist. So is that okay? Very persuasive. Is that okay? Who thinks that it is not okay. Not everybody. I think it is not okay. It's disingenuous. Can you imagine you'll be so mad when you find out that you are just talking to a liar. All right, you can lie about anything. Maybe this is something that it can", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4510095_ms_-_4592559_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4510095, "end_ms": 4592559}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4578711 ms - 4650831 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4578711 ms - 4650831 ms\n\nContent: it is not okay. It's disingenuous. Can you imagine you'll be so mad when you find out that you are just talking to a liar. All right, you can lie about anything. Maybe this is something that it can do that this organization can do. But it could be something totally wrong. How can you say like that? I mean, why? I mean, I wouldn't donate to anybody who's lying to me just so that I will donate. Right? It's absolutely not okay. And so the question is. So we need to be factual. Can we just use Wiki Chat? Because you can take LLM, you know, the Wiki Chat pipeline. Right? You eliminate anything that is not right. But you are using LLM to generate in the first place. And then we do factor checking and we remove whatever that is wrong. Would that work? Who thinks it does? Who thinks it does it? Why doesn't it? Maria's definitely not in Wikipedia. Regardless of whether she. You substitute it with the website information. You know, everything about Save the Children you're drawing from that.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4578711_ms_-_4650831_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4578711, "end_ms": 4650831}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4637015 ms - 4702257 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4637015 ms - 4702257 ms\n\nContent: Why doesn't it? Maria's definitely not in Wikipedia. Regardless of whether she. You substitute it with the website information. You know, everything about Save the Children you're drawing from that. You're not doing Wikipedia. Sorry, I'm using the Wiki Chat pipeline. Definitely not Wikipedia. It doesn't have anything. Suppose I'm doing the Save the Children organization. Yeah, there's like a different goal here. Like if I don't want to donate to Save the Children, I can probably already infer that people like Maria exist. There's like another issue that needs to be solved. Like persuasion. Not just telling me about the kids that he's saving. So if you apply to this example, what happens to the result if you use the Wiki Chat pipeline? I guess it would just be like a list of facts about the Good work that safety. And for this particular response, what would be the result? Will you replace Maria with an actual child? We would like to replace Maria with an actual child. No, I only", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4637015_ms_-_4702257_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4637015, "end_ms": 4702257}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4685865 ms - 4759215 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4685865 ms - 4759215 ms\n\nContent: the Good work that safety. And for this particular response, what would be the result? Will you replace Maria with an actual child? We would like to replace Maria with an actual child. No, I only eliminate. I only eliminate. That's a problem. Wiki chat will give you the first sentence and it stops. It's not capable. I mean, remember the pipeline is eliminate. But what you suggested is exactly what we need to do is to replace it with the right person. You take advantage of the persuasiveness, but you have to add the factuality. So the concept here is that we take the statement, we extract the strategy, if it does not fact check, we take the strategy and we go IR and say, could you fulfill this strategy? And now you have a totally different story. So this is an example. You take the utterance and you go through basically wiki chat. But we ask it, what is the strategy? And then after fact checking, if it says oh, Maria doesn't exist, it actually goes through another search. In this case", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4685865_ms_-_4759215_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4685865, "end_ms": 4759215}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4744243 ms - 4817951 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4744243 ms - 4817951 ms\n\nContent: and you go through basically wiki chat. But we ask it, what is the strategy? And then after fact checking, if it says oh, Maria doesn't exist, it actually goes through another search. In this case it is very insistent now. And then it says that. So because I know the strategy is to tell an impact story, it changes the query to inspiring stories of children saved by Save the Children. And then with the IR it actually returns you people that actually exist. It is interesting how there is Maha and Maya very close to Maria for some reason, but it is all actually a story from their website. And now you have the best of both worlds. Okay, what should the evaluation metrics be? I think I'm out of time. Let me just go straight to it. You want it to be persuasive, it has to be relevant, it has to be natural. And one thing that we really want is that it has to be honest. It doesn't try to pretend it's a human, it's disingenuous. So to prove that we have this zero shot capability, we just", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4744243_ms_-_4817951_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4744243, "end_ms": 4817951}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4802749 ms - 4881991 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4802749 ms - 4881991 ms\n\nContent: to be natural. And one thing that we really want is that it has to be honest. It doesn't try to pretend it's a human, it's disingenuous. So to prove that we have this zero shot capability, we just download the domains, we did not change the prompt and then just let it go. The assessment is that the question is can we use simulated users and simulated evaluation we again generate users with different profiles. The first experiment showed that it works really well and perhaps too well. What do you do? You let it go? Or what do you have to think about when it is working too well? Yeah, like when can you make it fail? How do you make it fail? You want to make it fail. So that's the important thing, you want to make it fail. So we did. And the way we made it fail is to talk about a tough user, a person who just keeps saying no. Okay, this is what is different from an Amazon Turk worker. They would Just go along, play along. But in real life there are people who really don't want to donate.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4802749_ms_-_4881991_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4802749, "end_ms": 4881991}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4867437 ms - 4936123 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4867437 ms - 4936123 ms\n\nContent: a person who just keeps saying no. Okay, this is what is different from an Amazon Turk worker. They would Just go along, play along. But in real life there are people who really don't want to donate. So once you make it tougher, everything becomes harder. Because before we get to the tough user, we cannot distinguish between us and GPT almost because it just goes along and it does really well. So here's an example of conversation, but it's all checked, fact checked. You can read it later. I'm kind of out of time. And here's another one, when it gets to be really hard, GPT made up more stories, okay? And which is just completely false. So that's when you tell the story, the difference. And then. So now we do the simulated conversations. 20 personalities of true soft and tough users. We keep simulated experiments and we try to do both the GPT 3.5 and llama 3 because the reviewers required us to, frankly. So the question next is can we use simulated evaluation? Who thinks yes on the", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4867437_ms_-_4936123_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4867437, "end_ms": 4936123}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4919731 ms - 4995137 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4919731 ms - 4995137 ms\n\nContent: keep simulated experiments and we try to do both the GPT 3.5 and llama 3 because the reviewers required us to, frankly. So the question next is can we use simulated evaluation? Who thinks yes on the conversational side, not the factuality conversational side? Who thinks no. Hide. Why? We're just guessing no. I think it would tend to create a standard conversation like how you simulate the top users different profiles. If there is confusion perspectives, I would believe yes. And the answer is no. I'm sorry I tricked you into doing this, but the truth is that we tried that. We did the automatic evaluation and then the only way to submit that is to also check it with human evaluation. And we discover that it doesn't correlate at all on the conversational tasks. And the bots favor their own evaluation because now we're talking about persuasion. It's not about facts, okay? It's not just being friendly, but when it comes to persuasion, they like their own strategy and it actually leads you", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4919731_ms_-_4995137_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4919731, "end_ms": 4995137}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 4983425 ms - 5064185 ms", "content": "Title: CS224V Lecture 10 > Transcript > 4983425 ms - 5064185 ms\n\nContent: because now we're talking about persuasion. It's not about facts, okay? It's not just being friendly, but when it comes to persuasion, they like their own strategy and it actually leads you to wrong conclusions when you compare against all of them. And so this is all done by hand. Could you believe it? But we have no choice. This is how we end up saying we discovered that the persuasive bot using 3.5 has pretty much the best results across them. And we did it on three different applications and they are consistent. So we felt really good about that and we said that LLMs have better strategies. We actually analyze the strategies and discover that there are for each different application they have different number of strategies. And so are we done analyzing the performance? Who thinks no? Why? I'm just guessing. Ok, why somebody else not guessing? Have we finished the experiment? It's the same question last time on wikichat and the answer is no. Why? We haven't started yet. You haven't", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_4983425_ms_-_5064185_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 4983425, "end_ms": 5064185}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 5044385 ms - 5126131 ms", "content": "Title: CS224V Lecture 10 > Transcript > 5044385 ms - 5126131 ms\n\nContent: I'm just guessing. Ok, why somebody else not guessing? Have we finished the experiment? It's the same question last time on wikichat and the answer is no. Why? We haven't started yet. You haven't what? We haven't started analyzing it yet. Analyzing what? We Just did all kinds of analysis in terms of how many strategies, what is missing? Same thing. What real users. Thank you. We haven't done the real user test. You have to do the real user test and the real user test, it checked out. It was very happy. We are very happy that the real users actually did well on that. Here are just some more examples. You can read it later. I didn't go into the details but when you look at the comments, the most important information is in the comments. And what we learn is that it is lacking in one area and that is it is too repetitive. And this is not something that we focused on. It is a little bit harder because it's a persuasion task. We keep harping on the same thing. I want you to donate. So it", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_5044385_ms_-_5126131_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 5044385, "end_ms": 5126131}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 5115019 ms - 5175645 ms", "content": "Title: CS224V Lecture 10 > Transcript > 5115019 ms - 5175645 ms\n\nContent: that is it is too repetitive. And this is not something that we focused on. It is a little bit harder because it's a persuasion task. We keep harping on the same thing. I want you to donate. So it turns out that you keep repeating the arguments which is not the same with the wiki chat conversation. So this is something that we learn in this is something that still needs to be taken care of. So in conclusion here is that the LLM based system development is different. It is not just tuning the hyper parameters. You have to create the tough examples to assess and repeat. At some point you can evaluate simulated and real conversations, automatic and manual depending on your problem. And you have to deploy and learn from the real world. And then the last thing here is that we're very excited with the multilingual wiki chat and we are evaluate. We need to evaluate with users in the wild. And then for the persuasive act, the persuasive bot we see it looks like we do have a zero shot bot", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_5115019_ms_-_5175645_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 5115019, "end_ms": 5175645}}
{"document_title": "CS224V Lecture 10", "section_title": "CS224V Lecture 10 > Transcript > 5161925 ms - 5214865 ms", "content": "with the multilingual wiki chat and we are evaluate. We need to evaluate with users in the wild. And then for the persuasive act, the persuasive bot we see it looks like we do have a zero shot bot which is very, very cool. The key here is that in this case you have to do manual evaluation as far as we know because it just does not match. So don't take it for granted that it works on one case. It will work on your case. Okay, so this is what I'm going to be thinking of when I look at your projects to figure out if you have done a good job. Now, it takes time. It is fine if you say I should do this, but I did not in the next N weeks. But you should be aware of the limitations with whatever you do. Okay? All right, I'll see you on Monday.", "block_metadata": {"id": "CS224V_Lecture_10_>_Transcript_>_5161925_ms_-_5214865_ms", "document_type": "transcript", "lecture_number": 10, "start_ms": 5161925, "end_ms": 5214865}}
