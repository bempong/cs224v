{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > A few thoughts for the class", "content": "Title: CS224V Lecture 12 > Chapter Summaries > A few thoughts for the class\n\nContent: All my slides are missing the most important sentences. They just start with quiz and there is no answer. The notes are not supposed to be read by themselves. Looking at my slides is a really bad idea. I hope that makes it more interesting to come to class.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_A_few_thoughts_for_the_class", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 98735, "end_ms": 168663}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Building Agents with Structured and Unstructured Data", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Building Agents with Structured and Unstructured Data\n\nContent: Today we're going to talk about building agents that use structured and unstructured data. We came up with a language called sukl that extends SQL to include free text operations. The whole concept here is to make it easy for people to build agents.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Building_Agents_with_Structured_and_Unstructured_Data", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 168839, "end_ms": 665587}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Introduction to Summary and Answer in SQL", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Introduction to Summary and Answer in SQL\n\nContent: The design of SQL is very simple. You just add two free text and primitives into SQL and they are summary and function and answer. How do you implement summary and answer? Here's the quiz.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Introduction_to_Summary_and_Answer_in_SQL", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 665731, "end_ms": 930761}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Quantum Intelligence and the SQL framework", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Quantum Intelligence and the SQL framework\n\nContent: Inside MySQL I am using answer functions for free text only. This is really to say I have many reviews, many restaurants. Find me even under Palo Alto. And then inside your SQL it's actually numbers. What if the question, the embedding is very different from the answer?", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Quantum_Intelligence_and_the_SQL_framework", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 930793, "end_ms": 1283125}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > In the language of summary and answer", "content": "Title: CS224V Lecture 12 > Chapter Summaries > In the language of summary and answer\n\nContent: The whole point here is that we're doing IR and you can ask for anything in a single answer. So strictly speaking, you don't need a summary. The reason why the summary is there is because it is very natural. Make it actually easy for the semantic parser to generate that statement.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_In_the_language_of_summary_and_answer", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 1283505, "end_ms": 1374295}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > SQL in a hybrid QA database", "content": "Title: CS224V Lecture 12 > Chapter Summaries > SQL in a hybrid QA database\n\nContent: Aaron: We talked about this hybrid QA database data set with tables and hyperlinks. It really just means tables and paragraphs, it turns out. I want to see if you know how to write SQL statements.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_SQL_in_a_hybrid_QA_database", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 1375595, "end_ms": 1581065}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Pushing the semantic parser in C#", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Pushing the semantic parser in C#\n\nContent: When was the flat barrel of Rio's Olympic war? You see, the sentence is very short. But it actually requires you to do this to find the answer. Things turn into simple calculations in a sense. The parse is very succinct and very clear.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Pushing_the_semantic_parser_in_C#", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 1581885, "end_ms": 1912829}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Semantic Parsing for Business and Personal Data", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Semantic Parsing for Business and Personal Data\n\nContent: S UQL can be very useful for just about all product searches. It can combine information that is structured and unstructured. Do you have examples where you find this useful?", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Semantic_Parsing_for_Business_and_Personal_Data", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 1912997, "end_ms": 2389505}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > In SQL, Where is the hallucination?", "content": "Title: CS224V Lecture 12 > Chapter Summaries > In SQL, Where is the hallucination?\n\nContent: The level of hallucination depends on the embedding quality. I only retrieve things that you trust because you give me the corpus. There are a lot of things hidden underneath the implementation. How do you evaluate SQL as a design, as a proposal?", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_In_SQL,_Where_is_the_hallucination?", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 2392445, "end_ms": 2674129}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > What is the cost of natural language processing in Suql?", "content": "Title: CS224V Lecture 12 > Chapter Summaries > What is the cost of natural language processing in Suql?\n\nContent: What is the most important AI? What is the standard AI problem that we worry about in natural language processing? The semantic parser. How am I accurate? Can I translate the natural language into code? And the thing is the speed, the efficiency.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_What_is_the_cost_of_natural_language_processing_in_Suql?", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 2674177, "end_ms": 2740325}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Quantum SQL Algorithm: The Implementation", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Quantum SQL Algorithm: The Implementation\n\nContent: Ben: We have to update the semantic parser because we no longer generate SQL, we generate SuQL. Does it know how to generate S uQL? Ben: Here are the three main ideas that are implemented in the compiler. How would you improve it?", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Quantum_SQL_Algorithm:_The_Implementation", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 2741145, "end_ms": 3227077}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > In the world of SQL,", "content": "Title: CS224V Lecture 12 > Chapter Summaries > In the world of SQL,\n\nContent: You have to work it into your agent. All the stuff that we are talking about, because otherwise it just doesn't run. These optimizations are possible only because we are optimizing across the whole query. Is the optimization necessary? I would say yes.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_In_the_world_of_SQL,", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 3227261, "end_ms": 3341015}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Quantum Intelligence and the evaluation of our AI", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Quantum Intelligence and the evaluation of our AI\n\nContent: This is a real life Yelp bot. It actually uses all of Yelp data. The platform we're using is Prolific. How do you do evaluation? There are so many difficulties.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Quantum_Intelligence_and_the_evaluation_of_our_AI", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 3342235, "end_ms": 3785185}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Podesta: parsing errors and query evaluation", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Podesta: parsing errors and query evaluation\n\nContent: There are actually two parsing errors only for this small domain. The query evaluation on the other hand has issues. The parsing, however, in this data set is not a huge problem.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Podesta:_parsing_errors_and_query_evaluation", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 3785525, "end_ms": 3873535}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Yelpbot's 'Give Us Feedback'", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Yelpbot's 'Give Us Feedback'\n\nContent: The chatbot is available on genie, yelpbot and Stanford Edu. If you are looking for a restaurant in the Bay Area, go give it a try and see what it says. Give us feedback.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Yelpbot's_'Give_Us_Feedback'", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 3873835, "end_ms": 3954597}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Hybrid QA: The challenge", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Hybrid QA: The challenge\n\nContent: In our preliminary assessment, we discovered that hybrid QA is definitely harder than SQL. It is hard for the semantic parser to identify the right text column. Once we put those common patterns in that it improves the system.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Hybrid_QA:_The_challenge", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 3954741, "end_ms": 4322105}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > The evaluation of our Sota", "content": "Title: CS224V Lecture 12 > Chapter Summaries > The evaluation of our Sota\n\nContent: We are building a zero shot system. We only do the evaluation on the test. All the other work is just not relevant to our project. There's a huge difference in the cost in getting the data and doing the training. You start with error analysis, and then you figure out what to do.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_The_evaluation_of_our_Sota", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 4323205, "end_ms": 4683031}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > Semantic Parsing with LLM", "content": "Title: CS224V Lecture 12 > Chapter Summaries > Semantic Parsing with LLM\n\nContent: The true errors we have is about 39%. Out of that 39% is semantic parsing LLM based functions. We actually have to do more work on semantic parsing and we will talk about that in the next class. With the 85% true accuracy potential true accuracy, S uql works well for natural queries on small databases.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_Semantic_Parsing_with_LLM", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 4683223, "end_ms": 4978201}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Chapter Summaries > How to upload a corpus in 2 Minutes", "content": "Title: CS224V Lecture 12 > Chapter Summaries > How to upload a corpus in 2 Minutes\n\nContent: Harshit wants to tell you a tool that you may want to use for your project. We are making it easier for you to index your corpus. What we expect from here is we just want you to provide us your documents which are already chunked. Then you can plug it in into Storm, wikichat and other services and use it.", "block_metadata": {"id": "CS224V_Lecture_12_>_Chapter_Summaries_>_How_to_upload_a_corpus_in_2_Minutes", "document_type": "chapter summary", "lecture_number": 12, "start_ms": 4978313, "end_ms": 5096325}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 98735 ms - 177007 ms", "content": "Title: CS224V Lecture 12 > Transcript > 98735 ms - 177007 ms\n\nContent: Let's start on time. All right. How's everybody? Good. So I don't know if you guys have noticed that all my slides are missing the most important sentences. They just start with quiz and there is no answer. And those are the most important pieces of information. And maybe we can remind everybody, you know, the notes are not supposed to be read by themselves. If you just look at the notes, you are missing the whole point because I believe that if I just tell you what they are, they don't stick. So I'm hiding them from you. And that's, I hope that makes it more interesting to come to class. And I think that that is, you know, I want to make it to be something that you have to think actively as you listen to the lectures. So I just want to remind everybody that looking at my slides are a really bad idea. I mean, only look. Looking only at my slides are really bad idea. All right, today we're going to talk about building agents that use structured and unstructured data. And I think that", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_98735_ms_-_177007_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 98735, "end_ms": 177007}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 163415 ms - 236961 ms", "content": "Title: CS224V Lecture 12 > Transcript > 163415 ms - 236961 ms\n\nContent: bad idea. I mean, only look. Looking only at my slides are really bad idea. All right, today we're going to talk about building agents that use structured and unstructured data. And I think that there are a lot of projects in this class that can take advantage of, of this platform. So I just want to summarize what we learned from last time. So we said that if you pick any domain from the restaurants to the course, the course advisor and so forth, there are many of these agents require hybrid data sources. We talked about various methods. One is to say, okay, we have ir, we have databases, we decide which one to use and then we call one or the other. You can do the binary classifier upfront or you can do it afterwards, or you say, let's combine them. And we talked about two methods. One is that you turn all the structured tables and databases into text and you just use IR to retrieve everything. And we say, look, but you are losing the advantages of databases relational algebra where", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_163415_ms_-_236961_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 163415, "end_ms": 236961}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 222057 ms - 292177 ms", "content": "Title: CS224V Lecture 12 > Transcript > 222057 ms - 292177 ms\n\nContent: that you turn all the structured tables and databases into text and you just use IR to retrieve everything. And we say, look, but you are losing the advantages of databases relational algebra where you can sort and you can do all kinds of things including joins. Joins in IR really, you know, are just difficult. Then you say, okay. Then there's another method that tries to combine them. And it was a one hop method. You retrieve some from the tables, retrieve some from the text, and then you say LLM, answer the question based on these, this evidence. Okay, and the, and in the LLM it looks, and in the system it looks at some patterns to do sort and so forth. It doesn't have the full power of the relational algebra operators. So now what do we do? Well, you already know, we've discussed it and you know about the solution, but I just want to walk through what are the goals here? You want to keep, you want to Keep the hybrid data sources. You don't want to mash up your, you know, smash your", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_222057_ms_-_292177_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 222057, "end_ms": 292177}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 279433 ms - 354151 ms", "content": "Title: CS224V Lecture 12 > Transcript > 279433 ms - 354151 ms\n\nContent: you know about the solution, but I just want to walk through what are the goals here? You want to keep, you want to Keep the hybrid data sources. You don't want to mash up your, you know, smash your knowledge base into text and you don't want to turn text into knowledge base. You have to do multi hop. You have to compose the knowledge arbitrarily and you want to have the efficient searching of free text. You know, we are able to search the entire Internet, all of wikipedias and so forth. We have to take advantage of that. We also have to take advantage of algebra. So you really want to use the sources in their original, you know, in the, you know, in the original form they are stored. So what we are talking about is that since LLM understand SQL, let's put the free text retrieval into SQL. And that's why we came up with a language called sukl. This S U Q L Stanford University query language, right? If you have a hard time remember the name, just remember where you are hearing it from,", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_279433_ms_-_354151_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 279433, "end_ms": 354151}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 339915 ms - 421653 ms", "content": "Title: CS224V Lecture 12 > Transcript > 339915 ms - 421653 ms\n\nContent: And that's why we came up with a language called sukl. This S U Q L Stanford University query language, right? If you have a hard time remember the name, just remember where you are hearing it from, right? But it is structured and unstructured query language and it extends SQL to include free text operations. And based on this S UQL as an intermediate representation, I would say we have created a agent framework. I talked too loud. Maybe you can turn it down a little bit. So the whole concept here is that we want to make it easy for people to build agents. The developer only says, it only shows me the schema of the SQL and all the sources that are in free text. If your table has free text, we want to see the table because we want to index it. Okay, so you just provide me with this. I provide you with an agent from front to end. Okay? So that's a framework using the SQL. So SQL itself is a high level language, like a database language. It's domain independent. You can use it to put any", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_339915_ms_-_421653_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 339915, "end_ms": 421653}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 407077 ms - 490567 ms", "content": "Title: CS224V Lecture 12 > Transcript > 407077 ms - 490567 ms\n\nContent: you with an agent from front to end. Okay? So that's a framework using the SQL. So SQL itself is a high level language, like a database language. It's domain independent. You can use it to put any information in there. I mean it's totally domain independent. We just do it once and for all. It works for all domains, create you agents in all different domains. The SQL divides the whole conversation, the implementation of the agent into two parts. First of all, SQL becomes the target for semantic parser. We take natural language down to S uQL and that's an LLM based parser. Once you describe it in S uQL, we go and execute it. We have a compiler, it's an optimizing compiler that efficiently handles the implementation of the SQL queries. So the S UQL is kind of like the intermediate representation separating the AI part from a deterministic part. And this reduces the amount of work that has to be done by the AI part and increase improving the accuracy and performance throughout. So that's", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_407077_ms_-_490567_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 407077, "end_ms": 490567}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 475381 ms - 558355 ms", "content": "Title: CS224V Lecture 12 > Transcript > 475381 ms - 558355 ms\n\nContent: separating the AI part from a deterministic part. And this reduces the amount of work that has to be done by the AI part and increase improving the accuracy and performance throughout. So that's a whole system design with SQL being the key abstraction. So that's really important, you know, if you can take away some of the complexity you are, you know, you don't have to rely on being so accurate at the, at the LLM base level, which obviously is statistical. So with this framework, what do you have to do as a user, as a developer? You provide the schema. The schema is like SQL except that we pay attention to the things that we call text like popular dishes, the reviews. This is not what we have discussed before because these are the free text fields and you provide the schema, you provide me all the free text because the SQL framework will create the vector store for the information retrieval. Okay, so this is the backend and if you, it is also desirable for you to give me some examples", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_475381_ms_-_558355_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 475381, "end_ms": 558355}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 538898 ms - 621061 ms", "content": "Title: CS224V Lecture 12 > Transcript > 538898 ms - 621061 ms\n\nContent: all the free text because the SQL framework will create the vector store for the information retrieval. Okay, so this is the backend and if you, it is also desirable for you to give me some examples and we can show, put the examples into the few shot system as of a few shot prompt. So for example, this is for the restaurant and it would say, oh, a family friendly restaurant that serves burgers. Then you show me the target of what you want me to generate and we go on from there. So we don't just look at the name of the schemas, but actually you get to see how the schema fields are used and we can do a better job. So this is some few shots. One of the experiments that we are doing. We are working on arbitrarily many. I mean we are working with 75,000 different tables and we don't have few shots. Okay, so we can also handle 00shot to some extent. So this is what we're gonna do today in the lecture. We're gonna start by the design of SQL. We don't normally do designs of languages in AI", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_538898_ms_-_621061_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 538898, "end_ms": 621061}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 604917 ms - 684261 ms", "content": "Title: CS224V Lecture 12 > Transcript > 604917 ms - 684261 ms\n\nContent: shots. Okay, so we can also handle 00shot to some extent. So this is what we're gonna do today in the lecture. We're gonna start by the design of SQL. We don't normally do designs of languages in AI classes, but hey, this is an agent and you have to do work and it is your friend. So we are going to talk about the language, the design and the rationale and the implementation of the semantic parser that takes you to SQL and the compiler will connect to the databases and the IR vector tool and then we're going to do evaluations. We skipped the evaluation of Yelp long time in the first lecture. I mean on Monday because we said we really need hybrid data. So we're going to pick it up here. We introduced hybrid QA from yesterday Monday class and we want to show you the evaluation of that large database as well. Okay, so four topics. The design of SQL is very simple. You just add two free text and primitives into SQL and they are summary and function and answer. So for example, we've seen", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_604917_ms_-_684261_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 604917, "end_ms": 684261}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 664323 ms - 754183 ms", "content": "Title: CS224V Lecture 12 > Transcript > 664323 ms - 754183 ms\n\nContent: database as well. Okay, so four topics. The design of SQL is very simple. You just add two free text and primitives into SQL and they are summary and function and answer. So for example, we've seen this a few times I want a family friendly restaurant in Palo Alto. We are searching with a filter on location. Nicos for Palo Alto. You want a family friendly restaurant, it's coming from the reviews. And because we want to be friendly, we want to show you a summary of the reviews. Okay, so those are the two functions. If you look at this statement, this is a perfectly legal SQL query. By the way. The summary and answer are just two kind of like programmer provided functions. Syntactically it is nothing but an SQL statement. So the minute you provide those two functions to LLM and you say I have these two functions, here's some example, it can generate SQL syntax because this is well known. So we're very careful to make sure that we take, we leverage what LLMs know about SQL. Now, how do", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_664323_ms_-_754183_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 664323, "end_ms": 754183}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 737623 ms - 827265 ms", "content": "Title: CS224V Lecture 12 > Transcript > 737623 ms - 827265 ms\n\nContent: these two functions, here's some example, it can generate SQL syntax because this is well known. So we're very careful to make sure that we take, we leverage what LLMs know about SQL. Now, how do you implement summary and answer? Here's the quiz. We have already talked about how you handle free text. So how do you implement summary answer? Yeah, Yvette? Yeah. You could ask LLM to summarize. Just like, just summarize the specific text. Sorry, excuse me. Oh, could you like just ask the LLM like just summarize this piece of text? It seems like something, what, what am I giving to the LLM to summarize like in this, like, just like the text, the full text. What's in the reviews? All the reviews. Okay, maybe not. We are going to call LLM with text to summarize. And what text is it in this case? Yeah, it's what gets returned in that star with the limit one, such as any family friendly restaurant. But then it has to do the answer, it has to call the answer function to decide if it's a family", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_737623_ms_-_827265_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 737623, "end_ms": 827265}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 814065 ms - 895101 ms", "content": "Title: CS224V Lecture 12 > Transcript > 814065 ms - 895101 ms\n\nContent: Yeah, it's what gets returned in that star with the limit one, such as any family friendly restaurant. But then it has to do the answer, it has to call the answer function to decide if it's a family friendly restaurant. Right, the answer function for summary. Summary. We just want a summary. Okay, so first of all at the bottom you are going to return a bunch of records and it has a reviews field and there are many, many reviews. So at least I'm talking about the reviews of the result that is returned by the SQL statement and the reviews. You can try to summarize all of it or you can try to summarize if the reviews are rated, the top reviews or the people, the reviews that people want. And so that is the summary part. Okay, we're just going to be using LLM and you just prepare the data for it. And hopefully if I am, you know, if I have limited down to just one record, it will be the reviews for that one record. Okay, what about answer? Yeah, Michael, I guess you would call it for all", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_814065_ms_-_895101_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 814065, "end_ms": 895101}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 873171 ms - 946247 ms", "content": "Title: CS224V Lecture 12 > Transcript > 873171 ms - 946247 ms\n\nContent: it. And hopefully if I am, you know, if I have limited down to just one record, it will be the reviews for that one record. Okay, what about answer? Yeah, Michael, I guess you would call it for all of the things that apply to anything else in your query. So you'd first probably filter to location equals Palo Alto. And you could call it per row. Basically some prompt where you give it this question and actually I wanted to ask you about answer. Do you do any scaffolding for like you want a binary yes or no or is it like the answer could be an open ended type answer as well. So what do we do here? Is that how do you retrieve. Okay, you are saying that I'm just going to find all the records and I'm going to read it one by one. That's a naive thing that I think. Well, I think you would want to continue. You know how many Palo Alto restaurants are there? Probably a lot. Yeah. How many reviews are there for each restaurant? Oh yeah, Even long. That's a lot of reviews. So what do we do when", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_873171_ms_-_946247_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 873171, "end_ms": 946247}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 932177 ms - 1010227 ms", "content": "Title: CS224V Lecture 12 > Transcript > 932177 ms - 1010227 ms\n\nContent: continue. You know how many Palo Alto restaurants are there? Probably a lot. Yeah. How many reviews are there for each restaurant? Oh yeah, Even long. That's a lot of reviews. So what do we do when you have a lot of free text? Huh? Maybe you aggregate some of the. How do I aggregate? Yes. What's your name? J. J. You do RAG on it. You rag? What does RAG do? It identifies the relevant information out of like you have a bunch of texts. How do I implement rag? I don't know. What did we say? Yes. Vector embedding. Vector embedding. I was trying to remind you, you give me text, I have to index it. We did mention Colbert, for example, as an implementation. I did not tell you exactly how Colbert is done, which we will do later. But the whole concept here is just to remind everybody is that you have a set of documents. You use a neural network that has been trained so that it has a good embedding for both documents and questions so that they can find each other by using cosine similarity.", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_932177_ms_-_1010227_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 932177, "end_ms": 1010227}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 994489 ms - 1068923 ms", "content": "Title: CS224V Lecture 12 > Transcript > 994489 ms - 1068923 ms\n\nContent: have a set of documents. You use a neural network that has been trained so that it has a good embedding for both documents and questions so that they can find each other by using cosine similarity. Okay, so you are using a pre trained model that can take your question into an embedding and then you go and do a cosine similarity with all the. And all the vectors that you compute for this, the corporate you have. So this is why at the beginning I say for the SQL framework, you supply me with a free text. I do the index so that when you ask me a question, I go find it here. I have to use IR for that purpose. Yes. What if the question, the embedding is very different from the answer. So for example, you're asking what's the analysis of certain trends? And then inside your SQL it's actually numbers. Inside MySQL I am using answer functions for free text only. Right? I mean everything else I'm using, if it is a number, if it is a field, I do numbers. This is really to say I have many", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_994489_ms_-_1068923_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 994489, "end_ms": 1068923}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1052107 ms - 1127497 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1052107 ms - 1127497 ms\n\nContent: numbers. Inside MySQL I am using answer functions for free text only. Right? I mean everything else I'm using, if it is a number, if it is a field, I do numbers. This is really to say I have many reviews, many restaurants. Find me even under Palo Alto. Find Me a review that talks about family friendly. Okay. And this is just to retrieve you the article. And then with that article you ask the LLM, does it say yes, the embedding is usually very loose, right? Suppose I say does it have good parking? Sometimes people say it has horrible parking. All right? But when I do an embedding, I'm not very precise whether it is good parking or bad parking. I just find you something that talks about parking. So when you retrieve the articles, you still use an LLM to answer the specific question that you are talking about here so that you can distinguish between the good and bad parking. As an example, you don't just retrieve and you assume that if I retrieve something, it has the answer. No, it", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1052107_ms_-_1127497_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1052107, "end_ms": 1127497}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1115841 ms - 1190729 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1115841 ms - 1190729 ms\n\nContent: you are talking about here so that you can distinguish between the good and bad parking. As an example, you don't just retrieve and you assume that if I retrieve something, it has the answer. No, it just. You just have found the relevant paragraphs and then you use LLM to find the answer. So after you retrieve it gives you a whole bunch of vectors. Each one of them correspond to some chunk in the text. And you can rank them and then you pick the pick them one by one until you find enough, exam, enough data. In this case, I just want one, for example. Then you just check with that and then you come back. We do not want to run this LLM on all the text one by one. So this is how we combine IR with database access. If I don't have the ir, then I will have to retrieve all the free text and now find a review that talks about family friendliness. Okay, got it. All right, so that is the. That is it. This is SQL and it has this ability that anywhere in any of these statements, any clause could", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1115841_ms_-_1190729_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1115841, "end_ms": 1190729}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1168735 ms - 1257741 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1168735 ms - 1257741 ms\n\nContent: find a review that talks about family friendliness. Okay, got it. All right, so that is the. That is it. This is SQL and it has this ability that anywhere in any of these statements, any clause could be a free text operation. So any questions about this? I only have two functions. Is that enough or is it too many? Hi Michael. Is it all left up to the LLM to resolve disagreements? Like what if some people think it's family friendly and some don't? Do you just leave it to the LLM? At the last stage. You can improve this if you like. And it is easy to see that people want to have, you know, once I pull out the articles that has Family friendly, it's relatively easy for you to change the prompt to do whatever you want with the information. Okay, but I'm keeping it simple. But that's a good point, right? So for example, summary of reviews. Should I summary every, you know, all the reviews or should I just pull a few? I mean, that's a little bit, you know, that's something that you can tune", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1168735_ms_-_1257741_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1168735, "end_ms": 1257741}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1245053 ms - 1319411 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1245053 ms - 1319411 ms\n\nContent: right? So for example, summary of reviews. Should I summary every, you know, all the reviews or should I just pull a few? I mean, that's a little bit, you know, that's something that you can tune for the, for the purpose in restaurants. People don't care that much and you wouldn't be doing as much and it will still be okay. All right. But for other purposes, I mean, I want to summarize all the drug side effects. Then everything has to be, you know, I'm not looking for limit one. I'm not looking for a few reviews, okay? So you can tune it with your specific domain. So how about the number summary and answer? Do you feel like we have enough functions or do you have. Why do we have summary? Why can't we just say, answer the summary, Give me the summary. That's an answer. That's a question. Give me a summary of this review. Why do we have two and why not more? Does it sound right to you? We actually have a discussion, George and I, about whether we actually need a summary even. Because", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1245053_ms_-_1319411_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1245053, "end_ms": 1319411}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1301899 ms - 1381347 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1301899 ms - 1381347 ms\n\nContent: Give me a summary of this review. Why do we have two and why not more? Does it sound right to you? We actually have a discussion, George and I, about whether we actually need a summary even. Because you can just answer, get a summary. What's the difference? So the whole point here is that we're doing IR and you can ask for anything in a single answer. Okay? I mean, whatever that it is that you want to know, it is a natural language. You just put it in there. So strictly speaking, you don't need a summary. But the reason why the summary is there is because it is very natural. There is no yes or true or false in that question. And it is very natural for you to say, just give me the summary. It is just important enough that we feel like that would make it very easy for. Make it actually easy for the semantic parser to generate that statement knowing exactly what it actually is trying to do. So that's the basic idea. So we talked about this hybrid QA database data set with tables and", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1301899_ms_-_1381347_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1301899, "end_ms": 1381347}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1366271 ms - 1462491 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1366271 ms - 1462491 ms\n\nContent: easy for the semantic parser to generate that statement knowing exactly what it actually is trying to do. So that's the basic idea. So we talked about this hybrid QA database data set with tables and hyperlinks, with text and tables, and we talked about these various types. And by the way, I just discovered that we talked about that T and S. It turns out to be a typo in the original paper. It really just means tables and paragraphs, it turns out. So what I want to do is to see if you know how to write SQL statements. I want to get you thinking about this. And again, it is kind of hard to write SQL statements. So I'm going to give you SQL statement and you tell me what the English is. What is this? You start from the bottom. You should. What is this question? Can you read it? So this is a table of flag bearers and it has name the flag bearer. This is the picture I show you, right? The gender. And then there are several text boxes which gives you flag bearer info and the event year", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1366271_ms_-_1462491_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1366271, "end_ms": 1462491}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1448327 ms - 1548857 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1448327 ms - 1548857 ms\n\nContent: a table of flag bearers and it has name the flag bearer. This is the picture I show you, right? The gender. And then there are several text boxes which gives you flag bearer info and the event year info. Okay? And so this is the question, this is the query itself. What does it ask for? Aaron, where was the 31st Olympiad Hill. Yeah. Okay. Yeah. So you are Looking at this table flag, and then you pull out the name, has to be 31st, you pull out the record, and then it looks at the event year info, which is text box, and you ask, where is this event held? And the answer is, oh, wow. Where it is held. So that's the right answer. So here's a second question. Oh, it's the same one, isn't it? Oh, this is a different one. Sorry. I give you the answer already. It's like, where is this event held? Looking at the. In this case, what does it actually say? You have already picked out the event location, Rio. And this information is only found in the free text. So you have to go find the record with", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1448327_ms_-_1548857_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1448327, "end_ms": 1548857}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1532865 ms - 1663391 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1532865 ms - 1663391 ms\n\nContent: at the. In this case, what does it actually say? You have already picked out the event location, Rio. And this information is only found in the free text. So you have to go find the record with that free text in it. And then you ask for the name, and then you say, what was the name of the Olympic event held in Rio? Okay, so the last one, you're going from. From the table to paragraph. And here you're going from a paragraph to the table. Okay. It's all compositional. If you look at the English sentence, you don't. You know, it could be any of these things. I mean, but it's very natural. But you need to compound the search, the access. So here is the third one, which is this. What does it say? Yes, thank you. So when was the flat barrel of Rio's Olympic war? You see, the sentence is very short. It's very natural, but it actually requires you to do this to find the answer. All right, fourth one, getting longer. What is it? Is it. Is it easy for people or can you do it? Is everybody able", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1532865_ms_-_1663391_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1532865, "end_ms": 1663391}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1618005 ms - 1753337 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1618005 ms - 1753337 ms\n\nContent: It's very natural, but it actually requires you to do this to find the answer. All right, fourth one, getting longer. What is it? Is it. Is it easy for people or can you do it? Is everybody able to do it? I just want to know. Yeah, all right. What do you think? Which male flag bearer participate in the men's 100 kilogram event? Yeah. Perfect. Because they can be both one female and one male bearers. I mean, you know, when they carry the flags. So thank you. Very good. What about this one? How about this one? Yes. It's the youngest that was competing between 20. Exactly. Given these two events, when was the younger flatbearer born? Because I'm taking the max on their date. The max on the date is the youngest person. Okay. So you see that Very natural. Things turn into simple calculations in a sense. But the parse is. It's very succinct. I mean, it. It's very succinct and very clear. What is it that you want? Okay, and this is where the interpretability comes from. Right? If you ask me", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1618005_ms_-_1753337_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1618005, "end_ms": 1753337}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1734591 ms - 1834023 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1734591 ms - 1834023 ms\n\nContent: a sense. But the parse is. It's very succinct. I mean, it. It's very succinct and very clear. What is it that you want? Okay, and this is where the interpretability comes from. Right? If you ask me for a question, I give you an answer. I know exactly what I'm looking for. I can tell you what it is. I don't just say this person because you don't know if it is right or wrong. Okay, so this is really important. Okay, the last one, what does this say? Is that the oldest flag there? Yes. It'S flipped. We are sorting in descending order. The later date it is, the youngest, the younger it is. Where did Burmese come from? Because it's a table. The table itself is a Burmese table. Okay, so you have this question. It will pull out that table. Okay. It is not, it is not quite complete in terms of going backwards, but that's where that came. Okay, so the table was given. It is actually the Burmese flap. Very good. But I just want you to get so used to the concept that even these fancy state, you", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1734591_ms_-_1834023_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1734591, "end_ms": 1834023}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1821127 ms - 1886105 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1821127 ms - 1886105 ms\n\nContent: backwards, but that's where that came. Okay, so the table was given. It is actually the Burmese flap. Very good. But I just want you to get so used to the concept that even these fancy state, you know, it generates fancy queries, but they are very natural sentences. So you really need this level of support and complexity. And at a glance, these are the things that we just went, we just talked about. And if we just look at where answer is, it's just, it can show up as a filter, it can show up as the answer. You can compute on top of the answer and so forth. It is just arbitrary combinations. Yes. One of the things that we're doing in SQL was let's say the where filter, when you have to quote, it has to be exact match. And so I was wondering in this case other ways that when you pull the answer from answer is there are ways to still compete compared between for example, men's 100kg event and maybe the man. M is lowercase. Could you still compare the right one using other functions? So", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1821127_ms_-_1886105_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1821127, "end_ms": 1886105}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1871431 ms - 1945111 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1871431 ms - 1945111 ms\n\nContent: from answer is there are ways to still compete compared between for example, men's 100kg event and maybe the man. M is lowercase. Could you still compare the right one using other functions? So our compiler takes care of things like that. We overload those equals with the right using LLM calls actually. So this is the nice part is all these details like what you discuss guys discussed is hidden from the semantic parser and it's now taken care of by the compiler and it is done once and for all for everybody working using. Very good, thank you. So we have seen restaurants before, you know, it is just another longer example, you know. Do you have a recommendation for a first date restaurant in Palo Alto? We're thinking sushi, but not sure what's going around here. There are many pieces here and it turns into this statement that talks about the cuisines, the location and you can ask whether it is good for a first date. We just pull it out and see what happens and so forth. So this is for", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1871431_ms_-_1945111_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1871431, "end_ms": 1945111}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1927093 ms - 2012737 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1927093 ms - 2012737 ms\n\nContent: and it turns into this statement that talks about the cuisines, the location and you can ask whether it is good for a first date. We just pull it out and see what happens and so forth. So this is for restaurants. It can actually be very useful for just about all product searches. Product searches have a lot of attributes, descriptions, reviews and so forth. And so here is an example. I need a laptop with this function and 16 gigabyte RAM and for my workstation setup. So when we do the semantic parsing, we know the workstation setup is not interesting. Okay. You don't translate that into the parse and then you put the rest of it in. So here is an example where I need to know if it has the Thunderbolt 3 connection. But then the problem now is which text do you want to get it out of? And what we have discovered in our experiment is that it is often that the answer is in the text, but you don't know where it is. Okay. And if you just ask it to use just one description, it may fail. So it", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1927093_ms_-_2012737_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1927093, "end_ms": 2012737}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 1997711 ms - 2095501 ms", "content": "Title: CS224V Lecture 12 > Transcript > 1997711 ms - 2095501 ms\n\nContent: we have discovered in our experiment is that it is often that the answer is in the text, but you don't know where it is. Okay. And if you just ask it to use just one description, it may fail. So it is a simple thing for you to do because we have IR anyway. It's just to broaden it to text field. If something doesn't return, you try other text fields just to get around that error. So you don't have to be that precise. So this is the kind of the interactions between semantic parsing and these field selections. So what else can we use S UQL for? We talked about restaurants, we talked about flag bearers. Do you have examples where you find this useful? Maybe your own projects? How useful is it? Can you think of one? Yeah, tons of information that is structured and unstructured. We have a chat that lets you talk about donations. And the first thing happened is that it says, here is a pack. And then you look at that and say, I have no clue what that is. You know, I've never seen that word,", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_1997711_ms_-_2095501_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 1997711, "end_ms": 2095501}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2078563 ms - 2157949 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2078563 ms - 2157949 ms\n\nContent: that lets you talk about donations. And the first thing happened is that it says, here is a pack. And then you look at that and say, I have no clue what that is. You know, I've never seen that word, that pack before. And it's like, what is that pack? So you need to look at information, even though the table is all. Even though the donation table is all numbers and all in columns. But there is a lot of extra information that you need in order to interpret the table that is in free text. So that's an example. What else? I know that I talked to some of you working on the KQED project where you can answer questions about podcasts. And there are many, many podcasts. They are all free text, but you also have metadata. Even something like that has metadata, such as the date of the podcast, the people involved, and so forth. And now if I can use that, I can easily reduce the scope of the search and be better at that. Okay, so even if you give me text like the African Times, it's all different", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2078563_ms_-_2157949_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2078563, "end_ms": 2157949}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2141661 ms - 2219719 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2141661 ms - 2219719 ms\n\nContent: involved, and so forth. And now if I can use that, I can easily reduce the scope of the search and be better at that. Okay, so even if you give me text like the African Times, it's all different episodes, but there are metadata that can help refine the search. I say anything that is after 1900. So then you can pull it out from the episodes, from the issue information, and so forth. So almost anything that we see here has a Combination. Sometimes this is table heavy, sometimes this is text heavy, sometimes it's just a combo. Any other suggestions ideas? Yeah, there was like a trip planning. Thing and I think like you could have something where it's like find me a flight with some that's cheap and some are warm. And then maybe you could like join a flights database with like descriptions of vacation destination. Yeah, all product searches have that flavor and that's definitely the case. Yeah. Find the potential co founder candidate leaving Palo Alto mountaineer or Menlo park who is", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2141661_ms_-_2219719_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2141661, "end_ms": 2219719}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2200023 ms - 2280733 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2200023 ms - 2280733 ms\n\nContent: of vacation destination. Yeah, all product searches have that flavor and that's definitely the case. Yeah. Find the potential co founder candidate leaving Palo Alto mountaineer or Menlo park who is interested in conversational AI and finished Stanford. Very good. Let's go find those people. Yeah. And where is the free text in this case? Well, you would be interested in and it would be thinking going against LinkedIn profiles right now. Sounds good. Yeah. I participated in a project where we're analyzing propaganda in authoritarian regimes and it was extremely exhausting because we had to go to newspapers of these different types of countries and spend most of the time retrieving the metadata, first putting into table and then kind of annotating. So I guess these tool might reduce the friction. So keep that in mind. If you are doing free text, there may be the structured data there too. So. So the whole concept here is that once we put this into a framework, it doesn't matter, you", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2200023_ms_-_2280733_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2200023, "end_ms": 2280733}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2265859 ms - 2333265 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2265859 ms - 2333265 ms\n\nContent: friction. So keep that in mind. If you are doing free text, there may be the structured data there too. So. So the whole concept here is that once we put this into a framework, it doesn't matter, you know, you take the full power. You know, some. So what I saw is that some of these projects I just have a little metadata. Can I just code that in specially? It's like, well, if you have a system like this, when the IR part is as good as any IR that you can do rack that you can do, then you don't have to worry about and you can take the advantage of the full the strengths of both IR and databases. Okay. So they don't usually look like what we showed you, which is kind of half and half. So I just want to summarize the advantages of SQL is that this is a formal representation that combines the two. It is very succinct, very expressive. Expressive meaning that I can mix them arbitrarily, totally domain independent. This is the power of programming languages. And then you can combine the", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2265859_ms_-_2333265_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2265859, "end_ms": 2333265}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2319819 ms - 2405585 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2319819 ms - 2405585 ms\n\nContent: the two. It is very succinct, very expressive. Expressive meaning that I can mix them arbitrarily, totally domain independent. This is the power of programming languages. And then you can combine the techniques. The compiler compile combines the techniques of IR and relational algebra. You don't have to think about it. Very interpretable. I can read the query back to you, so you know what questions I'm answering. So when I was doing the databases, I said once you give me the query I can read, I can execute it. The answer is also so always correct. In this case, is it always correct. You turn the natural language into formal and I say the answer to this, to your MySQL query is this. Can you guarantee that it is correct like we did with SQL? Very important question. Who thinks yes. Who thinks? No. All right. Why not? Why not? When you do rag, there could be a hallucination. In terms of like the retreat, what's. The level of hallucination you expect? It depends on the embedding quality.", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2319819_ms_-_2405585_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2319819, "end_ms": 2405585}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2386429 ms - 2470373 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2386429 ms - 2470373 ms\n\nContent: No. All right. Why not? Why not? When you do rag, there could be a hallucination. In terms of like the retreat, what's. The level of hallucination you expect? It depends on the embedding quality. So basically like whether you can retrieve the right information. So how does that affect hallucination? So the embedding, if it's like actually very relevant information, retrieves, they can maybe generate the right answer. But if the embedding quality is bad, they might retrieve the wrong information and then generate. I retrieved the information is still correct. Where is the hallucination? So basically the retrieved one could be incorrect because the embedded. I only retrieve things that you trust because you give me the corpus, right? I'm retrieving from the corpus you give me. If you trust it, I'm not hallucinating. With respect to the corpus, the question is, where is the hallucination? What do you expect that level to be? Using the methods that we have talked about in class? I just", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2386429_ms_-_2470373_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2386429, "end_ms": 2470373}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2449265 ms - 2527507 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2449265 ms - 2527507 ms\n\nContent: it, I'm not hallucinating. With respect to the corpus, the question is, where is the hallucination? What do you expect that level to be? Using the methods that we have talked about in class? I just want you. What is it? My point here is that don't jump right into the. Everybody says, oh, there's hallucination, right? And you have to think harder. It depends on the method. And when I was talking about the the project with Wikichat, we were at 97%, but it's not 100%. So the answer is yes, I definitely can make a mistake. But the level of the hallucination depends on what kind of rag you're doing. And the rag we are talking about is that I retrieve and then I only filter. So I reduce the hallucination down to very little. So that's the important thing to know, right? But one thing I want to point out, which you were getting at, which is very important, is the quality of the ir. I don't worry about reducing. I don't worry about hallucination because of bad ir. I worry about false", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2449265_ms_-_2527507_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2449265, "end_ms": 2527507}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2514211 ms - 2592061 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2514211 ms - 2592061 ms\n\nContent: want to point out, which you were getting at, which is very important, is the quality of the ir. I don't worry about reducing. I don't worry about hallucination because of bad ir. I worry about false negatives. I may not have retrieved the article that has the answer and I cannot give you an answer. Okay, so I won't hallucinate, but I may not give you the right answer. So there is a false positive problem and false negative. Okay, so there is because we are using free text functions. You have those two problems. You don't have that in SQL. Okay, so this is a little bit different and this is the best we know how to do. So that's really important. And then the other Thing is that SQL is a high level programming language. There is a lot of things hidden underneath the implementation and we will get to it. So these are the potential advantages. But SQL is new. How do you evaluate SQL as a design, as a proposal? What do you worry about? You know, what SQL does? What do you worry about?", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2514211_ms_-_2592061_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2514211, "end_ms": 2592061}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2573901 ms - 2658089 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2573901 ms - 2658089 ms\n\nContent: will get to it. So these are the potential advantages. But SQL is new. How do you evaluate SQL as a design, as a proposal? What do you worry about? You know, what SQL does? What do you worry about? What do you want me to tell you to convince you to use suv, in other words? Yes, Michael, I can think of two things. One would be like expressivity, like what are queries that you can ask with SQL that you couldn't ask with SQL? And another is like cost because LLM queries are going to be cost. Okay? The expressiveness, we have full compositionality, okay, we found a little hole and we make sure that everything can compose, but that is in our implementation as opposed to the language. So we know the scope is that we have all of relational algebra and we have all of IR because that's the function that you have and it's fully compositional. So that's a strap. The second question is the cost. That's a good question. What other questions are there? The cost is in the LLM, but there is another", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2573901_ms_-_2658089_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2573901, "end_ms": 2658089}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2640223 ms - 2717623 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2640223 ms - 2717623 ms\n\nContent: that you have and it's fully compositional. So that's a strap. The second question is the cost. That's a good question. What other questions are there? The cost is in the LLM, but there is another question of cost is how fast am I going to execute that query? Okay, like you said, if I'm going to bring them in one text at a time, now that is expensive without even thinking about the LLM part because you are bringing in too much data. So there is the cost and there is one very, very important question, and that is anybody else, anybody worrying about suql? What would you worry about? You give me the data, I'm just a shell. I mean, I'm just the implementation. You provide me with the data, right? It's the developer's problem. What is the most important AI? What is the standard AI problem that we worry about in natural language processing? The semantic parser. How am I accurate? Can I translate the natural language into code? Okay, so what is the accuracy? The accuracy comes from two", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2640223_ms_-_2717623_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2640223, "end_ms": 2717623}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2702487 ms - 2767787 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2702487 ms - 2767787 ms\n\nContent: that we worry about in natural language processing? The semantic parser. How am I accurate? Can I translate the natural language into code? Okay, so what is the accuracy? The accuracy comes from two places. In general, it's accuracy. The semantic parser is the first one and we just talked about the execution accuracy. Am I giving you the answer even though the query is. Even if the query is correct? May not be. We just talked about how it is not necessarily correct. Those are the two important questions. And the thing is the speed, the efficiency. Okay, so that's what we're gonna cover. So now let's talk about the implementation. So remember, this is the SQL agent design where you have the classifier, the semantic parser, go to the SQL compiler. In this case, we augment it with the Classification. Because we know that matching to classification needs a little bit more help than just throwing it in the semantic parser. Remember the cafe, the coffee example. So it is added to the", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2702487_ms_-_2767787_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2702487, "end_ms": 2767787}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2756489 ms - 2828891 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2756489 ms - 2828891 ms\n\nContent: Classification. Because we know that matching to classification needs a little bit more help than just throwing it in the semantic parser. Remember the cafe, the coffee example. So it is added to the compiler to find the word that may be in the in the database and if it is no results, you tell, we tell you and then otherwise you go to execute and fetch the database, fetch from the database and then you respond. So this is the SQL. What does S uQL look like? It turns out that a basic SQL has two parts. Of course I'm adding the summary execution and the answer execution and that is it at the high level. It will run, it won't be fast. And that's the optimizing compiler part we will talk about next. So in other words, we have to update the semantic parser because we no longer generate SQL, we generate SuQL. Does it know how to generate S uQL? Actually we haven't even really fully covered how well it can execute SQL because we said let's defer the evaluation. I just want to remind you", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2756489_ms_-_2828891_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2756489, "end_ms": 2828891}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2815299 ms - 2896477 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2815299 ms - 2896477 ms\n\nContent: we generate SuQL. Does it know how to generate S uQL? Actually we haven't even really fully covered how well it can execute SQL because we said let's defer the evaluation. I just want to remind you where we are at. Ok, so. And once you have a language, the compiler can hide the details from the users. Oh, oh, oh, oh. The implementation of this agent is still a few shot prompt for the semantic parser. It has the schema and it has examples where the summary and answers are used. And that was it, just a few examples. And it seems to do pretty reasonably well. You know, it seems to do reasonably well. For example, for Yelp on the slide you can click into it, it will show you the entire implementation of semantic parser. It's just a very small prompt. The compiler is a lot more complicated, which is what I'm going to talk about next. So as I said, summary and answer are just functions. You don't have to do anything. It will run, it will be very slow. Okay, we just talked about how are we", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2815299_ms_-_2896477_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2815299, "end_ms": 2896477}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2881347 ms - 2960969 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2881347 ms - 2960969 ms\n\nContent: is what I'm going to talk about next. So as I said, summary and answer are just functions. You don't have to do anything. It will run, it will be very slow. Okay, we just talked about how are we going to implement the fields and so forth. Okay, but it is too slow. So let us take a look at an example and you can tell me how would you improve this. Suppose I take the same thing. I have a little bit of. We have a free text answer function and a summary wrapped with this simple SQL statement and the location. Okay, so this is the statement. Let me show you a naive implementation. The first thing I'm going to do is that I execute a where clause. You say where answer is reviews and this is a statement. I will use IR to retrieve all the relevant reviews. Okay, not well, you know, you find out which reviews have the family friendly restaurant right mentioned and then you filter on location and then you get the reviews. Because I'm a chatbot, I am not doing research as much. I'm going to", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2881347_ms_-_2960969_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2881347, "end_ms": 2960969}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 2947353 ms - 3037571 ms", "content": "Title: CS224V Lecture 12 > Transcript > 2947353 ms - 3037571 ms\n\nContent: which reviews have the family friendly restaurant right mentioned and then you filter on location and then you get the reviews. Because I'm a chatbot, I am not doing research as much. I'm going to report the first few. This is a naive implementation. How would you improve it? Yes, I cannot remember. Ben, you should filter on anything that doesn't require summary or answer first. So filter on location first. Very good. Yeah, that's the first thing. What else? Yes, what is your name? Flora. Yeah, I guess like it just asks like I want a family friendly restaurant, so you just really need to return one bfu. But if you use the slide star or slide master. So if I start with okay, suppose I. So that's good. That's very good. You don't want to retrieve all the answers because at the end I'm going to report only a few. Very good. So these are the main considerations. So how would you. So here are the three main ideas that are implemented in the compiler. The first one is order filtering. Okay,", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_2947353_ms_-_3037571_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 2947353, "end_ms": 3037571}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3021481 ms - 3104515 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3021481 ms - 3104515 ms\n\nContent: to report only a few. Very good. So these are the main considerations. So how would you. So here are the three main ideas that are implemented in the compiler. The first one is order filtering. Okay, you always execute the structure predicates first and then that makes a huge difference. The second thing is to return only the necessary results, which is what Flora mentioned. Okay, so I don't need to return all the answers. So I'm going to just use the embed. I'm using the embedding model vector similarity. I return the top candidates and I just give you those answers and then you use that to do the answer function. So here's an example that is a little bit trickier. So here I want to know if it is family friendly. I also want to know that I have good parking and then I do location equals for Palo Alto. So first of all I reduce the list to only the restaurants in Palo Alto number one. But now what do I do? I have two answer functions. Do I need to find. The problem is that I have an", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3021481_ms_-_3104515_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3021481, "end_ms": 3104515}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3083621 ms - 3169667 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3083621 ms - 3169667 ms\n\nContent: for Palo Alto. So first of all I reduce the list to only the restaurants in Palo Alto number one. But now what do I do? I have two answer functions. Do I need to find. The problem is that I have an and okay, if I retrieve the. If I just retrieve five of the family friendly restaurants, the top 10, the top five, none of that may be. May have parking information in there. So I cannot stick with the small number how many do I have to do? It really changes. I mean I am not here to find all the combinations, but I have to find some combinations. So in other words, this is what we mean by lazy evaluation. We evaluate them, we bring in a few and then we see if I can filter it on the second, second, the second clause. If the answer is empty, I have to go back and fetch more. And so this is the concept of lazy evaluation. And then I can stop when I actually find an answer. So on the whole here is that the implementation is done by modifying The SQL compiler, what I show you is just the main", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3083621_ms_-_3169667_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3083621, "end_ms": 3169667}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3149835 ms - 3234989 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3149835 ms - 3234989 ms\n\nContent: concept of lazy evaluation. And then I can stop when I actually find an answer. So on the whole here is that the implementation is done by modifying The SQL compiler, what I show you is just the main concepts. But when you work on a compiler you have to deal with the fact that it has a grammar. It means that it can handle arbitrary complex nested statements. So I just give you a rough picture of what it means is that we have to process the syntax tree of the SQL recursively because it composes and we will start at the bottom and we rewrite it to do the optimizations. And then you keep repeating and you go to the top and in the rewrite. What we have to do is that you work on anything that has the free text functions like answers in it. You apply the optimization, you store the result in a temporary table and then you substitute this table and you move up and then you have to have the execution engine that handles the lazy evaluation. Okay, it is kind of non trivial and the question", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3149835_ms_-_3234989_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3149835, "end_ms": 3234989}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3218159 ms - 3291449 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3218159 ms - 3291449 ms\n\nContent: a temporary table and then you substitute this table and you move up and then you have to have the execution engine that handles the lazy evaluation. Okay, it is kind of non trivial and the question here is that if I don't have this compiler, you build it using a classic agent approach. What does that mean? Who will be doing this optimization for you? You have to work it into your agent. All the stuff that we are talking about, because otherwise it just doesn't run. People talk about function calling, but there's a lot of data potential that you have to go through and you have to optimize. This is the advantage of having a high level abstraction like a SQL and it will just hide the details and take care of it once and for all. Okay, get it. So these optimizations are possible only because we are optimizing across the whole query. I can tell you I don't just jump on the first subquery and work on it. I look at the whole query, the full semantics and I pick the ones that I have to do.", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3218159_ms_-_3291449_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3218159, "end_ms": 3291449}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3278641 ms - 3346251 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3278641 ms - 3346251 ms\n\nContent: we are optimizing across the whole query. I can tell you I don't just jump on the first subquery and work on it. I look at the whole query, the full semantics and I pick the ones that I have to do. This is classic in a sense. It is a version of the classic database query optimizations. People have done a lot of work on this area and this is why we get to take advantage of it. Is the optimization necessary? I would say yes. If I don't have it, if I just use function calling, I do not want to spend all the time debugging how the agent gets the efficiency done. Why, if I can do it deterministically with a very simple high level programming language, that's the way to go. So keep that in mind when you design your system. Factor out the things that needs to be done by AI. Natural language interfaces. Yes. Anything else you can do outside, you do it outside. You get the better control accuracy and interpretability and so forth. So now let's come to the evaluation. Let's start with Yelp.", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3278641_ms_-_3346251_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3278641, "end_ms": 3346251}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3331539 ms - 3410615 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3331539 ms - 3410615 ms\n\nContent: Yes. Anything else you can do outside, you do it outside. You get the better control accuracy and interpretability and so forth. So now let's come to the evaluation. Let's start with Yelp. We've been talking about Yelp. Let's start with Yelp. And I mentioned the first time is that when we did the experiment using our own energy, it just doesn't work because our annotations were bad. So this is an experiment. This is one of our first agents that we have built. This is a real life Yelp bot. It actually uses all of Yelp data. Okay. It is not a small table that we do for a academic project. It is a full life Yelp data. We scrape the reviews and popular dishes and so forth. And we handle San Francisco, Palo Alto, Cupertino in Sunnyvale. It's pretty large. LLM is GPT 3.5. We are doing IR. We're using Coco. Dr. In this case. And we have a compiler, an optimizing compiler, fully working. And let's talk about evaluation, how we know that we're not annotating and then doing the gold comparison,", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3331539_ms_-_3410615_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3331539, "end_ms": 3410615}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3394222 ms - 3468428 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3394222 ms - 3468428 ms\n\nContent: using Coco. Dr. In this case. And we have a compiler, an optimizing compiler, fully working. And let's talk about evaluation, how we know that we're not annotating and then doing the gold comparison, because we have failed to do that in the last time. So we care about real users. So we went and did a study with real users. We're doing crowdsourcing. The platform we're using is Prolific. By the way, we're not using amturk. Prolific is a. Is a company that they kind of have, you know, they have employees to do this kind of work and they are a lot more dependable than people that you get from Amazon Turks. And this is what we have been using for pretty much all our projects. We do not tell the workers what fields are available in the database. Okay, you just talk. You don't know what we managed to put in the database at all. So this is a very tough test, I would say. So we have two parts. The first one is just Q and A. It's 100 crowdsourced questions about restaurants. And the second", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3394222_ms_-_3468428_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3394222, "end_ms": 3468428}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3452381 ms - 3537345 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3452381 ms - 3537345 ms\n\nContent: to put in the database at all. So this is a very tough test, I would say. So we have two parts. The first one is just Q and A. It's 100 crowdsourced questions about restaurants. And the second one's conversational 20 conversations, 96 turns. Why is it so small? Because how are we going to do evaluation? All right, so the first thing I want to note is that out of those 100 questions for single turn, over 50% here, you know, requires hybrid data. And inside the conversation it is 40% is hybrid. So this is a very important functionality. So the question now is, how do you do this evaluation? Because I cannot. There are so many difficulties. If I give you a query, there are a lot of possible answers and it is not really important to match exactly the answer that you asked for because there are many choices. And for the case in real life, you don't need to see all the choices of restaurants anyway. And I cannot label all of them and say any one of these will do. I mean, that would be just", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3452381_ms_-_3537345_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3452381, "end_ms": 3537345}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3526729 ms - 3593623 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3526729 ms - 3593623 ms\n\nContent: many choices. And for the case in real life, you don't need to see all the choices of restaurants anyway. And I cannot label all of them and say any one of these will do. I mean, that would be just too much work. So what we do is that we manually inspect whether the restaurants retrieved by a system satisfy all the criteria. This is all done manually. And you calculate the precision, which is the number of correct answers over the number of results that you're looking for. And the experiment has a baseline, and the baseline is to take data that has been linearized. So just to remind everybody what linearization means is that you take the database and you turn them into sentence by sentence, record by record, and then you throw it into the embedding model, which is the ir, and then you match the question. You find the common, the best similarity, you know, the most similar restaurant, and then you return this. Okay, this is IR information retrieval with a flattened or linearized", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3526729_ms_-_3593623_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3526729, "end_ms": 3593623}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3577677 ms - 3656381 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3577677 ms - 3656381 ms\n\nContent: match the question. You find the common, the best similarity, you know, the most similar restaurant, and then you return this. Okay, this is IR information retrieval with a flattened or linearized database. So if you ask a question, in this case an Italian restaurant with a romantic atmosphere, it would just return you this one. It mentions atmosphere, but it doesn't have anything to do with being romantic. So this is an example. But of course, if you're doing s uQL, we are separating them into looking for the cuisine and then for the reviews, we are doing IR on those reviews. We are. And then you can, you can put it together and you get a better answer. So the experimental result shows that in my, in the single turn case, if I just return one answer, you are correct 57%. If I give you. Let you give me three answers. With the three answers, we check the three answers and we say that the, the, you know, the accuracy dropped. Whereas in SQL we are at the 94% for single turn, 90% for the", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3577677_ms_-_3656381_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3577677, "end_ms": 3656381}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3639509 ms - 3716223 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3639509 ms - 3716223 ms\n\nContent: you give me three answers. With the three answers, we check the three answers and we say that the, the, you know, the accuracy dropped. Whereas in SQL we are at the 94% for single turn, 90% for the conversation. Okay, so there is still an error in here, but it is now in the 90s as opposed to in the 50s and 60s, and it is very noticeable when that happens. Okay, so how do you get incorrect answers? Results? I think we discussed that already. Okay, we can get incorrect answers. And the one question is, can I get all the answers or can I get incomplete answers? And so what happened here is that in this case, for the restaurants, we cannot give you all the answers because we just take the first few, find something that matches what you asked for, and give you the answer. We are not going after completeness, but in other projects that we do, we care about completeness. We can just run the whole thing. It's not really a limitation on the approach. So one thing I want to point out is that", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3639509_ms_-_3716223_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3639509, "end_ms": 3716223}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3703135 ms - 3770907 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3703135 ms - 3770907 ms\n\nContent: completeness, but in other projects that we do, we care about completeness. We can just run the whole thing. It's not really a limitation on the approach. So one thing I want to point out is that you have to be defensive so that you know, if you get it wrong, what you know that the user also gets it wrong. And that is you have to Verbalize the user query. So in this particular case you say, oh, I searched for the five star restaurants that serve kids food and unfortunately couldn't find any. Is there anything I can help you with? I tell you exactly what I'm trying to do. You can now go and refine this statement. Maybe I'm not asking for that. I will see that, oh, I didn't mean this. And you can even correct your queries in the conversation. So the error analysis. So there are two sides. One is did I give you the restaurant that satisfy what you want? And the other one is did I miss any restaurants? And you ask, say, oh, there is no such restaurant when there is such a restaurant. So", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3703135_ms_-_3770907_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3703135, "end_ms": 3770907}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3759339 ms - 3835285 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3759339 ms - 3835285 ms\n\nContent: One is did I give you the restaurant that satisfy what you want? And the other one is did I miss any restaurants? And you ask, say, oh, there is no such restaurant when there is such a restaurant. So that's a false negative. And so we did the analysis on that. Out of the 100 questions on the single term, there are 14 false negatives. So we want to dig them up and see what it is and why. And there are actually two parsing errors only for this small domain. So we ask for the accuracy of semantic parser. It's not that much of a problem. The query evaluation on the other hand has issues. We have two kinds of information retrieval. One is structured, one is unstructured. It turns out that for the structured one there are some quite a bit of it is half of it is unsupported. When you are building any system, that is one of the things to watch out for is just you have not implemented that feature and you cannot answer the questions. Okay. And so we also have opening hour questions. And then", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3759339_ms_-_3835285_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3759339, "end_ms": 3835285}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3819475 ms - 3891735 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3819475 ms - 3891735 ms\n\nContent: any system, that is one of the things to watch out for is just you have not implemented that feature and you cannot answer the questions. Okay. And so we also have opening hour questions. And then we discovered that you are looking for favorite dishes. We look at the popular dishes, but we forgot we did not look at the review. So this is what I mean that sometimes for text fields you really want to pick them up from all the text columns and that would be better. And so there are four there for the free text we saw solve four problems. That is false negatives. And this is because the answer function is not, you know, is not correct. So we see errors at various levels. The parsing, however, in this data set is not a huge problem. All right, we ask for feedback. This is something that I want you guys to do. If you get to the point of getting users get the feedback. It's such a joy these days to be able to get to the point of getting when in the past we just give you some numbers just not", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3819475_ms_-_3891735_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3819475, "end_ms": 3891735}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3878243 ms - 3940597 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3878243 ms - 3940597 ms\n\nContent: you guys to do. If you get to the point of getting users get the feedback. It's such a joy these days to be able to get to the point of getting when in the past we just give you some numbers just not usable. And here are some examples of the negative feedbacks which are all very reasonable and nothing terribly wrong with it. I mean, of course you can improve it. We're not a product group and we did not Worry about it. And then there are a lot of positive feedback from saying that. There is actually nothing I didn't like about this chatbot. I would actually use it on a regular basis. It was fast, gave me detailed responses. And I like the last one. It's like shocked at how good the restaurant suggestions were. I even asked for something with better prices and got that too. Now I'm hungry. It's just being really funny. Okay, so you can try it for yourself. It is on genie, yelpbot, genie, Stanford Edu. If you are looking for a restaurant in the Bay Area, go give it a try and see what it", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3878243_ms_-_3940597_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3878243, "end_ms": 3940597}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3927587 ms - 4009723 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3927587 ms - 4009723 ms\n\nContent: It's just being really funny. Okay, so you can try it for yourself. It is on genie, yelpbot, genie, Stanford Edu. If you are looking for a restaurant in the Bay Area, go give it a try and see what it says. Okay, Give us feedback. Right. But that is a small test, only about less than 200 statements. So we have to do work on the larger data set and we use the hybrid qa. Okay, so hybrid QA data set is pretty interesting. It is the one that truly has integration of of knowledge base, table and multi hop with a sizable number of data. It's 70,000 questions. I want to walk through how they come up with 70,000 questions. Because you guys are preparing for experiments. You want to know how people do it. So they have 13,000 tasks and each crowdsourced worker, this is Amturk, they're given a table and it's hyperlinked passages. Remember the flag bearer, all the linked passages. They are supposed to come up with six questions and answers. Each task takes around 12 minutes and they spend 2.3", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3927587_ms_-_4009723_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3927587, "end_ms": 4009723}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 3997143 ms - 4080965 ms", "content": "Title: CS224V Lecture 12 > Transcript > 3997143 ms - 4080965 ms\n\nContent: and it's hyperlinked passages. Remember the flag bearer, all the linked passages. They are supposed to come up with six questions and answers. Each task takes around 12 minutes and they spend 2.3 dollars on it. And this is a 25,000 plus experiment. Okay, this data set, and this is the statistics. And one thing that I cannot emphasize more is that they paid graduate students to go over every one of the question answers. And if they don't do that, the data will be very dirty. Okay, this is 100% important. We've been burned by that. All right, so they are instructed to include table reasoning and text reasoning. That's why the questions that I show you all kind of go from, you know, it's a little bit more complicated using both passages and text. So what we want to do now is to use SQL on hybrid qa. And this problem is very different from Yelp. Every question comes with its own table. There are 13,000 tables. So each question has a table. And we're doing zero shots. We cannot do few", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_3997143_ms_-_4080965_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 3997143, "end_ms": 4080965}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4062895 ms - 4138426 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4062895 ms - 4138426 ms\n\nContent: hybrid qa. And this problem is very different from Yelp. Every question comes with its own table. There are 13,000 tables. So each question has a table. And we're doing zero shots. We cannot do few shots. Nobody is writing those examples for each of the 13,000 tables. So we want to push it at that end. But if you are building a product of a particular, for a particular data set, of course you can put an example. So this experiment is done with zero shot. No table specific examples. It has examples to teach it about answers and summary, but it is on an arbitrary table. It is not the table that is being asked to handle. Okay, so that's why we call it zero shot for that particular. For the. For the question that you're asking. So in our preliminary assessment, we discovered that hybrid QA is definitely harder than SQL. It is hard for the semantic parser to identify the right text column. We learned that also from Yelp. So what we do is that if I cannot find the answers in the text column", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4062895_ms_-_4138426_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4062895, "end_ms": 4138426}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4125882 ms - 4198381 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4125882 ms - 4198381 ms\n\nContent: definitely harder than SQL. It is hard for the semantic parser to identify the right text column. We learned that also from Yelp. So what we do is that if I cannot find the answers in the text column that the semantic parser picks, we will also try to find it in other columns. You can take care of that in the compiler. So that's good. And then we find that it's still not good enough. And then by eyeballing, all right, you have to look at your result, as I mentioned several times. And what we realize is like when I actually run the query, there is a very clear feedback cycle, feedback signal. And that is that if I run it, it returns no answer. You know that, you know that's not desirable. So we just do a simple thing. We just try again. We try it again and we did it at most three times. So we modified this pipeline. Given what we saw is that we. Let's use the feedback, very simple signal. There are no answers to your query, so let's throw the feedback in. So when we see that, we go no", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4125882_ms_-_4198381_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4125882, "end_ms": 4198381}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4182885 ms - 4264253 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4182885 ms - 4264253 ms\n\nContent: we modified this pipeline. Given what we saw is that we. Let's use the feedback, very simple signal. There are no answers to your query, so let's throw the feedback in. So when we see that, we go no results, then I go through this parse and then I generate yet another SQL, yet another SQL query. And you can click on this and you will see the few shots. This is the prompt. The prompt shows you some examples of how you want to perturb the queries once it gets it wrong. And this is what we learned by looking at the results, some of the common patterns. Okay, so once we put those common patterns in that it improves the system. All right, so let's go back to the data set. It turns out that these tables, so we have these tables, these are coming all from Wikipedia. So they have these tables and they are not very long tables. And the SQL, before we go through the semantic parser, we take these tables and turn them into SQL tables so that we can use SQL operators. And the tables are on", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4182885_ms_-_4264253_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4182885, "end_ms": 4264253}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4246963 ms - 4324745 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4246963 ms - 4324745 ms\n\nContent: and they are not very long tables. And the SQL, before we go through the semantic parser, we take these tables and turn them into SQL tables so that we can use SQL operators. And the tables are on average just 4.4 columns and 16 rows. And if you look at the SOTA, which we discussed last class, their solution is that they feed the entire table into the LLM system. Can you imagine doing that for Yelp? It's just not possible. Okay, so they're not doing anything fancy. They don't need to do IR they have the passages, they have the tables, they just feed it in so they have a much easier question and it cannot handle anything that is not small in terms of size. So I just want to make sure that you understand that there is a huge difference between the numbers I'm going to show you. I mean between this method and our method and our method. We're doing SQL query, it doesn't matter how big the table is. And we are doing ir. I can handle tons and tons of text. Are you ready for the results?", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4246963_ms_-_4324745_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4246963, "end_ms": 4324745}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4311805 ms - 4402291 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4311805 ms - 4402291 ms\n\nContent: this method and our method and our method. We're doing SQL query, it doesn't matter how big the table is. And we are doing ir. I can handle tons and tons of text. Are you ready for the results? What do you think the results look like? So the SOTA here is trained, this is the training data. So basically if you look at the split is that 62,000 trained, 3,400 deaf and 3,400 test. We are building a zero shot system. We don't have any training data. We just say here are a few prompts, here is a new problem, go solve it. We only do the evaluation on the test. All the other work is just not relevant to our project. So it changes the amount of time and money that you need to collect the training data. To collect data for us you just have to use 3400 for them in order to get to the sota they have to, they need the full 70,000 questions because that's a huge difference. Well, guess what, this is the answer. The Sota here is 68% which is what we discussed yesterday. Do you remember that", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4311805_ms_-_4402291_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4311805, "end_ms": 4402291}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4386035 ms - 4472961 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4386035 ms - 4472961 ms\n\nContent: have to, they need the full 70,000 questions because that's a huge difference. Well, guess what, this is the answer. The Sota here is 68% which is what we discussed yesterday. Do you remember that pipeline? They have this fancy retrieval that tries to fix some of the problems in the data set and then they bring in all the evidence and then an LLM answer it, but it has only single hop, okay, on one table and one passage. So this is the result. So before us there are other people who tried to do zero shot using LLM. Oh, the Sota S3HQ8 also uses LLM, but only in one, the final stage. So this is after LLMs came out. So the work by Zeng, their answer is at the 20ish percentage Xi and so forth. And for us we, which is the last row here we got to the 59% in exact match 68 F1 on deaf and and then our test is about the same. We're not training, we don't take the benefit of the training. There's a huge difference in the cost in getting the data and doing the training. So first of all we noticed", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4386035_ms_-_4472961_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4386035, "end_ms": 4472961}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4458025 ms - 4554261 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4458025 ms - 4554261 ms\n\nContent: our test is about the same. We're not training, we don't take the benefit of the training. There's a huge difference in the cost in getting the data and doing the training. So first of all we noticed that among all the people doing prompts, we outperformed them significantly from the 20ish percent. The last one is the 48% we got it to. Basically we should look at the test, the Exact match. We go from the 48 to 59. Okay. It's a significant difference. We are losing. Compared to the trained model, we are within about 9% of exact match, 7% of F1. But they have been trained on 62,000 examples. Okay, so this is the evaluation. What should we do next? Are we happy with this answer? It's not 100% any. You know, we're still just at 60% and they are better. So what do we. What. What should we. What should we conclude? Or what can we do now? Not very satisfying. Right? The answers are not that good, in other words, Right? Yes. No. Yeah, not so good. What do we do now? What do we do now? You", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4458025_ms_-_4554261_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4458025, "end_ms": 4554261}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4529541 ms - 4602333 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4529541 ms - 4602333 ms\n\nContent: What should we conclude? Or what can we do now? Not very satisfying. Right? The answers are not that good, in other words, Right? Yes. No. Yeah, not so good. What do we do now? What do we do now? You must know the answer. What do we do now? We got the first. You know, we got the evaluation results. You can write it up. What else should we do? I'm not satisfied with this answer. I'm not satisfied with this answer. So what should we do? Yeah. Yeah. Thank you. You have to do error analysis. When I ask you this question, you can always just say error analysis. I'll be very happy. Okay. But you guys stick too long. Okay. The whole point here is like, I don't. I don't like the answer. I want it higher. What do I do? You start with error analysis, and then you figure out what to do. Don't just try something different. So it's an answer that is very easy to give because you don't have to think about what to do next because you can't without doing error analysis. So let's do the error", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4529541_ms_-_4602333_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4529541, "end_ms": 4602333}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4590741 ms - 4661797 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4590741 ms - 4661797 ms\n\nContent: just try something different. So it's an answer that is very easy to give because you don't have to think about what to do next because you can't without doing error analysis. So let's do the error analysis. It turns out that out of the. We cannot do all of it. We randomly sample. Very good trick. We discovered that 60% of the E or evaluation issues, such as we're missing formats. They want me to say Johnson City. Oh, they want Johnson City. And we say Johnson City, Tennessee. And that's considered a mismatch. Of course, that was a perfectly good answer. Another one is that there is only one gold answer, but as a matter of fact, there are multiple answers. So we didn't give it the one that they want, and they dinged us for it. And that is 23%. Okay. And it is really, you know, this is the reasonable answer, but not when you're doing an evaluation. From our perspective is that. In other words, you have to be very careful anytime you pick up a data set. All right, If I start by saying I", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4590741_ms_-_4661797_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4590741, "end_ms": 4661797}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4647365 ms - 4723817 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4647365 ms - 4723817 ms\n\nContent: reasonable answer, but not when you're doing an evaluation. From our perspective is that. In other words, you have to be very careful anytime you pick up a data set. All right, If I start by saying I need to improve the system because I don't want to be at 60% if I don't do the error analysis, I will be just doing Waste of, you know, wasting my time because the data set actually has more, you know, the 6 over 50% of the problem is in the data set itself. I cannot touch it, I cannot do anything about it. All right, the true errors we have is about 39%. You can get into this one 20%, 22%. Out of that 39% is semantic parser LLM based functions. We know that that's a possibility. And then there is some type related conversational errors and you can now go and fix some of that. We are now down to. So one thing that is very common is that there are a lot of format problems. The good. But how did the fine tune people get it higher? The fine tuned people also are faced with these issues, but", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4647365_ms_-_4723817_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4647365, "end_ms": 4723817}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4707527 ms - 4784677 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4707527 ms - 4784677 ms\n\nContent: to. So one thing that is very common is that there are a lot of format problems. The good. But how did the fine tune people get it higher? The fine tuned people also are faced with these issues, but they did get 10% better than us. What is it? After you fine tune with 62,000 examples, you learn the format. They are better at guessing the form, putting the answers in the format that they want. But I would say that it is not intrinsically important because the humans look at it and you just tell me it is. Tennessee, that's perfectly good. So in other words, I actually don't think that we are worse than them. Okay, they match the goal better, but we know that they're 40%. You know, they also have this problem. But fine tuning has a chance of adapting to their format better than we can because we don't know what the format is. Okay. So really, really you have to keep that in mind. And if after this evaluation our true accuracy may reach 84% okay, because you know, we are talking about our", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4707527_ms_-_4784677_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4707527, "end_ms": 4784677}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4766243 ms - 4846909 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4766243 ms - 4846909 ms\n\nContent: we don't know what the format is. Okay. So really, really you have to keep that in mind. And if after this evaluation our true accuracy may reach 84% okay, because you know, we are talking about our accuracy is about 60% and this is the breakdown of the errors, you know, that adds up to 100% of the errors. But when you multiply that together, our true error, which is 39% of the errors that we see, we're talking about losing only 15%. Am I happier with this? Yes. And we actually will show you in the next class even smarter semantic parsing for more complicated stuff because hybrid qa, even though there are many, many tables, each table is actually relatively small. We actually have to do more work on semantic parsing and we will talk about that in the next class. But you know, this is the. You just have to be careful. If you are picking up a data set, you don't waste time on the. So in conclusion, here is that we're pretty happy with the design of S uql. It has a. It is a good", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4766243_ms_-_4846909_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4766243, "end_ms": 4846909}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4828949 ms - 4906087 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4828949 ms - 4906087 ms\n\nContent: is the. You just have to be careful. If you are picking up a data set, you don't waste time on the. So in conclusion, here is that we're pretty happy with the design of S uql. It has a. It is a good representation. We can keep improving the semantic parser, but it is a good way of hiding all the low level details from the user. The most important thing is that it unifies the combination of free text and database knowledge. It's arbitrarily compositional. We saw how you go back and forth between tables and paragraphs. So it is one whole corpus even though they are represented very, very differently. We now have a domain agnostic so called based Gini framework which means that you just apply the tables of pretext examples and we give you an agent. We have done a few and we need more experiments with it, but I think it is a very good starting point. And the implementation is separated into two things. It's a zero shot semantic part parser as well as optimizing compiler. Bringing systems", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4828949_ms_-_4906087_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4828949, "end_ms": 4906087}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4892679 ms - 4970285 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4892679 ms - 4970285 ms\n\nContent: with it, but I think it is a very good starting point. And the implementation is separated into two things. It's a zero shot semantic part parser as well as optimizing compiler. Bringing systems in at the right place simplifies your AI problem. It's better than just keep, you know, banging, you know, keep working on the problem using only AI techniques. In terms of the evaluation, we found that hybrid, you know, you have to support hybrid hybrid queries for real. And I would conclude that with the 85% true accuracy potential true accuracy, I would say that in context learning with LLMs works well for natural queries on small databases. Okay. We had only cheat. We have only been testing on small data, small schemas. We will talk about some horribly large databases of large knowledge graphs in the next talk and we will bring in the agentic approach. Okay. So it just gets deeper and deeper. All right, so thank you and I will see you on Monday and don't forget your weekend reports. Oh,", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4892679_ms_-_4970285_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4892679, "end_ms": 4970285}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 4954007 ms - 5035691 ms", "content": "Title: CS224V Lecture 12 > Transcript > 4954007 ms - 5035691 ms\n\nContent: in the next talk and we will bring in the agentic approach. Okay. So it just gets deeper and deeper. All right, so thank you and I will see you on Monday and don't forget your weekend reports. Oh, oh, if you have a few more minutes, this is all extra. We have a few more minutes. I forgot, sorry. Harshit has wants to tell you a tool that you may want to use for your project. Just two minutes. Yeah, yeah, yeah, yeah. I think this is very. I forgot, sorry. No, no, no. Yeah. So most of you will be working with like I think like a lot of you have interest in working, working with Storm, co Storm and WikiChat. So we are making it easier for you to index your corpus. So there's this website that you can go to this URL, I'll share this on ED as well, which is Search Genie, Stanford Edu uploadcollection. And what we expect from here is we just want you to provide us your documents which are already chunked. There is, there are like information on this, on this webpage about how you can chunk", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_4954007_ms_-_5035691_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 4954007, "end_ms": 5035691}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 5023899 ms - 5095581 ms", "content": "Title: CS224V Lecture 12 > Transcript > 5023899 ms - 5095581 ms\n\nContent: And what we expect from here is we just want you to provide us your documents which are already chunked. There is, there are like information on this, on this webpage about how you can chunk them. So basically you extract the documents. You chunk them, we provide you guides for how do you split the text. Then you can extract sections for this structuring, for example, how you can get metadata for the headers and then you save it in a JSON lines format which should look like this. And once you're done with that, you can upload it here and it will tell you if the JSON lines format the file that you have uploaded, if it is correctly formatted or not. If it is correctly formatted, then you can just send us a mail to genie CS Stanford edu and we will index the corpus for you. And then you'll get a URL unique URL which will be your vector embedding server. And then you can plug it in into Storm, wikichat and other services and use it. Yeah, so that's it. If you have any questions like I can", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_5023899_ms_-_5095581_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 5023899, "end_ms": 5095581}}
{"document_title": "CS224V Lecture 12", "section_title": "CS224V Lecture 12 > Transcript > 5079141 ms - 5096325 ms", "content": "a URL unique URL which will be your vector embedding server. And then you can plug it in into Storm, wikichat and other services and use it. Yeah, so that's it. If you have any questions like I can take them after this.", "block_metadata": {"id": "CS224V_Lecture_12_>_Transcript_>_5079141_ms_-_5096325_ms", "document_type": "transcript", "lecture_number": 12, "start_ms": 5079141, "end_ms": 5096325}}
