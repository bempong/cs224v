{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > First day of class at Stanford", "content": "Title: CS224V Lecture 1 > Chapter Summaries > First day of class at Stanford\n\nContent: First day of class. How's everybody doing? Amazing. Did you have a good summer? Good, good, good. And for those of you who are new at Stanford, welcome to Stanford.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_First_day_of_class_at_Stanford", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 71565, "end_ms": 133265}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Monica Lam at the Introduction to Deep Learning", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Monica Lam at the Introduction to Deep Learning\n\nContent: At Stanford, I teach two quizzes every year. I teach 224 volts, obviously. And I also teach 243. Natural language processing 243 and 224 is kind of the same to me. I want to make sure that with the work that we do in the future, everything will be in natural language.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Monica_Lam_at_the_Introduction_to_Deep_Learning", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 134285, "end_ms": 285541}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Teaching Staff introductions", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Teaching Staff introductions\n\nContent: All right, so that's me. Let me introduce our teaching staff. Harshad is the head TA for the course. There are six LRTIs because we have a pretty large course. Would you like to get a seat? You okay? Too much sitting.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Teaching_Staff_introductions", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 285693, "end_ms": 372025}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > A revolution in the language of learning", "content": "Title: CS224V Lecture 1 > Chapter Summaries > A revolution in the language of learning\n\nContent: NLP professor: We really have a revolution in the making. With this revolution, things are automated and people are no longer spending hours just doing mundane physical work. That gave rise to the concept of knowledge work. The next big thing is the Internet.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_A_revolution_in_the_language_of_learning", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 372145, "end_ms": 773805}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > ChatGPT", "content": "Title: CS224V Lecture 1 > Chapter Summaries > ChatGPT\n\nContent: ChatGPT, introduced in November 2022, completely changed the game. It passed the medical licensing exam. There is a difference between passing exams and doing work. The world does not have enough doctors and lawyers in the world.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_ChatGPT", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 775345, "end_ms": 1031685}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Meta's Galactica", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Meta's Galactica\n\nContent:  Meta created an AI assistant for scientific articles. It didn't work and they have to withdraw it within one day. There is a mismatch between what people think it can do and what it can actually do. There are right ways to use it.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Meta's_Galactica", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 1032305, "end_ms": 1390887}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > GPT and the hallucination of neural networks", "content": "Title: CS224V Lecture 1 > Chapter Summaries > GPT and the hallucination of neural networks\n\nContent: GPT is very good at all the popular questions. If you don't know the answer, it can give you something and you wouldn't know what to make of it. There are many, many issues associated with using GPT. Can we just train a bigger and better LLM to eliminate Hallucination?", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_GPT_and_the_hallucination_of_neural_networks", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 1390951, "end_ms": 1680737}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > An LLM with its deterministic properties", "content": "Title: CS224V Lecture 1 > Chapter Summaries > An LLM with its deterministic properties\n\nContent: What we want to aim for is to turn LLM from just being advisory to being deterministic. It can automate the routine for the things that we already know how to do. And even more exciting is that it can elevate experts to do things that they couldn't do before.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_An_LLM_with_its_deterministic_properties", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 1680921, "end_ms": 2115301}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Cognitive Skills and the LLM", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Cognitive Skills and the LLM\n\nContent: To get to the knowledge work, we have to add the executive control from the prefrontal cortex. It's not just language skills. How do humans teach cognitive skills? How can we teach computers how to do knowledge work?", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Cognitive_Skills_and_the_LLM", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 2115453, "end_ms": 2586711}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Conversation on Learning & Memory", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Conversation on Learning & Memory\n\nContent: How do we teach LLMs these cognitive skills? And I would say we can emulate what humans do step by step, algorithmically. How fast do you think the progress has been since ChatGPT came out last?", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Conversation_on_Learning_&_Memory", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 2586763, "end_ms": 2772983}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > All right, so let's talk about the point", "content": "Title: CS224V Lecture 1 > Chapter Summaries > All right, so let's talk about the point\n\nContent: We need RAG retrieval, augmented generation. Only about 60% of the facts are grounded in retrieved information. Unlike GPT, the answers are very dry. You need to ask GPT because it provides additional information. These different times you put them together, you actually get a good answer.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_All_right,_so_let's_talk_about_the_point", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 2773119, "end_ms": 3086675}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Cognitive Processes and GPT", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Cognitive Processes and GPT\n\nContent: Christopher Nolan cast Tom Hanks and Michael Caine in the new film Oppenheimer. The technique combines the best of both worlds: drafting and refining with fine tuning. As you teach it the cognitive skills, you can get faster and faster with each round.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Cognitive_Processes_and_GPT", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 3088015, "end_ms": 3626733}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Building a knowledge revolution with the LLM", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Building a knowledge revolution with the LLM\n\nContent: The foundation for this knowledge revolution is that we learn how to teach human beings. We're using LLM as a speech center and we wrap it with executive control. Now we can build on top of it and build a lot more important things.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Building_a_knowledge_revolution_with_the_LLM", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 3626869, "end_ms": 4004235}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Projects in the World of Wikidata: Introduction", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Projects in the World of Wikidata: Introduction\n\nContent: There are three parts to this course. I want you to learn the state of the art and also advance it. The bulk of this is the project. This is going to be heavily supervised. We truly believe that having supervision makes the education experience a lot better.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Projects_in_the_World_of_Wikidata:_Introduction", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 4004935, "end_ms": 4481235}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Can a Stanford AI Research Project get a paper published on arX", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Can a Stanford AI Research Project get a paper published on arX\n\nContent: Storm turns any topic into a research paper for free. It is very general, it applies to any domain. But it is more the non technical domain as we discussed. You can all try it and you will be doing homework on this.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Can_a_Stanford_AI_Research_Project_get_a_paper_published_on_arX", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 4481395, "end_ms": 4569933}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > Projects in NLP 2", "content": "Title: CS224V Lecture 1 > Chapter Summaries > Projects in NLP 2\n\nContent: Project uses system we call the GENIE worksheet, which allows you to build a powerful agent with a few lines of code. The purpose of this project is to see what it means to building a tool for developers. And then you will learn the weaknesses and strengths.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_Projects_in_NLP_2", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 4570109, "end_ms": 4685567}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > How to Enroll in a Course", "content": "Title: CS224V Lecture 1 > Chapter Summaries > How to Enroll in a Course\n\nContent: What does it really mean to do courses so we all know how to enroll? Look at the form that you have to fill out. By the time you're finished with this homework you are ready to do a lot of projects. The hardest part is to come up with the research project itself.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_How_to_Enroll_in_a_Course", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 4685591, "end_ms": 4988363}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Chapter Summaries > The Introduction to Greatscope", "content": "Title: CS224V Lecture 1 > Chapter Summaries > The Introduction to Greatscope\n\nContent: The class is full. Because we have heavy supervision then we are limiting the class. If you don't take, if you plan to not take the class, please get out of the class so we can give the spots to other people. Unfortunately, we also have a very long waitlist.", "block_metadata": {"id": "CS224V_Lecture_1_>_Chapter_Summaries_>_The_Introduction_to_Greatscope", "document_type": "chapter summary", "lecture_number": 1, "start_ms": 4988539, "end_ms": 5188245}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 71565 ms - 179835 ms", "content": "Title: CS224V Lecture 1 > Transcript > 71565 ms - 179835 ms\n\nContent: Hello, hello, Hello. All right, we're ready to start. Hi, everybody. Well, welcome to CS224V. And for those of you who are new at Stanford, welcome to Stanford. First day of class. How's everybody doing? Amazing. Amazing. All right. Did you have a good summer? Yeah. Who has a good summer? Good, good, good. I had a good summer, too. We did a lot of work and we did a lot of play. So to start, I think I'll introduce myself a little bit. Is this too loud? It's okay. All right, so I'm Monica Lam, and I'm the instructor for this class. And at Stanford, I teach two quizzes every year. And I teach 224 volts, obviously. And I also teach 243. What is 243? Well, it is the advanced compiler course. Okay. And that's, of course, you know, this is in AI, this is in systems. And then you ask, why am I doing this? What kind of a person am I? And the point is that I actually do both. I've been doing a lot of work on systems, everything from architecture, operating systems, to compilers and programming", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_71565_ms_-_179835_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 71565, "end_ms": 179835}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 167147 ms - 241745 ms", "content": "Title: CS224V Lecture 1 > Transcript > 167147 ms - 241745 ms\n\nContent: this? What kind of a person am I? And the point is that I actually do both. I've been doing a lot of work on systems, everything from architecture, operating systems, to compilers and programming languages. And I wrote the Dragon book, which is the most popular textbook, and compilers, and that's what we'll be teaching in 2243. And about eight years ago, I saw the writing on the wall. Okay. With deep learning, I knew that it has completely changed vision, and I know it's going to completely change. Natural language processing 243 and 224 is kind of the same to me. They're all languages, right? Natural language, formal language. And you will see that we actually benefit from working in these two domains at the same time. So about eight years ago, I started working on natural language processing, and we started 224V about four years ago. But I must say, in the last two years, the technology has just accelerated to. It just took off because. But the technology is ripe, and now you can do", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_167147_ms_-_241745_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 167147, "end_ms": 241745}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 226389 ms - 302463 ms", "content": "Title: CS224V Lecture 1 > Transcript > 226389 ms - 302463 ms\n\nContent: and we started 224V about four years ago. But I must say, in the last two years, the technology has just accelerated to. It just took off because. But the technology is ripe, and now you can do so many things with it. And because of my background, I'm a little bit different from a typical AI person. If I have a system with accuracy of 80%, that's absolutely not good in AI, you would say, oh, wow, 80%. It's like, no, especially with LLM, we are now talking about making things that work. And I want to make sure that with the work that we do in the future, everything will be in natural language. We will have natural language interfaces to all programs, and people just talk. People are not going to be slow using a GUI and playing with your phone like this. And that's what we're going after. All right, so that's me. Let me introduce our teaching staff. Harshad. And he's the head TA for the course. He is a PhD student in my group and he knows everything about what we're doing very, very,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_226389_ms_-_302463_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 226389, "end_ms": 302463}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 285693 ms - 373577 ms", "content": "Title: CS224V Lecture 1 > Transcript > 285693 ms - 373577 ms\n\nContent: All right, so that's me. Let me introduce our teaching staff. Harshad. And he's the head TA for the course. He is a PhD student in my group and he knows everything about what we're doing very, very, very well. We also have, there are six LRTIs because we have a pretty large course and we really want to help you with all your projects and I think some of them are flying in, some of them got flight delays. Who's here? Introduce yourself. Adam. My name is Adam. Currently a co term student and I'd say my expertise is in multimodal levels. Nice to meet you. Hi everyone, I'm shu. I'm a second year semester and I'm happy to work with you, Mr. Choir. All right. I like. Yes. Hi, I'm Singh Hee. I'm a second year CS Master's student. Nice to meet y'all. Alright, anybody else here? Well, you will see them all in due time. All right, so there are a couple of people standing there. Would you like to get a seat? You okay? Too much. There's too much sitting. All right, that's fine too. All right, so", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_285693_ms_-_373577_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 285693, "end_ms": 373577}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 355191 ms - 427295 ms", "content": "Title: CS224V Lecture 1 > Transcript > 355191 ms - 427295 ms\n\nContent: them all in due time. All right, so there are a couple of people standing there. Would you like to get a seat? You okay? Too much. There's too much sitting. All right, that's fine too. All right, so let's get started with this, with our first lecture and we're going to give you an introduction. This course, I mean for this lecture there are really three parts. I want to talk about how exciting things are getting in the space of nlp. We really have a revolution in the making. And the second part, I want to talk about the foundation. We're not going to cover all the material for the course. So I just want to talk about the foundation, how we approach this revolution and then we talk about the course. Okay, so that's the plan. So why did I say it's a revolution in the making? I think that this is a very exciting time because it is as big as what happened in the 18th century and when we had the Industrial Revolution. Industrial revolution completely changed the workforce. People were", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_355191_ms_-_427295_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 355191, "end_ms": 427295}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 413623 ms - 487397 ms", "content": "Title: CS224V Lecture 1 > Transcript > 413623 ms - 487397 ms\n\nContent: this is a very exciting time because it is as big as what happened in the 18th century and when we had the Industrial Revolution. Industrial revolution completely changed the workforce. People were using manual labor mostly for their work. And with this revolution, things are automated and people are no longer spending hours just doing mundane physical work. And that gave rise to the concept of knowledge work. And this was a term that was introduced by Peter Drucker, 1959. Imagine somebody realizing, oh wow, we're doing knowledge work. Okay? And it says knowledge work is someone whose job requires them to think for a living. You would think that that's what it, you know, everybody should have been doing. But that's not true until the 19th, you know, in the 1960 period. So pretty much everybody, we, a lot of people. Most of the people working today, they are knowledge workers. Of course, there are still people who are doing physical labor, but for the most part, most of the people are", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_413623_ms_-_487397_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 413623, "end_ms": 487397}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 476997 ms - 542081 ms", "content": "Title: CS224V Lecture 1 > Transcript > 476997 ms - 542081 ms\n\nContent: we, a lot of people. Most of the people working today, they are knowledge workers. Of course, there are still people who are doing physical labor, but for the most part, most of the people are doing. And if you look at what happens with the development of tools, you probably. Well, some of us know the introduction of the calculators. It's a very interesting story. I mean, calculators, I mean, that's boring for you. This is all times we have computers, but at the time when the calculators were first introduced, people were asking, should we let students use calculators? Right. What does it mean? It means that they don't have to know how to do arithmetic anymore. They just punch in the numbers. So there was a debate about that. Imagine that. And can you imagine doing physics exams without a calculator or anything? I mean, there is just no question that of course you will be letting students use calculators for their class and for the exams and so forth. And that is history. And guess", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_476997_ms_-_542081_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 476997, "end_ms": 542081}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 530226 ms - 598431 ms", "content": "Title: CS224V Lecture 1 > Transcript > 530226 ms - 598431 ms\n\nContent: a calculator or anything? I mean, there is just no question that of course you will be letting students use calculators for their class and for the exams and so forth. And that is history. And guess what's happening today? People are asking if we should let students use GPT for their homework or for their exams and so forth. And the reason I brought up the calculator is that the answer is clear. You have to, because it can help you do things that you couldn't do before at a speed that you couldn't do before. So that's. They're kind of like the beginning of tools with the calculators, of course, the computers. I would say the next big thing is the Internet. You guys have not seen the days before the Internet. Can you imagine doing research without the Internet? That's a joke. It was hard. There was this thing called libraries. In the good old days, we'd go to the libraries to find things and how long does it take for the things to appear in the library? Crazy, huh? Then of course, we", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_530226_ms_-_598431_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 530226, "end_ms": 598431}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 584227 ms - 659583 ms", "content": "Title: CS224V Lecture 1 > Transcript > 584227 ms - 659583 ms\n\nContent: There was this thing called libraries. In the good old days, we'd go to the libraries to find things and how long does it take for the things to appear in the library? Crazy, huh? Then of course, we got deep learning. This is only. It's not that long ago. All right, and so what happened? So with deep learning, the first applications are envision. And then we started doing a lot of natural language processing. And by the introduction of LLMs, starting with GPT, I would say, I mean, it completely changes. It completely changed what you can do with natural language processing. And this is the transformer model, right? And today we still have the transformer model. And the reason why this is very, very substantial here is that if you look at the world's knowledge, a lot of it, most of it I would say, is in the written word. Things have been written down. And if you can process writing, you process knowledge. Okay, so this is going to change knowledge work and it's going to be a", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_584227_ms_-_659583_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 584227, "end_ms": 659583}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 645245 ms - 713387 ms", "content": "Title: CS224V Lecture 1 > Transcript > 645245 ms - 713387 ms\n\nContent: of it I would say, is in the written word. Things have been written down. And if you can process writing, you process knowledge. Okay, so this is going to change knowledge work and it's going to be a revolution, like the Industrial revolution, right? It changes people's way of work completely. And now we're going to do the same thing with knowledge work. A lot of the basic mundane things that a lot of people are spending nine to five hours doing, moving one information from one form to another and so forth, that's all going to be automated and they will go away. And it is going to change the workforce. And I was asked by the educators to give a talk about this and it was very interesting because at the very beginning I was going to say, I feel sorry for the students because by the time they grow up, all the jobs are gone. All the jobs that we are familiar with, that we are doing, the adults are doing, they'll be gone. What does that mean? And then by the time I finished preparing for", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_645245_ms_-_713387_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 645245, "end_ms": 713387}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 702755 ms - 762583 ms", "content": "Title: CS224V Lecture 1 > Transcript > 702755 ms - 762583 ms\n\nContent: grow up, all the jobs are gone. All the jobs that we are familiar with, that we are doing, the adults are doing, they'll be gone. What does that mean? And then by the time I finished preparing for the talk, I said that was such a stupid idea. Why? Imagine that at the time of the Industrial Revolution, I say, oh, I feel sorry for you guys. You don't get to work anymore. You don't get to do manual work anymore. I mean, it makes sense back then because people are losing jobs. But the truth of the matter is that who wants to do that? Who wants to do mundane knowledge work? We get to do the creative knowledge work. And this is the reason why it is very, very exciting. It will change, it will change the landscape of the jobs that you can find. But it's all for the better in a sense, because anything that we can automate at this basic level, hey, who wants to spend human time on it? Okay, but this is a huge change in the way we are going to work, going into the future. So this is a", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_702755_ms_-_762583_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 702755, "end_ms": 762583}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 749031 ms - 825285 ms", "content": "Title: CS224V Lecture 1 > Transcript > 749031 ms - 825285 ms\n\nContent: anything that we can automate at this basic level, hey, who wants to spend human time on it? Okay, but this is a huge change in the way we are going to work, going into the future. So this is a revolution in the making. It's if you go, you know, we're talking about industrial revolution, that's in the 80s, 1700s, 1800s. This is one of the big events in the world. So it was clear that the ChatGPT, which was introduced in November 2022, completely changed the game. It was interesting to look at this picture. Everybody knows about social media, how it, you know, how they took off and so forth. But what is going on here is that this is a plot on the smaller, the smallest number, the number of months it takes to reach 100 million user users. And ChatGPT beat all these really, really popular social media entertainment apps. Why, why, why? What do you guys think? I'm not going to talk. All I want to hear from you guys. Yeah, conversation is so natural to us. That anyone can do it. Anybody", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_749031_ms_-_825285_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 749031, "end_ms": 825285}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 805925 ms - 895637 ms", "content": "Title: CS224V Lecture 1 > Transcript > 805925 ms - 895637 ms\n\nContent: social media entertainment apps. Why, why, why? What do you guys think? I'm not going to talk. All I want to hear from you guys. Yeah, conversation is so natural to us. That anyone can do it. Anybody can do it. Conversation. Yes, it's good. Anybody else yeah, it was saving time and effort. Saving time and effort in what? In other tasks. In tasks. In all the work that people are doing. Before you're talking about, are you interested in spending time looking at pictures and playing and talking to people? But now Everybody Works, and ChatGPT is helping you with your work. And this is the reason why it just took off in two months, 100 million users. Okay, so this is fundamental because everybody works, from adults to students, Everybody is dealing with knowledge, and this is the growth. So are we there yet? Are we there yet? Who thinks yes? Who thinks no? So why. Why do you say no? ChatGPT passed the medical licensing exam. That is one of the hardest degrees that people can get. You know,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_805925_ms_-_895637_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 805925, "end_ms": 895637}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 873659 ms - 948955 ms", "content": "Title: CS224V Lecture 1 > Transcript > 873659 ms - 948955 ms\n\nContent: we there yet? Are we there yet? Who thinks yes? Who thinks no? So why. Why do you say no? ChatGPT passed the medical licensing exam. That is one of the hardest degrees that people can get. You know, you can make tons of money once you get this degree. It passes that exam. So that's pretty good, right? Not a lot of people can make it to the. To passing the medical exam. Oh, the bar exam, it passes it too. So why are you saying no? Well, this is a funny. So the question here is that you. I just want to make sure that people understand. It's like it really changes the world because the world today does not have enough doctors and lawyers in the world. Okay, I'm not talking about what you see here in America, perhaps. I mean, even in America, there are people who cannot afford the medical care, the legal services. But outside of America, there are a lot of people who could use an automated version of the doctors and the lawyers we have today. But the truth of the matter is that it is not", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_873659_ms_-_948955_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 873659, "end_ms": 948955}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 935649 ms - 1011797 ms", "content": "Title: CS224V Lecture 1 > Transcript > 935649 ms - 1011797 ms\n\nContent: the legal services. But outside of America, there are a lot of people who could use an automated version of the doctors and the lawyers we have today. But the truth of the matter is that it is not quite there. There is a difference between passing exams and doing work. I love this story. This was in 2023. There's a lawyer who says, oh, I have to work. Let's use GPT, and asked GPT to prepare the case, and it comes up with some fake cases. And so he put it into his argument and submitted it. And this was like crazy, right? The judge is saying, he said, what on earth are you doing? And you know what he said? He says, they asked him, why are you doing this? He says, well, you know, I use GPT. And I even asked GPT if this is all true. We all laugh because we know GPT, of course, thinks it well, thinks that the right answer is to say yes. It doesn't think that it is true, but it thinks that the right answer is yes, because that's what you want to hear, okay? And that's what GPT is like. So", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_935649_ms_-_1011797_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 935649, "end_ms": 1011797}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 999445 ms - 1077859 ms", "content": "Title: CS224V Lecture 1 > Transcript > 999445 ms - 1077859 ms\n\nContent: thinks that the right answer is to say yes. It doesn't think that it is true, but it thinks that the right answer is yes, because that's what you want to hear, okay? And that's what GPT is like. So there is a difference. Passing the exams were set up for humans. We wouldn't do things like this. But if you take the exam and give it to a GPT. They can ace it, but it doesn't mean that they can practice. So there is a huge gap. So that is kind of a silly lawyer, right? So how many people have heard about Galactica from Meta? Facebook? Of course, Meta has done tons and tons of AI work, including the open source model, the llama model and so forth. They are the prose here. And they created this assistant for scientific articles. How many people have heard of it? How come the rest haven't heard of it? You know why it was withdrawn after one day, just one day. Why? It sounds like a really great idea, right? You train a GPT and it reads all the scientific articles and you can talk to it and", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_999445_ms_-_1077859_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 999445, "end_ms": 1077859}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1057851 ms - 1131127 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1057851 ms - 1131127 ms\n\nContent: of it? You know why it was withdrawn after one day, just one day. Why? It sounds like a really great idea, right? You train a GPT and it reads all the scientific articles and you can talk to it and you can ask it for anything that you want. Frankly, I was shocked. I was shocked in several ways. The first thing is that I would not have attempted this model, this project in the first place. Why? Because we all know how GPT works. It is, people like to say it's a black box. It's not a black box, okay? It's a few equations that you can easily read and it is just searching for the unknowns to fit the equations. It is very, very, very explicit what it does when you train it for next word prediction, you get next word prediction, okay? If it does anything else, it is all in our interpretation of how humans behave. Okay? A computer in here, this is a statistical model. It gives you the next word. But it is incredibly good at it. It remembers tons and tons of things. You give it enough", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1057851_ms_-_1131127_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1057851, "end_ms": 1131127}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1119263 ms - 1192065 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1119263 ms - 1192065 ms\n\nContent: of how humans behave. Okay? A computer in here, this is a statistical model. It gives you the next word. But it is incredibly good at it. It remembers tons and tons of things. You give it enough context, it will spew out the next word. But scientific articles cannot be done this way. Okay, can you imagine, you start describing an experiment and then you start because of the prediction, you switch to the result of another experiment, right? And different experiments and the work they build on each other, they sometimes refute the work from before and you just read them all word by word, single word by word prediction. You're not going to get any good results. So this is a little bit surprising for Meta to attempt this. I don't mind doing pre training, but don't put it out as a usable chat service. The second thing I was really surprised at is that it released it. So you build it, you try it, you know it doesn't work. It doesn't take much to know that it doesn't work. Why are they", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1119263_ms_-_1192065_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1119263, "end_ms": 1192065}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1175615 ms - 1251599 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1175615 ms - 1251599 ms\n\nContent: chat service. The second thing I was really surprised at is that it released it. So you build it, you try it, you know it doesn't work. It doesn't take much to know that it doesn't work. Why are they releasing it? Of course they released it. Everybody was excited, everybody used it. And then, as you'd expect, it didn't work and they have to withdraw it within one day. Okay, I thought that this is very strange, really Important to pay attention to this because this is meta. Can you imagine all the companies out there who think so high? You know, it's like, it looks really good, right? There are so many things that you, you can expect it to. You have seen it, you have seen it work. But there are ways that are wrong to use, you know, there are wrong ways to use it. There are right ways to use it. I have a company, it's a big motor company who came to me and said, we did the pre training, what we wanted to do is to help with, say, repairs. And they discovered that it was that GPT was", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1175615_ms_-_1251599_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1175615, "end_ms": 1251599}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1230011 ms - 1310645 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1230011 ms - 1310645 ms\n\nContent: use it. I have a company, it's a big motor company who came to me and said, we did the pre training, what we wanted to do is to help with, say, repairs. And they discovered that it was that GPT was manufacturing new car parts that don't exist. I said, what are you expecting? There are tons and tons of car parts, so many of them. And you do word prediction and you expect it to come out with the right car part for what you need. There is a mismatch between what, what people think it can do and what it can actually do. But of course, everybody here, we all understand that this is a transformer behind this, solving a bunch of equations. Next word prediction. You cannot get more, you know, get out of it more than what you expect. Okay? So there is this mismatch. You have to be careful. So just to. Just to make sure that we know what we're talking about, let's try something about. Let's look at some little examples. Here's a question. What is the biggest country in Europe by population?", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1230011_ms_-_1310645_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1230011, "end_ms": 1310645}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1296365 ms - 1383995 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1296365 ms - 1383995 ms\n\nContent: to. Just to make sure that we know what we're talking about, let's try something about. Let's look at some little examples. Here's a question. What is the biggest country in Europe by population? What's the answer? Without looking at GPT, what's the answer? Germany. Any other else? Uk. Uk. Well, guess what? Huh? Aha. Somebody yelled out the right answer. GPT3 says Germany. So those of you who said Germany, you have a reason to say Germany or GPT3 has a reason to say Germany, but the answer is actually Russia. Okay? So here is an example of a question that is pretty simple. And you would think that it will know, but it doesn't. Okay, here's another question. Where does the name Melbourne come from? Anybody? No? Well, let's ask GPT. GPT. It comes from the Latin word malburnum, meaning blackburn or blackbird. Pretty clear. Very exciting. Nobody else knows this. What do you guys think? I turn around and I ask, GPT, what does the Latin word malburnum mean? And it says, it doesn't exist.", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1296365_ms_-_1383995_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1296365, "end_ms": 1383995}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1365981 ms - 1439349 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1365981 ms - 1439349 ms\n\nContent: or blackbird. Pretty clear. Very exciting. Nobody else knows this. What do you guys think? I turn around and I ask, GPT, what does the Latin word malburnum mean? And it says, it doesn't exist. Okay? Because I don't know. I mean, I don't even know which are Latin words. So it turns out that Melbourne is named after William Lamb, the second Viscount Melbourne. All right, so the problem here is that some of these are popular questions, some of these are not popular. Questions. And what happened here is that when people, when people get GPT, they're so excited they will ask it questions that they know. And it's not surprising that GPT also knows those answers. Okay. Because it is very good at all the popular questions. And so the problem here is that if you know the answer, it can help you confirm it. If you don't know the answer, it can give you something and you wouldn't know what to make out of it. So you really have to fact check the answers. So here's another one that is also very", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1365981_ms_-_1439349_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1365981, "end_ms": 1439349}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1424317 ms - 1506847 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1424317 ms - 1506847 ms\n\nContent: confirm it. If you don't know the answer, it can give you something and you wouldn't know what to make out of it. So you really have to fact check the answers. So here's another one that is also very illustrative. Where did Geoff Hinton get his PhD? Do you know who Jeff Hinton is? Yeah. Who is it? Was any the person backpropagation or. Yeah, he did a lot of work on neural networks from way back in the 1980s and he won a Turing Award for his work on neural networks along with two other guys. So hey, this is like the father of GPT. And GPT would know, right? So we asked it the question, where did Geoff Hinton get his PhD? And it says, Jeff Hinton received his PhD in artificial intelligence from the University of Edinburgh in 1978. All perfect. His doctoral thesis was titled Relational Learning of Pattern Matchable Templates. But the problem here is this is not his thesis. His thesis is actually Relaxation in its Role in Vision. The title is reasonable. Yeah. And it sounds like a thesis.", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1424317_ms_-_1506847_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1424317, "end_ms": 1506847}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1483695 ms - 1567757 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1483695 ms - 1567757 ms\n\nContent: of Pattern Matchable Templates. But the problem here is this is not his thesis. His thesis is actually Relaxation in its Role in Vision. The title is reasonable. Yeah. And it sounds like a thesis. It even matches kind of like the time frame, perhaps. But that's not his thesis. And what is interesting about this answer is that you didn't ask for the thesis. Okay. GPD offers this answer to you. And this is kind of like, you know, it doesn't have to give you this information. And humans don't behave like this. Right. You offer the information and you ask them about the information for things that they don't know. They just don't make things up. Or do they? I said that there are people who laughed as well. There are individuals who like to make things up. But hey, we're in university and I don't in my classes. I just thought I would let you know that if I don't know, I tell you I don't know. Because hey, you don't know everything. But at any rate, this is offering new information that you", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1483695_ms_-_1567757_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1483695, "end_ms": 1567757}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1556973 ms - 1633305 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1556973 ms - 1633305 ms\n\nContent: in my classes. I just thought I would let you know that if I don't know, I tell you I don't know. Because hey, you don't know everything. But at any rate, this is offering new information that you didn't ask for. At most people didn't really suspect even that the information could be wrong. So there are many, many issues associated with using GPT. And this is the issue that we have to deal with. This is an interesting question. Can we just train a bigger and better LLM to eliminate Hallucination. What do you think? How many people think? Yes. How many people think? No. All right. Well, I have to say that this is a discussion that I have with a lot of my colleagues in the NLP lab and we have a difference in opinion. So it is fine for you to have a different opinion from mine, which is that I don't think so. I don't think so because I believe that generative AI means it is hallucinating. Okay? It needs to. It is generating things. And does it know when it is generating and when it is", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1556973_ms_-_1633305_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1556973, "end_ms": 1633305}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1617735 ms - 1689805 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1617735 ms - 1689805 ms\n\nContent: I don't think so. I don't think so because I believe that generative AI means it is hallucinating. Okay? It needs to. It is generating things. And does it know when it is generating and when it is not generating? I don't think so. It is the same algorithm. You can try to improve it, but by definition you cannot just train. I believe that you can just train an LLM because the training is in next word. If you want to eliminate hallucination, you can train it to do different things. Okay? And it is possible for you to get neural networks to do all kinds of things. But if I'm just doing next word prediction, I will not be able to get hallucination out of the system, is what I believe in. So let's conclude with what we talked about in this section here is that, yes, it is a revolution in the making. It can bring about a knowledge revolution, but it does not yet. And you have to be careful with what you do with it. All right, so now let's talk about the foundation. How do we get beyond LLMs", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1617735_ms_-_1689805_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1617735, "end_ms": 1689805}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1674877 ms - 1757569 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1674877 ms - 1757569 ms\n\nContent: It can bring about a knowledge revolution, but it does not yet. And you have to be careful with what you do with it. All right, so now let's talk about the foundation. How do we get beyond LLMs with its basic properties? So with this, I want to tell you that the. What are we aiming for? What we want to aim for is to turn LLM from just being advisory. I can advise you you want a new name for your paper. It's really good at it, things like that. They can advise you. You can take it or leave it, but I really want to go way beyond that. I want to turn this into reliable services, solving things that are essential. If it says something, I have to be able to trust it. If I can make this leap from being advisory to being deterministic, in a sense that I can trust, it changes what it can possibly do. It can automate the routine for the things that we already know how to do. And what is even more exciting is that it can elevate experts to do things that they couldn't do before. It's not just", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1674877_ms_-_1757569_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1674877, "end_ms": 1757569}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1741601 ms - 1816397 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1741601 ms - 1816397 ms\n\nContent: do. It can automate the routine for the things that we already know how to do. And what is even more exciting is that it can elevate experts to do things that they couldn't do before. It's not just speed up what you can do, but I can actually process a lot more information than I could possibly do without tools. So this is the two things that I want, which is automate the basic things. I don't want it to give me advice and still have a human go over it. And at the same time, I wanted to help the experts and do things we could not possibly do before. So this is what we are trying to do. And it's a long way to go from where we were, where we are today, to this point. But that's the goal of this project, of our research. So how do we think about it? I turn to neuroscience to help us a little bit with this problem we are facing. What exactly is an LLM? And this is my own hypothesis. I actually believe that we all have something called an LLM in our brain, and that is that in our brain,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1741601_ms_-_1816397_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1741601, "end_ms": 1816397}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1800393 ms - 1868281 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1800393 ms - 1868281 ms\n\nContent: bit with this problem we are facing. What exactly is an LLM? And this is my own hypothesis. I actually believe that we all have something called an LLM in our brain, and that is that in our brain, there are different parts of our brain, and we have a speech area, the brokers area. And, you know, this is the area where it deals with speech. The reason that I can talk really fast and you can understand every word that I'm saying, even though I'm going really fast, is because we have a neural network that process the words. And as it turns out, if you look at the talk to the bio neurobio people, neuroscience people, they have found that as we are listening, we are actually predicting the next word. This is why you can hear words so quickly. So, for example, when you learn a foreign language, you just much slower, because you have to take each word in and then you parse it. And the same thing, you're much slower when you are talking in a language that you barely know. And that is because", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1800393_ms_-_1868281_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1800393, "end_ms": 1868281}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1855855 ms - 1929135 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1855855 ms - 1929135 ms\n\nContent: you just much slower, because you have to take each word in and then you parse it. And the same thing, you're much slower when you are talking in a language that you barely know. And that is because it is not your neural net, your speech center at work. You are involving other parts of your brain, and they go a lot slower. And when I give a talk like this, it is not just my neural network running, not just my speech center running. Okay, I actually have executive control. I have to decide what I'm trying to, you know, what point I'm trying to make. It is not just, you know, a whole bunch of words that are streaming out of my mouth. And that is what we have learned from neuroscience. And so besides the speech center that is involved in talking, there's also the prefrontal cortex. The prefrontal cortex is the one that is responsible for inhibition and attention. And it stops you from saying things that you shouldn't be saying. I have so many examples that I can recall with this notion.", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1855855_ms_-_1929135_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1855855, "end_ms": 1929135}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1908145 ms - 1990547 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1908145 ms - 1990547 ms\n\nContent: cortex is the one that is responsible for inhibition and attention. And it stops you from saying things that you shouldn't be saying. I have so many examples that I can recall with this notion. I'll give you an example. I still remember my mother had a surgery, and she was coming out of surgery. Of course, she was kind of still under the effects of anesthesia. And my father walked in and she said, you're so fat. I was like, my goodness, my mother never said that of my dad, and she would never say that. To anybody. Okay. And my father was heavy and he had been heavy. And the whole point here is that her inhibition is not working. Okay? He normally would not. She would normally not say that, but the minute he walked in, she just blurted that out. And that is her speech center at work. So there are the two levels of thinking as we speak. And I have many other examples where I'm tired and you have too much to drink and so forth. You know that there is a difference between how you talk", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1908145_ms_-_1990547_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1908145, "end_ms": 1990547}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 1977091 ms - 2046641 ms", "content": "Title: CS224V Lecture 1 > Transcript > 1977091 ms - 2046641 ms\n\nContent: there are the two levels of thinking as we speak. And I have many other examples where I'm tired and you have too much to drink and so forth. You know that there is a difference between how you talk with or without involving your big brain, the prefrontal cortex, and that's inhibition and attention, working with long term memory, planning and monitoring external signals and so forth. So how do people know that? Well, it turns out that there was a very famous patient, Phileas Gage. He had a. He worked at the railroad and have a railroad road spike that went up his head. Okay? It turns out that that didn't kill him. And he was actually really good at talking still. But he was a changed person. The personality was completely different from what he was before. And all the speech functions are still there, but the prefrontal cortex is out of whack and it is just a totally different person from the content of what he was actually saying. So that's the reason why we know that the prefrontal", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_1977091_ms_-_2046641_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 1977091, "end_ms": 2046641}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2033201 ms - 2104353 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2033201 ms - 2104353 ms\n\nContent: still there, but the prefrontal cortex is out of whack and it is just a totally different person from the content of what he was actually saying. So that's the reason why we know that the prefrontal cortex and the speech center have to work together to make the speech that you are familiar with. And what does that say? Well, obviously the prefrontal cortex is also neural. Okay, you could have a neural network that represent the prefrontal cortex, but I doubt if you would be training your prefrontal cortex with next word prediction algorithms. Okay. The data set, the data that you are training with is different if you really want to train at the thinking level. And just imagine as an adult if everything that we know has to go through a neurotraining process. I don't think we are learning as much as we do today. There are different parts of the brain. And this is the reason why I think that LLM is one part is a really important part. Because if I don't have a speech center working,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2033201_ms_-_2104353_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2033201, "end_ms": 2104353}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2092799 ms - 2167147 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2092799 ms - 2167147 ms\n\nContent: as much as we do today. There are different parts of the brain. And this is the reason why I think that LLM is one part is a really important part. Because if I don't have a speech center working, there's no way I can communicate with you at a decent or in any way. In a sense, I need to have language. And that's what the speech center is. And that's what I think the LLM is. Now with the LLM, we can now talk about how you get the. With the. We have the speech center, which helps with the natural language processing. Now the question is, how do we do all the other things that we need to do in order to do knowledge work? Okay, what do you guys think of this theory? Do you buy it? Yeah. Anybody who wants to discuss this? Yeah, I would be curious about. Emotions play a role in here. Oh, it's everywhere. And even your smell has, you know, the smell function gives you input into the emotion feel into the feelings that you have. So many parts of the brain are involved in. For example,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2092799_ms_-_2167147_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2092799, "end_ms": 2167147}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2149625 ms - 2226541 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2149625 ms - 2226541 ms\n\nContent: it's everywhere. And even your smell has, you know, the smell function gives you input into the emotion feel into the feelings that you have. So many parts of the brain are involved in. For example, emotions. For sure. That's a very good question. So anyway, so this is what we believe in. It's like, this is important, especially there's no way I can handle natural languages without a speech center. But it's not enough. So if you buy this story, then we'd say that the language model gives you a speech center is a very, very good one, by the way. Much better than the one that you have, than the one we have humans have. But it is still natural language skills. It is not knowledge skills. And so what we want to do is to say, let's. To get to the knowledge work, we have to add the executive control. What you see, for example, from the prefrontal cortex. And we need to have cognitive skills. It's not just language skills. So there is a difference between these two things. This is what I was", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2149625_ms_-_2226541_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2149625, "end_ms": 2226541}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2211005 ms - 2289899 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2211005 ms - 2289899 ms\n\nContent: What you see, for example, from the prefrontal cortex. And we need to have cognitive skills. It's not just language skills. So there is a difference between these two things. This is what I was alluding to. I know that for kids, they learn languages. They are actually better at learning languages than adults because they are just so much faster in training their neural networks, in a sense. But after you pick up the language, there are a lot of things that we learn. The cognitive skills that we learn, we learn it from the teachers. And there's a lot of learning going on from the time you are able to speak and hear to the 20s and of course, the 30s and so forth. And I think that this is what we need to look at as a process. Look at the human learning process to figure out how we create, how we can teach computers how to do knowledge work. And so how do humans learn? How do humans teach cognitive skills? So in education, this is a very famous taxonomy, Bloom's Taxonomy, where they have", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2211005_ms_-_2289899_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2211005, "end_ms": 2289899}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2269283 ms - 2343931 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2269283 ms - 2343931 ms\n\nContent: we can teach computers how to do knowledge work. And so how do humans learn? How do humans teach cognitive skills? So in education, this is a very famous taxonomy, Bloom's Taxonomy, where they have identified, the educators have. This is education for humans. Nothing to do with computers. Okay, this is from the education department. In this sense, they are saying that when you teach, there are different levels at which the different skills that you have to teach for teach at the very basic level is just to remember facts. And that is at the bottom, you know, you can recall facts and get the basic concepts. As you move up, the next level will be understand. I actually can understand. I can explain the ideas of the concepts. The next level I can apply, I can analyze, and I can do the evaluation to justify, you know, a decision and so forth. And at the very top is create. When we say create here, I'm not talking about creative writing, I'm talking about creative from a knowledge", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2269283_ms_-_2343931_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2269283, "end_ms": 2343931}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2329865 ms - 2400061 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2329865 ms - 2400061 ms\n\nContent: evaluation to justify, you know, a decision and so forth. And at the very top is create. When we say create here, I'm not talking about creative writing, I'm talking about creative from a knowledge perspective. This is the kind of thing that, for example, researchers do. They are trying to create things that are novel. But this is all based on the learning that we know from the past. I mean, from. We have to study literature, we have to understand the concepts, and then we create original work. And that's kind of on the top of this pyramid. And that is created. And I would say that LLMs are better than us in remembering. It may get some things wrong, but it is still much better than the memory that we have. But anything above that, it's a little bit trickier. Okay, it looks like it understands some things because, you know, you ask it things and you can explain things, apply, analyze, evaluate and create. There is a little bit of that going on in our interpretation. But it is doing", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2329865_ms_-_2400061_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2329865, "end_ms": 2400061}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2387815 ms - 2457195 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2387815 ms - 2457195 ms\n\nContent: some things because, you know, you ask it things and you can explain things, apply, analyze, evaluate and create. There is a little bit of that going on in our interpretation. But it is doing next word prediction, okay, the chain of thoughts and so forth. It is trying to get it to go through the prediction process. But if you take something that it create and you turn it around and you say, do you mean this? Or whatever, you flip the sense of the word and it will give you a different answer, showing that it actually does not really understand. But it looks like it understands. Because in human terms, if you can give me answers like that, it means that you understand, but in computer terms, it is different. So the question here is that we believe that it remembers a lot of things, but the rest of it, we have to explicitly teach the system to do in order to do things reliably. So that's the kind of philosophy that we have in our research, in this line of research. And as you do", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2387815_ms_-_2457195_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2387815, "end_ms": 2457195}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2438567 ms - 2505197 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2438567 ms - 2505197 ms\n\nContent: but the rest of it, we have to explicitly teach the system to do in order to do things reliably. So that's the kind of philosophy that we have in our research, in this line of research. And as you do different skills, you can perform different jobs. So for example, you know, remember, it's just very basic. But if you have to do, if you, if you want to be an advisor or employee training, you have to really understand. You have to be able to answer the questions when people ask you the questions. And if I apply means that I can read things and I can kind of interpret it and apply it, I can do analysis if I'm a data scientist, if I have analysis skills, I can be a data scientist, journalist, recruiters, evaluation. I can be judges and auditors and compliance officers and so forth. And the very top, if you're talking about create, it would be the researchers, investigative journalists and so forth. And along with all these different jobs, you have to have different skills. It's not just", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2438567_ms_-_2505197_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2438567, "end_ms": 2505197}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2494851 ms - 2559337 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2494851 ms - 2559337 ms\n\nContent: top, if you're talking about create, it would be the researchers, investigative journalists and so forth. And along with all these different jobs, you have to have different skills. It's not just remembering that you need. So for example, I have to be able to read, I have to understand what I am reading. I have to be able to follow instructions, I have to, if I want to be able to analyze, I have to do knowledge based retrieval and literature research, evaluate evaluation. I have to do formal reasoning and not just guess at an answer. And if I want to do create, I have to look at the evidence, I have to code it and I have to analyze it and say statistically this is true, it's not. You just pull it out of your head. Okay, and so these are the skills that humans acquire in order to do all these fancy jobs. And we now need to make this possible in an AI system building on top of the remembering, you know, the language, the natural language processing skills and the remembering skills of", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2494851_ms_-_2559337_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2494851, "end_ms": 2559337}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2545265 ms - 2618313 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2545265 ms - 2618313 ms\n\nContent: fancy jobs. And we now need to make this possible in an AI system building on top of the remembering, you know, the language, the natural language processing skills and the remembering skills of LLM. If I don't have the LLMs, I won't be able to do the rest, frankly. Okay, so this completely makes the rest of it possible. We can learn a lot of these skills without doing neuro training. And in the same way we can build a system that uses an LLM and couple with it other things in order to achieve the functions that we actually want. Okay, so that's the story of what we wanted. Do any questions? Comments? Yes. So I feel like LLMs and AI are progressing at a very accelerated pace. Even the summer there have been many model releases by top companies. And so do you. How fast do you think the progress, say since ChatGPT came out last, has there been any progress in terms of this activity? How fast you can get progress? I believe that they are really good at remembering and they are very good", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2545265_ms_-_2618313_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2545265, "end_ms": 2618313}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2603991 ms - 2680913 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2603991 ms - 2680913 ms\n\nContent: say since ChatGPT came out last, has there been any progress in terms of this activity? How fast you can get progress? I believe that they are really good at remembering and they are very good at making the next word prediction because it is trained to do that. You can accelerate and improve the performance, which I will do too. Okay, but you have to be explicit about teaching them these things. Some of them, as you will see. Why don't you hold on that for a minute and I will show you how we can accelerate LLM training by knowing that this is what we need to do. Okay, just hold on for a second. And so now we say that these are the human skills. How do we teach LLMs these cognitive skills? And I would say we can emulate what humans do step by step, algorithmically. Yes. You have a question? Yeah. Human beings are. People. What if human beings are better? Not as good. If al are better remembering than human beings, but human beings are better than at creation and LLMs, then maybe my", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2603991_ms_-_2680913_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2603991, "end_ms": 2680913}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2665815 ms - 2732945 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2665815 ms - 2732945 ms\n\nContent: Yeah. Human beings are. People. What if human beings are better? Not as good. If al are better remembering than human beings, but human beings are better than at creation and LLMs, then maybe my question is like looking at the triangle, it sort of seems as if like it's a, it's a. Each Step builds upon each other. But I see it more as like a, like first of all, I know I'm really badly memory, but I'm very able to of understand and apply and analyze this. And like if human beings have maybe evolved not to remember but try to do more Crazy. Very good point. Should this be looked in a different way in terms of like what the steps are and how. That's a very good point. Can you imagine if we don't know how to write things down, you cannot remember much of anything. We augment our memory by with the written word. Once we can, once we have invented writing, you can pass on knowledge from previous generations to this point. So we are relying on external memories, right? So this is how we", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2665815_ms_-_2732945_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2665815, "end_ms": 2732945}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2717571 ms - 2788863 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2717571 ms - 2788863 ms\n\nContent: with the written word. Once we can, once we have invented writing, you can pass on knowledge from previous generations to this point. So we are relying on external memories, right? So this is how we compensate for the fact that we don't have the same kind of memory LLMs, but we don't need it because we know how to look it up, right? And that's part of the story because even though it has memory, it doesn't have a perfect memory. It doesn't have perfect memory, especially with all the, the long tail. All right, so that's the basic story, but if you don't have any basic memory, it doesn't work either. So you really need a combination. Very good question. So we say that look, I need to teach cognitive skills. How do we do that? And so I say do what humans do to start. All right, so let's talk about the point that you are making. We don't remember everything, but we can read, we can search, we can retrieve. And so everybody knows about. Heard of how many people have heard of rag? Yeah, of", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2717571_ms_-_2788863_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2717571, "end_ms": 2788863}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2775311 ms - 2851993 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2775311 ms - 2851993 ms\n\nContent: about the point that you are making. We don't remember everything, but we can read, we can search, we can retrieve. And so everybody knows about. Heard of how many people have heard of rag? Yeah, of course, we need RAG retrieval, augmented generation. And there are many, many commercial examples, including Bing Chat. And what is interesting is that there is a paper that was written by Nelson, Tianyi and Percy and they did the evaluation and they show that only about 60% of the facts are grounded in retrieved information. If you ask Bing a question, it reads it, but it actually still augments it 40% of the time. It is not coming up from the retrieved answer. Why? So one of the things that we have in our research, we discovered a very simple basic fact. And that is when you try to retrieve, you get something and sometimes what you retrieve doesn't answer the question right. You say you give it to GPT. It says, well, this doesn't answer the question, but I want to answer the question. So", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2775311_ms_-_2851993_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2775311, "end_ms": 2851993}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2840265 ms - 2910841 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2840265 ms - 2910841 ms\n\nContent: you get something and sometimes what you retrieve doesn't answer the question right. You say you give it to GPT. It says, well, this doesn't answer the question, but I want to answer the question. So they make it up. All right, so they fill in the blanks. So this is the first thing that they would do. So even we all heard about rag, but RAG by itself is you have to be very careful with reg. It is not just pulling it and do a search. And they say answer this question based on that because you know it may not have the right answer. And the second problem here is that unlike GPT, the answers are very dry. If you are doing reg, it will give you exactly what you are looking for. It doesn't offer you information like the doctoral thesis and things that make GPT very, very interesting. Okay, so those are the basic problems with reg. So what do we do? So what I recommend, so what we have been doing in our work in this line of research is that we emulate what the humans do. So suppose I ask", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2840265_ms_-_2910841_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2840265, "end_ms": 2910841}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2894185 ms - 2963847 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2894185 ms - 2963847 ms\n\nContent: those are the basic problems with reg. So what do we do? So what I recommend, so what we have been doing in our work in this line of research is that we emulate what the humans do. So suppose I ask you for something you don't know the answer to right off the top of your head. What do you do? One thing is that you search the Internet with a query, okay? And you often, you know how many times it comes back and sometimes it doesn't have the information that you want. You filter out the irrelevant information. So that's one path. What other things do you do? If I ask you the question today to look up information today, what would you do? Ask GPT. Okay, we ask GPT. So that's the second thing that we do. We ask GPT, but we don't take the answer as it is because we have to dissect the answers into claims. We take each claim. When we fact check each claim, we search the Internet with a claim and then you filter out the incorrect claims. And then together you can draft it and you can refine it", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2894185_ms_-_2963847_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2894185, "end_ms": 2963847}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 2951023 ms - 3018705 ms", "content": "Title: CS224V Lecture 1 > Transcript > 2951023 ms - 3018705 ms\n\nContent: claims. We take each claim. When we fact check each claim, we search the Internet with a claim and then you filter out the incorrect claims. And then together you can draft it and you can refine it and I give you the answer because of the filter at the bottom. Anything that I say back to you is actually correct. All right? The reason why we want to pass is that you can be asking me for something that didn't even that GPT wasn't even trained with. You have to do with the search. You have to ask, you can. You need to ask GPT because it actually provides additional information that is interesting, but you have to fact check it. And this is the reason why we can get a much get a, a good answer, interesting answer and at the same time it is factual. So in other words, here is that the green part you have to do to search Internet. The rest of all the parts you can ask GPT to do, okay, we're asking GPT to do the basic things like can you summarize the information? Can you filter the", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_2951023_ms_-_3018705_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 2951023, "end_ms": 3018705}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3006281 ms - 3076907 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3006281 ms - 3076907 ms\n\nContent: green part you have to do to search Internet. The rest of all the parts you can ask GPT to do, okay, we're asking GPT to do the basic things like can you summarize the information? Can you filter the information? They're actually pretty good at that. You can trust it. But these are little subroutines. Once they do put out all the little you call GPT, all These different times you put them together, you actually get a good answer. You don't just take the answer directly from GPT assets. So I'll give you an example. Suppose we ask, what do you think about the casting of Oppenheimer? So at the time, so the time we did this experiment, GPT knows of the movies, but doesn't know, but it does not. They know that a movie is being planned, but it doesn't know what happens to the rest of it. So the first on the left branch we just call the Internet. And by then, of course we know the answer. The Internet replies by saying that the cast signed between September and April and Cillian Murphy is to", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3006281_ms_-_3076907_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3006281, "end_ms": 3076907}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3060203 ms - 3143601 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3060203 ms - 3143601 ms\n\nContent: first on the left branch we just call the Internet. And by then, of course we know the answer. The Internet replies by saying that the cast signed between September and April and Cillian Murphy is to play Robert Oppenheimer. So that answers the questions in the. To the answers the question up there. But in the same, at the same time, we call GPT. We say, okay, what do you think about the casting of Oppenheimer and GPT says Christopher Nolan. The director is known for his meticulous casting choices. Very good. He often casts well known actors in his films. And this time he has selected Tom Hanks and Michael Caine. You have seen the movie? No. Well, Michael Caine, if you haven't. Michael Caine and Tom Hanks are not in this movie. Okay, so what happened here is that we extract the claims, we split them up into different sentences and we have we. The green means that it is shown to be correct. The pink here says we don't have the evidence of that. So we filter them out and now we put the", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3060203_ms_-_3143601_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3060203, "end_ms": 3143601}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3124735 ms - 3199473 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3124735 ms - 3199473 ms\n\nContent: split them up into different sentences and we have we. The green means that it is shown to be correct. The pink here says we don't have the evidence of that. So we filter them out and now we put the answers together. And what is going on here is that we actually never ask GPT directly to give it the answer. What is key here is that when I do the retrieve, I ask it only to summarize and to filter. Those are the things it does well. If you ask it for the answers, it will fill it in if it feels that the answer is not correct. So you're using GPT in the right way, then you put it together and then here is the first draft. And the final answer is that Christopher Nolan has cast Cillian Murphy as the titular role of Oppenheimer, keeping with his tradition of meticulous casting choices. I'm sure it'll be an accident. Excellent movie. Okay, so it has the feel of GPT, but it is actually the correctness of what's happening today. So this is kind of like a super rag, you would say, which really", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3124735_ms_-_3199473_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3124735, "end_ms": 3199473}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3186321 ms - 3266561 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3186321 ms - 3266561 ms\n\nContent: it'll be an accident. Excellent movie. Okay, so it has the feel of GPT, but it is actually the correctness of what's happening today. So this is kind of like a super rag, you would say, which really combines the best of both worlds. So if you look at this, what happened is that we are calling GPT many, many times and it takes a while. And how can we improve that, so we then do the fine tuning, we do the next step. And a way to explain that is to compare this with, for example, how we learn to play piano. Okay. When you are, when you get a new piece, you play a little bit slower. But as you practice, you get to go really fast. Okay, because you committed to your, to your memory, your motor memory. I can do the same thing with my speech center. If you ask me questions that I'm very familiar with, I can answer it faster. Okay, you can actually train the LLMs to do certain things better by practice. So what we show here, when we did this refinement, this process, it is kind of slow. We", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3186321_ms_-_3266561_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3186321, "end_ms": 3266561}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3247115 ms - 3327773 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3247115 ms - 3327773 ms\n\nContent: with, I can answer it faster. Okay, you can actually train the LLMs to do certain things better by practice. So what we show here, when we did this refinement, this process, it is kind of slow. We call LLM, you know, 20 times sometimes. And we sometimes split the process into multiple sections because I want each subroutine to be something it is actually good at. But now I want to speed it up. I can take the many inputs, different inputs, I generate outputs using this slow method and I can distill it and then I can fine tune an LLM, I can make it run faster. So in this way we can learn harder and harder skills. And we can. The best part is that because we are, you know, we, we are not, not relying on LLM for memory. We, it works pretty well with a smaller and local LLMs. It is actually better for confidentiality and efficiency. And I hope I answered your question earlier, which is that yes, you can get it to behave better according to your goals, your teaching goals. In this case, it", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3247115_ms_-_3327773_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3247115, "end_ms": 3327773}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3312925 ms - 3384573 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3312925 ms - 3384573 ms\n\nContent: for confidentiality and efficiency. And I hope I answered your question earlier, which is that yes, you can get it to behave better according to your goals, your teaching goals. In this case, it is the cognitive skills of how do you answer a question? Okay, I am retrieving, but at the same time, I, for example, instead of drafting and refining, I can in one step give you a much better answer by doing fine tuning. So as you teach it the cognitive skills, you can distill this down and you can get faster and faster with each round. But the goal here is that I'm not just giving it more and more text, I'm actually teaching it cognitive skills. Because according to education department, the cognitive skills transfer. If I learn how to learn, I can learn and learn many more things. For example, in this case we are teaching it reading skills. And reading is just so very basic. And so here's an experiment that we did where we distill GPT4 to llama and we did this experiment which is that we", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3312925_ms_-_3384573_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3312925, "end_ms": 3384573}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3369831 ms - 3446777 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3369831 ms - 3446777 ms\n\nContent: in this case we are teaching it reading skills. And reading is just so very basic. And so here's an experiment that we did where we distill GPT4 to llama and we did this experiment which is that we create a factual check at factual chat using Wikipedia, use GPT4, we were at 27 seconds, but with wiki chat, with Llama, we were down to seven point seconds. So now you can make the cognitive skills faster. But of course, when I'm doing distilling, I'm not as good as GPT4. But it turns out that with this experiment we show that it behaves almost like a GPT 3.5, the process that I just described. Okay, you say I want you to practice. How do you answer questions? I practice enough. Then I take the input and output and I do a fine tuning. And now it becomes second nature. So that's the difference. LLM was trained with next words, but I'm trying to use the next word prediction in the way that perform the basic cognitive skills that we want. And now I get better with the cognitive skills. Okay,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3369831_ms_-_3446777_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3369831, "end_ms": 3446777}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3431633 ms - 3511211 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3431633 ms - 3511211 ms\n\nContent: LLM was trained with next words, but I'm trying to use the next word prediction in the way that perform the basic cognitive skills that we want. And now I get better with the cognitive skills. Okay, so the focus here are on the skills as opposed to more and more facts that you supply or more and more input and answers. I'm actually learning it the subroutines and improving the subroutines, if you will, rather than the direct answers, because the skills are what are more transferable. So I teach it how to read. And I want to show you that with this, the evaluation. We test it with a study where the user reads the first sentence of a new Wikipedia page and then they have a conversation. So the experiment is comparing GPT4 versus the version that we have that combines this super rag pipeline. And what is interesting is that our rating is actually higher. It is slower. People complain about that, but they still give us a better rating. That means that we are not losing the", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3431633_ms_-_3511211_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3431633, "end_ms": 3511211}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3498391 ms - 3565801 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3498391 ms - 3565801 ms\n\nContent: rag pipeline. And what is interesting is that our rating is actually higher. It is slower. People complain about that, but they still give us a better rating. That means that we are not losing the conversationality. But in the meantime, our factuality is now at like 98%. We miss a little bit and there are people who ask me, you still miss a little bit. What do you do? It turns out that we miss, for example, because of ambiguity in natural language, we're confusing two things that have similar names and so forth. And I would have to say that with natural language you cannot get to the 100% because there is ambiguity. We maybe will do better, but this is about as more or less as good as we can get, which is totally different. Right. If I get to the 98%, I can trust it for the most part. And what is interesting is that when you look at the ratings the people who gave GPT gave us 3.8 versus 3.4. These users are not aware that half of the statements are actually false. They're not dinging", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3498391_ms_-_3565801_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3498391, "end_ms": 3565801}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3553697 ms - 3633893 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3553697 ms - 3633893 ms\n\nContent: what is interesting is that when you look at the ratings the people who gave GPT gave us 3.8 versus 3.4. These users are not aware that half of the statements are actually false. They're not dinging it for errors. Okay, so it means that this is an example of how we can kind of get back the, you know, the ability to talk and but in a way that you can actually trust. So I would say that for the most part we have for this for conversations about things that are general like in Wikipedia, we have got rid of hallucination and we actually got an award for doing that from Multi pedia. Sorry, which is the right word? This is from Wikimedia. Wikimedia is the parent company of Wikipedia. Okay? So they own Wikipedia and they gave us an award for showing that we can stop the hallucination of LLM chatbots by few shot grounding on Wikipedia. Okay? And these are the students and the interesting. And this is actually a live demo. You can go to our website, you can try it. And I want to. I cannot", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3553697_ms_-_3633893_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3553697, "end_ms": 3633893}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3616717 ms - 3694311 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3616717 ms - 3694311 ms\n\nContent: chatbots by few shot grounding on Wikipedia. Okay? And these are the students and the interesting. And this is actually a live demo. You can go to our website, you can try it. And I want to. I cannot emphasize more the difference between before LLM and after LLM. Before we have built many projects. We never release it on the web. It's just too clunky, it's got a cliff, it makes mistakes. But now we can actually build things that we can put out and let people use. And this is what makes it really exciting. All that work is just to teach the system how to read. We are exploiting the LLM capabilities. We're using the generative power and understanding of the world. And we use its summary. The filter claims splitting and entailment, but we limit its use. We fact check everything. We only give it small paragraphs. We do not let it answer questions directly. And I cannot emphasize more that the devil is in the details. It looks very simple, right? What I showed you, it actually took three", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3616717_ms_-_3694311_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3616717, "end_ms": 3694311}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3680703 ms - 3752925 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3680703 ms - 3752925 ms\n\nContent: it small paragraphs. We do not let it answer questions directly. And I cannot emphasize more that the devil is in the details. It looks very simple, right? What I showed you, it actually took three students three months in order to get to the 98% effectuality. We just keep rejecting it. We keep seeing, no matter what we try, still mix things up. We spend a lot of time in order to get to this point. And one and it's like, why? And one of the simple examples that you can easily understand is that LLM, they don't understand time, okay? When they are trained, everything looks. They are in the past. They are in the past. But today if I read an article that was for example, written in the summer about. Written in the spring about an event in the summer, it is now all past, okay? But GPT didn't realize that everything just looks like text to it. And so those are the things that we have to work into the prompts to find out what are the things that it actually has no clue about. No clue about", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3680703_ms_-_3752925_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3680703, "end_ms": 3752925}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3741415 ms - 3813445 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3741415 ms - 3813445 ms\n\nContent: realize that everything just looks like text to it. And so those are the things that we have to work into the prompts to find out what are the things that it actually has no clue about. No clue about real world time. You fix it once and for all. And now we have a subroutine that actually gives you no hallucination on general knowledge. Okay? And now we can build on top of it and build a lot more important things. So that's the approach that I want to teach and this class. You have to be careful, get it to the 98%. And now you can build things that people can build on top of and actually use in practice without wondering whether it is actually correct at any point in time. So what we see here is that we're using LLM as a speech center and we wrap it with executive control. And in this case it is this pipeline, right? We do these 20 steps in a sense, in order to get it done. So here, before we build our neural prefrontal cortex, we are using software to provide the executive control,", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3741415_ms_-_3813445_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3741415, "end_ms": 3813445}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3797189 ms - 3874607 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3797189 ms - 3874607 ms\n\nContent: it is this pipeline, right? We do these 20 steps in a sense, in order to get it done. So here, before we build our neural prefrontal cortex, we are using software to provide the executive control, that control, what's the LLM as a subroutine. So in this particular case, it's a software program that implements the reading pipeline. It calls the LLMs and subroutines, and it interfaces to external system components. We don't rely on the LLM memory, we're relying on Wikipedia, okay? And this is how we can build a reliable system. And that's the combination of LLM software over it and connecting to external system components. This is the architecture that we will be seeing throughout this class. So to summarize, what we talked about is the foundation for this knowledge revolution is that we learn how to teach human beings. And that is using the Bloom's taxonomy. And we go back to that when we try to build systems that uses to build systems that use nlp. Okay? You don't just say, here is", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3797189_ms_-_3874607_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3797189, "end_ms": 3874607}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3860167 ms - 3927871 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3860167 ms - 3927871 ms\n\nContent: learn how to teach human beings. And that is using the Bloom's taxonomy. And we go back to that when we try to build systems that uses to build systems that use nlp. Okay? You don't just say, here is LLM and let's just use it to be a judge to do all these things. No, it is not going to be enough. You need to combine that with executive control. What goes into the prefrontal cortex and the executive control today is a software program to implement the cognitive skills. So the core concept here is to focus on the skills. Then it becomes a powerful system you can apply to many things, which is very different from 224 N. You know, every problem has its own training data, right? And if you don't have the training data, you cannot solve this problem. LLM is very generic. You can prompt, you can do all kinds of things. But what we are saying here is that if we now endow it with extra components to give you the cognitive skills, then I can come, they can use it and apply it to all the", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3860167_ms_-_3927871_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3860167, "end_ms": 3927871}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3916775 ms - 3981587 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3916775 ms - 3981587 ms\n\nContent: you can do all kinds of things. But what we are saying here is that if we now endow it with extra components to give you the cognitive skills, then I can come, they can use it and apply it to all the different domains. We just build it once and we apply to all the different domains. And that's the difference between the 224N and 224V. I would say, okay, it's like we want to build something that is generally intelligent on top by building it on top of LLMs. So the design of this course is that our focus is the tools we want to build reliable assistance easily by non AI experts. The approaches that we're going to do more application driven research because if I don't look at the application, I don't know what is wrong, but it is. The goal here is not to make one application work, it is to make all the different applications work. But we're using application to drive our research. So and the approach here is to create new LLM based cognitive skills by modeling human processes. And I just", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3916775_ms_-_3981587_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3916775, "end_ms": 3981587}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 3967747 ms - 4038401 ms", "content": "Title: CS224V Lecture 1 > Transcript > 3967747 ms - 4038401 ms\n\nContent: all the different applications work. But we're using application to drive our research. So and the approach here is to create new LLM based cognitive skills by modeling human processes. And I just show you how we did reading. We're going to do it for writing, for creating, for analyzing, for all these other things. And we are using LLM as a tool and then the project is to experiment with an initial application and then we apply it and generalize to other applications. So that's the basic idea. So here is the objectives for this course. I want you to learn the state of the art and also advance it. This is the cool part. It is really quite early. This is early stage. It is easy enough for everybody to participate in advancing it. And we have been doing so. And I want this class to get the experience of this. So there are three parts to this course. The first one is that you're going to do two homeworks and I want to bring you up on the state of the art tools. So now you know how to", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_3967747_ms_-_4038401_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 3967747, "end_ms": 4038401}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4025665 ms - 4101135 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4025665 ms - 4101135 ms\n\nContent: experience of this. So there are three parts to this course. The first one is that you're going to do two homeworks and I want to bring you up on the state of the art tools. So now you know how to build things, right? Within two and a half weeks. Because this is a project class. Once you do these two homeworks, you can imagine many projects that you couldn't imagine without these homeworks because you don't know how to get anything done yet. Right, I presume. All right, but you go through these things is that you can build all kinds of things already. Then of course there is a series on the, of lectures to teach you the techniques. And the bulk of this is the project. This is going to be heavily supervised. I have been educating a lot of PhDs for many, many years and we truly believe that having supervision makes the education experience a lot better. We supervise our PhD research. We are going to supervise the research, I mean the projects here. And you can do two things. You can", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4025665_ms_-_4101135_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4025665, "end_ms": 4101135}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4086749 ms - 4165155 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4086749 ms - 4165155 ms\n\nContent: having supervision makes the education experience a lot better. We supervise our PhD research. We are going to supervise the research, I mean the projects here. And you can do two things. You can develop new tools or you can take the tools and apply it. And you will find out that it's going to have weaknesses and now you can improve it. So you have really these two routes. Whether you're doing more tools or you're doing more apps, you know, it's like you will, you know, no matter what, you will be generating new knowledge in this space. So this is the fourth time, but so what happened is that things changed a lot with the, with the arrival of 224V and at the beginning of 224V in 2023, we give them the Wiki Chat, you know, the reading tool that I just talked to you about. And Wiki Chat was actually built a year before in 224V in 2022. Okay? That's when we built it. So we use it as a tool in 2023. So in day, in week one, everybody was able to build a chat that does not hallucinate", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4086749_ms_-_4165155_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4086749, "end_ms": 4165155}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4147995 ms - 4223935 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4147995 ms - 4223935 ms\n\nContent: was actually built a year before in 224V in 2022. Okay? That's when we built it. So we use it as a tool in 2023. So in day, in week one, everybody was able to build a chat that does not hallucinate because they just used the tool. And so that's the beginning. And we have many projects, we have about 90 some students, we have about 40ish projects. And out of the 40 sub projects, at the end of that quarter, we had two submissions that got accepted as papers. So we got two. There's just a single quarter project and they got accepted and presented in the summer of this year. And it also laid the groundwork for three other publications that just got accepted. And we also have live demos which I'm very, very excited about. You can go try them. There are several of them. And this was last year. This is the picture from last year. This is the picture for this year. Okay. You guys are so much, you know, you are so much luckier in a sense because what is given to you is a much bigger platform.", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4147995_ms_-_4223935_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4147995, "end_ms": 4223935}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4210811 ms - 4289877 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4210811 ms - 4289877 ms\n\nContent: This is the picture from last year. This is the picture for this year. Okay. You guys are so much, you know, you are so much luckier in a sense because what is given to you is a much bigger platform. Not only can you read, not only can you do question answering, we can actually write full reports. It is not just writing, it is also a collaborative reports. We can do coreference browsing and you can actually build a fully working conversational agent. It is connected to, for example, the Google Wikipedia Wikidata. You may not know about Wikidata Wikipedia, everybody knows about. Wikidata is a structured knowledge base also from Wikimedia. And it has something like 15 billion triples. It has tons and tons of knowledge. But it was really hard to get access to, for people to get access to because you have to write in this really hard query language called Sparkle. And what we have created is an interface so that you can use natural language to reach to be to get to the 15 billion triples", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4210811_ms_-_4289877_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4210811, "end_ms": 4289877}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4275099 ms - 4356063 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4275099 ms - 4356063 ms\n\nContent: you have to write in this really hard query language called Sparkle. And what we have created is an interface so that you can use natural language to reach to be to get to the 15 billion triples in Wikidata. Okay, so these are the public knowledge bases. You can also attach your own documents, your own corpus, your own knowledge bases. And we have two tools. One is suql. It is a. It's the query language that takes that. So what we do is that we take natural language. We go through a semantic parser, we generate a formal queries using represented in SQL. It is the first query language that allow you to access both Structured and unstructured. Everybody knows about SQL, that's for structure. But there are also things that are unstructured. So SQL combined them so people now can use natural language and you can get access to both structured and unstructured. GNE worksheet allows you to build a new build an agent with very small number, very small number of lines of code. And this is a", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4275099_ms_-_4356063_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4275099, "end_ms": 4356063}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4340135 ms - 4417037 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4340135 ms - 4417037 ms\n\nContent: language and you can get access to both structured and unstructured. GNE worksheet allows you to build a new build an agent with very small number, very small number of lines of code. And this is a task and knowledge box. This is available to you for you to build projects on. Okay. This is so much better than the previous years and we are building all the skills we talked about. Everything else, everything that we mentioned here, we have already gotten some results in. Okay. So we are much more sophisticated than the year before. So what we're going to do is that we're going to do two homeworks. In this first homework you're going to learn how to do data curation. Curation means it's going beyond just Q and A. You know, that's kind of too basic. So what you will be building on is a system called Storm. That was the project from a year, from 224v last year. So this Storm project, it writes you a Wikipedia like article from scratch by Reese searching the Internet and you on top of", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4340135_ms_-_4417037_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4340135, "end_ms": 4417037}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4401541 ms - 4475235 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4401541 ms - 4475235 ms\n\nContent: called Storm. That was the project from a year, from 224v last year. So this Storm project, it writes you a Wikipedia like article from scratch by Reese searching the Internet and you on top of Storm. From the first quarter last year we have built Code Storm which added collaboration to Storm. So your homework is that you get to use Code Storm and to make things exciting you get to use cold Storm on archive. Right? This is all brand new. It's all bleeding etched material here. We do not know what it looks like, but we know that it doesn't work very well. Okay. Because there's a huge difference between ARXIV and Wikipedia and this is why we have a lot of research to do. Okay. So you will get to play with LLM based pipelines. So within the first home, after you finish this homework, you will be familiar with putting these pipelines together and how people have put pipelines together before and you will find out problems because we are all looking for problems in the research. In this", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4401541_ms_-_4475235_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4401541, "end_ms": 4475235}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4463869 ms - 4537759 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4463869 ms - 4537759 ms\n\nContent: be familiar with putting these pipelines together and how people have put pipelines together before and you will find out problems because we are all looking for problems in the research. In this course I want you to find the problem so that you can propose new projects to fix the problems. So we know that it does something with arXiv, but it doesn't do it very well. Just keep in mind this is the nature of these projects. I just want to tell you a little bit about Storm. It was released in July, I think that was the second release and we started seeing a lot of users and we discovered that other people have made YouTube videos of our project and they put it online and they say secret AI research tool from Stanford turn any topic into a research paper for free. And I like this a lot. It says perplexity pages alternative, but this is insanely better. This is what happens when other people are doing commercials for you. Because at Stanford we don't talk like this. But anyway, we", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4463869_ms_-_4537759_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4463869, "end_ms": 4537759}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4524895 ms - 4601489 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4524895 ms - 4601489 ms\n\nContent: It says perplexity pages alternative, but this is insanely better. This is what happens when other people are doing commercials for you. Because at Stanford we don't talk like this. But anyway, we generated a lot of traffic. It broke our system a couple of times because we ran out of quota for our APIs. We have many users, many articles and this is pretty good. It is very general, it applies to any domain. But it is more the non technical domain as we discussed. You can all try it and you will be doing homework on this. This is storm and then we build code Storm on it. The second homework is that we will be creating an agent. When people think about nlp, the classic use is conversational agents, right? You want to be able to interface to data, to everything. This project uses this system we call the GENIE worksheet, which allows you to build a powerful agent with a few lines of code. It's task oriented. So this particular example we are doing is a project that was built last year year", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4524895_ms_-_4601489_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4524895, "end_ms": 4601489}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4587961 ms - 4657393 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4587961 ms - 4657393 ms\n\nContent: call the GENIE worksheet, which allows you to build a powerful agent with a few lines of code. It's task oriented. So this particular example we are doing is a project that was built last year year in this class and it does, it helps you with course enrollment. All right, you know how to enroll in courses and now you can now talk. I want to enroll in 224V. Sorry, it's too full. It's full, but that's a different story. And then it is also knowledge oriented because most of the time when you want to enroll, you want to ask about the courses. Is there a lot of work in 220 and so forth? People don't just enroll. If you have an agent, you want to talk. So it actually covers all this. And the purpose of this project is that you will get to see what it means to build a tool for developers. In this case it is the developer that is using the worksheet to generate new agents of any topic that you like. And then you will learn the weaknesses and strengths. It is better than anything we knew", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4587961_ms_-_4657393_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4587961, "end_ms": 4657393}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4643383 ms - 4713183 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4643383 ms - 4713183 ms\n\nContent: this case it is the developer that is using the worksheet to generate new agents of any topic that you like. And then you will learn the weaknesses and strengths. It is better than anything we knew outside, but it has still got its own weaknesses. And I want to show you that. You know, there are a lot of NLP papers every day actually. But there is a big difference between projects that people use and a paper. And I want you to experience that. So don't be too upset if things don't work completely as you expect. But it is a lot better than what we have been able to do. Okay? Okay. What does it really mean to do courses so we all know how to enroll? Look at the form that you have to fill out. This is AI biology everything, right? There are so many different requirements and the kind of things that we want to answer is that what course would you like to enroll in? People just don't give you a course. There's a kind of answer that people would like to be able to say, I'm a sophomore", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4643383_ms_-_4713183_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4643383, "end_ms": 4713183}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4700799 ms - 4767705 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4700799 ms - 4767705 ms\n\nContent: things that we want to answer is that what course would you like to enroll in? People just don't give you a course. There's a kind of answer that people would like to be able to say, I'm a sophomore studying cs. I want to complete this significant implementation requirement. What are some of the options that require the least amount of work? Is that you? I know that there are people who ask that questions. So how do you answer that question? You actually have to look up a lot of knowledge, a lot of databases from the degree requirement, from the bulletin from Carter to figure out if there's a lot of work. So to answer that question, you have to look at the program sheets and so forth, to look at the grades, look at how much work, and then it just keeps going. And wouldn't it be nice to have an agent that will answer your question? So that's the project that we have built and that's the one that you're going to build on top of. And in this case, the developers really just have to", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4700799_ms_-_4767705_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4700799, "end_ms": 4767705}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4756497 ms - 4824103 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4756497 ms - 4824103 ms\n\nContent: to have an agent that will answer your question? So that's the project that we have built and that's the one that you're going to build on top of. And in this case, the developers really just have to provide a worksheet and the knowledge bases and that's it. The system takes over and do the rest. So the worksheet kind of looks like this. You, for enrollment, you have to know the course number and the student information and so forth. And this you can do without knowing anything in AI. Okay, it just writes down what you need and then you have to tell me what the databases look like and it finishes the rest. So these are several databases. And then a question like this, I'm a sophomore, blah, blah, blah, blah. The same question that we have, it turns into, through the AI system, a formal query like this. And this is the language S uQL that pulls the information out and then it answers your question. And the architecture looks like this. We are asking the developers to describe what you", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4756497_ms_-_4824103_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4756497, "end_ms": 4824103}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4810303 ms - 4887083 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4810303 ms - 4887083 ms\n\nContent: like this. And this is the language S uQL that pulls the information out and then it answers your question. And the architecture looks like this. We are asking the developers to describe what you want to achieve. And GENIE interprets it, okay, In a sense, kind of like a compiler, it interprets it and it actually gives you the agent that you want that you are specifying. So so much easier than doing dialogue trees and anticipating what people will ask and so forth. You just basically supply the basic and we fill in the blanks. So I will. So this is, this is an actual. We'll do this later because we're running out of time. It actually answers the question quite well. You know, you can ask about the time offered and sort by the rating and figure out credit and no credit and all this stuff. This is all automatically done and you will be building on top of that in your homework too. Okay. You just Extend this in homework too. So by the time you're finished with this homework you are ready", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4810303_ms_-_4887083_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4810303, "end_ms": 4887083}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4875171 ms - 4947431 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4875171 ms - 4947431 ms\n\nContent: is all automatically done and you will be building on top of that in your homework too. Okay. You just Extend this in homework too. So by the time you're finished with this homework you are ready to do a lot of projects. So we're going to have lectures and then we're going to do the projects. And I want to emphasize that we. The there are. The hardest part about projects is to come up with the research project itself. And we have put together over 20 projects. It's on the website. I recommend that you check it out as soon as you can. And there are interesting domains from journalism, history, medicine and so forth. And you're also welcome to initiate your own. And we're going to do weekly group meetings and all the projects are done in groups of two. And this is roughly the schedule where we got to do the first of all it's a homework get use the research project ideas. We then discuss the projects, then we do the lectures and then we come back from Thanksgiving and then we present the", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4875171_ms_-_4947431_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4875171, "end_ms": 4947431}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4935633 ms - 5026451 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4935633 ms - 5026451 ms\n\nContent: we got to do the first of all it's a homework get use the research project ideas. We then discuss the projects, then we do the lectures and then we come back from Thanksgiving and then we present the final projects. So this is. And we will have weekly check ins with your mentors in order to make sure that everybody is making good progress. And it is really our responsibility to make sure that if you do what we want you get an A out of course. Okay. And if there is, there is a problem then you will know about it as we go. Okay so that's the basic idea and this is the course, the course breakdown in terms of the distribution. And we will talk about the projects later in the rest of the quarter. All right. Any so. So that's the overview and maybe I'll anybody else, anybody wants to ask a question or two otherwise I'll let you go. There's a question. The attendance. I am going to try to learn everybody as you come in and then I'll see who is missing. We are here. You will find out that we", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4935633_ms_-_5026451_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4935633, "end_ms": 5026451}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 4997509 ms - 5092671 ms", "content": "Title: CS224V Lecture 1 > Transcript > 4997509 ms - 5092671 ms\n\nContent: or two otherwise I'll let you go. There's a question. The attendance. I am going to try to learn everybody as you come in and then I'll see who is missing. We are here. You will find out that we got to know who is in the class because I actually think that it is important to have students in class. They contribute and you learn by interacting with each other. There are so many discussions on projects and I don't think that it is pretty obvious if you are not making the class. Okay, yes, I'm sorry, the class is full. So because we have heavy supervision then we are limiting the class. So some of you got enrolled and I think some of you are what you call it? Testing. You know you are still figuring out if you're going to take the class. If you don't take, if you plan to not take the class, please get out of the class so we can give the spots to other people. Yes. Logistics. But I know that the last Wednesday is a three hour class. If we have a class that's supposed to be right after", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_4997509_ms_-_5092671_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 4997509, "end_ms": 5092671}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 5075183 ms - 5157765 ms", "content": "Title: CS224V Lecture 1 > Transcript > 5075183 ms - 5157765 ms\n\nContent: please get out of the class so we can give the spots to other people. Yes. Logistics. But I know that the last Wednesday is a three hour class. If we have a class that's supposed to be right after this with mandatory attendance, what can we do about that? We will let you go first. But this is. This is. That's the only way we can pack everybody in, we think. Yes. When are we supposed to do the readings? What are you supposed to do with the readings? No, when are we supposed to do which readings? Oh, the. You will see from the lectures which readings go with which. Okay. There are a lot of readings and maybe we should mark. Which is mandatory because it includes a lot of papers that not everybody will have time to read. And we will try to make it more clear. That's a good question. Anything else? Yes, you are in. What? No. No. Okay. What? You are. You are enrolled. If you are officially enrolled inaccess. Yes, unfortunately, we import everything in Greatscope into Greatscope, including", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_5075183_ms_-_5157765_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 5075183, "end_ms": 5157765}}
{"document_title": "CS224V Lecture 1", "section_title": "CS224V Lecture 1 > Transcript > 5135025 ms - 5188245 ms", "content": "else? Yes, you are in. What? No. No. Okay. What? You are. You are enrolled. If you are officially enrolled inaccess. Yes, unfortunately, we import everything in Greatscope into Greatscope, including all the people on the waitlist. Unfortunately, we also have a very long waitlist. Anything else? Yes. So we'll be sending out a form, put together credits. Please fill it out because you'll have to use the API key for the homeworks. So we'll publish it today. So try to like fill it up by tomorrow. All right. Okay. It's a fast moving course. I think that's good for today. And I'll see you on Wednesday.", "block_metadata": {"id": "CS224V_Lecture_1_>_Transcript_>_5135025_ms_-_5188245_ms", "document_type": "transcript", "lecture_number": 1, "start_ms": 5135025, "end_ms": 5188245}}
