{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > How To Passcode During a Meeting", "content": "Title: CS224V Lecture 4 > Chapter Summaries > How To Passcode During a Meeting\n\nContent: Please enter the meeting passcode followed by the pound sign. I think anybody who is on Zoom should mute themselves. Okay. That work?", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_How_To_Passcode_During_a_Meeting", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 69405, "end_ms": 187925}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Quarter 3 Projects", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Quarter 3 Projects\n\nContent: The homework in the second homework is based on the material we discussed in the last lecture. You get to play with a live knowledge and task assistant and you get to make one yourself. After doing that, it will probably give you a better idea of what you can attempt in this course.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Quarter_3_Projects", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 188305, "end_ms": 284879}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Projects", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Projects\n\nContent: So today we are going to be. talking about projects. This is the first day because devote quite a few days on projects. And what we have done is that we have asked the people who are pitching projects to come in person or on Zoom to talk about their projects.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Projects", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 285047, "end_ms": 321225}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > One small step for people with autism", "content": "Title: CS224V Lecture 4 > Chapter Summaries > One small step for people with autism\n\nContent: Lynn Kanford: Our team has been working with Monica and her amazing team to work on some treatment for people with autism. Just finished a randomized clinical trial. Would like to reach 50,000 people this quarter in the US alone. Looking at employment issues.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_One_small_step_for_people_with_autism", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 321925, "end_ms": 656075}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Improvement in the autism communication", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Improvement in the autism communication\n\nContent: Sh. Major: How did you measure improvement to compare between the experimental and control? The conversation is inherently a multimodal form of conversation. Are there any plans to incorporate this?", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Improvement_in_the_autism_communication", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 669455, "end_ms": 984439}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Autism and the Multodal Experience", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Autism and the Multodal Experience\n\nContent: If anybody in this class wants to add multimodal to any of this interaction we have with people with autism, it is very, well, welcome. There's a lot of technology that can be used. Next speaker is Trevor, who is also going to be talking to us over zoom.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Autism_and_the_Multodal_Experience", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 984487, "end_ms": 1052004}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > African History from Under the Hood", "content": "Title: CS224V Lecture 4 > Chapter Summaries > African History from Under the Hood\n\nContent: Trevor Goetz: This is African History from below, a collaboration that the center for African Studies is participating in. It's based on the African Times, a newspaper created by people of African descent in the Atlantic in the late 19th century. We want to see what these articles contribute that is different.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_African_History_from_Under_the_Hood", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 1052904, "end_ms": 1248755}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > African Times", "content": "Title: CS224V Lecture 4 > Chapter Summaries > African Times\n\nContent: Trevor: We want to analyze the difference between the African Times relative to older media. We can prove that this tool is adding to knowledge. Giving us a new and richer and different perspective on the past. It'S going to be talking together with Sino on the next project.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_African_Times", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 1264425, "end_ms": 1364645}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > How to create knowledge with the Internet", "content": "Title: CS224V Lecture 4 > Chapter Summaries > How to create knowledge with the Internet\n\nContent: Sina is a senior PhD student in my research group. Their project is to understand how to create knowledge. They will be working with large set of newspapers that the Library of Congress has. They hope to use AI systems to extract knowledge from them.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_How_to_create_knowledge_with_the_Internet", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 1365105, "end_ms": 1806675}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Projects 6, Communication Engagement", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Projects 6, Communication Engagement\n\nContent: Shi Cheng: In this project we propose to develop a data talk conversational engine. The core innovation part of it lies in a language that takes in both structured data and free text. There's real possibilities in being able to use this kind of platform or tool to be able to level up small newsrooms.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Projects_6,_Communication_Engagement", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 1833385, "end_ms": 2185761}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > The list of counties with ICE contracts", "content": "Title: CS224V Lecture 4 > Chapter Summaries > The list of counties with ICE contracts\n\nContent: We just wanted to see where ICE contracts were in the state and see if there was potential for a story there. It was surprising that San Diego was not surprising. But second was Santa Clara county, which was a little surprising. They were almost all data sharing.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_The_list_of_counties_with_ICE_contracts", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 2185833, "end_ms": 2255115}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Storm and Cold Storm: The Future of Intelligence", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Storm and Cold Storm: The Future of Intelligence\n\nContent: There are over 160 topics collaboratively contributed by all of you across more than 20 domains. Allows users to see the system autonomously driven and occasionally played by the users to steer the directions of the AI system. Could be a good directions for all the AI products in the future.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Storm_and_Cold_Storm:_The_Future_of_Intelligence", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 2262785, "end_ms": 2665505}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > In-Depth Analysis with the STORM system", "content": "Title: CS224V Lecture 4 > Chapter Summaries > In-Depth Analysis with the STORM system\n\nContent: The Storm system surveys like many like background information and pulls out information sources from the general Internet. The second project we are proposing here is having a deep dive conduct in depth analysis with the knowledge curation systems. Can they combine have a good performance on the use cases like the investigative journalism.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_In-Depth_Analysis_with_the_STORM_system", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 2666405, "end_ms": 2855295}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Projects for the Game Tutor", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Projects for the Game Tutor\n\nContent: Sony engineer presents his pitch for a game tutor bot. The bot would be able to understand different gameplay styles according to the for example player history. It would cater to the need of the player regarding that specific gameplay style. If you guys are interested, you can contact them.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Projects_for_the_Game_Tutor", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 2861605, "end_ms": 3173845}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Sony PlayStation 4.1", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Sony PlayStation 4.1\n\nContent:  PlayStation won't be able to provide the real game or the player data due to the privacy concern. We encourage you to look for any open source environment or game simulation environment. If there are good results coming out of this collaboration, we will be happily continue for the future.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Sony_PlayStation_4.1", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 3175905, "end_ms": 3301625}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > PlayStation Tutorials with PS4", "content": "Title: CS224V Lecture 4 > Chapter Summaries > PlayStation Tutorials with PS4\n\nContent: Either a single player or multiplayer. There will be a next level of complicacy. Currently the PlayStation has some game tutorial. Those could be your source if you're able to gather your own data. But unfortunately we won't be able to provide those data.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_PlayStation_Tutorials_with_PS4", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 3303125, "end_ms": 3391845}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Findings in the Wikipedia Knowledge Corpus", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Findings in the Wikipedia Knowledge Corpus\n\nContent: We are working with Wikipedia as a knowledge corpus. By the end of the quarter we ideally want to be able to run the system on the entire Wikipedia, on more than 600 million articles. If you want to find inconsistencies, especially at this scale, you will run into some interesting cases.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Findings_in_the_Wikipedia_Knowledge_Corpus", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 3404385, "end_ms": 3654249}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Building a deep-level news analysis system", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Building a deep-level news analysis system\n\nContent: Another project that I'm proposing is to build a systems that can analyze or answer very deep questions about what is being reported in the news. The goal of this specific project now is to building a similar system for KQED's audio archives.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Building_a_deep-level_news_analysis_system", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 3654297, "end_ms": 4091095}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > The Conversational Agent for Lectures", "content": "Title: CS224V Lecture 4 > Chapter Summaries > The Conversational Agent for Lectures\n\nContent: The project goal itself is just to create a conversational agent that takes video audio transcripts of lectures as input. The goal should be to be able to do this for any sort of lecture that Stanford might have. You'll get to build on an existing structured audio pipeline and integrate lecture content, additional materials.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_The_Conversational_Agent_for_Lectures", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 4095925, "end_ms": 4213865}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Transcription in the LLM", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Transcription in the LLM\n\nContent: Have you experimented with using any synthetic data to help these agents out? Because I assume because text to speech is like. Lecture videos and the ground truth would be quite difficult. Is there any questions on this one?", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Transcription_in_the_LLM", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 4219565, "end_ms": 4305525}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > 3 challenges in creating AI-based Agents with GENIE", "content": "Title: CS224V Lecture 4 > Chapter Summaries > 3 challenges in creating AI-based Agents with GENIE\n\nContent: Genie worksheets aims to provide developers ease of coding up chatbots. The system outperforms state of the art methods in academic data sets. There are several other projects that we want to branch out from this.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_3_challenges_in_creating_AI-based_Agents_with_GENIE", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 4316625, "end_ms": 4687275}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Chapter Summaries > Together AI: Week 1", "content": "Title: CS224V Lecture 4 > Chapter Summaries > Together AI: Week 1\n\nContent: Please fill out your Together AI credits form by end of day today. We'll send the accounts to together AI tomorrow. Maybe you're looking for project project partners. Let us know. I'll see you next week on Monday.", "block_metadata": {"id": "CS224V_Lecture_4_>_Chapter_Summaries_>_Together_AI:_Week_1", "document_type": "chapter summary", "lecture_number": 4, "start_ms": 4691575, "end_ms": 4830425}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 69405 ms - 256755 ms", "content": "Title: CS224V Lecture 4 > Transcript > 69405 ms - 256755 ms\n\nContent: Please enter the meeting passcode followed by the pound sign. Okay, perfect. That work? I think anybody who is on Zoom should mute themselves. Okay. All right. How's everybody? It's hot. Hot out there. Anyway, today we have an exciting class because we're going to be talking about projects. But before I do that, let's do a little bit of class logistics. The homework is due today. I hope everybody has done the homework and we are putting out another homework and that should be out within the next couple of hours. Next few hours. Okay. So the homework in the second homework is based on the material we discussed in the last lecture. And you get to play with a live knowledge and task assistant and you get to make one yourself as well. It is a very lightweight exercise. It's not like you're going to do something very, very fancy because we just want to make sure everybody has their hands on experience with doing something. But that will, you know, working with an agent. And what we expect", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_69405_ms_-_256755_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 69405, "end_ms": 256755}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 242707 ms - 308661 ms", "content": "Title: CS224V Lecture 4 > Transcript > 242707 ms - 308661 ms\n\nContent: to do something very, very fancy because we just want to make sure everybody has their hands on experience with doing something. But that will, you know, working with an agent. And what we expect is that after doing that, it will probably give you a better idea of what you can attempt in this course, what is easy and what is hard. Some students came up to me and said, oh, can I do this for my quarter project? And I say, try it on GPT. If GPT works as is, then this is not the project. Okay. It's easy to try whatever you want to do. Try it. If you can find problems with GPT, then you may have a project. So that's how it goes. So today we are going to be. We're going to talk about projects. This is the first day because devote quite a few days on projects. And this is the first day. And what we have done is that we have asked the people who are pitching projects to come in person or on Zoom to talk about their projects. It's a very short introduction to the projects. You can find out", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_242707_ms_-_308661_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 242707, "end_ms": 308661}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 296365 ms - 368777 ms", "content": "Title: CS224V Lecture 4 > Transcript > 296365 ms - 368777 ms\n\nContent: we have done is that we have asked the people who are pitching projects to come in person or on Zoom to talk about their projects. It's a very short introduction to the projects. You can find out more information on the proposal or you can talk to the people afterwards according to your interest. Okay, so let's get started. And the first up is Lynn. Is Lynn in there? Is Lynn ready on Zoom? I'm here. Can you hear me? Yes, we can hear you. So, okay, great. Let's kick that off. Thanks, Lynn. Great. Okay, thank you, Monica. It's so nice to be here today. I've been really fortunate to have met Monica several years ago and have been working with her amazing team for a while. And I'm from the School of Medicine, so we combined. I do treatment for individuals with autism Spectrum disorder and I've been just. Our team has been working with Monica and her amazing Monica and Her amazing team to work on some treatment for people with autism. So let me just go to the next slide. We have really", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_296365_ms_-_368777_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 296365, "end_ms": 368777}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 353601 ms - 422013 ms", "content": "Title: CS224V Lecture 4 > Transcript > 353601 ms - 422013 ms\n\nContent: I've been just. Our team has been working with Monica and her amazing Monica and Her amazing team to work on some treatment for people with autism. So let me just go to the next slide. We have really seen an increase in the prevalence and incidence of autism in the last few decades. When I started and my career was 1 in every 2,500. Now, the CDC last year points to the fact that 1 in every 36 children are being diagnosed with autism. And this creates a huge problem. There aren't enough providers to deal with all these kids and adolescents and adults. They are not what A lot of the providers aren't well trained. So the individuals aren't getting very good intervention. There's long wait lists for the intervention, and there's not quite. There's more research and services available to young kids, but not as many to adolescents and adults. So it's really a crisis right now for people that have autism. They also have the lowest employment rate of any group, any disability group. Most of", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_353601_ms_-_422013_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 353601, "end_ms": 422013}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 408283 ms - 477671 ms", "content": "Title: CS224V Lecture 4 > Transcript > 408283 ms - 477671 ms\n\nContent: kids, but not as many to adolescents and adults. So it's really a crisis right now for people that have autism. They also have the lowest employment rate of any group, any disability group. Most of them have very few friends and most of them don't have very many social activities. So it's a real problem. And over the last few years, Monica's team and our team developed nura, which is a web app program to help clinicians implement interventions that are usually face to face. But we've gotten these apps to be able to target these areas, which mostly are challenging areas for our folks. And we have modules that help with empathy, asking relevant questions, listening and responding, talking the right amount, not talking too much or too little, giving the right amount of personal information, not too much personal information if you've just met somebody, tactful responses, giving compliments, understanding sarcasm, which can be very challenging for some people on the autism spectrum, and", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_408283_ms_-_477671_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 408283, "end_ms": 477671}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 464825 ms - 528841 ms", "content": "Title: CS224V Lecture 4 > Transcript > 464825 ms - 528841 ms\n\nContent: not too much personal information if you've just met somebody, tactful responses, giving compliments, understanding sarcasm, which can be very challenging for some people on the autism spectrum, and also staying on topic during conversations. So we've gotten. We've developed these modules that can help in all these different areas. And we're very, very excited that we just finished a randomized clinical trial. It took us over a year to do, but we had 30 participants, 15 in the experimental, 15 in the control group. And we were very excited when we ran the statistics. And the experimental group showed significant improvements with medium to large effect sizes compared to the control group. And this was after only four weeks of using the app. And we started with a low dosage. We just wanted to see if we could kind of do a minimal number of trials. So all they had to do were like 10 examples a day, which could take, depending on the person, just a few minutes to a little bit longer. But", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_464825_ms_-_528841_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 464825, "end_ms": 528841}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 517493 ms - 579115 ms", "content": "Title: CS224V Lecture 4 > Transcript > 517493 ms - 579115 ms\n\nContent: to see if we could kind of do a minimal number of trials. So all they had to do were like 10 examples a day, which could take, depending on the person, just a few minutes to a little bit longer. But just with a minimal practice, they really improved a lot. And what was also exciting was that almost all the participants rated the program as positive. Or very positive and said they would recommend it to others. We also, Monica and I got mentioned in Scientific American article which was kind of exciting because the article was basically saying that people were putting out all these things for individuals with autism, but some of them aren't scientifically proven to help them and they could even hurt them. So it's kind of a risky thing, especially with a population that might not really under, you know, fully understand that it's an app. And so we really are trying to not be their friend, but to give them really some positive skills so that they can more easily make friends and engage.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_517493_ms_-_579115_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 517493, "end_ms": 579115}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 566847 ms - 636265 ms", "content": "Title: CS224V Lecture 4 > Transcript > 566847 ms - 636265 ms\n\nContent: you know, fully understand that it's an app. And so we really are trying to not be their friend, but to give them really some positive skills so that they can more easily make friends and engage. So future directions, this next quarter, we would really like to increase the availability and just have this help change everything for people with ASD. We'd like to reach 50,000 people this quarter in the US alone, there's over 75 million people with autism spectrum disorder. So we want to start getting to these people to really help and to create a mass feedback analysis system. And we also would like to look at some of the employment issues. There's a lot of these individuals, if they do get jobs, they have coaches that are go to them to the job site, but most of them are not well trained. They don't really help them improve in communication. They don't really reward their attempts and things like that. So we'd like to develop another app that would really help them help job coaches", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_566847_ms_-_636265_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 566847, "end_ms": 636265}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 622817 ms - 718735 ms", "content": "Title: CS224V Lecture 4 > Transcript > 622817 ms - 718735 ms\n\nContent: They don't really help them improve in communication. They don't really reward their attempts and things like that. So we'd like to develop another app that would really help them help job coaches improve their feedback to the person with autism. So thank you. That's it for mine. My email is Lynn Kanford. Edu, but I'm sure Monica also has everybody's emails listed to get in touch. Great. Thank you so much. And we look forward to working with you if you're interested in this area. Okay, so I'll sign off. Should I sign off? We have a question here for you. Can you hear me? Oh. Wait, wait. We have a question for you. Okay. How did you measure improve? Sorry, I can't hear. I think the sound is off. Oh, I see an in the chat. Let's see. Can you hear us? Can you hear us? I can hear you now. Yeah. Hi, this is. My name is Sh. My question was how did you measure improvement to compare between the experimental and control? Oh, that's a great question. So we had a couple different measures. One", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_622817_ms_-_718735_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 622817, "end_ms": 718735}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 705607 ms - 780541 ms", "content": "Title: CS224V Lecture 4 > Transcript > 705607 ms - 780541 ms\n\nContent: Hi, this is. My name is Sh. My question was how did you measure improvement to compare between the experimental and control? Oh, that's a great question. So we had a couple different measures. One measure is we just look at the app and see if they improved from their first week to their last week. And the second thing is, what was more important to us was does it generalize? So we had one of our. One of our team members called them before and after and we looked at Improvements. So we only had participants that had challenges at the beginning, and then we went through the treatment, and then we looked at their improvement score. So they had to score 60% or below. So we did do a conversation sample to make sure it generalized into everyday conversation. Anybody else? I was just wondering how you designed. The like, user interface in terms of, like, how people were interacting with this app and like, I guess how specifically it was helping. Was this, like, a conversation? Like. Like a.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_705607_ms_-_780541_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 705607, "end_ms": 780541}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 770261 ms - 853283 ms", "content": "Title: CS224V Lecture 4 > Transcript > 770261 ms - 853283 ms\n\nContent: you designed. The like, user interface in terms of, like, how people were interacting with this app and like, I guess how specifically it was helping. Was this, like, a conversation? Like. Like a. Like a practice conversation that we used? Sorry, I missed the last part. You wonder how we designed. What is the app that's not on? Sorry. Oh, sorry. This one is on. Okay. Can you hear me now? Yes. Okay, great. Yeah, I was just wondering how you. Designed the interface, essentially. Yeah. How did the users use it, and. What did you find was most useful to them? Yeah, so we. I showed briefly a little how. What it kind of looked like. So. Oops, let me see. I can show you what the app look like. It sort of. Let's see, where am I? This is what our app looked like. On the right. You can see it. Nora. And so, for example, for the empathy module, it might. Nora. The app might say, oh, I was running this morning and I twisted my ankle. It really hurts. And if the autistic person said, huh, Then it", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_770261_ms_-_853283_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 770261, "end_ms": 853283}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 835995 ms - 912047 ms", "content": "Title: CS224V Lecture 4 > Transcript > 835995 ms - 912047 ms\n\nContent: And so, for example, for the empathy module, it might. Nora. The app might say, oh, I was running this morning and I twisted my ankle. It really hurts. And if the autistic person said, huh, Then it would say, give feedback. Or if they said, oh, it'll get better, it gave feedback and it said, you know, a better answer would be maybe showing that you care about my sprained ankle. Like, how are you feeling? So it would give feedback or tell the person if it had a correct answer. And if you're interested, I know we're short of time today, but if you're interested, feel free to email me and I can show you how it works in real life. Becomes available. We close it because of the clinical trial, and we make it available. And the short answer also is that the interface was designed as the way they, you know, the therap. You know, it's what the therapist was doing, was doing with them, with the patients, with the users. Hello, my name is Kevin. Coming from the perspective of a psychology.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_835995_ms_-_912047_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 835995, "end_ms": 912047}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 896515 ms - 969567 ms", "content": "Title: CS224V Lecture 4 > Transcript > 896515 ms - 969567 ms\n\nContent: they, you know, the therap. You know, it's what the therapist was doing, was doing with them, with the patients, with the users. Hello, my name is Kevin. Coming from the perspective of a psychology. Major, the conversation is inherently a multimodal form of conversation. Like, it's multimodal. There's not only just the words that people say, but also how they say it, what the inflection is, body language, facial expressions. And these are also precisely a lot of the skills that individuals with autism. May need help with. So are there any plans to incorporate this? And how do you. Or, like, just sort of. Are there any thing that exists right. Now that can incorporate these modal multimodal inputs. You're absolutely right. There's a lot of kind of pragmatic areas, too, that are challenging. In this one, we've mostly focused on the verbal, and the participants were all able to carry on a conversation, so areas like eye contact and loudness and things like that. We. We didn't", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_896515_ms_-_969567_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 896515, "end_ms": 969567}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 955389 ms - 1016255 ms", "content": "Title: CS224V Lecture 4 > Transcript > 955389 ms - 1016255 ms\n\nContent: challenging. In this one, we've mostly focused on the verbal, and the participants were all able to carry on a conversation, so areas like eye contact and loudness and things like that. We. We didn't specifically focus on this was more focused on just what kind of verbal response you give. But I did a couple of the language samples that, you know, during the. During the study, and I noticed a huge difference. Like, on one of them, the guy said. I said to the guy, oh, you know, I twisted my ankle running. And he said, I don't know what to say. And then after four weeks, I said something similar, like, oh, you know, Lizzie broke her shoulder, or my friend broke her shoulder when she was riding her horse. And he goes, oh, I'm so sorry to hear that. Is she okay? So really, we could see a big difference in some of the participants. Most of the participants improve, so it was really exciting. You have to move on because we have many more. Thank you so much. Well, thank you. Thank you, Lynn.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_955389_ms_-_1016255_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 955389, "end_ms": 1016255}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1004123 ms - 1070801 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1004123 ms - 1070801 ms\n\nContent: in some of the participants. Most of the participants improve, so it was really exciting. You have to move on because we have many more. Thank you so much. Well, thank you. Thank you, Lynn. And just to add to that particular question, if anybody in this class wants to add multimodal to any of this interaction we have with people with autism, it is very, well, welcome. You know, you are welcome to do that. Okay, so that would be a good. We talked about it, and it would be a really good topic to explore. There's a lot of technology that can be used. All right, thank you, Lynn. Thank you. Thank you. So we move on to the next speaker, and that is Trevor, and he is also going to be talking to us over zoom. Trevor, please introduce yourself. Hi, everybody. My name is Trevor Goetz, and I'm an African historian. And I'm going to talk to you about African history from below, which is a collaboration that the center for African Studies is participating in, along with folks from. Folks, you", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1004123_ms_-_1070801_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1004123, "end_ms": 1070801}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1057064 ms - 1119659 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1057064 ms - 1119659 ms\n\nContent: African historian. And I'm going to talk to you about African history from below, which is a collaboration that the center for African Studies is participating in, along with folks from. Folks, you know, from this lab. And I guess what I really want to start by saying is that none of these people that you see in front of you historically appeared in narratives about the 19th century, but they're all people who appear in the African Times, which is a newspaper that was created by people of African descent in the Atlantic in the late 19th century. And we want to talk to them because history is a conversation that you hold with the past. And these kinds of people are historically marginalized. We hear about us, and in many cases, we don't need to hear about them less because they created things like newspapers and large data sets. That there just aren't. There isn't the funding, there isn't the history of interacting with them. Unfortunately, what that means is that if you talk to", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1057064_ms_-_1119659_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1057064, "end_ms": 1119659}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1108827 ms - 1179945 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1108827 ms - 1179945 ms\n\nContent: things like newspapers and large data sets. That there just aren't. There isn't the funding, there isn't the history of interacting with them. Unfortunately, what that means is that if you talk to ChatGPT or to a lot of other large LLM models, you don't get the perspectives and the stories of these folks. So we created History Chat and it's based on this or this instance of it is based on this newspaper, the African Times, which is an amazing newspaper with lots of history from the period, from the particular perspective of these English speaking, mainly West Africans, people of African descent. And we took it in, we can browse it, we can search it. And now using Storm, it's articles. We want to see what these articles contribute that is different. We're not looking for. Right, wrong. We're looking for how the perspective of the people who wrote the African Times might be different from the perspective of people who wrote this thing, which is the UNESCO General History of Africa,", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1108827_ms_-_1179945_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1108827, "end_ms": 1179945}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1168753 ms - 1238719 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1168753 ms - 1238719 ms\n\nContent: We're looking for how the perspective of the people who wrote the African Times might be different from the perspective of people who wrote this thing, which is the UNESCO General History of Africa, which is a set of books. It's an older technology, it's from before. In some ways people work corpora like the African Times. So the first thing we're looking for is people who can help us to analyze and interpret the differences and similarities, working with folks from African Studies and figure out why and how the African Times is giving new knowledge and different knowledge. And if we can do that, we can start how this kind of thing is replicable and how different types of information from the past, from marginalized communities, from newspapers and documents produced by children or by women or by people with disabilities in the 1960s and 1970s or whatever might become also a kind of source that we can communicate with to hear their perspectives and to learn them. So this is African", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1168753_ms_-_1238719_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1168753, "end_ms": 1238719}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1224803 ms - 1307405 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1224803 ms - 1307405 ms\n\nContent: or by people with disabilities in the 1960s and 1970s or whatever might become also a kind of source that we can communicate with to hear their perspectives and to learn them. So this is African History from below and I the first of, I think the two things that we're going to be presenting today. I don't know if anybody has questions. Hi Trevor, super interesting project. Could you repeat again? What are the goals of what are you seeking? I understood briefly that you want to analyze what are the difference between the African Times relative to older media or different direction. Thank you. Yeah, no, right. So we have some existing media and so we want to prove the concept that, that the articles that are produced through the, through this LLM augmented tool can actually add to knowledge, can actually change the way we think the past happened, can actually give us a different perspective on the past. So the way that we want to do this is by doing a comparison between the articles", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1224803_ms_-_1307405_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1224803, "end_ms": 1307405}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1296909 ms - 1396655 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1296909 ms - 1396655 ms\n\nContent: can actually change the way we think the past happened, can actually give us a different perspective on the past. So the way that we want to do this is by doing a comparison between the articles generated by the African Times on certain topics and articles that exist or chapters that exist in pre existing text source. And if we can do that. We can prove that this tool is adding to knowledge. Right. Giving us a new and richer and different perspective on the past. So it's that comparison that is right now the thing that we think we can achieve in a short period. And I know Sina can also add to this explanation probably. All right. It'S going to be talking together with Sino on the next project. Right. Okay, so. So let's get started. So this is Sina. Sina is a senior PhD student in my research group and in the very first class we talk about wikichat that is Sina's work and there are many other papers that you probably will be running across in the next couple of months. That's a good", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1296909_ms_-_1396655_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1296909, "end_ms": 1396655}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1375633 ms - 1479047 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1375633 ms - 1479047 ms\n\nContent: group and in the very first class we talk about wikichat that is Sina's work and there are many other papers that you probably will be running across in the next couple of months. That's a good background. I just want to bring, just connect what you heard with what we have been talking about in the first class. We talked about the Bloom's taxonomy where there are the basic understanding. And then if you look at the tip of the pyramid, it is about creation. And what we are doing with Trevor in this project is to understand how to create knowledge. And there are lots of interesting things we have already discovered such as how do you recognize that a story in the African times has significance, has historical significance. Okay. That kind of information is not written down anywhere. And we have been picking Trevor's brain on what it means to have a significant, original, significant story. And it has given us a lot of interesting theories about how we can help people to be creative. So", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1375633_ms_-_1479047_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1375633, "end_ms": 1479047}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1461391 ms - 1556055 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1461391 ms - 1556055 ms\n\nContent: have been picking Trevor's brain on what it means to have a significant, original, significant story. And it has given us a lot of interesting theories about how we can help people to be creative. So one of the things that we learn is that we really have to be able to extract the tacit knowledge that experts have about these topics. Because at the top of the pyramid when we are doing curation, the work is based on a lot of experience and none of them, a lot of that information is not written down. So if you want to automate it, we really have to create a tool that we can work with the experts and collect kind of the expertise so that we can actually do things that are not are creative. Okay. So this has been a very exciting project so far and Sina is going to talk about the next project. And the African time was available in digital form so we had to run text extraction to extract it. But it turns out there are thousands of other corporal in the form of newspapers, handwritten reports", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1461391_ms_-_1556055_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1461391, "end_ms": 1556055}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1542579 ms - 1618527 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1542579 ms - 1618527 ms\n\nContent: the African time was available in digital form so we had to run text extraction to extract it. But it turns out there are thousands of other corporal in the form of newspapers, handwritten reports and you know, all these things would be potentially we will be able to work with them with AI systems and extract knowledge from them. The only limiting factor is that, you know, we don't have access to their text. So all these retrieval based models and an AI system won't be able to work with them that easily. So this one is a French corpus, is documents of basically travel logs of these steamships that would travel from country to country, from coast to coast, and they would write reports. This is some of the challenges here is that these are in tabular form and this is not a regular structure for a document. And sometimes you have these kind of odd, unusual structures that you have to be able to deal with them if you want to work with their text. So specifically for this project, we will", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1542579_ms_-_1618527_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1542579, "end_ms": 1618527}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1602743 ms - 1674015 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1602743 ms - 1674015 ms\n\nContent: for a document. And sometimes you have these kind of odd, unusual structures that you have to be able to deal with them if you want to work with their text. So specifically for this project, we will be working with this very large set of newspapers that the Library of Congress has. They have been scanned. And for example, this is one from the Titanic. I think it's right after it happened. So I think early, early 20th century. And this is one of the examples that they have. And you can see that the structure is kind of complex. You have a title, you have these small titles, and then you have columns and you have images. So this is kind of very challenging for existing systems to be able to work with extracted text from and so on. In fact, the Library of Congress also, you know, runs OCR and makes the text available. So this is an example of a text for this section. If you can read that is their OCR here says hundred of telephone rails and telegraphic inquiries something into the office", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1602743_ms_-_1674015_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1602743, "end_ms": 1674015}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1658679 ms - 1728181 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1658679 ms - 1728181 ms\n\nContent: makes the text available. So this is an example of a text for this section. If you can read that is their OCR here says hundred of telephone rails and telegraphic inquiries something into the office question mark way last night asking information as T. So it's kind of like, you know, some of the words are there, but a lot of it is also, you know, extracted incorrectly. So this is actually very useful. But if useful if you want to do keyword search, so if you search for Titanic, for example, it will show you this article, but then you have to read through it and hundreds of other articles about the Titanic to make your own conclusions. So what we're proposing to do in this project is to use all these new technology that, for example, 3D 4.0 has very good vision capabilities that we actually used to extract the text from the African Times newspaper. But you want to extend that to be able to handle all these complex structures of newspapers, be able to break them down into smaller", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1658679_ms_-_1728181_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1658679, "end_ms": 1728181}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1717253 ms - 1801503 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1717253 ms - 1801503 ms\n\nContent: used to extract the text from the African Times newspaper. But you want to extend that to be able to handle all these complex structures of newspapers, be able to break them down into smaller articles and detect where the images are, detect where the text is, and then be able to connect that to all these RAC systems like Storm that you have seen in your first assignment and so on. So basically you want to build a prototype, you want to evaluate it on. There are some benchmarks, and we also can look at them manually. And it will be very helpful. I think there is going to be thousands, if not more groups of scholars who are working with these corpora around the world, and they are just ready to work with these systems once they are available. Yeah, I'm happy to answer questions. Well, I just want to say that, I mean this is, you know, for people working with the past, whether it's economists or people working with disease epidemiology in the past, or historians as we think of them or", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1717253_ms_-_1801503_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1717253, "end_ms": 1801503}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1789141 ms - 1883885 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1789141 ms - 1883885 ms\n\nContent: want to say that, I mean this is, you know, for people working with the past, whether it's economists or people working with disease epidemiology in the past, or historians as we think of them or whatever, this is really exciting stuff and will really kind of transform how we can, how we can do this work. Hi everyone, I'm Shi Cheng. I'm presenting the first project Data talk on the proposal manual. I'm Cheryl Phillips, I teach data journalism and computational journalism here and co presenting. So this first slide is a story that we did with the New York Times Investigative Reporting Fellowship where we spent months trying to parse through contract documents and campaign finance contributions in the state of Hawaii. Basically a fifth of the campaign contributions were tied to 15 companies that got big contracts from the state of Hawaii and from some of the counties. And that is just like a bare minimum of what's actually happening. That was the best we could do because we were doing", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1789141_ms_-_1883885_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1789141, "end_ms": 1883885}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1870629 ms - 1932553 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1870629 ms - 1932553 ms\n\nContent: that got big contracts from the state of Hawaii and from some of the counties. And that is just like a bare minimum of what's actually happening. That was the best we could do because we were doing this like using computational methods, using, you know, Python and doing OCR and doing all kinds of things. But the records were really bad. Sometimes they were non existent and there are just a lot of challenges. And so it took literally, yeah, 100 plus hours. We had three different journalists working on it extensively from for months. So I think the main point here is that investigative journalism relies on data a lot of times Data is something that takes a lot of laborious time and some of the things you're doing in this class will help eliminate some of those time resources that constrain the ability to do really powerful and impactful journalism. Thanks, Cyril. So in this project we propose to develop a data talk conversational engine. The conversational engine should be able to take", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1870629_ms_-_1932553_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1870629, "end_ms": 1932553}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1920913 ms - 1980167 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1920913 ms - 1980167 ms\n\nContent: the ability to do really powerful and impactful journalism. Thanks, Cyril. So in this project we propose to develop a data talk conversational engine. The conversational engine should be able to take in both graph databases, relational database and free text and be able to expose interface for users to communicate with the agent. The core innovation part of it lies in a language we created that takes in both structured data and free text. That augments SQL, which we call SUQL stands for structured Non Structured query language. So with this language, what we could do is instead of having the journalists doing all these 100 plus hours of navigation, the agent will be able to answer questions pulling from both structured data and unstructured text. So we have actually built a prototype for this that everybody can access at DataTalk, Gini, Stanford EDU. Right now the agent uses election data published from FEC and Open Secrets and it gives you up to date information on questions you can", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1920913_ms_-_1980167_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1920913, "end_ms": 1980167}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 1967423 ms - 2022019 ms", "content": "Title: CS224V Lecture 4 > Transcript > 1967423 ms - 2022019 ms\n\nContent: this that everybody can access at DataTalk, Gini, Stanford EDU. Right now the agent uses election data published from FEC and Open Secrets and it gives you up to date information on questions you can ask about your senators or your House of Representatives. It does this by going in a Gentec approach. It explores the database as you can see on the top part that it has intermediate thoughts. It presents the SQL, it explains the SQL to you, it gives you the result with links to the FEC website. Then it also suggests some follow up queries. We also build a navigation engine. So if you click on this, you should be able to pop to the right hand side where people can share queries and publish share public dashboards. As I was just looking at the presentation, there has been much more shared queries that just came up and everybody can look through them, get insights. That's because my journalism class is using this and we're working with three or four different newsrooms to publish stories", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_1967423_ms_-_2022019_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 1967423, "end_ms": 2022019}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2012041 ms - 2069919 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2012041 ms - 2069919 ms\n\nContent: that just came up and everybody can look through them, get insights. That's because my journalism class is using this and we're working with three or four different newsrooms to publish stories right now, like in the next two weeks up to the, up to the election. So there's real possibilities in being able to use this kind of platform or tool to be able to level up small newsrooms, to be able to do important journalism. We think there's room to do more beyond FEC data, beyond just structured data, as Sicheng mentions. And so one of these, these are just a few of the examples of census data, open elections data, campaign finance data from the state of California and also Agenda Watch. I'm going to talk a little bit more about agendawatch. So Agenda Watch is a platform that we built. I run an organization here at Stanford called Big Local News. It's a data sharing platform and collaborative in support of local journalism. And we've also built this platform called Agenda Watch. Agenda", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2012041_ms_-_2069919_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2012041, "end_ms": 2069919}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2059383 ms - 2112903 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2059383 ms - 2112903 ms\n\nContent: an organization here at Stanford called Big Local News. It's a data sharing platform and collaborative in support of local journalism. And we've also built this platform called Agenda Watch. Agenda Watch scrapes local government agendas in minutes from across the country. Right now it's very much, I wouldn't even call it beta, it's alpha, but it's. We have a couple of hundred agencies in there right now. We have built the scraping infrastructure. We think we're going to be moving up to about 3,000 agencies very soon within the next couple of months. And it allows you to keyword search so you can search across the agendas in minutes and look for patterns. Homelessness, pedestrians, bicycles, certain company names, you name it. There's a lot more you could do with it if you used an agent. So I'm gonna hand it back to you. Oh, I guess, yeah. This is an example of how you can do journalism out of it that's actually even more accountability focused, investigative focused. One of my classes", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2059383_ms_-_2112903_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2059383, "end_ms": 2112903}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2103399 ms - 2162383 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2103399 ms - 2162383 ms\n\nContent: I'm gonna hand it back to you. Oh, I guess, yeah. This is an example of how you can do journalism out of it that's actually even more accountability focused, investigative focused. One of my classes last year took the Agenda Watch corpus for Santa Clara county and we got the contracts for ice, that ICE has contracts with all kinds of local governments, even though there's a sanctuary law In California that prohibits them from data sharing. The Santa Clara county was continuing to ink new contracts, pay more money to ICE and sharing data with ICE at the same time that in their public meetings they were saying they were not doing that. So it was a really great way to link these two disparate sources of information to show kind of how they were, how the county was not doing what they were supposed to be doing. Great. So in this research, we'll expand on a prototype and tackle a lot of more problems. Some of the problems that we're encountering is how do we automatically evaluate. We", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2103399_ms_-_2162383_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2103399, "end_ms": 2162383}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2151631 ms - 2215555 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2151631 ms - 2215555 ms\n\nContent: supposed to be doing. Great. So in this research, we'll expand on a prototype and tackle a lot of more problems. Some of the problems that we're encountering is how do we automatically evaluate. We think simulations with users would be a great way to go. We also would like to building a fact that what Monica was talking about with tasks and knowledge. We'd like to be able to continuously improve the agents through more user testing and gather more knowledge about the data set. That was a goal at the end. We aim to enable generalists without AI knowledge that can bring up the Data Talk Navigator very quickly and be able to get insights from it. That's it. Thank you. I was wondering, I don't know if this is on or if it needs to be like for the ICE example, like, what information did you have to know about it before getting all that information about Santa Clara county and the ice? Like, did you have to know that there was maybe potentially something suspicious going on or did the", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2151631_ms_-_2215555_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2151631, "end_ms": 2215555}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2204931 ms - 2278521 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2204931 ms - 2278521 ms\n\nContent: have to know about it before getting all that information about Santa Clara county and the ice? Like, did you have to know that there was maybe potentially something suspicious going on or did the computer like come up with that for you? We just wanted to see where ICE contracts were in the state and see if there was potential for a story there. So it was a little bit of an exploratory mission. So we just grabbed all the ICE contracts and we thought, let's just see which county has the most ICE contracts. And it was surprising that San Diego was not surprising. It was number one, because they have facilities down there. But second was Santa Clara county, which was a little surprising. It had the most contracts of any other county and they were almost all data sharing. So. Thank you. So for next two projects. So just like one question. Have everyone finished their homework? One right now. Howlstorm and Cold Storm. Anyone want to share some insights or experience using it? Perhaps some", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2204931_ms_-_2278521_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2204931, "end_ms": 2278521}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2263977 ms - 2338487 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2263977 ms - 2338487 ms\n\nContent: next two projects. So just like one question. Have everyone finished their homework? One right now. Howlstorm and Cold Storm. Anyone want to share some insights or experience using it? Perhaps some bugs you find sound like. Yeah. One thought I had that I thought could be useful now is because it's so good at pulling information from different articles. Using it to get candidate profiles for. Elections really quickly could be super useful, especially with the upcoming presidential election. Good idea. Good idea. Anyone else? No? Okay. Yeah. I tried to prompt it to use certain domains for its sources and this. Is, I believe, a future complexity model. But it's like If I want to. Have good reviews for a restaurant, I don't want to look at news articles, I want to look at other people. So I want to prompt people on social media. I think that would be really useful. The system will have the awareness of users preferences over the sources where the user can explicitly specify the sources.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2263977_ms_-_2338487_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2263977, "end_ms": 2338487}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2324039 ms - 2397393 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2324039 ms - 2397393 ms\n\nContent: want to prompt people on social media. I think that would be really useful. The system will have the awareness of users preferences over the sources where the user can explicitly specify the sources. Yeah, and perplexity has it. So if you want to search only. Academic articles and it only searches the domains that are scholarly articles, search social media. You can search a lot of different types of websites. That's cool. So right now just before the lecture we see that There are over 160 topics collaboratively contributed by all of you across more than 20 domains. Some of them are present here. We see a lot of good feedback. Feedbacks and thank you for all of your feedbacks and contributions. So like, so if you wonder like what Storm and Cold Storm does like if you just want a comprehensive service from scratch that you want to spend minimal effort and want to know everything about like something about everything, Storm can like Storm can do it right. And if you want to do an open", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2324039_ms_-_2397393_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2324039, "end_ms": 2397393}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2382939 ms - 2466569 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2382939 ms - 2466569 ms\n\nContent: service from scratch that you want to spend minimal effort and want to know everything about like something about everything, Storm can like Storm can do it right. And if you want to do an open ended exploration assisted by AI and you can like drive the conversation directions with the AIs then custom can do it. And I would just want to show a Twitter I spot this morning Andrew Kapathy post a Twitter post saying that the input optional product is something that's really interesting to people because coming up with a good input is really hard and become a barrier to many AI products. So allowing the users to see the system autonomously driven and occasionally played by the users to steer the directions of the AI system will be a good directions for all the AI products in the future. Resonates with this philosophy to some extent. But like Cold Storm and Storm doesn't cover everything we want in the knowledge curation or satisfy all the information needs. So what if we want to write", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2382939_ms_-_2466569_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2382939, "end_ms": 2466569}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2452247 ms - 2532441 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2452247 ms - 2532441 ms\n\nContent: with this philosophy to some extent. But like Cold Storm and Storm doesn't cover everything we want in the knowledge curation or satisfy all the information needs. So what if we want to write something about like plan a trip. Plan a 7 day, 5 day trip to some places. And what about like write literature survey or related works of the academic work you are working on. And what about doing investigative journalism questions just like George presented in the previous slides. All of them contain some shared perspectives which is like they're all goal oriented like information search which is not like quite open ended. You have a goal in mind and the output is usually structured in a certain way. So for example if we are planning a five day trip in for example in Arizona, if you just use Storm or Cold Storm it will show something like this. It will survey national parks, we'll survey potential activities and some trips like tips about the Travel and something else like just everything. But", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2452247_ms_-_2532441_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2452247, "end_ms": 2532441}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2517817 ms - 2586019 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2517817 ms - 2586019 ms\n\nContent: or Cold Storm it will show something like this. It will survey national parks, we'll survey potential activities and some trips like tips about the Travel and something else like just everything. But if you just instructed to, could you summarize all the points here and give me a five day trip itinerary? It doesn't do it very well to some extent, especially if you want to be in the loop saying that I want to modify the second day to be more like indoor activities. And then Storm can hopefully it can modify the itinerary to that part only while keeping all other parts intact. This structure, awareness, planning awareness, language model system is not like really there in Storm and Cold Storm right now. So can we design a system to achieve this and could we possibly include the human in the loop? And that is the project one which is planning with the AI. We are doing like human AI collaborative writing on the how to articles. So we're going to in this project we're going to survey like", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2517817_ms_-_2586019_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2517817, "end_ms": 2586019}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2573899 ms - 2643459 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2573899 ms - 2643459 ms\n\nContent: loop? And that is the project one which is planning with the AI. We are doing like human AI collaborative writing on the how to articles. So we're going to in this project we're going to survey like existing methods like the language model, chatbots, Storm, Cold Storm, how do they perform on this type of task. And we're going to conduct proof of concept design and potentially conduct the automatic and human evaluation on such system. Any questions so far on this one? Yeah. Understand it would essentially be trying to. Align an LLM chatbot to output a. Certain type of text or text style. It's not exactly the text style you can imagine. Like after some search, if you just prompt the language model given the Storm article just instructed to give me itinerary. Right. It probably do a decent job. But what if you want to interact with the system more? Talking about like I don't like your planning on this day, could you modify that and I want to include more like indoor activities on day", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2573899_ms_-_2643459_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2573899, "end_ms": 2643459}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2632179 ms - 2698871 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2632179 ms - 2698871 ms\n\nContent: job. But what if you want to interact with the system more? Talking about like I don't like your planning on this day, could you modify that and I want to include more like indoor activities on day three, could you go out, search and then incorporate your result in day three? Like this kind of, you have a clear outline structure of the output and you want to modify some of it. And even if you want to collaborate the language model to modify the overall structure, current language system doesn't really do a good job at this one. Any more questions on this one? All right, so the next one is an example investigative journalism writing. So for example, if you're going to write a topic about like PAC contributions to candidates for the US elections this year, this is kind of the outline you will expect from the Storm system. Basically it surveys like many like background information and pull out information sources from the general Internet. But if you take a close look at this section,", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2632179_ms_-_2698871_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2632179, "end_ms": 2698871}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2685551 ms - 2756597 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2685551 ms - 2756597 ms\n\nContent: will expect from the Storm system. Basically it surveys like many like background information and pull out information sources from the general Internet. But if you take a close look at this section, like Talking about the 2024 election here, some of the sources are even pulled from 2023 or 2022. And some of the sections you see here is very general, at least some general categories, but not all of them. So if you want to write a very comprehensive in depth analysis of a specific event or a specific angle about this report, the STORM doesn't really go in depth but go in breadth about everything. So you may wonder, this is just a few of them. Could we have all of them? Could we include accurate data from untrusted sources into the writing? If you even take a closer look at what the STORM system does in the background when you click on the see brainstorming process button in the website, you can see the perspective guided QAs and you see their questions. They're pretty like in general", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2685551_ms_-_2756597_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2685551, "end_ms": 2756597}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2744149 ms - 2815005 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2744149 ms - 2815005 ms\n\nContent: system does in the background when you click on the see brainstorming process button in the website, you can see the perspective guided QAs and you see their questions. They're pretty like in general it doesn't like ask in depth questions about specific data. Could you give me a list of all possible available like categories? It doesn't ask questions like that. So we're thinking can we design a language model system that asks this type of question that ask a very specific question that requires like output of accurate data from the data sources or the databases just like what we presented in the previous one, the DataTalk project. It will convert this kind of search queries into program search queries here and then augment the generation with the accurate results from the database and write an in depth comprehensive report in combination with the STORM system. So the second project we are proposing here is having a deep dive conduct in depth analysis with the knowledge curation", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2744149_ms_-_2815005_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2744149, "end_ms": 2815005}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2798993 ms - 2897717 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2798993 ms - 2897717 ms\n\nContent: write an in depth comprehensive report in combination with the STORM system. So the second project we are proposing here is having a deep dive conduct in depth analysis with the knowledge curation systems. We're going to figure out can we design a database retrieval module and integrate that with the STORM system and can they combine have a good performance on the use cases like the investigative journalism. And if necessary we have to design the in depth question asking modules to instruct the language model to ask in depth questions to these retriever modules and we conduct the evaluations. So any questions on this one? Okay, then I hand over to next one. Any of these projects interesting to you guys? Yes. No. Okay, I see some hands, so that's good. And if you guys are interested, you know who they are, you can contact them. Thank you everyone. It is a great honor to be able to present our pitch for the project here. I'm a software engineer at Sony Interactive Entertainment. As you", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2798993_ms_-_2897717_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2798993, "end_ms": 2897717}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2870869 ms - 2969341 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2870869 ms - 2969341 ms\n\nContent: who they are, you can contact them. Thank you everyone. It is a great honor to be able to present our pitch for the project here. I'm a software engineer at Sony Interactive Entertainment. As you know, PlayStation. And I'm coming with my coworker Michael Stapler here and he is the academic manager here who is in charge of this academic relationship collaboration. Let's just dive in, let's think about this. So have you. Yeah, thank you. Let's take a. Take a minute to think. Have you ever experienced or have you ever heard of anyone around you experience that? The frustration when they try to push through a game but they spend hours playing like trying to beat certain boss or trying to solve certain puzzles. And that is the starting point of this project. So our proposal is to try to create a game tutor bot that could potentially help the user to go through these struggles. And it may sound a little bit vague to you that what does it mean, a game tutor, Right? Let me break it down step", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2870869_ms_-_2969341_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2870869, "end_ms": 2969341}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 2951849 ms - 3027555 ms", "content": "Title: CS224V Lecture 4 > Transcript > 2951849 ms - 3027555 ms\n\nContent: a game tutor bot that could potentially help the user to go through these struggles. And it may sound a little bit vague to you that what does it mean, a game tutor, Right? Let me break it down step by step so that it may be a little clearer for you how to achieve this goal. Let's think from basic. What does a game tutor needs to do if it's going to help a user to play a game? Fundamentally, the game tutor needs to know how to play the game. That's the basic function. That's step one. The game tutorial will be able to supposedly will be able to be dropped into any random gameplay scenario and be able to play the game episode by itself. That's the first step, the first level of feature that we will be looking for and then comes to a second level of the feature. So let's imagine you play the game and I play a game. We might play the same game but in a different game playing style. For example, some people, they prefer to collect all the collectibles or try to just explore the", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_2951849_ms_-_3027555_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 2951849, "end_ms": 3027555}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3014473 ms - 3091625 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3014473 ms - 3091625 ms\n\nContent: play the game and I play a game. We might play the same game but in a different game playing style. For example, some people, they prefer to collect all the collectibles or try to just explore the environment in the game. But some people, they prefer to just go through a checkpoint and then reach the goal as soon as possible. Different players, they have different gameplay styles. That will be the second level of the feature that we are looking for which is to have this game tutor that are able to understand different gameplay styles according to the for example player history or just player inputs. And they can cater to the need of the player regarding that specific gameplay style. There can be more features adding on that. But the third level feature that we will be happy to have is to have the game tutor to be able to deliver a instruction that takes cater to the user's specific requirements of the granularity. For example, sometimes I'm in a more adventurous mood. I would prefer", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3014473_ms_-_3091625_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3014473, "end_ms": 3091625}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3071099 ms - 3153035 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3071099 ms - 3153035 ms\n\nContent: have the game tutor to be able to deliver a instruction that takes cater to the user's specific requirements of the granularity. For example, sometimes I'm in a more adventurous mood. I would prefer that the game tutor to give me instructions that's more high level and just tell me strategic wise what I can do to go through this struggle instead of wasting hours trying to solve it by myself. But other times I would prefer the game tutor to just tell me down to very specific which button I should press to just get through this hard. For example boss fight, those can be changed just based on this even if it's the same player. Those will be the third level of feature that would be good to have in a game tutor that we would love to the game. Sorry, we would love the game tutor to be able to cater to the player's requirements specifically. Those are the three layers of features that will be good to have for this project. It might sound a lot for you and it is a lot. Even each single", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3071099_ms_-_3153035_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3071099, "end_ms": 3153035}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3137215 ms - 3206793 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3137215 ms - 3206793 ms\n\nContent: to cater to the player's requirements specifically. Those are the three layers of features that will be good to have for this project. It might sound a lot for you and it is a lot. Even each single individual feature itself is a hard research problem. So please don't push yourself to try to accomplish all, but try to focus on any one of those. And just for the presentations or the projects earlier that's been presented here, some of those could be potentially helpful for accomplish this goal. After talking about the features then I would like to just briefly talk about the simulation environment and the data that we could use. So unfortunately for this class, PlayStation won't be able to provide the real game or the player data due to the privacy concern. So we encourage you to look for any open source environment, game simulation environment or any open source game data that you could find. And we don't want to limit your creativity on this. And we also provide some of the reference", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3137215_ms_-_3206793_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3137215, "end_ms": 3206793}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3194937 ms - 3267087 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3194937 ms - 3267087 ms\n\nContent: open source environment, game simulation environment or any open source game data that you could find. And we don't want to limit your creativity on this. And we also provide some of the reference of the environment on the game project document. So hopefully Monica can share that with you and feel free to refer to those if you would prefer. Last but not least, we look forward to you joining us and if there are good results coming out of this collaboration, we will be happily continue for the future. And also hopefully we'll be able to publish some research paper together. Thank you. Yes. Yeah, I think this is a very interesting feature. According to your description of the feature. Does it require some level of interoperability between the like games and their account systems? Or you think this feature is based. On one game interoperability? Interoperability of the. You know, the. You know, because you mentioned like there are this. It should be able to understand different games and", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3194937_ms_-_3267087_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3194937, "end_ms": 3267087}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3255775 ms - 3344937 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3255775 ms - 3344937 ms\n\nContent: this feature is based. On one game interoperability? Interoperability of the. You know, the. You know, because you mentioned like there are this. It should be able to understand different games and the gaming style history. Yeah, sorry, I forgot to mention that. And very good question. That is one key point here. We all talk about large multimodal models and we don't talk about it because it's popular, we talk about it because it's useful. We encourage you to leverage the capability of this multimodal models like LLM or LVM or whatever else that you could find to try to build this interpretability between the user and the agent. Yeah, very good point. Thank you. Any other. Okay. I was just going to ask, is there a specific type of game, different categories, RGP or FPS or just any type of game? Just any type of game, yeah. Okay, cool. Thank you. Yeah, either a single player or multiplayer. But multiplayer. There. There will be a next level of complicacy. Yeah. So it's up to. I just", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3255775_ms_-_3344937_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3255775, "end_ms": 3344937}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3314701 ms - 3468941 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3314701 ms - 3468941 ms\n\nContent: of game? Just any type of game, yeah. Okay, cool. Thank you. Yeah, either a single player or multiplayer. But multiplayer. There. There will be a next level of complicacy. Yeah. So it's up to. I just wanted to ask what are the current efforts at Sony in implementing this and has it been implemented in certain games or in tutorials or anything? Currently the PlayStation has some game tutorial, but those are pre recorded videos by other players or by community shared videos. Those could be your source if you're able to gather your own data. But unfortunately we won't be able to provide those data. Yeah, so thank you. Any other questions? Yeah. Thank you so much. Thank you. Which one do you want to go first? An example of this is how let's say a knowledge corpus like Wikipedia is created, or if you look at the entire Internet as a whole, how it's created, or let's say even a software documentation. So there are a lot of different people over a long period of time. In the case of", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3314701_ms_-_3468941_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3314701, "end_ms": 3468941}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3456805 ms - 3517563 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3456805 ms - 3517563 ms\n\nContent: or if you look at the entire Internet as a whole, how it's created, or let's say even a software documentation. So there are a lot of different people over a long period of time. In the case of Wikipedia, maybe 20 years, more than 20 years, and each person only looks as a small part and gradually they add on top of each other, create this corpus of knowledge that search engines and large language models then use. But you know, if you want to find inconsistencies, especially at this scale, you will run into some interesting cases which you want to tackle in this project. So let's say there's a simple statement, factual statement. Professor Monica Lam is a professor at Stanford University. And I want to see if there is anything else in the knowledge corpus that can contradict this. I could search for this, for example, but what I would find is that, you know, the statement turns out to be look inconsistent because you know, there's another person with the same name as the professor who", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3456805_ms_-_3517563_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3456805, "end_ms": 3517563}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3506995 ms - 3570291 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3506995 ms - 3570291 ms\n\nContent: search for this, for example, but what I would find is that, you know, the statement turns out to be look inconsistent because you know, there's another person with the same name as the professor who is, you know, not the person you're looking for. So basically it is at the service level it looks inconsistent because you have different people. But in fact, if both of these people have Wikipedia pages, which they do, there is no problem there. Another example is within Wikipedia, for example, you can ask under specified questions like when was the first high water book published? You have to specify which country. And then even if you do that in the case of Iran, you have to specify which calendar system you're using. So that's not a typo, it's just a different calendar system. So if you have an automated system just reading Wikipedia, trying to find, you know, inconsistencies, this would look like a problem, that would be a false positive. So you know, as I mentioned, we are working", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3506995_ms_-_3570291_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3506995, "end_ms": 3570291}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3557141 ms - 3615637 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3557141 ms - 3615637 ms\n\nContent: have an automated system just reading Wikipedia, trying to find, you know, inconsistencies, this would look like a problem, that would be a false positive. So you know, as I mentioned, we are working with Wikipedia as a knowledge corpus and by the end of the quarter we ideally want to be able to just run the system, build the system first, but then run this on the entire Wikipedia, on, you know, more than 600 million, more than 6 million articles, and you know, find these pieces of information that, okay, this piece of information from this article actually contradicts this other one. Maybe one of them is outdated, maybe one of them is actually incorrect, or maybe they are using different measures of the same thing and so on and so forth and just have a list of this to be able to add that to the public for the public use. And you know, there are cases where you can fix them. There are cases where you have to refer to a human or a Wikipedia volunteer to fix these things. And that would", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3557141_ms_-_3615637_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3557141, "end_ms": 3615637}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3602223 ms - 3676815 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3602223 ms - 3676815 ms\n\nContent: to the public for the public use. And you know, there are cases where you can fix them. There are cases where you have to refer to a human or a Wikipedia volunteer to fix these things. And that would be very helpful. So obviously if you have a prototype, it's uses GPT4 to break down documents into claims and compare them across different documents. But there are some interesting challenges when you want to scale that and when you want to add all these nuances to it, which we will be working on this quarter. So any question for this project so far? Okay, and feel free to contact me right away about any of these projects if you're interested. So another project that I'm proposing is, you know, trying to build a systems that can analyze or answer very deep questions about what is being reported in the news, which is also in its own term a reflection of what is happening in the, in the real world. So, you know, there are many use cases, in fact, you know, interesting use cases for", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3602223_ms_-_3676815_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3602223, "end_ms": 3676815}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3664873 ms - 3730985 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3664873 ms - 3730985 ms\n\nContent: reported in the news, which is also in its own term a reflection of what is happening in the, in the real world. So, you know, there are many use cases, in fact, you know, interesting use cases for organizations that want to collect data on, let's say humanitarian crises that are going on around the world. They want to prepare, they want to inform governments how to prepare, allocate resources or journalists who want to report on, let's say the war, the war around the world, the number of casualties and you know, what can be done to help people with those situations. So one of the, one of these organizations is called ACL ad. We actually just published a paper with them this morning. And it is basically the paper goes over how can we build automated system using LLMs that take the code book, take the definitions of what is of interest to them. For example, let's say they want to, they want to look at, you know, have a map of all the striking locations in Ukraine during the last year.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3664873_ms_-_3730985_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3664873, "end_ms": 3730985}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3717739 ms - 3789177 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3717739 ms - 3789177 ms\n\nContent: take the definitions of what is of interest to them. For example, let's say they want to, they want to look at, you know, have a map of all the striking locations in Ukraine during the last year. So, you know, the way we can do this is we build the system and then we run it on, you know, every available news article that we can find, especially in other languages, especially in Ukrainian and Russian in this case and probably English. And you know, you find all these locations from, from these systems and then you can have these high level analysis on, you know, what is going on and how it can be reported and how, how the situation can be improved potentially using this data in some way. So it is actually a difficult difficult task. So ACLED, for example, has, you know, 200 people around the world looking at news reports in different languages with a high accuracy to try to be able to put these data reports together so that, you know, other people can use in a reliable way. But there", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3717739_ms_-_3789177_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3717739, "end_ms": 3789177}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3771639 ms - 3851905 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3771639 ms - 3851905 ms\n\nContent: the world looking at news reports in different languages with a high accuracy to try to be able to put these data reports together so that, you know, other people can use in a reliable way. But there are a few things that we think we can do to help further than just automating or trying to automate and trying to help with that. One is what if we want to let users ask any question not just about conflicts, not just about political conflicts, but let's say a question is, how are news media reporting in Canada about American sports teams? That's just a very random question. Or you can ask about your news reporting bias, you can ask about the topics they cover or anything like that. But all these questions could be answered using the system that looks at individual articles, millions of them in fact, and then tries to extract something from each of them and put them together. So for this project, we will have the code for this paper ready. It basically has a base system that works with", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3771639_ms_-_3851905_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3771639, "end_ms": 3851905}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3839293 ms - 3982887 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3839293 ms - 3982887 ms\n\nContent: in fact, and then tries to extract something from each of them and put them together. So for this project, we will have the code for this paper ready. It basically has a base system that works with conflict data if we want to expand it. And also we have this cleans corpus of 600 million user articles from the web covers the last eight years in 100 languages. So, you know, it's a good, it's a good test bed for, you know, all these language models, their multilingual capabilities, and how we can extract information at scale and make them useful for people. So any questions for this one? People can hear me now. That could basically answer questions grounded in data, which is something that this class highlights a lot. The goal of this specific project now is to build a similar system for KQED's audio archives. We want to make this interactive and we want to leverage new techniques which we've seen this class like SU SQL Storm Hybrid search and Rags and other things, and just build on top", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3839293_ms_-_3982887_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3839293, "end_ms": 3982887}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 3966943 ms - 4031063 ms", "content": "Title: CS224V Lecture 4 > Transcript > 3966943 ms - 4031063 ms\n\nContent: audio archives. We want to make this interactive and we want to leverage new techniques which we've seen this class like SU SQL Storm Hybrid search and Rags and other things, and just build on top of this existing audio transcription pipeline line. And just a little bit of background on KQED. It's the largest public radio station in the U.S. they have 45 weekly programs and podcasts and they have a bunch of diverse content. And you would have access to all this data, which is really cool because you can sort of directly apply this data into your project. And just to highlight some of the key project components, you know, you're working with ll embeddings on audio and trying to categorize content and diarization. So, you know, when there's a specific name that might be mentioned in a news article, I mean, sorry, in sort of like a radio station, you know, you want to be able to segment that name and correctly output it. And on top of that, you know, enhancing the retrieval system. So", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_3966943_ms_-_4031063_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 3966943, "end_ms": 4031063}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4018495 ms - 4081419 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4018495 ms - 4081419 ms\n\nContent: article, I mean, sorry, in sort of like a radio station, you know, you want to be able to segment that name and correctly output it. And on top of that, you know, enhancing the retrieval system. So building like hybrid vector search with dense and sparks vectors, building knowledge graphs, maybe even a re ranking system. And then another goal would be to make sort of each specific thing feel curated to that specific radio station. And we have some examples of this which I won't get into right now. And just some challenges and considerations to highlight are handling diverse content types, ensuring accuracy, especially in temporal cases. So if I ask a question that might have an outdated answer, you don't want to output that. So you have to really have like a lot of metadata to avoid those things. And also just gracefully acknowledging limitations of the system itself. You also get to work with domain experts. Steve Hen, who is at Stanford, big local news, he also teaches the", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4018495_ms_-_4081419_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4018495, "end_ms": 4081419}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4068795 ms - 4135517 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4068795 ms - 4135517 ms\n\nContent: those things. And also just gracefully acknowledging limitations of the system itself. You also get to work with domain experts. Steve Hen, who is at Stanford, big local news, he also teaches the computational journalism class, he's at npr. And also Lowell Robinson, who's at kqed. So that's the first project. Any questions? Okay, the next one I'm going to go over is again, it's going to be an audio transcription pipeline, but this is specifically for sort of answering questions about lectures like this recorded one right here and being able to interactively, you know, deal with the transcription archives. And so just the overall project statement here is right now, lecture notes are one sided. You know, this is a recording right now, you can listen to it later, but you won't be able to really search over the content very well or get answers to the things that were talked about. There's in general just a lack of interaction between anybody who's going to watch this later. And there's a", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4068795_ms_-_4135517_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4068795, "end_ms": 4135517}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4125445 ms - 4188359 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4125445 ms - 4188359 ms\n\nContent: search over the content very well or get answers to the things that were talked about. There's in general just a lack of interaction between anybody who's going to watch this later. And there's a highlight on the importance of active engagement and learning. So being able to interactively communicate through the archives. So the project goal itself is just to create a conversational agent that takes video audio transcripts of lectures as input and being able to sort of create a question answer system grounded in the data itself. And really there's an emphasis on making sure that you bring up the accurate transcription content. And you also want to incorporate additional study materials that might be relevant to the course, which could be things like the textbook syllabus, notes, et cetera. And specifically we're going to prototype this conversational agent From Stanford Med 275 from diagnosis to Dialogue, which is taught by Professor Brian Lynn. And we're going to build it", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4125445_ms_-_4188359_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4125445, "end_ms": 4188359}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4174279 ms - 4253421 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4174279 ms - 4253421 ms\n\nContent: et cetera. And specifically we're going to prototype this conversational agent From Stanford Med 275 from diagnosis to Dialogue, which is taught by Professor Brian Lynn. And we're going to build it specifically for this podcast to start. But I think the goal should be to be able to do this for any sort of lecture that Stanford might have. And just a couple of things to highlight. You'll get to build on an existing structured audio pipeline and integrate lecture content, additional materials and again try to deal with domain specific errors. And this is my contact information. Thank you. Is there any questions on this one? Hopefully, hopefully, hopefully they will. Any other questions? Yeah. Have you experimented with using any synthetic data to help these agents out? Because I assume because text to speech is like. Okay. And I assume like getting all these. Lecture videos and the ground truth would be quite difficult, especially for mathematics where. You'Re not going to have law", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4174279_ms_-_4253421_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4174279, "end_ms": 4253421}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4242915 ms - 4320441 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4242915 ms - 4320441 ms\n\nContent: because text to speech is like. Okay. And I assume like getting all these. Lecture videos and the ground truth would be quite difficult, especially for mathematics where. You'Re not going to have law tech. For every math lecture, right? Yeah, I mean I think it's, it's kind of the verse like speech to text. Right? So you're going to transcribe the audio into text. And I think the initial goal is you really want to ground it in what it's said. Right. Because LLM has all sorts of information and the goal is to specifically sort of parse, you know, the chunks that you transcribe and bring the most relevant ones forward. But of course, like there's totally room for hallucination, right. I ask a follow up question and sort of like that isn't in the context of this class, the LLM still has knowledge that's external and you want to make sure as tightly as possible to sort of ground against that. Yeah, that would be the ideal case. All right, thank you. So, hey everyone, I'm pitching couple of", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4242915_ms_-_4320441_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4242915, "end_ms": 4320441}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4287367 ms - 4376415 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4287367 ms - 4376415 ms\n\nContent: that's external and you want to make sure as tightly as possible to sort of ground against that. Yeah, that would be the ideal case. All right, thank you. So, hey everyone, I'm pitching couple of projects which are related to the Genie worksheets work that we have been doing. So all of you are aware of OpenAI, they have their ChatGPT and everything. You would expect them to use AI like LLM based agents for, let's say helping you out with any issue. But if you go to their customer service, they still have a dialog tree based chatbot where you have to interact with the, you have to click on these different buttons and then they have these different dialog flows. And this kind of illustrates how challenging this problem is that for all these transactional dialogue agents, like how do you reliably create them, how do they interact with the users? So I want to highlight three challenges. So the first one is how do you create these effective agents but by providing developers ease of coding", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4287367_ms_-_4376415_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4287367, "end_ms": 4376415}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4360495 ms - 4429791 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4360495 ms - 4429791 ms\n\nContent: create them, how do they interact with the users? So I want to highlight three challenges. So the first one is how do you create these effective agents but by providing developers ease of coding these up. So people use LangChain guidance and autogen kind of libraries to create these chatbots, but it requires a lot of effort. You have to create ad hoc pipelines. So this is one of the key main challenges in creating these reliable agents. Then the second one is that the agent should support composition of query languages and task information. So if the user is providing you some information, they also might want to gather some information from different knowledge sources. So These agents should be able to respond effectively to these queries while also performing the task. And then the third problem is, as the conversations become longer and longer, keeping track of the important fact in the conversation becomes more difficult. So here's an example, let's say a student. So this is the", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4360495_ms_-_4429791_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4360495, "end_ms": 4429791}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4418927 ms - 4479295 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4418927 ms - 4479295 ms\n\nContent: problem is, as the conversations become longer and longer, keeping track of the important fact in the conversation becomes more difficult. So here's an example, let's say a student. So this is the scenario of course advisor where the student initially says that I want to take some AI course for later grade and for four units. And then the user interacts with the agent exploring different courses. Let's say they're like 15 different courses, talking about the ratings, the reviews and everything. And eventually when the user says that, oh, I'll take let's say math theory for ML course, the agent should know that the user wants to take this course for a letter grade and four credit of units. So this again like keeping track of such long conversations becomes problematic with if you're just purely using LLMs. So our genie worksheet, the developer has to provide APIs, some examples and there's a high level specification which you'll read about in the assignment that we'll release today.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4418927_ms_-_4479295_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4418927, "end_ms": 4479295}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4465971 ms - 4533075 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4465971 ms - 4533075 ms\n\nContent: purely using LLMs. So our genie worksheet, the developer has to provide APIs, some examples and there's a high level specification which you'll read about in the assignment that we'll release today. You just provide this to the agent. It has different modules with help with semantic parsing, they help with agent policy and then generating the response. And you get a conversational agent where the user can talk with the agent, they can gather information and complete the task. So we did some quick evaluation. This is on one of the static datasets. So traditionally all these static datasets for conversation task oriented agents, they were just like slot filling where you just have to do slot filling, you have a JSON file, you have fields and you just fill out specific values. So we show that in such academic data sets, GENIE outperforms the state of the art methods. But then we also conduct a real user study where we had 62 users and we had three different agents. One was for restaurant", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4465971_ms_-_4533075_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4465971, "end_ms": 4533075}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4517163 ms - 4586227 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4517163 ms - 4586227 ms\n\nContent: in such academic data sets, GENIE outperforms the state of the art methods. But then we also conduct a real user study where we had 62 users and we had three different agents. One was for restaurant reservation, one was for ticket submission, which is similar to ServiceNow kind of agent, which helps you file complaints. And then the course enrollment, but we gave the users to, we gave a set of user GPT4 with function calling, creating the whole pipeline and other set of users genie. And then we evaluated them across execution, the dialogue act accuracy and if the users were able to complete the goal or not. So the last row you can see that for ticket submission and course enrollment, our system like just outperforms GPT4 by a lot. But this is not all. We have multiple. So this is just the beginning of the project. There are like several other projects that we want to branch out from this. So the first one is trying out new Application domains with GENIE worksheets, one of the", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4517163_ms_-_4586227_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4517163, "end_ms": 4586227}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4575499 ms - 4636507 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4575499 ms - 4636507 ms\n\nContent: just the beginning of the project. There are like several other projects that we want to branch out from this. So the first one is trying out new Application domains with GENIE worksheets, one of the application domain could be FAFSA where undergrad students they have to like file for student aid. This process is very difficult. You have to gather a lot of information. So this could be like one of the application domain which you can use, you can bring your own application domain. The other one is like, I talked to a few people, they said that they also want a module in GENIE worksheet where the LLM can ask questions on its own. So when you want to dive deeper into some conversations, there should be some module which controls when to ask. From this, let's say the specification that we have versus the dlm. So this could be another interesting project that one can work on. Then there are a few other things such as how can we use worksheets for creating web agents? Because you have to", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4575499_ms_-_4636507_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4575499, "end_ms": 4636507}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4622235 ms - 4687275 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4622235 ms - 4687275 ms\n\nContent: versus the dlm. So this could be another interesting project that one can work on. Then there are a few other things such as how can we use worksheets for creating web agents? Because you have to store information, you have to store what actions you took previously. And can we use that and use the context handling mechanism that we have to create such web agents? And then there's another one which is kind of like more of improvement over GNIE worksheets which is using some sort of an agentic approach which performs semantic parsing and then also improves the response generation quality and handling errors. And finally there are some like, if you're interested, we can also work on how do you actually evaluate these systems. So for this study we had to conduct user study, but you can cannot always conduct user studies. So if there could be so we can work together to find out like what are the ways of automatically evaluating such systems. Thank you. If you have questions, let me know.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4622235_ms_-_4687275_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4622235, "end_ms": 4687275}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4674535 ms - 4757843 ms", "content": "Title: CS224V Lecture 4 > Transcript > 4674535 ms - 4757843 ms\n\nContent: always conduct user studies. So if there could be so we can work together to find out like what are the ways of automatically evaluating such systems. Thank you. If you have questions, let me know. Okay, there's one more slide. Please fill out your Together AI credits form by end of day today we'll send the accounts to together AI tomorrow and it's on Ed. Just fill it out and because tomorrow we'll send it to them and after that you won't like if you haven't filled it out, you won't get the credits. Thank you. We're gonna do today. So here's the plan for the next week. There's a lecture on Monday and then on Wednesday we're gonna have an open mic system. Okay, Maybe you're looking for project project partners. Maybe you have some ideas that you just want to pitch to your fellow classmates and get their feedback. And so let us know. We're gonna have a sign up sheet and we can kind of run this or this kind of thing around again, but this time with your, you, you guys get to talk. All", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4674535_ms_-_4757843_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4674535, "end_ms": 4757843}}
{"document_title": "CS224V Lecture 4", "section_title": "CS224V Lecture 4 > Transcript > 4745225 ms - 4830425 ms", "content": "and get their feedback. And so let us know. We're gonna have a sign up sheet and we can kind of run this or this kind of thing around again, but this time with your, you, you guys get to talk. All right? Okay, I'll see you next week on Monday. Thank you. Thank you to all. The speaker. It.", "block_metadata": {"id": "CS224V_Lecture_4_>_Transcript_>_4745225_ms_-_4830425_ms", "document_type": "transcript", "lecture_number": 4, "start_ms": 4745225, "end_ms": 4830425}}
