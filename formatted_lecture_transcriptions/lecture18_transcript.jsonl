{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language Models and Data Centricity", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Language Models and Data Centricity\n\nContent: I'm giving a guest lecture today on a variety of data centric language modeling topics. The reason why LMS do so well is because they're trained on incredibly large amounts of data. But studies show that language models struggle on niche entities. How can retrieval augmentation harm the language model's performance?", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Language_Models_and_Data_Centricity", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 26035, "end_ms": 872163}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > language modeling: the knowledge gap", "content": "Title: CS224V Lecture 18 > Chapter Summaries > language modeling: the knowledge gap\n\nContent: As entity popularity goes down, language models do worse. The Internet data that we train on for language modeling is quite a precious resource. How can we more data efficiently train models? That's a very fundamental question to continue progress in building future language models.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_language_modeling:_the_knowledge_gap", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 872179, "end_ms": 1083173}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Data Selection, Training, and Scalability", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Data Selection, Training, and Scalability\n\nContent: How do we select good data for pre training? In a way that will give us more capable, you know, better models. There's data, there's scaling, and then there's an attention to detail in the engineering.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Data_Selection,_Training,_and_Scalability", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 1083309, "end_ms": 1151015}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Machine Learning's Data Pipeline", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Machine Learning's Data Pipeline\n\nContent: Of the three pillars, data is the most unknown. There has been some work trying to open source, replicate data pipelines to train highly effective models. Are there simple, some principled approaches to classifying based on BI grabs like that would be cool?", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Machine_Learning's_Data_Pipeline", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 1151055, "end_ms": 1508577}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Why Algorithmic Data Selection is so Hard", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Why Algorithmic Data Selection is so Hard\n\nContent: The cost is to train a thousand different models. It's hard to get a lot of data. If you want to analyze a model, don't ever train your own model. Use publicly available models. To understand the relationship between training data and downstream performance.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Why_Algorithmic_Data_Selection_is_so_Hard", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 1508681, "end_ms": 1853325}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language Algorithms and the Web", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Language Algorithms and the Web\n\nContent: Lambada: As an algorithm, it's very simple. All I do is I measure log losses on websites. I look at the websites that have the highest correlation between log likelihoods and downstream benchmarks. This turns out to be kind of an effective data selection algorithm.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Language_Algorithms_and_the_Web", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 1854825, "end_ms": 2098215}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Pre-training and data centricity", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Pre-training and data centricity\n\nContent: Language effects are incredibly clear cut. English looks totally different from, you know, French or Spanish or any other language. We also see really clear differences in the difficulty of the web page, like the entropy of the website. Language and entropy are two really core ones.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Pre-training_and_data_centricity", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 2099835, "end_ms": 2174545}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Pre-Training and Validation in Machine Learning", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Pre-Training and Validation in Machine Learning\n\nContent: How do we know that the research conclusions we reach are valid? One thing I'm excited about trying recently is to think about pre registration. Data selection for pre training is very important. And then I want to talk about a very different thinking about the data regime in the future.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Pre-Training_and_Validation_in_Machine_Learning", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 2175565, "end_ms": 2440031}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Data efficiency and continued pre-training", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Data efficiency and continued pre-training\n\nContent: We know that language models struggle when there's not enough data. This is going to become a problem as we start to either exhaust data or think about more niche domains. Here's a new more data efficient algorithm that lets me put the knowledge in. these books into the parameters of the model.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Data_efficiency_and_continued_pre-training", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 2440223, "end_ms": 2893121}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language Model Training", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Language Model Training\n\nContent: One of the core challenges is how do we do evaluation. The idea here is to take reading comprehension dataset in particular. As I generate more and more synthetic tokens from rephrasing, I actually see pretty predictable linear gains.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Language_Model_Training", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 2893313, "end_ms": 3402275}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Neuroanatomy 6, Introduction to Entigraph", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Neuroanatomy 6, Introduction to Entigraph\n\nContent: Antigraph scales much more nicely and more efficiently than just naively rephrasing. We can pretty handily beat GPT4, sort of closed book in doing QA. And I think we can hopefully continue to scale with additional token inputs.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Neuroanatomy_6,_Introduction_to_Entigraph", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 3403535, "end_ms": 3595215}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > RAG vs LLAM: empirical evaluation", "content": "Title: CS224V Lecture 18 > Chapter Summaries > RAG vs LLAM: empirical evaluation\n\nContent: We built a RAG system that is incredibly accurate. Within the top eight articles, 99% of the time the right article is in there. Even against this strong baseline, if we apply continued pre training, we still see like 2 to 3% gains on this.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_RAG_vs_LLAM:_empirical_evaluation", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 3596115, "end_ms": 3692055}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Synthetic Data and its implications", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Synthetic Data and its implications\n\nContent: Tatsu: We need to think very carefully about what is synthetic data buying us, what is it not buying us. Synthetic data is describing the relation between two entities that may not necessarily be directly in the document. We can use the same playbook to improve knowledge of language models.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Synthetic_Data_and_its_implications", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 3693515, "end_ms": 3975365}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Pre-training and the data-", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Pre-training and the data-\n\nContent: The central role of data and how we can algorithmically intervene on data in many ways. The place at which I'm going to intervene is on this kind of pre training style training. That's the place where we can deploy the most compute and that might have the greatest leverage.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Pre-training_and_the_data-", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 3975865, "end_ms": 4054065}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > How do we train language models?", "content": "Title: CS224V Lecture 18 > Chapter Summaries > How do we train language models?\n\nContent: Psychology major: How to present knowledge to models in a way that's more easily learnable. Do data augmentation emulate how humans learn? How do we close the gap? I just don't know what the answer is.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_How_do_we_train_language_models?", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 4054105, "end_ms": 4173933}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language models: Creativity", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Language models: Creativity\n\nContent: The goal of frontier researchers is like, to create new knowledge. But the biggest contributor to that is actually the alignment process. Continued pre training does, broadly speaking, make language models lose their capabilities.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Language_models:_Creativity", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 4174029, "end_ms": 4342923}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Anatomy 2, Creativity", "content": "Title: CS224V Lecture 18 > Chapter Summaries > Anatomy 2, Creativity\n\nContent: If you change something about the prompt, you'll get a totally different response. But if you don't vary the prompt you'll often cap out. Creative is a double edged sword. You want creativity in that. But in a way alignment is really focusing towards the average case.", "block_metadata": {"id": "CS224V_Lecture_18_>_Chapter_Summaries_>_Anatomy_2,_Creativity", "document_type": "chapter summary", "lecture_number": 18, "start_ms": 4342979, "end_ms": 5238965}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 26035 ms - 498371 ms", "content": "Title: CS224V Lecture 18 > Transcript > 26035 ms - 498371 ms\n\nContent: It, it. All right, so I'll get started. I'm giving a guest lecture today on, well, a variety of kind of data centric language modeling topics that I hope you will find useful. I know you all have been learning about kind of grounding and hallucination and hopefully some of this will be of interest in thinking about those things. So kind of the starting point for this is, you know, thinking about where do language models sort of remarkable capabilities come from. Right. Um, I think many of you actually maybe started in the post LLM era in this field. So like, this is just all natural to you. It's like, of course language models can do all the things, but this is actually pretty surprising to many of us, you know, who started in the field way long before that. And so, you know, it's, it's kind of amazing that all of these, you know, different NLP tasks can just get shoved into a language model in sequence to sequence form and they can do well. I put T5 up here because it's one of the", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_26035_ms_-_498371_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 26035, "end_ms": 498371}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 486555 ms - 548795 ms", "content": "Title: CS224V Lecture 18 > Transcript > 486555 ms - 548795 ms\n\nContent: kind of amazing that all of these, you know, different NLP tasks can just get shoved into a language model in sequence to sequence form and they can do well. I put T5 up here because it's one of the earliest models that really did some of this. And then, you know, below that, if you look at kind of what does real world usage of a chatbot really look like, it's incredibly varied. The second plot Here is from LMSYS1M, which is a pretty cool resource. It's from the Berkeley folks that run Chatbot Arena. They went and then they analyzed like, what do people use chatbots for? And they found all sorts of different things like discussing software errors and solutions and creating business strategies. I don't know if I trust my alums with that. And some stuff that's a little bit less, um, safe for work. Um, maybe surprisingly there isn't like a, you know, homework help category, but you can kind of see all sorts of different things that people are asking for help with from a language model.", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_486555_ms_-_548795_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 486555, "end_ms": 548795}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 538023 ms - 592463 ms", "content": "Title: CS224V Lecture 18 > Transcript > 538023 ms - 592463 ms\n\nContent: work. Um, maybe surprisingly there isn't like a, you know, homework help category, but you can kind of see all sorts of different things that people are asking for help with from a language model. Um, and I think the reason why LMS do so well is because, you know, they're trained on incredibly large amounts of data, right? Pre training data is incredibly varied. It's a huge slice of the Internet and on the Internet people are doing lots of different things, right? So because of that, you know, we got great generalization, but at the same time we know these systems don't really do well everywhere, right? That's the whole point of trying to understand and improve LM chatbots. So, you know, there have been really nice studies. This one's by some folks at UW showing that language models really struggle on niche entities. What's kind of an interesting fact is, you know, on really, really popular entities like the ones to the right here, this is measured by I think like page accesses on", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_538023_ms_-_592463_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 538023, "end_ms": 592463}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 582567 ms - 631445 ms", "content": "Title: CS224V Lecture 18 > Transcript > 582567 ms - 631445 ms\n\nContent: struggle on niche entities. What's kind of an interesting fact is, you know, on really, really popular entities like the ones to the right here, this is measured by I think like page accesses on Wikipedia or something. Um, you know, these models are really good. In fact, they're better than even a retrieval augmented system, which is kind of surprising. So you might as well just ask the language model directly. Um, but as these entities get less and less popular, as you go to the left of this plot, you know, your unassisted language model just does worse and worse and retrieval augmentation starts to do better. Right. So there's a pretty clear correlation between the popularity of entities and how well these models do. And so that might make you think about, you know, the pre training data. What are people building the LLMs really interested in? So if you go to, I don't know, anthropic OpenAI or something and you talk to those folks and you're like, what are you really excited about?", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_582567_ms_-_631445_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 582567, "end_ms": 631445}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 622665 ms - 674745 ms", "content": "Title: CS224V Lecture 18 > Transcript > 622665 ms - 674745 ms\n\nContent: What are people building the LLMs really interested in? So if you go to, I don't know, anthropic OpenAI or something and you talk to those folks and you're like, what are you really excited about? They're going to be talking about like, oh, cutting edge knowledge. We want our language model to do like, I don't know, quantum physics. We want it to be able to like, you know, make the next generation of language models themselves. Yes. How can retrieval augmentation harm the language model's performance? Yeah. Like if you put irrelevant contexts in, in, you can certainly harm the performance of the model. You might also retrieve, you know, some misleading but correct looking things and put them into the context. It's not really always the case, especially for like really, really obvious facts, you know, that retrieval augmentation is a big win. Right. Like I don't need to retrieve augmentation to know that the earth is round. Um, similarly. Not similarly. Sorry for going back to kind of", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_622665_ms_-_674745_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 622665, "end_ms": 674745}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 663401 ms - 716815 ms", "content": "Title: CS224V Lecture 18 > Transcript > 663401 ms - 716815 ms\n\nContent: you know, that retrieval augmentation is a big win. Right. Like I don't need to retrieve augmentation to know that the earth is round. Um, similarly. Not similarly. Sorry for going back to kind of cutting edge knowledge, you know, so there's a lot of interest in trying to develop language models that can do really well on scientific knowledge. That's rare on the Internet. Right. Um, and finally, if you're thinking about bias and fairness. Right. We know that these models do not do well, you know, uniformly across the globe. Um, as people who, you know, speak and interact in English, we are very much privileged to be able to take advantage of an incredible amount of high resource, you know, English languages on the Internet, which makes these models really, really good at English. Right. But on the other hand, if you're operating in much lower resource languages, these models do much, much worse. Right. So why am I bringing up these three different examples? They really all come back", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_663401_ms_-_716815_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 663401, "end_ms": 716815}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 707935 ms - 756831 ms", "content": "Title: CS224V Lecture 18 > Transcript > 707935 ms - 756831 ms\n\nContent: But on the other hand, if you're operating in much lower resource languages, these models do much, much worse. Right. So why am I bringing up these three different examples? They really all come back to the data. Right. Like the pre training data at some sense drives base model capabilities. In term, base model capabilities, derive your instruction tune model capabilities. And if you're building a system on top of it, it's going to really affect those as well. Right. So it all kind of comes back down to so what's in the pre training set of your language model. So I think, you know, I don't know how many of you have have tried to build your own language model or have looked at a language model pre training dataset, but one exercise that I'm going to go through now is to, you know, say, let's think about the pre training data, like, what does that look like? Right? So I think if you have like a kind of naive view of what pre training data looks like, I think it looks a little bit like", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_707935_ms_-_756831_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 707935, "end_ms": 756831}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 748831 ms - 794983 ms", "content": "Title: CS224V Lecture 18 > Transcript > 748831 ms - 794983 ms\n\nContent: think about the pre training data, like, what does that look like? Right? So I think if you have like a kind of naive view of what pre training data looks like, I think it looks a little bit like this on the slide right now. So on the left, you know, this is pre training and pre training is nice and clean and it's like stack exchange, it's stuff that, you know, you can read, it's high quality stuff. On the right, you know, you are going to evaluate it. And this is a, you know, a very badly named eval called Human Eval that everyone uses for code evaluation. It's something like, oh, here's a function, here's an instruction in comments. Now synthesize this Python code on the bottom right. And in many ways there's a good alignment between the left and the right. On the left you've got this like QA format, you've got some code in the middle. And if I transform this a little bit, you can imagine that it's kind of similar to the evaluation, right? There's good overlap between the two. And", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_748831_ms_-_794983_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 748831, "end_ms": 794983}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 786331 ms - 846371 ms", "content": "Title: CS224V Lecture 18 > Transcript > 786331 ms - 846371 ms\n\nContent: like QA format, you've got some code in the middle. And if I transform this a little bit, you can imagine that it's kind of similar to the evaluation, right? There's good overlap between the two. And so I think this leads lots of people to say like, oh yeah, of course language models work. You know, pre training data covers everything and is very similar to evaluation data. Um, but really pre training data, the vast majority of it, if you take a random web scrape, looks nothing like your evaluation data. Um, as part of a course that I was teaching last spring, we went through an exercise of just picking a random page from common crawl. This is the first page in one particular common crawl dump called 00084 smartcode.com and what is it? As far as I can tell, it's a blog spam. It's just talking about, it has random outbound links, it's talking about test prep training. I think it has no real knowledge, content value in this thing. Um, and so the vast majority of the web, if you do not", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_786331_ms_-_846371_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 786331, "end_ms": 846371}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 833513 ms - 888771 ms", "content": "Title: CS224V Lecture 18 > Transcript > 833513 ms - 888771 ms\n\nContent: about, it has random outbound links, it's talking about test prep training. I think it has no real knowledge, content value in this thing. Um, and so the vast majority of the web, if you do not filter it in an intelligent way, looks like this. And if you train your models on this, I mean, you know, you can easily imagine the model is going to behave very badly. But also you've Wasted. Very, very precious resource, which is your pre training compute. Right. And so for all of these reasons, there is a lot of interest in thinking about how do we select and curate the data on which our models are trained. Right. Because that's the foundation of all of the knowledge and all the things that these models have. Right. So I'll talk about that first. The other thing I want to talk about is kind of this knowledge gap for, you know, lower resource domains. So we know that as entity popularity goes down, language models do worse. Right. This is just kind of, you know, fact that's been observed", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_833513_ms_-_888771_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 833513, "end_ms": 888771}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 877539 ms - 933655 ms", "content": "Title: CS224V Lecture 18 > Transcript > 877539 ms - 933655 ms\n\nContent: of this knowledge gap for, you know, lower resource domains. So we know that as entity popularity goes down, language models do worse. Right. This is just kind of, you know, fact that's been observed over and over. And so, you know, can we somehow overcome this gap? You know, I have some knowledge out in this tail that I really want the language model to understand that like a deep, you know, fundamental level. Is there a way to do that? And so, you know, there's some questions about whether that's possible. And I think in a way this is a very interesting question. I'm going to approach it much more from the basic science, like is it possible to do X or Y kind of perspective. But I think it's related to very important, broader concerns in language modeling. On the left, you know, if you're much more of like a I want to build applications, I care about applications in the outside world, then this left figure may be of interest to you. You know, it's some folks who are, you know,", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_877539_ms_-_933655_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 877539, "end_ms": 933655}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 922437 ms - 981155 ms", "content": "Title: CS224V Lecture 18 > Transcript > 922437 ms - 981155 ms\n\nContent: you're much more of like a I want to build applications, I care about applications in the outside world, then this left figure may be of interest to you. You know, it's some folks who are, you know, analyzing the progress of AI, sort of, you know, arguing that really public data, you know, there isn't that much of it, it's going to get rapidly used to train language models. Really, you know, the next stage of value really comes from, you know, proprietary data like, you know, chat logs inside a company, internal wikis, like all those things are very important and valuable in building the next generation of AI systems. On the right here, this is a really kind of fun article, I'll call it, from Epoch AI where they were asking the question like in what year will we run out of LLM pre training data given the rate of current progress? Right. And that's kind of almost a ridiculous thing to ask about because there's a lot of data on the Internet, but as I said before, there isn't that much", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_922437_ms_-_981155_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 922437, "end_ms": 981155}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 971149 ms - 1019641 ms", "content": "Title: CS224V Lecture 18 > Transcript > 971149 ms - 1019641 ms\n\nContent: data given the rate of current progress? Right. And that's kind of almost a ridiculous thing to ask about because there's a lot of data on the Internet, but as I said before, there isn't that much high quality data on the Internet. Right. And so the Internet data that we train on for language modeling is actually quite a precious resource. And if you think about it, the compute being used to train these models, you know, is scaling up very rapidly, both from, you know, investment like people are buying their clusters, but also Nvidia is just making faster and faster GPUs right. And so the blue line is kind of a projection of how many tokens, you know, different model runs use. And you know, their argument is like roughly somewhere around 20, 30, you know, we're just going to like exhaust all the tokens on the available Internet. Right. And so if that's kind of the future, then you really start to care about, okay, how can we more data efficiently train models? That's a very", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_971149_ms_-_1019641_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 971149, "end_ms": 1019641}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1010641 ms - 1068061 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1010641 ms - 1068061 ms\n\nContent: exhaust all the tokens on the available Internet. Right. And so if that's kind of the future, then you really start to care about, okay, how can we more data efficiently train models? That's a very fundamental question to continue progress in building future language models. So the approach in this talk is going to be just like a very simple, very direct way of thinking about these problems. Right. I'm not going to give you like a deep algorithmic innovation. I'm going to play it safe. And what I'm going to do is I'm going to, you know, use the current existing, very effective paradigm. And what is that? That's just pre training big language models at scale. We know that works. I'm not going to change that. Right. But I'm going to try to combine that with some very simple ideas to try to break or to improve upon those two things that I talked about before. The first one being how do we select good data? The second one being can we train models more data efficiently when we don't have", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1010641_ms_-_1068061_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1010641, "end_ms": 1068061}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1057083 ms - 1115261 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1057083 ms - 1115261 ms\n\nContent: break or to improve upon those two things that I talked about before. The first one being how do we select good data? The second one being can we train models more data efficiently when we don't have that many documents? So this is based on some, two recent works that I'm excited about. Um, you know, the, the details of the work may be less important, um, but they have some fun ideas that hopefully you'll take away and kind of enjoy thinking about. Right. So the part, first part of this is gonna be, uh, talking about data selection. How do we select good data for pre training? Um, in a way that will give us more capable, you know, better models. So let's get started with thinking about data selection. Like, why does data selection matter? Um, I can make my own arguments, but I think it would be maybe more credible to let you know, a Frontier Lab training big language models do the talking for me. Right? So you can go download and look at the Llama 3 tech report and you know, they're", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1057083_ms_-_1115261_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1057083, "end_ms": 1115261}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1104169 ms - 1158407 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1104169 ms - 1158407 ms\n\nContent: be maybe more credible to let you know, a Frontier Lab training big language models do the talking for me. Right? So you can go download and look at the Llama 3 tech report and you know, they're pretty upfront about what's important or what they thought was important in training. You know, the biggest, most capable open source models at the time. Right. And they say, okay, there's three things that are important. There's data, there's scaling, and then there's an attention to detail in the engineering. Right. It's not necessarily like deep algorithmic innovation, which saddens me as a researcher. But really, you know, of these three, right. Scaling is really just like, get more GPUs and do the systems engineering necessary to orchestra orchestrate those in attention. The detail is, you know, train your models in ways that don't blow up. But really. So then the variable component, the part that's at least to me most mysterious is this data thing, right? Of the three sort of pillars,", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1104169_ms_-_1158407_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1104169, "end_ms": 1158407}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1148367 ms - 1207447 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1148367 ms - 1207447 ms\n\nContent: know, train your models in ways that don't blow up. But really. So then the variable component, the part that's at least to me most mysterious is this data thing, right? Of the three sort of pillars, data is the most unknown. So you know, but also at the same time data is something that's incredibly opaque. Um, and this is partially due to the current, you know, crop, I would say, of, of copyright lawsuits that have targeted every foundation model training lab in existence. Um, but if you look at for example the Llama 3.1 report, um, they will say, you know, they have pre trained from a variety of data sources. And I'm sure many of you have seen kind of this viral video of former CTO Mira Murati. You know, in an interview the interviewer asks her like, so what did you train your video generation models? And she's like, publicly available data, right? You know, this is the kind of situation we're in. None of the labs like to talk about what data they use. And so it's very much a", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1148367_ms_-_1207447_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1148367, "end_ms": 1207447}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1196671 ms - 1256961 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1196671 ms - 1256961 ms\n\nContent: generation models? And she's like, publicly available data, right? You know, this is the kind of situation we're in. None of the labs like to talk about what data they use. And so it's very much a mystery, I think to almost everyone working in the field what the exact training data is. So there has been some work trying to open source, replicate data pipelines and to train highly effective models. And I'll talk about one example as I continue through this talk called Data Comp lm. Right? And Data Comp LM was this attempt by some folks at Apple and Toyota Research Institute to basically build a full pre training filtering pipeline that results in an effective model and they end up training a pretty reasonable 7 billion parameter model. And the one thing to take away from this figure on the bottom here is it's a pretty complex hand engineered pipeline. You've got like English filtering, you've got page length filtering, repetition filtering, deduplication, you know, this algorithmic", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1196671_ms_-_1256961_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1196671, "end_ms": 1256961}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1244329 ms - 1304573 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1244329 ms - 1304573 ms\n\nContent: the bottom here is it's a pretty complex hand engineered pipeline. You've got like English filtering, you've got page length filtering, repetition filtering, deduplication, you know, this algorithmic filter that I'll talk about in a moment and then you select about 1.4% of your original. I think they start with like 200, 300 trillion tokens, all the way down to a couple trillion tokens at the end, right? So this is a really highly engineered data filter that you get. And so, okay, so you take a look at this and you're like, all right there, there's a highly effective data pipeline. I would like to understand, you know, like what is the state of the art? And so if you look at the state of the art in some ways it is, it is deeply disappointing because the best performing algorithmic data filtering pipeline right now is a bigram classifier that attempts to distinguish websites, random websites on the Internet from Reddit's explained like I'm5 subreddit and a instruction tuning data set", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1244329_ms_-_1304573_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1244329, "end_ms": 1304573}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1291803 ms - 1353197 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1291803 ms - 1353197 ms\n\nContent: pipeline right now is a bigram classifier that attempts to distinguish websites, random websites on the Internet from Reddit's explained like I'm5 subreddit and a instruction tuning data set called Open Hermes. Right. Um, and it is kind of, you know, doubtless that this pipeline works like the model that they train is good. Um, but this seems totally ad hoc and arbitrary and it is deeply dissatisfying to me as a researcher that like, you know, some random subreddit is a core component of a data filtering pipeline. Right. And so one thing that I would like to understand is are there simple principled alternatives? Are there simple, some principled approaches to classifying based on BI grabs like that would be cool, right? And so here is the simple game that we will play. The inputs are a target benchmark. This is the thing I would like to do well on the token count, this is the size of my data set and then the pre training pool, this is the pool of data that I can filter from and then", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1291803_ms_-_1353197_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1291803, "end_ms": 1353197}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1343453 ms - 1400477 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1343453 ms - 1400477 ms\n\nContent: benchmark. This is the thing I would like to do well on the token count, this is the size of my data set and then the pre training pool, this is the pool of data that I can filter from and then the output of the, the algorithm that I'll present is going to be a data filtering policy. This is a method by which I can filter my big pool and get a much smaller pre training dataset that I can train my language model. Right. So that's, that's kind of the name of the game here. And we are far from the first ones to think this, right? Like it is kind of an obvious thing to want to have an algorithmic way to select data to make your models better. This is like kind of a foundational question in machine learning. Many people have tried. Uh, the results have been deeply mixed. Um, I'll talk about some ideas that, you know, maybe you'll enjoy hearing about and have been sort of classic at this point. Um, one approach called data models has been, you know, proposed by some folks at mit. Um, and", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1343453_ms_-_1400477_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1343453, "end_ms": 1400477}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1388525 ms - 1440375 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1388525 ms - 1440375 ms\n\nContent: some ideas that, you know, maybe you'll enjoy hearing about and have been sort of classic at this point. Um, one approach called data models has been, you know, proposed by some folks at mit. Um, and this is a very, very simple idea. It says, okay, I wanna understand what good data looks like. Let me train a bunch of different models, removing data on each one and then let me build a big regression model that goes from, you know, which data was included to what my downstream performance was. Right? Like a very simple, let's just make a big regression model approach. Um, some folks here, Percy who's, you know, upstairs here, and others proposed influence functions. And you can think about this kind of as a Taylor approximation to how the model trains. Right. We can't reason possibly about how, you know, changing one piece of training data affects the whole training process, but maybe we can reason about small perturbations in like up weighting or down weighting. Data points and how", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1388525_ms_-_1440375_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1388525, "end_ms": 1440375}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1430159 ms - 1481687 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1430159 ms - 1481687 ms\n\nContent: how, you know, changing one piece of training data affects the whole training process, but maybe we can reason about small perturbations in like up weighting or down weighting. Data points and how that might affect, you know, my final model. Right. So I'm really glossing over the details of like the influence function style things, but you can just kind of think of it as like a Taylor approximation. And there's been many others that try to use like robust optimization or similarity, like n gram overlap of data points. Basically, if you have an idea, probably someone's thought about something related to that because this is a big community where everyone's tried all sorts of different things. And so this is an area with a lot of different ideas. But I think the really sad reality and one of the things that I think I've been kind of sad about here is none of this has really moved the needle on how you actually train language models. Like there's a lot of algorithms, but basically every", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1430159_ms_-_1481687_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1430159, "end_ms": 1481687}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1470799 ms - 1527633 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1470799 ms - 1527633 ms\n\nContent: the things that I think I've been kind of sad about here is none of this has really moved the needle on how you actually train language models. Like there's a lot of algorithms, but basically every study that has run data selection at scale has basically found like, oh, maybe actually just like pick randomly at uniform does the best. That's this, that's this paper by Nvidia or you know, the handcrafted Reddit subreddit classifier from Data Comp. Those were the kind of best performing things that people had found. Right. Um, and so empirically none of these algorithms have moved the needle and that, that's been very saddening for me. Um, and so let's think about why algorithmic data selection has remained so elusive and so hard. Well, there's a cost reason, right? If you want to understand how data affects a language model, well, you have to at some point like remove data and train a language model, right? Like that's the only way to really know whether you're right or wrong. And", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1470799_ms_-_1527633_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1470799, "end_ms": 1527633}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1518193 ms - 1571263 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1518193 ms - 1571263 ms\n\nContent: how data affects a language model, well, you have to at some point like remove data and train a language model, right? Like that's the only way to really know whether you're right or wrong. And that's an incredibly expensive endeavor. Um, another thing is, you know, really you can only do this thing, this counterfactual, you know, 10 to 15 times, 10 to 50 times. Because I can't train that many language models. It's hard to get a lot of data, right. And so this has led to a huge number of problems. And so let me just walk you through why this is like such a, such a difficult thing for a long time. Um, let's say your goal, right, like you've been put in charge of, I don't know what organization would be training 7B models now. Um, I don't know, let's say, let's say you've been put in charge of like a new team at Google or something. Um, and you're gonna be training a new 7 billion parameter language model, right? And, and your marching orders are, you gotta do well on mmlu, which is you", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1518193_ms_-_1571263_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1518193, "end_ms": 1571263}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1560911 ms - 1612261 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1560911 ms - 1612261 ms\n\nContent: of like a new team at Google or something. Um, and you're gonna be training a new 7 billion parameter language model, right? And, and your marching orders are, you gotta do well on mmlu, which is you Know, a question answering benchmark, right? So that's your goal. Um, so how are we gonna do that? How are we gonna select data to do well on the task? Well, if I want to use something like data models to do this, right? So this is the hypothetical. Um, I'm gonna train a thousand different small models, each one with a different data mix, and then I'm gonna measure how well each of these models do. And then I'm gonna build a regression model that goes from what I, you know, what the data mixture was for each of these thousand models short to the downstream performance. And once I have this regression model, you know, I can find the best possible mixture and I can train on that. Right. Very conceptually simple, incredibly expensive, and totally impl. Practical in practice because the cost", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1560911_ms_-_1612261_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1560911, "end_ms": 1612261}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1599845 ms - 1661051 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1599845 ms - 1661051 ms\n\nContent: model, you know, I can find the best possible mixture and I can train on that. Right. Very conceptually simple, incredibly expensive, and totally impl. Practical in practice because the cost is to train a thousand different models. And you know, I can't really train more than, you know, I can't estimate more than weights for a thousand domains. That seems, that seems pretty rough. Um, okay, wait there. Okay, so there's this fun idea that I and a couple of others have been working on for a bit the last couple months. The field moves fast. Um, which is actually whenever you want to like, analyze a model or like think about the behavior of a model, don't ever train your own model. All we're gonna do is we're gonna download a whole bunch of models from Hugging Face, right? Like, there's a lot of collective resources and intelligence that has been used to train at this point, over 100 high performance models that are now publicly available, right? It seems very silly for me to spend my own", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1599845_ms_-_1661051_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1599845, "end_ms": 1661051}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1648463 ms - 1702651 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1648463 ms - 1702651 ms\n\nContent: of collective resources and intelligence that has been used to train at this point, over 100 high performance models that are now publicly available, right? It seems very silly for me to spend my own money to train my own model when so many high performance models are already available. Um, and so, you know, if I can use these publicly available models, understand these models that are in a deep way to try to, you know, understand the relationship between training data and downstream performance, then maybe I have a way of understanding data kind of at scale, right? So the advantages here are no cost. You know, it's just downloading models. Um, it's heterogeneous. You know, there's coding models, there's multilingual models, there's all sorts of different models on the Internet. Um, and then, you know, I have 100 different models especially trained at scale. That's a decent number of data points, right? So the only issue here is what I said before. No one wants to talk about what they", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1648463_ms_-_1702651_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1648463, "end_ms": 1702651}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1691939 ms - 1745053 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1691939 ms - 1745053 ms\n\nContent: you know, I have 100 different models especially trained at scale. That's a decent number of data points, right? So the only issue here is what I said before. No one wants to talk about what they trained on, right? Like, none of these hundred models that I can download, almost none of them that I can download on Hugging Face, have publicly available pre training data, you know, out there for me to look at. So that seems really hard because what I want to understand is how pre training data affects model performance. How can I do that if I don't know what these models trained on? So really the fun little trick here, and this is the one, the one trick in the, in this talk, which is it turns out you don't need to know what these models trained on because there are things that we can measure just by using the parameters of these models. And in particular the thing that we can measure is the loss of these models, right? So what I'm going to do is I'm going to take these publicly available", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1691939_ms_-_1745053_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1691939, "end_ms": 1745053}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1735621 ms - 1786745 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1735621 ms - 1786745 ms\n\nContent: just by using the parameters of these models. And in particular the thing that we can measure is the loss of these models, right? So what I'm going to do is I'm going to take these publicly available models and I'm going to measure how well they compress text or you know, what loss they get that what their log likelihoods are on different pre training websites, right? And so a model, you know, might fit this website better, this other model might fit this other website better. And I'll have this big matrix where the rows are different models and columns are different websites on the Internet, right? And I'll get a log loss number or log likelihood number for, for each entry in this matrix and then I have, you know, the downstream performance for each of these models and I can regress from log loss onto downstream performance, right? This is a very simple regression problem. We use a very particular kind of a regression problem or, sorry, regression model called a single index model.", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1735621_ms_-_1786745_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1735621, "end_ms": 1786745}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1776317 ms - 1829939 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1776317 ms - 1829939 ms\n\nContent: log loss onto downstream performance, right? This is a very simple regression problem. We use a very particular kind of a regression problem or, sorry, regression model called a single index model. But you know, the point here is really conceptually this is very, very simple. And the key thing here, why this is an okay thing, right? Like why am I regressing log losses onto downstream performance? Um, you can show in a very, very simple way that you know, if you can fit this single index model and you can find non negative weights that make this regression model work, then what you can show is actually training directly on the weights. You know, sampling pages proportionally to these weights is actually a really good data selection policy. Right? Um, and one way to think about that is to say what I found is a way to weight my log losses across different websites to closely match my downstream performance, right? That's what this regression is doing. And if I can do that, I might as", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1776317_ms_-_1829939_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1776317, "end_ms": 1829939}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1819963 ms - 1877421 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1819963 ms - 1877421 ms\n\nContent: say what I found is a way to weight my log losses across different websites to closely match my downstream performance, right? That's what this regression is doing. And if I can do that, I might as well train on those log losses because that's probably also going to optimize the downstream performance, right? So, so this is just like a very simple idea that turns out to let us really directly select data using publicly available models. No model training required. It's a very nice thing that we can do. Don't know if anyone has questions here. Um, happy to answer any algorithmic things before I just kind of move on. Okay, cool. As an algorithm, it's very simple. I feel like maybe, you know, when I'm talking about single index models and whatever, you might get sort of lost or confused or lose interest, but really, if we just think about what I'm like actually mechanically doing, it's incredibly simple. All I do is I measure log losses on websites. I look at the websites that have the", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1819963_ms_-_1877421_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1819963, "end_ms": 1877421}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1866849 ms - 1922965 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1866849 ms - 1922965 ms\n\nContent: lose interest, but really, if we just think about what I'm like actually mechanically doing, it's incredibly simple. All I do is I measure log losses on websites. I look at the websites that have the highest correlation between log likelihoods and downstream benchmarks. I saw all the domains from most to least correlated and I select them in order. This turns out to be kind of an effective data selection algorithm. I'm going to skip the math and the theory here. Both of these slides. Okay. And so, you know, I was saying earlier that a lot of these algorithms don't really seem to work very well in practice. And so we have to actually do our own validation, right? We have to train our own language models and show that selecting data in this way is actually a reasonable thing to do. So what do we do? Well, we estimate, you know, our regression model, which is we call perplexity correlations on around 90 publicly available models. We take eight benchmarks that, you know, work even with", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1866849_ms_-_1922965_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1866849, "end_ms": 1922965}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1912109 ms - 1965333 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1912109 ms - 1965333 ms\n\nContent: do we do? Well, we estimate, you know, our regression model, which is we call perplexity correlations on around 90 publicly available models. We take eight benchmarks that, you know, work even with small models, and then we train and evaluate these models and we're going to compare to some reasonable baselines like, you know, the FAST X classifier or this thing called DSIR that was validated at scale. And then we're also going to compare to things like language filtering or no filtering at all, because past work showed that that, you know, is a surprisingly effective baseline. Okay, how well does this thing work? Actually turns out to work pretty well. So, you know, this is kind of the average performance across all the benchmarks. So this is the ranking that you end up getting amongst these baselines. You know, going to the left is better. So if we manually filter for the right language that the benchmark is in and also use the FAST text classifier, we get the best performance, which", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1912109_ms_-_1965333_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1912109, "end_ms": 1965333}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1955333 ms - 2009427 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1955333 ms - 2009427 ms\n\nContent: baselines. You know, going to the left is better. So if we manually filter for the right language that the benchmark is in and also use the FAST text classifier, we get the best performance, which is like 1.3 average rank perplexity correlations, which is the thing I was describing, requires no manual intervention and gets very close performance to that. And then all these other methods, you know, that I talked about, they basically do, you know, no better than not selecting any data at all. So that roughly matches, you know, what I was talking about earlier. You know, looking at kind of per benchmark performance, you know, we see basically the same story. Um, one thing that's kind of notable here. So on the, the columns here are different benchmarks that we can evaluate against. Um, different rows are basically different methods. So, you know, if I look at the lambda row and perplexity correlations. That's when I run the data selection algorithm with Lambada as the target. And really", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1955333_ms_-_2009427_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1955333, "end_ms": 2009427}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 1998555 ms - 2057285 ms", "content": "Title: CS224V Lecture 18 > Transcript > 1998555 ms - 2057285 ms\n\nContent: rows are basically different methods. So, you know, if I look at the lambda row and perplexity correlations. That's when I run the data selection algorithm with Lambada as the target. And really one of the notable things here, if you, if you look, is that, you know, when we target the English benchmarks, they are really selecting almost entirely for English. When we select the multilingual benchmarks down here, actually they don't remove English because there's a lot of high quality sort of like transfer from English domains onto non English domains. So you can kind of see all these sort of fun things happening. Another thing that I think is a kind of useful observation if you're working with language models and doing evaluations, is actually, you know, log likelihoods, like log losses of these models on different web crawled data is often highly correlated with downstream benchmark performance, right? I think when you first start working with language models, you think, oh, you train", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_1998555_ms_-_2057285_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 1998555, "end_ms": 2057285}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2046981 ms - 2100811 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2046981 ms - 2100811 ms\n\nContent: these models on different web crawled data is often highly correlated with downstream benchmark performance, right? I think when you first start working with language models, you think, oh, you train these models to like predict the next word, right? And you have this like log loss objective. And what you want to do is like qa. And those are very, very different things. They're like, you know, worlds apart. Um, but you can show like, you know, PI qa, which is a common sense dataset, you know, on the X axis here, this is kind of the actual benchmark performance. On the Y axis here, this is kind of how good we would predict that model is based on their perplexities, like their log likelihoods on different web pages. And so what that means is, you know, if you can predict, you know, the next word on webpage as well that's on the Y axis here, then you can actually, you know, know how well it will do on the downstream benchmark. And that's a surprisingly good correlation. The, the final", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2046981_ms_-_2100811_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2046981, "end_ms": 2100811}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2087811 ms - 2145265 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2087811 ms - 2145265 ms\n\nContent: word on webpage as well that's on the Y axis here, then you can actually, you know, know how well it will do on the downstream benchmark. And that's a surprisingly good correlation. The, the final thing I want to talk about for, you know, this set of results and this set of approaches is kind of, you know, I was talking about this big matrix that we do regression on, right? Where rows are different models, columns are different websites, and each entry is the log likelihood that a model attains on that. And if you think about it, I can do things like PCA or T SNE on this big matrix to try to understand like, oh, can I like visualize pre training data and understand, you know, which things are important features in pre training. And we see really two very salient features I think that are useful to know about. The first thing is language effects. Language effects are incredibly clear cut. When you do any pre training data work, right? English looks totally different from, you know,", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2087811_ms_-_2145265_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2087811, "end_ms": 2145265}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2134483 ms - 2188197 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2134483 ms - 2188197 ms\n\nContent: useful to know about. The first thing is language effects. Language effects are incredibly clear cut. When you do any pre training data work, right? English looks totally different from, you know, French or Spanish or any other language. And so when you try to visualize any of these like log likelihood matrices, you immediately see clear clustering between English and non English and other languages. We also see really clear differences in the difficulty of the web page, like the entropy of the website. And that's another thing that really clearly shows up in all of these kind of data centric analyses. So if you're thinking a lot about, okay, what are the ways in which pre training data is clearly different? Language and entropy are two really core ones. Okay. And the final thing that I'm kind of excited about, you know, I've snuck this in there. Even though this is, this is much less course material and much more, I don't know, research talk material, is thinking about, like, how do", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2134483_ms_-_2188197_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2134483, "end_ms": 2188197}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2178173 ms - 2226667 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2178173 ms - 2226667 ms\n\nContent: kind of excited about, you know, I've snuck this in there. Even though this is, this is much less course material and much more, I don't know, research talk material, is thinking about, like, how do we know that the research conclusions we reach are valid? Right. Like, I think this is a really difficult problem in machine learning and LLMs and so on. Um, you know, we can't really necessarily trust any of these results. And you shouldn't necessarily trust me when I come up here and tell you great results, right? Like, well, many of the past results haven't held up. You know, I'm doing these experiments at small scale. You know, It's n equals 1. Um, and so I'm trying variety of different things, but one thing I'm excited about trying recently is to think about pre registration, which, you know, if you've, you know, been in a psych class or a social sciences class has been a really popular approach. You know, you basically declare on the Internet or publicly, I'm going to run this", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2178173_ms_-_2226667_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2178173, "end_ms": 2226667}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2217903 ms - 2266915 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2217903 ms - 2266915 ms\n\nContent: you know, if you've, you know, been in a psych class or a social sciences class has been a really popular approach. You know, you basically declare on the Internet or publicly, I'm going to run this experiment before you run it and then you just commit to releasing the results no matter what you do. And so we're actually in the process of doing this now for this class of algorithms where we're basically scaling up about 100 times in compute. And we've kind of pre registered the experiment we're going to run. So we don't really have the ability to, you know, essentially fudge the results. Right. Like we have to run the experiments, we said, and then we have to report the results, results regardless of outcome. Um, and we did a similar thing for a past work that we recently released a couple months ago. This is a slightly different flavor result in which what we were trying to do was to predict essentially capabilities of language models on complex tasks, like let's say like, you know,", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2217903_ms_-_2266915_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2217903, "end_ms": 2266915}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2256291 ms - 2309275 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2256291 ms - 2309275 ms\n\nContent: months ago. This is a slightly different flavor result in which what we were trying to do was to predict essentially capabilities of language models on complex tasks, like let's say like, you know, being an agent and solving software engineering benchmarks. So those are very complex tasks and you might wonder how well can we predict, you know, how well a model does on these complex tasks on the basis of much simpler tasks like answering, you know, questions. Um, turns out benchmarks are very, very correlated. And you can very clearly predict even the performance of models on the hardest of benchmarks, like things that people call emergent, you can easily predict using simple regressions. And so we did a similar thing where we you know, first fit these like regression like things. We pre registered predictions for really large scale new models. Um, and then you know, we basically tested our approach. Turns out our predictions remained accurate even for the new Llama 3 models, which was", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2256291_ms_-_2309275_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2256291, "end_ms": 2309275}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2298861 ms - 2353067 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2298861 ms - 2353067 ms\n\nContent: predictions for really large scale new models. Um, and then you know, we basically tested our approach. Turns out our predictions remained accurate even for the new Llama 3 models, which was really cool to see. So that was, that was pretty exciting. Um, sorry, that was a bit of a digression. Um, but you know, to take away here, right, One of the two sort of important pillars or a important pillar now is data selection for pre training, right. Like if you take away one thing from this talk, it's kind of pre training data, very important, right. And, and data selection is kind of the core algorithmic piece of that. Um, and what we've done is try to reduce it to a much, much simpler problem which is, you know, fitting regressions on publicly available models. And I think there's like lots of ingredients here if you're interested in like pre training research or pre training approaches. Uh, I think there's pretty interesting ideas here about. Okay, like what we're gonna do is we're gonna", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2298861_ms_-_2353067_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2298861, "end_ms": 2353067}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2343323 ms - 2401897 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2343323 ms - 2401897 ms\n\nContent: ingredients here if you're interested in like pre training research or pre training approaches. Uh, I think there's pretty interesting ideas here about. Okay, like what we're gonna do is we're gonna fit regressions to losses or using small scale validation and pre registered scaling studies. Okay. If there's any questions there, I'm happy to stop because I'm going to talk about something pretty totally different in a moment. Yes. So are you still using all your pre trained data but just weighting it differently based on how those selected models. Good question. And in some ways that's a great segue for the next part. So one of the, I think mindsets that's useful to get into when you think about pre training is to think about like what is the resource constraint at play? Right. And right now almost for, I think literally for everybody, their compute constraint, right. Like your, your constraint is you have more data than you can train on, but you don't have enough compute to train on", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2343323_ms_-_2401897_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2343323, "end_ms": 2401897}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2392393 ms - 2445391 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2392393 ms - 2445391 ms\n\nContent: now almost for, I think literally for everybody, their compute constraint, right. Like your, your constraint is you have more data than you can train on, but you don't have enough compute to train on all of it. Right. So you don't want to wait the data because to weight the data you have to compute the forward and backward passes. You just want to select data because then you don't have to do any gradient steps on those. And so everything we do is in that part is data selection. And then the second part that I want to talk about next actually that's thinking about a very different regime that's thinking about the data limited regime where in some distant future we have more Compute than we know what to do with. But the Internet data is finite. So how do we deal with that? That's going to be kind of the second theme of the second part of this talk. Okay, cool, then I will keep going. Um, so the second part here, you know, I've already given you the teaser, but I'm not gonna be", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2392393_ms_-_2445391_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2392393, "end_ms": 2445391}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2428877 ms - 2487579 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2428877 ms - 2487579 ms\n\nContent: to be kind of the second theme of the second part of this talk. Okay, cool, then I will keep going. Um, so the second part here, you know, I've already given you the teaser, but I'm not gonna be selecting data because selecting data is really the game that you play when you're compute back, right? When you, when you can't train on everything. But you might be in a regime where you can train on everything. Maybe that's because what you want to do is update the model for a small niche domain where you don't have much data. Or we're thinking about the distant future in which, you know, we have a million GPUs and we can train on whatever we want. So you know, we know that language models struggle when there's not enough data, right? Language models have this big data efficiency problem, but so far I think no one really thinks about it because we're not data bound, right? We can keep training on more data. So data efficiency just doesn't seem like a big problem right now. Um, but really", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2428877_ms_-_2487579_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2428877, "end_ms": 2487579}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2476635 ms - 2533287 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2476635 ms - 2533287 ms\n\nContent: so far I think no one really thinks about it because we're not data bound, right? We can keep training on more data. So data efficiency just doesn't seem like a big problem right now. Um, but really this is going to become a problem as we start to either exhaust data or, or we start to think about more niche domains. And you know, we do know that if we want to make a model work well in some area where there's not that much pre training data, there's tried and true techniques for doing it. It's called, you know, continued pre training. Um, there's been a number of papers, um, here's a, you know, random small selection of successful continued pre training papers, um, that you know, adapt these models for things like law or math or medicine or whatever. These models generally work well. The math models being, you know, and, and code models actually being regularly used in, in a variety of downstream applications. Um, but if you look at this table and you really want to apply this kind of", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2476635_ms_-_2533287_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2476635, "end_ms": 2533287}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2522623 ms - 2576591 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2522623 ms - 2576591 ms\n\nContent: models being, you know, and, and code models actually being regularly used in, in a variety of downstream applications. Um, but if you look at this table and you really want to apply this kind of technique to make your model, you know, good at your own domain, you start to see something very disturbing right on the right column, right? The number of tokens that you need to run continued pre training. That's like 15 billion. To be safe, you probably want a 50 billion tokens, right? That's a ton of domain specific tokens that you need in order to make your model sort of smarter in a domain. And realistically speaking, we do not have that many tokens. If I want to teach a language model to know, I don't know Stanford's like administrative policies really well, you do not have that many domains or that many tokens. And so, you know, one thing you can do of course in that regime is you can like build a rag system. And you know, this is kind of the stuff that, that you've learned about with", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2522623_ms_-_2576591_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2522623, "end_ms": 2576591}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2566373 ms - 2622043 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2566373 ms - 2622043 ms\n\nContent: or that many tokens. And so, you know, one thing you can do of course in that regime is you can like build a rag system. And you know, this is kind of the stuff that, that you've learned about with grounding and so on in the course. But I want to maybe, you know, for the moment bear with me and talk through an entirely alternative, much more inefficient approach. Right. What I'm going to try to do is I'm going to start with 1.3 million tokens. Way too few to do continued pre training directly. But I would like to come up with a new more data efficient algorithm that lets me put the knowledge in, you know, these, these books into the parameters of the model. Right? So this is continued pre training with 10,000 times less data than sort of what people have been doing before. So let's think about that, you know, sort of why doesn't this work? Right, so standard pre training and standard continued pre training, both of these approaches, you know, do the well established pre training", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2566373_ms_-_2622043_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2566373, "end_ms": 2622043}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2610651 ms - 2661737 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2610651 ms - 2661737 ms\n\nContent: about that, you know, sort of why doesn't this work? Right, so standard pre training and standard continued pre training, both of these approaches, you know, do the well established pre training playbook. What is that? Well, you know, you have, you know, a piece of text, in this case, you know, the abstract to my paper and you know, you're gonna take as input some prefix and you're gonna predict, you know, what comes after. Right. We all, we all kind of know this. Um, but the issue with this, if you think about it for a little bit, is what are we learning? We're learning to predict what comes, you know, to the right. And this is a really data inefficient way to learn. And people have talked about, you know, something called the reversal curse, which I don't know if, if you've covered in this class. Um, but it's, it's a phenomenon that basically says when you're trying to sort of query the model for knowledge that's stored in the original direction of the text. So in this case, let's", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2610651_ms_-_2661737_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2610651, "end_ms": 2661737}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2650701 ms - 2705753 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2650701 ms - 2705753 ms\n\nContent: this class. Um, but it's, it's a phenomenon that basically says when you're trying to sort of query the model for knowledge that's stored in the original direction of the text. So in this case, let's say, you know, I ask what does synthetic CPT do? This is a very easy task because in the prefix I have the word synthetic. Continued pre training. In the suffix I have a definition of what that is. Right? So this first question is very easy. The reverse question actually is very difficult. If I ask like, what is a technique that synthesizes a large corpus artificially? Right now I have to start with the thing that I predicted and then map that to my inputs, right? I'm going from like Y to X rather than X to Y. People have shown basically that if you're, if you are querying the model in this Kind of reverse direction performance degrades very dramatically, right? So that's one evidence that these models are way less efficient than we are as humans at learning from text. Right? And so, you", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2650701_ms_-_2705753_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2650701, "end_ms": 2705753}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2693849 ms - 2745773 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2693849 ms - 2745773 ms\n\nContent: of reverse direction performance degrades very dramatically, right? So that's one evidence that these models are way less efficient than we are as humans at learning from text. Right? And so, you know, let's think about this for the moment, right? Like, we know that continued pre training, this approach of like doing a single passive, you know, autoregressive training on a corpus, does not work very well. We do, however, know that pre training works, right? That's how ChatGPT is made. And so what's the difference here? Well, the, the key difference here in some sense is the diversity in which we present the same fact. Right? On the left, if we look at continued pre training, let's say I have a single book, you know, I only have one instantiation of that book. There's only a single, you know, instantiation, a single presentation, very not diverse in a way, right? On the other hand, in pre training, if I want to learn about linear algebra, you know, there's a ton of different places", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2693849_ms_-_2745773_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2693849, "end_ms": 2745773}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2735749 ms - 2787223 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2735749 ms - 2787223 ms\n\nContent: know, instantiation, a single presentation, very not diverse in a way, right? On the other hand, in pre training, if I want to learn about linear algebra, you know, there's a ton of different places that are talking about, you know, the singular value decomposition, right? Probably there's a Stack Exchange post, there's a Wikipedia post, there's some Reddit discussion, right? There's hundreds of different ways in which the same fact is going to be rewritten. And in some sense that's really critical for language models to learn, given the way in which they're trained, right? And so, you know, I told you earlier in this, in this talk that I am not going to do anything sophisticated at all. And so I'm not going to do anything sophisticated. All I'm going to do is I'm going to take my continued pre training data. In this case, let's say it's a single book, and let's say hypothetically that I could take the single book and I could rewrite it in, you know, millions of different ways, and I", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2735749_ms_-_2787223_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2735749, "end_ms": 2787223}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2777751 ms - 2828481 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2777751 ms - 2828481 ms\n\nContent: pre training data. In this case, let's say it's a single book, and let's say hypothetically that I could take the single book and I could rewrite it in, you know, millions of different ways, and I could rewrite it in ways that look like Wikipedia or Stack Exchange or Reddit or whatever, right? I can emulate what this piece of text would look like if it was a whole bunch of Internet websites. Let's say I could do that. Right? Um, if we walk through that thought experiment, then in some sense, conceptually this is gonna be identical to pre training, right? Like, there should be no difference between pre training and this sort of weird augmented version of continued pre training that I'm doing. And so that's kind of the goal here. I'm gonna try to, in some sense replicate the diversity of ways in which knowledge is presented in, in pre training in the hopes that that will give us something Interesting out at the other side. And so I would like to vary kind of the content like what is", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2777751_ms_-_2828481_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2777751, "end_ms": 2828481}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2818963 ms - 2874625 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2818963 ms - 2874625 ms\n\nContent: ways in which knowledge is presented in, in pre training in the hopes that that will give us something Interesting out at the other side. And so I would like to vary kind of the content like what is the, you know, each website talking about. I'd like to ideally very style like how it's presented. Um, and then this will give us a lot of data diversity that will hopefully let us generalize. And if you followed any kind of you know, literature on synthetic data or model training, this is pretty different from I think how many other people have been taking a approach on synthetic data. Um, some people have been doing synthetic data for compute or size efficiency. So that's um, if you're familiar with what Microsoft has done with the Phi models, that's exactly this. Um, others, um, including myself and my lab have done like fine tuning work where we generate, you know, instruction tuning data using language models. So this is different from either of those really. Our goal is, is to just", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2818963_ms_-_2874625_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2818963, "end_ms": 2874625}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2864451 ms - 2928029 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2864451 ms - 2928029 ms\n\nContent: myself and my lab have done like fine tuning work where we generate, you know, instruction tuning data using language models. So this is different from either of those really. Our goal is, is to just increase the diversity of data rather than, you know, fine tune it or to, to make the size of the model smaller. Okay. Uh, maybe this is a good stopping point. If anyone has questions, feel free to feel free to ask them. Okay. Or not. Um, so I think with, with all machine learning things one of the core challenges is how do we do evaluation. I think you know the, the pitch that I gave you for okay, I'm just going to rewrite this book. I think that's, that's conceptually appealing but we have to be able to evaluate it in this very rigorous quantitative way and we have to ideally evaluate it without using too much compute. And so there is actually a very nice dataset or more generally a dataset construction strategy that works pretty well. So the idea here is I'm going to take reading", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2864451_ms_-_2928029_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2864451, "end_ms": 2928029}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2915345 ms - 2977387 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2915345 ms - 2977387 ms\n\nContent: it without using too much compute. And so there is actually a very nice dataset or more generally a dataset construction strategy that works pretty well. So the idea here is I'm going to take reading comprehension dataset in particular a reading comprehension dataset that's designed for long context. So this has very long books and I would like to have question answering questions associated with these books. There's exactly such a dataset called Quality published by some folks at NYU and Anthropic. They basically took a bunch of somewhat obscure public domain books that most language models don't really know very well. Knowledge, you know here in this case like kind of appears once or twice in the pre training dataset and there's high quality human written QA evals that are written against these books. And so you know, quality fulfills all these criteria. It's niche fiction, it's fairly small amount of tokens and there's high quality QA and summary evals just to contextualize", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2915345_ms_-_2977387_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2915345, "end_ms": 2977387}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 2965707 ms - 3022573 ms", "content": "Title: CS224V Lecture 18 > Transcript > 2965707 ms - 3022573 ms\n\nContent: against these books. And so you know, quality fulfills all these criteria. It's niche fiction, it's fairly small amount of tokens and there's high quality QA and summary evals just to contextualize performance from when I start to talk about some of the results GPT4 on this dataset achieves 51% accuracy. Random guessing is 20, 25%. And so clearly the models know something about, you know, these books. You know, it's been trained on them, of course, but nowhere close to human level accuracy, which is like in the 97 or 98 or something. If you put the book in front of the, per the, in front of people and you ask, it's in like the high 90s. Okay, so how can we teach a language model the contents of these books? Right, so that's kind of the question. Or it's a toy problem, but that's the problem I'd like to solve. So the very first thing I'm going to do is I'm going to do just continued pre training. This is the thing that I told you about before, right? I'm going to take all the books and", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_2965707_ms_-_3022573_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 2965707, "end_ms": 3022573}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3013141 ms - 3067867 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3013141 ms - 3067867 ms\n\nContent: I'd like to solve. So the very first thing I'm going to do is I'm going to do just continued pre training. This is the thing that I told you about before, right? I'm going to take all the books and I'm going to train my language models in the usual autoregressive way. Next word prediction. I'm going to run through all 1.3 million tokens or so now. So if, if I show you, you know what happens. Llama3ab, which is the model that we're going to be improving, that's the black dash line. That's kind of the baseline from which we start. Um, if I do this continued pre training approach, that's the green line over here, it actually does worse than the original model. Any, any non trivial amount of continued pre training actually makes the model worse because not only is the model not absorbing the knowledge, in some sense, we're also destroying its ability to follow good instructions and so it actually does worse. GPT 3.5 and 4, they're slightly higher up in this plot here. As I said before,", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3013141_ms_-_3067867_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3013141, "end_ms": 3067867}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3055443 ms - 3111021 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3055443 ms - 3111021 ms\n\nContent: knowledge, in some sense, we're also destroying its ability to follow good instructions and so it actually does worse. GPT 3.5 and 4, they're slightly higher up in this plot here. As I said before, it's about 51% performance with GPT4. So one of the things that I can do this is slightly inefficient in terms of the cost to generate the data. But let's say I take the books and I do kind of the simplest possible way of making the data set bigger. I'm going to call the GP like OpenAI API and I'm going to ask it, okay, here's the chunk taken from a book. Please rewrite it. Just like paraphrase it in whatever way you want, just rewrite it in many different ways, right? So I'm just going to rephrase that book and that's going to give me this blue line. As I generate more and more synthetic tokens from rephrasing, I actually see pretty predictable linear gains. If you've seen many different kinds of like language Model training plots. Some people would call this a scaling law on this sort of", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3055443_ms_-_3111021_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3055443, "end_ms": 3111021}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3100963 ms - 3164689 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3100963 ms - 3164689 ms\n\nContent: from rephrasing, I actually see pretty predictable linear gains. If you've seen many different kinds of like language Model training plots. Some people would call this a scaling law on this sort of Y axis here. And that's kind of exactly what you want to see, right? In some ways the success of modern language model pre training that's predicated on a scaling law. Once you start to see predictable improvements in performance from resource inputs, if it's economically valuable enough, you basically throw GPUs at this to make performance go up. But unfortunately performance goes up pretty slowly with rephrasing. We're barely beating GPT 3.5 in this plot after I think like 50 million tokens or something like that. And so that's not really great. And so we would like to ideally be more efficient. More, yeah, more token efficient than that. And so here is like a general, not tip, general successful approach in synthetic data generation. Um, the core problem here is that we're asking", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3100963_ms_-_3164689_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3100963, "end_ms": 3164689}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3149201 ms - 3207347 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3149201 ms - 3207347 ms\n\nContent: efficient. More, yeah, more token efficient than that. And so here is like a general, not tip, general successful approach in synthetic data generation. Um, the core problem here is that we're asking language models to come up with data or to come up with, you know, new kinds of data. But language models, especially the ones that are instruction tuned, are not terribly diverse. I'm sure many of you have attempted to use ChatGPT for your writing at some point. Hopefully not for your writing assignments, but for writing. Um, and you'll find that, you know, it's, it's very fixated on certain ways of writing, right? It's, it can't like generate all sorts of diverse outputs. And we've seen this both in sort of, you know, this, this rephrasing thing, but also in another recent work where a student of mine, student of mine was working on sort of using LMS to come up with research ideas. And one thing that's very striking that we saw this was with Claude 3.5, was as you ask the model to", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3149201_ms_-_3207347_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3149201, "end_ms": 3207347}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3194899 ms - 3248135 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3194899 ms - 3248135 ms\n\nContent: student of mine, student of mine was working on sort of using LMS to come up with research ideas. And one thing that's very striking that we saw this was with Claude 3.5, was as you ask the model to generate more and more ideas, it kind of eventually runs out of ideas. And after about a thousand ideas, you know, every new idea you're getting is almost a duplicate of the old one, right? So there's very low diversity for these models. And so because of this, a lot of really successful ways of generating synthetic data rely on an approach that I call kind of externalizing the diversity. Um, and what that means is you have some external source of randomness and you, you know, inject that randomness into the prompt of your language model and that allows you to get a much more diverse output on the out on the other side. And so to give some examples, you know, Stanford Alpaca, which was a really early instruction tuning model we built. You know, we have random human seed data and we", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3194899_ms_-_3248135_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3194899, "end_ms": 3248135}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3238461 ms - 3291619 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3238461 ms - 3291619 ms\n\nContent: on the out on the other side. And so to give some examples, you know, Stanford Alpaca, which was a really early instruction tuning model we built. You know, we have random human seed data and we condition on the past data. So you, when you generate, you always, you Know, sample a random set of other data you've already generated and like, try to generate something that is different from that. Um, some folks at Princeton came up with this approach called Skill Mix Instruct. And in that approach, what they do is they list a number of skills that, you know, you want. They're generating data, so they're asking like, oh, here's like 10 different skills. They pick random pairs of skills and they say, oh, generate a example that requires both of those skills. And our approach is going to be to randomize over different topics that you can talk about. And we're going to do that through a knowledge graph kind of analogy or formalism. And so, you know, I think in a way what we're going to do is", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3238461_ms_-_3291619_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3238461, "end_ms": 3291619}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3280267 ms - 3333893 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3280267 ms - 3333893 ms\n\nContent: over different topics that you can talk about. And we're going to do that through a knowledge graph kind of analogy or formalism. And so, you know, I think in a way what we're going to do is go back to some of the early ways to think about knowledge, right? So in a document you've got a lot of entities. These entities have relations. And what we would like is for our model to have a good understanding of all of the different relations between the entities. That's kind of the best that we can hope for. So the way that we do this is, you know, we start by prompting the language model to generate all the entities that appear in a book, right? So we come up with a big, long list of entities. And then what we're going to do is we're going to take these entities and then to generate a new synthetic piece of text, I'm going to sample some K random subgraph of the knowledge graph. So in this case it's a two subgraphs. I'm going to pick, you know, two random entities in the graph, right? So", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3280267_ms_-_3333893_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3280267, "end_ms": 3333893}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3322853 ms - 3372517 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3322853 ms - 3372517 ms\n\nContent: piece of text, I'm going to sample some K random subgraph of the knowledge graph. So in this case it's a two subgraphs. I'm going to pick, you know, two random entities in the graph, right? So I'm going to pick DaVinci and the Louvre. And those are kind of my, my two nodes that I'm going to care about. And then the Augmenter LLM, you know, takes these two randomly chosen entities as input. And the prompt is very simple. You know, it's something like describe the relation between the Louvre and DaVinci in the context of this document, right? And if the LM does, does this instruction correctly, then what it's going to do is in some sense it's going to traverse the knowledge graph, right? It's going to say something like, you know, the Louvre contains the Mona Lisa, which was painted by DaVinci. Right? And so we're basically surfacing these indirect relations about, you know, let's say da Vinci and the Louvre. There's never a direct connection between those two in the document. We're", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3322853_ms_-_3372517_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3322853, "end_ms": 3372517}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3362473 ms - 3414191 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3362473 ms - 3414191 ms\n\nContent: Right? And so we're basically surfacing these indirect relations about, you know, let's say da Vinci and the Louvre. There's never a direct connection between those two in the document. We're going to force the language model to spell that out and then put that back into the training set of this, of this model. Right. So you know, to put this all together, really what we are doing is that a document, you know, it has a lot of facts in it, right? It has a lot of implicit facts. If we read the document, we can form inferences about, you know, different entities. And we're taking those implicit facts that are never explicitly stated and we're sort of materializing them explicitly using the synthetic data strategy. And you know, it also serves as kind of a form of randomization. So what happens when we do this like this? This is an approach that my, my student called Antigraph. This is the salmon colored line in this plot here. It scales much more nicely and more efficiently than just", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3362473_ms_-_3414191_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3362473, "end_ms": 3414191}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3404311 ms - 3463029 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3404311 ms - 3463029 ms\n\nContent: when we do this like this? This is an approach that my, my student called Antigraph. This is the salmon colored line in this plot here. It scales much more nicely and more efficiently than just naively rephrasing. And you know, if we, if we just keep scaling to the right and the right, I think this is, this ends at about 600 million tokens. We can pretty handily beat GPT4, sort of closed book in doing QA. So that's actually quite nice. And I think we can hopefully continue to scale with additional token inputs. Of course we stopped here because we weren't quite as interested in continuing to double the amount of token budget that we had. One thing that you might be concerned by is that we're using GPT4 to generate the synthetic data. And so maybe it's just distilling a lot of knowledge from GPT4. And so we checked against this by saying, okay, like how well do we actually do relative to GPT4? And we improve significantly like 17% via antigraph, which, you know, once again exceeds, you", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3404311_ms_-_3463029_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3404311, "end_ms": 3463029}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3451587 ms - 3507431 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3451587 ms - 3507431 ms\n\nContent: GPT4. And so we checked against this by saying, okay, like how well do we actually do relative to GPT4? And we improve significantly like 17% via antigraph, which, you know, once again exceeds, you know, GPT4 by, by a decent bit. And I think the reason why, you know, this approach is very exciting to me is because what I would like to do is I would like to build language models that have, you know, knowledge in a very specific domain. Like I want to build language models that can help us do research. To do that they need to have a good understanding of, you know, scientific knowledge. And you know, when we fine tune these models, like these continued pre trained models using instruction tuning data, we actually see lots of multitask behaviors that we see with things like ChatGPT. Like we can ask the language model to, you know, brainstorm or not brainstorm, like give a list of how the practice of dentistry in the US has changed and it will give you a list using facts from the", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3451587_ms_-_3507431_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3451587, "end_ms": 3507431}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3496477 ms - 3548361 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3496477 ms - 3548361 ms\n\nContent: Like we can ask the language model to, you know, brainstorm or not brainstorm, like give a list of how the practice of dentistry in the US has changed and it will give you a list using facts from the documents or you know, we can ask it to compare two different articles in the set of articles and it can do that as well. It was, these were never really explicitly in the training data and so that's very encouraging to see, right, that we can do things like summarization or relate multiple articles in this very general way despite only being trained on task agnostic data. When we look at summarization, the model without looking at the books at all, right? This is all closed book evaluation can do summarizations of books, you know, we asked in this case like what is the book Cosmic yo yo about? It can nicely summarize that other baselines, you know, don't basically have the knowledge to summarize this book that what I've highlighted and read here are hallucinations, right? So continue pre", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3496477_ms_-_3548361_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3496477, "end_ms": 3548361}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3538017 ms - 3600859 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3538017 ms - 3600859 ms\n\nContent: It can nicely summarize that other baselines, you know, don't basically have the knowledge to summarize this book that what I've highlighted and read here are hallucinations, right? So continue pre training with our approach, you know, gives dramatically better hallucination rates for summarization. And when we evaluate quantitatively using GPT4 itself to evaluate, you know, how factual is the summary, how salient are the claims that are being made in the summary. Um, the salmon colored one which is entigraph once again does really well. It, it makes very few hallucinations while making uh, claims uh, that are salient, you know, comparable to sort of humans. The X axis is salience here and the Y axis is in some sense hallucination rate. Um, the same is true even when we, we carefully match total number of tokens and when we generate total number of synthetic tokens that we use to do this evaluation. Okay. Kind of the final thing that I want to sort of in terms of the empirical", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3538017_ms_-_3600859_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3538017, "end_ms": 3600859}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3586177 ms - 3646173 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3586177 ms - 3646173 ms\n\nContent: match total number of tokens and when we generate total number of synthetic tokens that we use to do this evaluation. Okay. Kind of the final thing that I want to sort of in terms of the empirical evaluation that I want to talk about is, all right, so you've been talking about, you know, spending lots of money and lots of GPU time to generate synthetic data to put knowledge into a language model, but why wouldn't you just use a RAG system? And this is a fair point, right? Like I think if you want to build a system today, you're obviously going to build a retrieval augmented system, right? But I think one of the things that's quite encouraging to me is that when you build a system that has more domain knowledge, turns out that you get benefits on top of rag. And in this case we built a RAG system that is incredibly accurate. So if you look at the recall at 8 number here, one of my students, Neil, I think he like dedicated two weeks of his life to just like making a really good RAG", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3586177_ms_-_3646173_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3586177, "end_ms": 3646173}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3634759 ms - 3690363 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3634759 ms - 3690363 ms\n\nContent: RAG system that is incredibly accurate. So if you look at the recall at 8 number here, one of my students, Neil, I think he like dedicated two weeks of his life to just like making a really good RAG system. And within the top eight articles, you know, 99% of the time the right article is in there, right? So it's very, very accurate in terms of the retrieval accuracy. Um, and even against this strong baseline, if we apply continued pre training, we still see like 2 to 3% gains on this, which I think is actually quite Encouraging. It means that, you know, even when the retrieval is near perfect, there's still a lot of room for improvement from just doing some simple domain specific, continued pre training. Right? So and another sort of way to contextualize this is to say, you know, the closed book performance that we got, right? This is without any rag system, 56% is almost as good as the llama llama rag performance. And it's 80% of the gain, 80% of the way there from retrieval, which", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3634759_ms_-_3690363_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3634759, "end_ms": 3690363}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3678843 ms - 3737043 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3678843 ms - 3737043 ms\n\nContent: book performance that we got, right? This is without any rag system, 56% is almost as good as the llama llama rag performance. And it's 80% of the gain, 80% of the way there from retrieval, which is, which is quite nice, I think. So the final thing that I want to talk about today is to think about, okay, Like I think synthetic data is, is a kind of interesting space because it's also accompanied with by a lot of hype, right? Like in some sense when, when people talk about synthetic data, they're like, oh yeah, like this is going to solve all of our data problems. We'll just generate data out of thin air and we will no longer have to think about things like data limits. But I think this is not the case. So I think we need to think very carefully about what is synthetic data buying us, what is it not buying us. And so one of my students, Zitong and Xuan Ping, who was a collaborator, sat down to try to think about, okay, like, what's the theory behind, you know, synthetic data here? Or", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3678843_ms_-_3737043_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3678843, "end_ms": 3737043}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3724219 ms - 3778817 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3724219 ms - 3778817 ms\n\nContent: is it not buying us. And so one of my students, Zitong and Xuan Ping, who was a collaborator, sat down to try to think about, okay, like, what's the theory behind, you know, synthetic data here? Or at least, you know, our synthetic data augmentation. And so why do we get gains from rewriting the same piece of data in many ways? So what we did was we built a simple like toy mathematical model. Let's say we have a set of V different entities in a single document, the source. And this is our knowledge graph that we talked about before. And within this document there's going to be a bunch of claims that appear directly. By directly, I mean things like X is Y or you know, like Tatsu is teaching, right? Like these very direct statements are direct edges in the graph. Right. And we're going to assume that, you know, just for mathematical convenience, that, you know, this document graph is just going to have a random graph structure, Erdos Reni graph where, you know, you just flip a coin for", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3724219_ms_-_3778817_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3724219, "end_ms": 3778817}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3769905 ms - 3819987 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3769905 ms - 3819987 ms\n\nContent: that, you know, just for mathematical convenience, that, you know, this document graph is just going to have a random graph structure, Erdos Reni graph where, you know, you just flip a coin for each edge and you know, that's going to determine whether that edge appears. Right. And one thing that we can show is we can like very precisely analyze the process that we have of saying like, okay, what does, you know, synthetic data do? Synthetic data is describing the relation between two entities that may not necessarily be directly in the document. Right? That's kind of what we're doing. We can formalize that very precisely as mathematics, mathematically and say, okay, what's happening is we're going to, you know, follow this knowledge graph path and then we're going to fill in the edge that didn't used to exist. And the salient question is, what can we possibly know about the entities? That's going to turn out to be a mathematical question equivalent to saying how much can we fill in", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3769905_ms_-_3819987_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3769905, "end_ms": 3819987}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3809883 ms - 3861363 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3809883 ms - 3861363 ms\n\nContent: didn't used to exist. And the salient question is, what can we possibly know about the entities? That's going to turn out to be a mathematical question equivalent to saying how much can we fill in this knowledge graph? Given a partially filled in graph and the ability to connect any two sort of nodes that are sort of within the same connected component and you can, you can give a very precise description. I'm going to, you know, save you from, from having to look at the asymptotics just to say, basically there is a fairly clean sort of final result that says, you know, if our graph is awfully connected, then that means that the document has the knowledge to describe all the entities. Then synthetic data can sort of get us all the pieces of knowledge in the document. Otherwise, if there's entities that are, you know, not described at all in the document or not connected in any way, then those don't get filled in in any way. So it's in some way sense an obvious statement, but one thing", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3809883_ms_-_3861363_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3809883, "end_ms": 3861363}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3851191 ms - 3904023 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3851191 ms - 3904023 ms\n\nContent: entities that are, you know, not described at all in the document or not connected in any way, then those don't get filled in in any way. So it's in some way sense an obvious statement, but one thing that's kind of nice is it does fit the empirical observations that we had in terms of how performance improves as a function of the number of synthetic tokens. If we take that formula and we fit it, we see fairly nice sort of clean empirical fits with respect to sort of the performance of this curve. So, okay, but the takeaway here I think now hopefully this was kind of interesting given that, you know, in a lot of the class you've been learning about grounding and how to put knowledge into language models. You know, I think the, the perspective here is to say, you know, not only can we, you know, put knowledge in by like prompting and these like post hoc approaches, we can really start to inject, you know, domain specific knowledge deep within the parameters of a language model. Of", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3851191_ms_-_3904023_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3851191, "end_ms": 3904023}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3894087 ms - 3946971 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3894087 ms - 3946971 ms\n\nContent: you know, put knowledge in by like prompting and these like post hoc approaches, we can really start to inject, you know, domain specific knowledge deep within the parameters of a language model. Of course, the cost here is that we are spending a ton of GPU time generating synthetic data and doing the equivalent of pre training again on a particular domain. Um, but really the top line conclusion here is that, you know, pre training isn't restricted to pre training. We can use the same playbook to improve knowledge of language models on a particular domain. Um, and we can make it effective at the really tiny 1 million token level. Um, and we can get a model that's almost as good as a retrieval augmented model without having any retriever, which is kind of surprising. And if you're really interested in kind of Pre training data efficiency research. I think one thing that I didn't talk about, but I, but I'm personally really excited about is that the setting of doing continued pre", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3894087_ms_-_3946971_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3894087, "end_ms": 3946971}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3937107 ms - 3991997 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3937107 ms - 3991997 ms\n\nContent: interested in kind of Pre training data efficiency research. I think one thing that I didn't talk about, but I, but I'm personally really excited about is that the setting of doing continued pre training in a data efficient way is also a really good kind of toy model for thinking about the future of language modeling where we're going to eventually run out of high quality Internet data. Right. Like when that happens, we're going to have to rely on techniques like this to effectively get, you know, more data efficient ways of learning from the same number of tokens. Right. So I think this is a really interesting place to kind of mess around and do algorithmic development and think deeply about, you know, like, how do we make these models work. Okay, so to wrap it all up and to put things together, right, what I wanted to talk about today was kind of the central role of data and how we can algorithmically intervene on data in many ways. Right. And the place at which I'm going to", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3937107_ms_-_3991997_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3937107, "end_ms": 3991997}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 3979985 ms - 4032745 ms", "content": "Title: CS224V Lecture 18 > Transcript > 3979985 ms - 4032745 ms\n\nContent: together, right, what I wanted to talk about today was kind of the central role of data and how we can algorithmically intervene on data in many ways. Right. And the place at which I'm going to intervene is on this kind of pre training style training because that's the place at which we can deploy the most compute and that might have the greatest amount of leverage. I think my perspective here is kind of the opposite of many of my academic colleagues. You know, in some sense pre training, very compute intensive, but also maybe the most exciting place to be doing algorithmic work because it has the greatest leverage. So the first part was kind of about data selection and that was basically saying there's this big wealth of information which is models other people have trained and we can use that to try to understand what does high quality data look like. And there's some promising signs that that's possible. In the second part, you know, I was much less talking about the compute bound", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_3979985_ms_-_4032745_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 3979985, "end_ms": 4032745}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4023401 ms - 4091187 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4023401 ms - 4091187 ms\n\nContent: that to try to understand what does high quality data look like. And there's some promising signs that that's possible. In the second part, you know, I was much less talking about the compute bound setting, but much more the data bound setting. And there, you know, synthetic augmentations and thinking about, you know, continued pre training allows you to build models that without using a retriever or any kind of like external augmentation can do really well in answering narrow domain specific questions. So that's kind of an exciting potential new direction coming up. So thanks a lot. If you have any questions, I'm happy to take them. Yes, I'm actually a psychology major. I study neuroscience and also doing plus in memory right now. And it seems that like for humans at least the strategy is to is when you study to form these connections, you want to get topics and see how they like. Once you link your topics to your overall knowledge, it's a lot more effective than just reading the", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4023401_ms_-_4091187_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4023401, "end_ms": 4091187}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4077123 ms - 4141747 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4077123 ms - 4141747 ms\n\nContent: is to is when you study to form these connections, you want to get topics and see how they like. Once you link your topics to your overall knowledge, it's a lot more effective than just reading the textbook. And are we seeing this plan to affect why the knowledge graph is much of a much better dataset than just naively picking data. And are there any other ways of potentially augmenting data to be more like how we train? Yeah, I think that's a very interesting question, and I think I'm still pretty on the fence about this in some ways about whether the human learning analogies are relevant or useful for training language models. On the one hand, I think we can think about curriculum and how to present knowledge to models in a way that's more easily learnable, inspired by human learning strategies. I think that's pretty interesting. On the other hand, we do know that when we do one pass of continued pre training, for example, models are infinitely less efficient than humans. I look at", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4077123_ms_-_4141747_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4077123, "end_ms": 4141747}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4131891 ms - 4182743 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4131891 ms - 4182743 ms\n\nContent: I think that's pretty interesting. On the other hand, we do know that when we do one pass of continued pre training, for example, models are infinitely less efficient than humans. I look at a textbook chunk, I'll roughly understand all the facts and their implications. You do one pass of autoregressive modeling on a text chunk, models are going to understand like 0% of what happened there. Right. And so I think there's this question of, like, do we need a new algorithm? Does like data augmentation emulate how humans learn? How do we close the gap? I think those are very interesting questions. I just don't know what the answer is. What I do know is, you know, one path training is very different from humans. So maybe when we close the gap, they will become similar. I see, yes. You mentioned the goal of a lot of frontier researchers is like, to create new knowledge. And like, with the empty graph framework, do you think think of that as like creating new connections or is it a better way", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4131891_ms_-_4182743_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4131891, "end_ms": 4182743}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4174877 ms - 4225321 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4174877 ms - 4225321 ms\n\nContent: the goal of a lot of frontier researchers is like, to create new knowledge. And like, with the empty graph framework, do you think think of that as like creating new connections or is it a better way to think about it? Yeah, yeah, that's right. I think. Well, I'm deeply skeptical of any kind of like, arguments that you can create knowledge with synthetic data. That just seems fundamentally really difficult. Like, I think about kind of two different things. One is, you know, text is often like a manifestation of interactions with the real world, right? Like the grounding. Like I go out and like, I don't know, have dinner at a restaurant. I have a bad experience, I route it, write about it online. That's a grounded experience that's been mapped to the Internet. Right. Since language models don't yet go out and have dinner, they can't possibly create this new piece of knowledge. What they can do, though, is they can look at all the bad reviews and say, you know, actually this restaurant", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4174877_ms_-_4225321_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4174877, "end_ms": 4225321}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4215385 ms - 4282445 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4215385 ms - 4282445 ms\n\nContent: don't yet go out and have dinner, they can't possibly create this new piece of knowledge. What they can do, though, is they can look at all the bad reviews and say, you know, actually this restaurant is probably on average bad. Right. So they can synthesize all these grounded interactions through reasoning. They can make, you know, implicatures based on that, and you can think of that as new knowledge, but they're essentially like what people might call deductive closures. Right. Like they're logical inferences from what's already available, not necessarily materializing new facts about the world. Oh yes, Creativity or new idea generated by. Because I remember you have another paper talk about the research idea generated by this. Yeah, I mean I think kind of yes, but also maybe that's not the major factor. So when I talk about kind of diversity losses and lack of creativity of LLMs, I think the biggest contributor to that is actually the alignment process. So if you look at different", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4215385_ms_-_4282445_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4215385, "end_ms": 4282445}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4270261 ms - 4325689 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4270261 ms - 4325689 ms\n\nContent: the major factor. So when I talk about kind of diversity losses and lack of creativity of LLMs, I think the biggest contributor to that is actually the alignment process. So if you look at different models at different stages of their training pipeline, the base model, the autoregressive next word prediction model, very creative in a way, it will output all sorts of crazy hallucinated stuff. But you'll be like, wow, that's very creative. You do supervised fine tuning, it's going to follow instructions better but less creative. And once you do all this RLHF alignment now you've really put the model on rails. And so I think that's where all the diversity losses come from. Continued pre training does, broadly speaking, make language models lose their capabilities. They're a little bit worse at in context learning. They're a little bit worse at basically picking the right options out of a multiple choice question. But those losses are generally small relative to the alignment tax, I guess", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4270261_ms_-_4325689_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4270261, "end_ms": 4325689}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4315001 ms - 4374605 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4315001 ms - 4374605 ms\n\nContent: in context learning. They're a little bit worse at basically picking the right options out of a multiple choice question. But those losses are generally small relative to the alignment tax, I guess that you might call it. Cool. Yes. So for our project we're actually part of it like we're generating stories. We're basically continuing the generation process of a story and we realize that the stories that's generating based on the previous plot, it's actually like pretty, it's decent. It's like pretty good. Yeah. And I just wanted, because you also talked about how it kind of like only knows a couple styles. Yes. So I'm curious about like, well, maybe we just haven't pushed it far enough so that all the stuff is the same. Yeah. I think maybe it only knows one thing is the wrong sort of maybe phrasing of it. Um, it's kind of like let's say you have a story but you want a million different continuations. Yeah, that's I think very difficult. Like I think models I think will eventually cap", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4315001_ms_-_4374605_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4315001, "end_ms": 4374605}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4365115 ms - 4416791 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4365115 ms - 4416791 ms\n\nContent: phrasing of it. Um, it's kind of like let's say you have a story but you want a million different continuations. Yeah, that's I think very difficult. Like I think models I think will eventually cap out on the different ways that it can continue the same story. Whereas I think if you have many different, you know, prefixes, many different starting points of the story, you'll get many different continuations, if you know what I mean. Like, this is what I mean by like externalized randomness is really important. If you change something about the prompt, you'll get a totally different response. But if you don't vary the prompt you'll often cap out. So it sounds like if we don't do the reinforcement, like if we just do the pre training part, autoregressive, that will be very creative. So why has is there like a model that does that? Well, I mean, yeah. Right. Creative I think is a double edged sword. Right. Like you want creativity in that. Oh wow, that's like I didn't think of that.", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4365115_ms_-_4416791_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4365115, "end_ms": 4416791}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4405103 ms - 4461363 ms", "content": "Title: CS224V Lecture 18 > Transcript > 4405103 ms - 4461363 ms\n\nContent: why has is there like a model that does that? Well, I mean, yeah. Right. Creative I think is a double edged sword. Right. Like you want creativity in that. Oh wow, that's like I didn't think of that. Creative, but also total gibberish is something you haven't thought about. Right. So I think there's this big balance and in a way there's a person here, Peter west, who's been kind of messing around with some of this research. But in a way alignment is really focusing towards the average case. Right. Like when you prompt a model and you need the answer to be like generally correct, you want alignment, you want boringness, you want inoffensive but good continuation. When there's a human in the loop and you're going to pick the best out of 1,000 different generations, the game is very different and the companies aren't really optimizing for the second use case. And there you might want a base model or a much more lightly aligned model. And I think those are very interesting variations of", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4405103_ms_-_4461363_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4405103, "end_ms": 4461363}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Transcript > 4451091 ms - 5238965 ms", "content": "and the companies aren't really optimizing for the second use case. And there you might want a base model or a much more lightly aligned model. And I think those are very interesting variations of this. Great. I think we're a few minutes early, but if there's no more questions, you know, I won't keep you. It, it, it.", "block_metadata": {"id": "CS224V_Lecture_18_>_Transcript_>_4451091_ms_-_5238965_ms", "document_type": "transcript", "lecture_number": 18, "start_ms": 4451091, "end_ms": 5238965}}
