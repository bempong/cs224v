{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Information retrieval and the structured data set", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Information retrieval and the structured data set\n\nContent: There's free text and structured data. For large documents, we want to do information retrieval using search. Then we started talking about structured data and hybrid, which combines them. This is a new data type, in a sense, it's a document set.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Information_retrieval_and_the_structured_data_set", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 7680, "end_ms": 561665}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Can we use information retrieval to make a decision about a resume?", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Can we use information retrieval to make a decision about a resume?\n\nContent: Can we use information retrieval to make this happen? Can we? Because if you're trying to compute trends, like if you only retrieve a subset of the documents, then you wouldn't label something for all of them. This is a very, totally different problem.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Can_we_use_information_retrieval_to_make_a_decision_about_a_resume?", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 561785, "end_ms": 696805}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > In the Elevator With LLMs", "content": "Title: CS224V Lecture 14 > Chapter Summaries > In the Elevator With LLMs\n\nContent: This is a manual document set analysis. This is not a CS project. It is a multi year, multi country project. Understanding the real world usage is very important. And then we're going to go and talk about what we have been doing in computer science.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_In_the_Elevator_With_LLMs", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 697185, "end_ms": 782389}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Event detection from social media for epidemic prediction", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Event detection from social media for epidemic prediction\n\nContent: A group of people were looking at event detection from social media for epidemic prediction. They applied it to another disease, the monkeypox disease. In a sense, if you are looking at the red line, it gives you earlier warning.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Event_detection_from_social_media_for_epidemic_prediction", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 782477, "end_ms": 1270015}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > The Search for Experts in Paper Reviews", "content": "Title: CS224V Lecture 14 > Chapter Summaries > The Search for Experts in Paper Reviews\n\nContent: To take one document, stick it into a structure once and for all. Once you are structured, I just do a query. If you know what to look for, you are already on your way being an expert. There are many projects going on in this class that can use this method.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_The_Search_for_Experts_in_Paper_Reviews", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 1270435, "end_ms": 1554115}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > The Code Book of Quantitative Coding", "content": "Title: CS224V Lecture 14 > Chapter Summaries > The Code Book of Quantitative Coding\n\nContent: How do you get a code book in the first place? How do you automate the qualitative coding? Can you just forward for more esoteric domains? What we are playing with is a generic Q and A system.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_The_Code_Book_of_Quantitative_Coding", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 1554855, "end_ms": 1760025}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Quantitative coding in the real world", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Quantitative coding in the real world\n\nContent: Aclid, the Armed Conflict Location and Event Data, collects data on violent conflict and protest in all countries and territories in the world. The big picture can only be put together by looking at all the specific instances. If you want to improve the world, the first thing you have to do is figure out what's happening.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Quantitative_coding_in_the_real_world", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 1760185, "end_ms": 2169229}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > What's the Problem with Annotation?", "content": "Title: CS224V Lecture 14 > Chapter Summaries > What's the Problem with Annotation?\n\nContent: How to differentiate what classifies an event and what doesn't and what should be the threshold to classify. Subjectivity in general, going off of what people have said. The standard in annotation and the quality of the result that is really hard to manage.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_What's_the_Problem_with_Annotation?", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 2169357, "end_ms": 2271045}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > The coding and sourcing process of CS", "content": "Title: CS224V Lecture 14 > Chapter Summaries > The coding and sourcing process of CS\n\nContent: Eclipse has covered 2 million events, manual, event, manually. 200 plus researchers. 80 plus languages, 243 countries or territories. The scale is huge. The accuracy is nothing like what we have seen in CS coding, CS annotations. How far can we make it so?", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_The_coding_and_sourcing_process_of_CS", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 2271585, "end_ms": 2491705}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Event detection and the semantic analysis", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Event detection and the semantic analysis\n\nContent: Event detection is already a long studied subject. The unit of analysis, sentence level, the extraction is done using keywords. The other thing that is very important is to understand spans that you extract, which we call extractive and abstractive.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Event_detection_and_the_semantic_analysis", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 2493125, "end_ms": 2692475}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Exhaustive vs Abstractive Linking in Microsoft SQL", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Exhaustive vs Abstractive Linking in Microsoft SQL\n\nContent: David Wheeler: Why do we need abstractive linking? To determine whether an event happens or not. To be specific about what you're referring as an event. Wheeler: Can do like many to many mapping which allows both to. Identify when they're referring to maybe the. Same entities in different ways.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Exhaustive_vs_Abstractive_Linking_in_Microsoft_SQL", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 2693375, "end_ms": 2915605}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Quantitative Coding", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Quantitative Coding\n\nContent: Sina: Let's talk about how we build it using the latest and greatest LLM technology. The first step is to figure out what the events are. The second one is for a particular event, I have to find the argument for those events. And the third one is abstractive entity linking.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Quantitative_Coding", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 2917145, "end_ms": 3270955}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > LLM 2.8", "content": "Title: CS224V Lecture 14 > Chapter Summaries > LLM 2.8\n\nContent: Given an event type, then you have to zoom in on the arguments. Arguments actually have the descriptions and this is what we rely on to get LLM to do it in zero shots. We're going to take advantage of the fact that LLMs actually know Python data structures. We take free text and turn them into Python classes.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_LLM_2.8", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 3271035, "end_ms": 3491405}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Structured Output in the LLM", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Structured Output in the LLM\n\nContent: Aaron: LLMs are actually good at creating structured out output. What it actually does is that it convert the Python cost definitions into a context free grammar. Can this affect performance of the LLM? Aaron: It's good to know that it's actually using grammar to guide its output.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Structured_Output_in_the_LLM", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 3491785, "end_ms": 4034673}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > The Rank GPT in C#", "content": "Title: CS224V Lecture 14 > Chapter Summaries > The Rank GPT in C#\n\nContent: The Rank GPT was originally designed for finding the best match among documents. There are 6.2 thousand entities and they won't fit in one prompt. Solution here is to split them up. You take the database and then you turn them into groups.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_The_Rank_GPT_in_C#", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 4034729, "end_ms": 4264085}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Llama 2.8: A high-quality data set", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Llama 2.8: A high-quality data set\n\nContent: 17,000 instances and highly, well, high quality annotated data sets. Can the automatic qualitative coding at QC help ACLID expand their coverage in terms of size of events of the different languages. Can we create new analysis with a lower level of effort? In context learning is doing extremely well compared to gold.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Llama_2.8:_A_high-quality_data_set", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 4274915, "end_ms": 4646385}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Chapter Summaries > Machine Learning 3.8", "content": "Title: CS224V Lecture 14 > Chapter Summaries > Machine Learning 3.8\n\nContent: The overall problem accuracy. We are at about close to 70%. These are not your, you know, we would like to be higher. That is still the bottleneck is the entity linking. As we get better, as the language model gets better with the lower resource languages, the accuracy will go up.", "block_metadata": {"id": "CS224V_Lecture_14_>_Chapter_Summaries_>_Machine_Learning_3.8", "document_type": "chapter summary", "lecture_number": 14, "start_ms": 4647365, "end_ms": 5161655}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 7680 ms - 168069 ms", "content": "Title: CS224V Lecture 14 > Transcript > 7680 ms - 168069 ms\n\nContent: Right. It's funny. D. All right, we're ready to start. All right, so in the last few lectures we're building up this picture, right? We say that, look, LLMs cannot be relied upon from, in terms of all the possible content. We are going to supply it with information so that it will be more helpful. And we talked about two sets of data. There's free text and structured data. And we started by talking about searching information from a large free text corpus. And that is information retrieval. You know, the basic idea is that if you, if you don't have to search, you just give it a single article, it will, you know, with LLM it can give you an answer anyway. So we say for large documents, we want to do information retrieval using search. And then we started talking about structured data and hybrid, which combines them. And we say that, look, there is another way of approaching this once you have structured, and that is you can do knowledge based queries, relational algebra. And with that", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_7680_ms_-_168069_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 7680, "end_ms": 168069}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 154025 ms - 223723 ms", "content": "Title: CS224V Lecture 14 > Transcript > 154025 ms - 223723 ms\n\nContent: which combines them. And we say that, look, there is another way of approaching this once you have structured, and that is you can do knowledge based queries, relational algebra. And with that you can do all kinds of things from filtering to projections or joins or ranks, all the things that you can do that you can see in SQL. And we say we can also go from tables to knowledge graphs because a lot of information cannot be really stored in tables. And you can do search, obviously you can find out anything about any particular movie that you are interested in. And you can do a lot more than that as a matter of you can do analytics, you can talk about the large, the min, you can sort them average, you can look at trends, you can look at all kinds of things. And then now the question is, we seem to be missing this corner, this square. Okay, what if I have free tax software, free text, and I want to do data analytics with it? What do we do? So we want, that's what we want to talk about", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_154025_ms_-_223723_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 154025, "end_ms": 223723}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 208661 ms - 275359 ms", "content": "Title: CS224V Lecture 14 > Transcript > 208661 ms - 275359 ms\n\nContent: seem to be missing this corner, this square. Okay, what if I have free tax software, free text, and I want to do data analytics with it? What do we do? So we want, that's what we want to talk about today is that suppose you have a document set. It's like lots of documents there. And I want you to analyze it and get you skip some analytics. It could be some decisions that we have to make. Maybe we are looking at trends, maybe we are accumulating them and so forth. So this is the, this is the topic of this class. It's a new data type, in a sense, it's a document set. And I have to look at every document. I cannot just say, oh, I need a restaurant. Just find me anything that looks like this and you just tell me one or two and I'll be very, very happy. That's a search problem. But if I really want to study and analyze it, I have to look at every document. And now that's Expensive. It depends on how big the document sets are. So here are some examples of things that go into this category.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_208661_ms_-_275359_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 208661, "end_ms": 275359}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 262645 ms - 327839 ms", "content": "Title: CS224V Lecture 14 > Transcript > 262645 ms - 327839 ms\n\nContent: want to study and analyze it, I have to look at every document. And now that's Expensive. It depends on how big the document sets are. So here are some examples of things that go into this category. And there are tons and tons of them. Okay. Starting from, for example, what I have to do. You know, every year we get a lot of students applying to come to Stanford. I have to go look at all the, all the students records and transcripts and resumes. And I have to decide like, you know, who do we want to interview? We have job applicants for faculty in general. You know, there's a big market on finding jobs hunters and so forth. There's a lot of resumes out there and you need to select the candidates. So here we want to rank them. You absolutely have to read every one of them. You just can't say, pull it out of the hat, oh, this looks pretty good, right? You really want to have the best of the set. So that's the document set analysis. We write papers and we review papers. We look at every", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_262645_ms_-_327839_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 262645, "end_ms": 327839}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 315943 ms - 385707 ms", "content": "Title: CS224V Lecture 14 > Transcript > 315943 ms - 385707 ms\n\nContent: pull it out of the hat, oh, this looks pretty good, right? You really want to have the best of the set. So that's the document set analysis. We write papers and we review papers. We look at every paper that is submitted. There are lots of papers these days submitted to AI conferences and we absolutely have to review them. Or we can write proposals here. Just some of the things that we do in academia, but of course in businesses there are plenty more. If you want to invest in companies or you want to evaluate how companies are doing, you have to look at the financial reports and you have to look at each one of them. And a lot of times here we are looking for the top n that we are interested in. So there is a sort function. But the key here is that you need to find all of them. So that's one set of problems. Another one is that you want to accumulate the results. So, for example, a lot of experimental results, they are working on a specific part of the genome. Or there is a project that", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_315943_ms_-_385707_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 315943, "end_ms": 385707}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 371251 ms - 439361 ms", "content": "Title: CS224V Lecture 14 > Transcript > 371251 ms - 439361 ms\n\nContent: set of problems. Another one is that you want to accumulate the results. So, for example, a lot of experimental results, they are working on a specific part of the genome. Or there is a project that is ongoing in this class which is to find out the relationship between drugs, proteins and diseases. And this relationship, every pair of these things requires a substantial amount of work. And there will be a paper that describes two or three of these relations. And now there are hundreds of thousands of such papers. So what do we know about drugs, diseases and proteins? You really have to put this information together. You have to accumulate this knowledge. Another example here is just events and news and so forth. You want to know what happened in the world, all around the world. And so you really want to put them all together so that you can ask questions, search for it, or do whatever it takes. So that is the knowledge base. So in this case, we may not be creating a knowledge base", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_371251_ms_-_439361_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 371251, "end_ms": 439361}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 428169 ms - 496593 ms", "content": "Title: CS224V Lecture 14 > Transcript > 428169 ms - 496593 ms\n\nContent: you really want to put them all together so that you can ask questions, search for it, or do whatever it takes. So that is the knowledge base. So in this case, we may not be creating a knowledge base that organizes everybody. We just have to pull out the best candidates that we should Consider, but here we absolutely want to put everything into one gigantic knowledge base. And you can do whatever knowledge base that you need for knowledge base cases. Another group of things that is interesting is that if you have a lot of these information from news, from social media and so forth. For example, if I'm doing user studies, you interview all the people and then after the interview, what does it say? You have to summarize it. And then you say, oh, it's a good idea or a bad idea. This concept here of coding or understanding what are the interviews is actually the concept is outside of computer science. This is what people do in psychology and so forth. Then in the same way, if I'm doing", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_428169_ms_-_496593_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 428169, "end_ms": 496593}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 478871 ms - 553185 ms", "content": "Title: CS224V Lecture 14 > Transcript > 478871 ms - 553185 ms\n\nContent: here of coding or understanding what are the interviews is actually the concept is outside of computer science. This is what people do in psychology and so forth. Then in the same way, if I'm doing studies on medical records, I want to detect the trends. I say, well, if you have this kind of drug, if you use these drugs, there seems to be this kind of an outcome. We are not so interested on what happens to the individual patient in this study. I am not putting every. It is not important in a sense to say exactly what happens to each individual. We're just looking at the trends and so forth. We're discovering knowledge. And so, for example, you don't have to, for social media, you may just sample them. You don't actually have to read every one of them. So there are different characterizations, but nonetheless all of them as asking the question is like, I give you a document set, they're usually of the same kind. It's not the Internet that contains everything. Okay, there are one kind", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_478871_ms_-_553185_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 478871, "end_ms": 553185}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 542449 ms - 626505 ms", "content": "Title: CS224V Lecture 14 > Transcript > 542449 ms - 626505 ms\n\nContent: but nonetheless all of them as asking the question is like, I give you a document set, they're usually of the same kind. It's not the Internet that contains everything. Okay, there are one kind of documents. And out of these documents you have to perform some of these functions. So can you do. Can we use information retrieval to make this happen? Can we? Yes, Michael, I want to say no. Because if you're trying to compute trends, like if you only retrieve a subset of the documents, then you wouldn't label something for all of them. And so who knows what you're missing. You try to retrieve a subset of the documents. So if you have to stretch it, you would say the information retrieval will retrieve all the relevant ones. First of all, can I locate them? And when I locate them, can I look? Can we then look at all of them that is returned? Because usually we just look at the high ranking ones. What metric would it be that I am looking? If I'm looking for resumes, what am I searching for?", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_542449_ms_-_626505_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 542449, "end_ms": 626505}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 610581 ms - 687817 ms", "content": "Title: CS224V Lecture 14 > Transcript > 610581 ms - 687817 ms\n\nContent: we then look at all of them that is returned? Because usually we just look at the high ranking ones. What metric would it be that I am looking? If I'm looking for resumes, what am I searching for? The resume cannot be asking one question at a time. It is like somebody have stronger recommendations, some have stronger publications. It is multidimensional in the evaluation. What kind of questions am I asking? So I can imagine asking the questions and polling things sometimes is it Semantic, Is it something that you index? Not clear. But suppose I just say, suppose I was able to retrieve them and how do I compare across them if I don't, If I just see a set of documents, suppose I give you a smaller set of documents, how do you evaluate them? All right, so those are the considerations, right? So we were saying that this is a very important problem, but information retrieval, the method that we have been talking about, is really invented for searching things on the Internet. Okay? That was", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_610581_ms_-_687817_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 610581, "end_ms": 687817}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 675319 ms - 740559 ms", "content": "Title: CS224V Lecture 14 > Transcript > 675319 ms - 740559 ms\n\nContent: So we were saying that this is a very important problem, but information retrieval, the method that we have been talking about, is really invented for searching things on the Internet. Okay? That was the beginning of that kind of a search. So this is a very, totally different problem. So that's why we are paying attention to this, because this is a very important problem. So this is the outline of what we're going to do today. We're going to start by talking about this concept of analysis across the data document set. We'll provide some more motivations. Then we're going to look at this very specific case of document set analysis. This is a manual document set analysis. Okay? This is not a CS project. It is a multi year, multi country project. And we're going to talk about that. It's called aclid. And understanding the real world usage is very important. And then we're going to go and talk about what we have been doing in computer science. It turns out that if you just pick one of", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_675319_ms_-_740559_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 675319, "end_ms": 740559}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 726865 ms - 804155 ms", "content": "Title: CS224V Lecture 14 > Transcript > 726865 ms - 804155 ms\n\nContent: aclid. And understanding the real world usage is very important. And then we're going to go and talk about what we have been doing in computer science. It turns out that if you just pick one of these use cases, which is event detection, people have spent a lot of time working on that in the NLP literature. Then after looking at that, we're going to talk about a paper that we have just finished and it is about how do we do automatic qualitative coding using LLMs. We talk about the techniques, talk about the data set. This is a very different data set because we take it out of Aclid. It is not hiring AmTurk users to annotate this. This is annotating for real. And then we talk about the evaluation. So this is what we're going to do today. Okay, so there are lots of work on event detection or analyzing data set. But this is a very interesting one that I want to pull out to you to use it as a driving example. So this group of people were looking at event detection from social media for", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_726865_ms_-_804155_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 726865, "end_ms": 804155}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 789325 ms - 866899 ms", "content": "Title: CS224V Lecture 14 > Transcript > 789325 ms - 866899 ms\n\nContent: or analyzing data set. But this is a very interesting one that I want to pull out to you to use it as a driving example. So this group of people were looking at event detection from social media for epidemic prediction. Well, we all just lived through Covid and at the time when it was new, you know, in May 2020, there were really 20 millions a day talking about COVID Okay? And so the question is if, you know, people are still learning about this new disease. When you have a new disease, the only signal is coming from the individuals. Okay? It's not like somebody declares there's A new disease, let's look for it. No, people are saying, I'm sick, I'm all this. You have to look at the individual reports to even notice that something is going on. So this detection is really important. So the whole idea is that the tweets are the individual documents and there are millions of them. And you want to look at them and you want to say, oh, there is a new outbreak. People don't use the word", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_789325_ms_-_866899_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 789325, "end_ms": 866899}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 853331 ms - 929307 ms", "content": "Title: CS224V Lecture 14 > Transcript > 853331 ms - 929307 ms\n\nContent: the whole idea is that the tweets are the individual documents and there are millions of them. And you want to look at them and you want to say, oh, there is a new outbreak. People don't use the word COVID 19. They don't even know what it is, and you have to detect it. That is the subject of this paper. And so in order to understand that this is relevant to an epidemic, there is an ontology. You say that, look, it is about an epidemic. Then I'm interested in the infection, the spread, the symptoms, how do you prevent it, how do you control it, how do you cure it? And then, of course, there are the reports of deaths, and people are tweeting about all these things. So you are using these words to gather the information. Okay, so, you know, it is not like, oh, I'm just going to look for the word COVID 19. It did not exist at that time. Okay? So this is the premise, and what they have done is to apply it to another disease. This is the monkeypox disease. In August 14, 2024, WHO declares", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_853331_ms_-_929307_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 853331, "end_ms": 929307}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 909331 ms - 983211 ms", "content": "Title: CS224V Lecture 14 > Transcript > 909331 ms - 983211 ms\n\nContent: word COVID 19. It did not exist at that time. Okay? So this is the premise, and what they have done is to apply it to another disease. This is the monkeypox disease. In August 14, 2024, WHO declares that monkeypox is a global health concern. Okay? But if you look at the tweets, you can already see that there is something going on that is different from before. So the blue signal here is the true trend. You know, they went back to the history in history and then they start to collect all the information and you see a nice curve like this, but you don't know what's going on at the beginning. There are not that many cases, but that's the blue curve. So what this group was doing is using automatic techniques to analyze the tweets. Obviously, there are too many tweets for you to be doing it manually. So they use this automatic technique, and that is that red line. The red line shows a little peak in June, a little peak in July, but you can absolutely see that it is not the same as before.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_909331_ms_-_983211_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 909331, "end_ms": 983211}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 969355 ms - 1062971 ms", "content": "Title: CS224V Lecture 14 > Transcript > 969355 ms - 1062971 ms\n\nContent: So they use this automatic technique, and that is that red line. The red line shows a little peak in June, a little peak in July, but you can absolutely see that it is not the same as before. And then it spikes up. And when it spikes up is when it kind of coincides with the time that WHO declares that as an epidemic? So in a sense, if you are looking at the red line, it gives you earlier warning. In fact, it gives you four to nine weeks earlier warning. In this case, yes. The bottom label says 2022. Oh, let me see. I feel like it was like longer ago. Maybe this is 2022, you're saying, of course, August 24th is too late. Yeah, I think that was a mistake. Good point. That would just be two, three months ago. So it is 2022. I think I copied it wrong. Thank you. Yeah, we're just the beginning of November 2024. Right. Why did it drop so sharply when who declared it? Or around that time? So the red is. That's an interesting question. I mean, I think that it is a matter of the. It's what", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_969355_ms_-_1062971_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 969355, "end_ms": 1062971}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1039897 ms - 1130249 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1039897 ms - 1130249 ms\n\nContent: of November 2024. Right. Why did it drop so sharply when who declared it? Or around that time? So the red is. That's an interesting question. I mean, I think that it is a matter of the. It's what people are talking about. Okay. They are not talking about it. It's not the new phenomenon. And so that is very different. I don't think that monkeypox is like Covid, where it just kind of keeps blowing up in big ways, but you can see the trend. So that's a very good example of what you can do with epidemics, with even detecting epidemics. And there's almost no other way, almost for you to find out information like that, because it really has to come from the individuals. So what we see here is that a typical technique is that you don't leave it in free text. You actually code it. So, for example, in this case with epidemics, you care about people talking about being sick, dying, and, you know, getting infected and so forth. The symptoms, okay? So those are the key words or the key concepts", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1039897_ms_-_1130249_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1039897, "end_ms": 1130249}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1116433 ms - 1187463 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1116433 ms - 1187463 ms\n\nContent: in this case with epidemics, you care about people talking about being sick, dying, and, you know, getting infected and so forth. The symptoms, okay? So those are the key words or the key concepts or key meanings of messages that you have to attend to. So in. So all this information about how you know, what is relevant is code is put into a code book. This word code book doesn't have anything to do with CS program code. It is actually a word that the psychologists are using for their purposes when they code up interviews, when they try to analyze across different interviews and so forth. So that's a code book. So this code book describes what is in this data collection. They kind of turned it into some structure. There's a structure, and they describe what the contents are associated with each of the fields in the structure and the layout and so forth. So that's what the code book looks like. So now it breaks it down into two parts. You take all the documents, you take every document", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1116433_ms_-_1187463_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1116433, "end_ms": 1187463}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1174415 ms - 1243913 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1174415 ms - 1243913 ms\n\nContent: each of the fields in the structure and the layout and so forth. So that's what the code book looks like. So now it breaks it down into two parts. You take all the documents, you take every document and code it, and then you now put it into some structure. Of course, you can have free text in some of the fields, and then you can analyze it and you can use the methods that we have been talking about in the last class. So that's the concept, and that is called qualitative coding. Okay? Not programming, but qualitative coding according to this code book. Sometimes we also say the word schema to refer to the code book. You know, the schema of a database. And qualitative coding has also been referred to as information extraction in CS literature. But qualitative coding is a more general term that is out used outside of computer science. So the code book is really to capture what the experts think of when they want to evaluate and analyze a document. And by the way, it can be very personal", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1174415_ms_-_1243913_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1174415, "end_ms": 1243913}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1226851 ms - 1295687 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1226851 ms - 1295687 ms\n\nContent: is out used outside of computer science. So the code book is really to capture what the experts think of when they want to evaluate and analyze a document. And by the way, it can be very personal because I'm sure that, you know, how I look at a resume or how I accept my, you know, if I do my own. Well, we do look at resumes and say, oh, I'm interested in this person and that person. And different professors will have different preferences or different evaluation metrics. Okay, so the codebook in some cases can be very, very personalized. So we talked about the different data sets and each data set has its own kind of things that you worry about. If you talk about news, we will talk about where the source of the news is, what are the events, what are the parameters or social media, maybe there are people have repl and ratings to talk about how popular it is. Resumes, the standards, things where everybody knows about them, the names, address, education, employment, publication and so", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1226851_ms_-_1295687_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1226851, "end_ms": 1295687}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1283803 ms - 1352087 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1283803 ms - 1352087 ms\n\nContent: there are people have repl and ratings to talk about how popular it is. Resumes, the standards, things where everybody knows about them, the names, address, education, employment, publication and so forth. So I may give you some form, I may not be. It's just maybe all free text and you want to kind of extract and distill out that information. Right? Once you distill it out, I can say, for example, oh, let me find out what are all the students that come from Stanford. Okay, so once you are structured, I just do a query. I'm not going to look at every document and ask LLM to say which school does this person attend? Because if you ask that question given a resume which is small enough in this particular case, the LLM can extract that. But what we are doing now is that we want to extract this information once and for all. To take one document, stick it into a structure once and for all. And now when I do search on the structure, it will be efficient. Okay, so that's the basic idea in", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1283803_ms_-_1352087_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1283803, "end_ms": 1352087}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1338829 ms - 1410575 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1338829 ms - 1410575 ms\n\nContent: this information once and for all. To take one document, stick it into a structure once and for all. And now when I do search on the structure, it will be efficient. Okay, so that's the basic idea in terms of paper reviews. We talk about the topic area, originality. It's very subjective in some of these cases. Research proposals, relevance, soundness, potential and so forth. And then got a little bit tired, so I just stick those things in and I called GPT and I said do you know what other relevant concepts? Sure enough, it does a pretty good job. Okay, so the concepts of having concepts associated with each topic is not new to people and definitely not new to GPT. So everybody, if you give me a general topic, I know the kind of things to look for, but that's the more general stuff. Okay. But things can get hard for any of the specialized domains. So for example, if you ask me to review an NLP experimental paper, one of the things I would ask is, does it use a synthetic dataset? Okay.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1338829_ms_-_1410575_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1338829, "end_ms": 1410575}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1395065 ms - 1471941 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1395065 ms - 1471941 ms\n\nContent: But things can get hard for any of the specialized domains. So for example, if you ask me to review an NLP experimental paper, one of the things I would ask is, does it use a synthetic dataset? Okay. Last time I talked about the fact that I don't care much about synthetic data sets because I have done enough experiments where I don't have to do much and it will be at 97% accuracy and it has zero. It means nothing to real world data. So that's an example of a metric or a characterization that is very narrow, very focused on the particular style of NLP research. And this kind of concepts are derived by. They are not written down anywhere. But if you ask the experts, they will tell you what are the things I look for if I see a patient, the first thing, given these general descriptions, I may know of a special question that I have to ask. Are you drinking enough water? It really is very, very specific. And if you know this map of things to look for, then you have a chance to behave like", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1395065_ms_-_1471941_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1395065, "end_ms": 1471941}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1454645 ms - 1531407 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1454645 ms - 1531407 ms\n\nContent: know of a special question that I have to ask. Are you drinking enough water? It really is very, very specific. And if you know this map of things to look for, then you have a chance to behave like an expert. And I would almost say that if you know what to look for, you are already on your way being an expert, because that is the key. Does that make sense to people here? What kind of examples do you guys have that you know of that is not on here? Any projects that can use this method? There are many projects going on in this class and I think some of them can use this method. Give me some examples. Yeah, Eric, sort of a meta example, like with a lot of these topics. One question that I would be interested in is like, what are the trends over time in certain sets of documents? Like how does the. How is this company's earnings changing over time? Trends are. Yes, totally. I know. And those are the. Another way of saying it is that these are the properties that you need to look for to do", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1454645_ms_-_1531407_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1454645, "end_ms": 1531407}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1515743 ms - 1584331 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1515743 ms - 1584331 ms\n\nContent: the. How is this company's earnings changing over time? Trends are. Yes, totally. I know. And those are the. Another way of saying it is that these are the properties that you need to look for to do the evaluation and make decisions. And looking at trends in general is very, very important. That's a very good point. But these are all the things that make a difference to what you need to do with this document. Like hiring a person or finding a diagnosis or a treatment. That's the idea. Is there any topic or any domain that you can think of that is not included here? That is pretty obvious. Is. Or that, that you find interesting. Yes, but when at least my research. We look at energy efficiency in buildings. Yes. So when we do that, there's like a bunch of things I would look for. Oh, you have a heat pump. What's the installation material? How like you have the solar panel. Yeah. Storage. Like all of these things Right. And these can all be like things. You look for and they just write", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1515743_ms_-_1584331_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1515743, "end_ms": 1584331}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1571659 ms - 1646365 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1571659 ms - 1646365 ms\n\nContent: a heat pump. What's the installation material? How like you have the solar panel. Yeah. Storage. Like all of these things Right. And these can all be like things. You look for and they just write it down somewhere. Yeah. We have a project going on. Who is the person is a student. Here is a project going on looking at justification of government budgets, of government spending. That's an example. There's a lot of. You know, in that case, it is kind of worse than any of these examples here because those budget justification is very, very long, you know, many, many, many pages. But it's the same thing. I'm looking for this. The arguments that I make me, you know, that make me. That allow us to decide if you can actually justify the budget. Okay, and how are they doing it? How you compare them? Because once you structure them, you can compare them. Right. All right. So it's important. Right? Agreed. Yes. Okay. So how do we do this? So there are really two kinds of questions. The first one", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1571659_ms_-_1646365_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1571659, "end_ms": 1646365}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1629397 ms - 1706959 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1629397 ms - 1706959 ms\n\nContent: Because once you structure them, you can compare them. Right. All right. So it's important. Right? Agreed. Yes. Okay. So how do we do this? So there are really two kinds of questions. The first one is how do you get a code book in the first place? As I said, it is the kind of things that an expert attend to for their job. And so one thing is to ask the expert, and they are kind of created by the experts because they know those are the important aspects. Another thing that we have been playing with is to say that sometimes you can't, you know, you just forward for more esoteric domains. I mean, it's not just the standard thing like resumes and so forth. They may not be able to come up with this, but if so, what we are playing with this idea is that if I give you a generic Q and A system, such as Storm or such as a chat, and you let the user ask questions, as the user or the expert ask questions, you understand why, you know what the users are asking for. And then you take that, and", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1629397_ms_-_1706959_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1629397, "end_ms": 1706959}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1692229 ms - 1758841 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1692229 ms - 1758841 ms\n\nContent: such as Storm or such as a chat, and you let the user ask questions, as the user or the expert ask questions, you understand why, you know what the users are asking for. And then you take that, and then you try to make a schema out of it. So, for example, if a doctor asks the question, does this patient smoke? And I say, oh, that might be interesting. So maybe we will ask. You can create a schema with questions about keeping track of whether they smoke or whether they drink or whether they have substance abuse and so forth. You can even just from some of the questions, you can come up with a schema without waiting for the questions to be asked. So these are the things that you can imagine doing. So that's the first thing is to get a code book. But a lot of times they exist. Then if I give you a code book, how do you automate the qualitative coding? How do you take a set of documents and give you the structure according to the code book? And this is the lecture today, is to focus on", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1692229_ms_-_1758841_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1692229, "end_ms": 1758841}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1745061 ms - 1821285 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1745061 ms - 1821285 ms\n\nContent: you a code book, how do you automate the qualitative coding? How do you take a set of documents and give you the structure according to the code book? And this is the lecture today, is to focus on the Second question. Okay, so now let's talk about qualitative coding in the real world. You know, a lot of times when we do nlp, we just talk about what other papers are doing. But I think it is actually really interesting to see things in practice. So this is a group called aclid, the Armed Conflict Location and Event Data. And this is their tagline. It's an independent, impartial, international, nonprofit organization collecting data on violent conflict and protest in all countries and territories in the world. That's hard and that is very broad. And this is also very important because if nobody is organizing this or doing this, we would have a very poor picture of what is going on in the world. So, for example, the UN's International Organization for Migration, they use ACLID to track", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1745061_ms_-_1821285_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1745061, "end_ms": 1821285}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1803325 ms - 1874221 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1803325 ms - 1874221 ms\n\nContent: is organizing this or doing this, we would have a very poor picture of what is going on in the world. So, for example, the UN's International Organization for Migration, they use ACLID to track the movement and needs of displaced people in over 80 countries. Another example is that the US Multi Agency Global Fragility Act Secretariat uses ACLID data to promote stability across five priority countries. Okay, so somebody has to do this research. And this is an organization that keeps track of very important topics around the world. So different organizations don't have to do their own research. And we are seeing more and more of that. So this is the, you know, you cannot just say, I'm a newspaper, I'm going to go write a paper about this. How am I going to make this happen? Because this is a pretty difficult task. And so they have amazing data. You should go visit their website. There are all kinds of information. So for example, they have a graph on the violence against civilians in", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1803325_ms_-_1874221_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1803325, "end_ms": 1874221}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1860613 ms - 1931987 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1860613 ms - 1931987 ms\n\nContent: a pretty difficult task. And so they have amazing data. You should go visit their website. There are all kinds of information. So for example, they have a graph on the violence against civilians in Ukraine. And there are many, many, many dots that talks about the violence targeting civilians, air and drone strikes. We read some newspapers at a high level, but here they tell you which location do they see a drone strike? Okay, this is coming from local reports by people in Ukraine. And this is how you can get the detailed information. And the big picture can only be put together by looking at all the specific instances. Okay, so. Or armed clashes, shelling events. The things that I highlighted are violence targeting civilians. And you see a lot of things by looking at this picture. This is the accumulation of results. If I give you a whole bunch of news articles, it's like, how much can you read? And by the way, what languages are them? Are they in? You cannot read them. But now when", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1860613_ms_-_1931987_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1860613, "end_ms": 1931987}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1918419 ms - 1994809 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1918419 ms - 1994809 ms\n\nContent: the accumulation of results. If I give you a whole bunch of news articles, it's like, how much can you read? And by the way, what languages are them? Are they in? You cannot read them. But now when they put the dots with different sizes on this graph, you notice a lot more things. You know, where the region being bombed, being attacked, and there's a lot of civilian attacks. Total events is 7,000, 381 just on that square. Okay, that's that particular button There. And there are tons and tons of events that have to be. Have to. These are all manually coded. There's a sexual violence around the globe. There's all the. The information. It is not everything. It is the place where they focus on. Okay, I'm sure that there are plenty more in America, but that is not the focus of their work. So you don't see it there. But, you know, I'm sure there, you know, they're just so. But if you look at the areas that they focus on, you can see that there's a lot of violence in Africa, and you can tell", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1918419_ms_-_1994809_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1918419, "end_ms": 1994809}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 1979923 ms - 2063345 ms", "content": "Title: CS224V Lecture 14 > Transcript > 1979923 ms - 2063345 ms\n\nContent: don't see it there. But, you know, I'm sure there, you know, they're just so. But if you look at the areas that they focus on, you can see that there's a lot of violence in Africa, and you can tell where it is down to the lowest level of details. India and so forth and in various places. And you can. So another example that is closer to home is tracking political demonstrations. So they do America. They do all around the world. This is September 2024. And what are these? These are violent and military events and demonstrations and strategic development. Okay, so how the events are related to the strategic development. So, for example, in September 2024, there are 693 demonstration events, which is 14% increase. And they are demonstrating against. And it's anti Trump demonstrations following the September 10th presidential debate. Okay, so what you're seeing is the granularity. It's not this big picture of. Oh, yeah, there are a whole bunch of things here. You can get to the specific", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_1979923_ms_-_2063345_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 1979923, "end_ms": 2063345}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2051841 ms - 2132955 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2051841 ms - 2132955 ms\n\nContent: the September 10th presidential debate. Okay, so what you're seeing is the granularity. It's not this big picture of. Oh, yeah, there are a whole bunch of things here. You can get to the specific date for the different purposes based on the reports. And I certainly did not know that there are 693 events in September 2024. If you want to improve the world, the first thing you have to do is to figure out what's happening in the world. This is the various radical groups and what's happening with the proud boys and so forth. It was a big deal in 2020, you know, the last election. And, you know, and so this is all kinds of information that you get from performing the same thing, which is analyzing the events that are reported in the news and social media. Okay, how do you do it? That is a lot of information, a lot of data. And where do you think the challenges are? Let's. What do you think? You know, I want to start this organization. And you say, but yes, I'm thinking maybe also, like", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2051841_ms_-_2132955_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2051841, "end_ms": 2132955}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2116822 ms - 2187903 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2116822 ms - 2187903 ms\n\nContent: of information, a lot of data. And where do you think the challenges are? Let's. What do you think? You know, I want to start this organization. And you say, but yes, I'm thinking maybe also, like the. How confident we are of the sources that we use. Yes. How can we choose the sources that are actually doing. This source is very important. Very good point. You know, you can't just. There's a lot of fake news out there, Right. You talk about fake news. Right. So you have to really know what your sources are. What else is scary? Yvette, Maybe like the data is probably like very unstructured. You have to speak up. Oh, sorry. Maybe the data is like very unstructured. Like you could imagine. They're all very unstructured. They're all just text somewhere. It's like, oh, you know, here's what happened here. Anything else people can think of that worries you? Yes, I'm also thinking how to differentiate what classifies an event and what doesn't and what should be the threshold to classify.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2116822_ms_-_2187903_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2116822, "end_ms": 2187903}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2172917 ms - 2249755 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2172917 ms - 2249755 ms\n\nContent: happened here. Anything else people can think of that worries you? Yes, I'm also thinking how to differentiate what classifies an event and what doesn't and what should be the threshold to classify. Yes, it's tricky. Tricky, right. It is not like how. It's not just a number. Right. What kind of event, how do I classify it? That's tricky. What else? Yeah, Michael. Well, if you're annotating based on a code book, you have to define the categories and people might refer to the same things in different ways and stuff like that. So you have to make sure you don't have like, don't categorize the same thing differently. Yes. It's very ambiguous. If I just say, oh, do it, how do you do it? And it's like, what are the chances we'll code the same way, different people the same way. And if you don't, the quality of the data is not good enough. So what you were talking about here is the standard in annotation and the quality of the result that is really hard to manage. Any other points that", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2172917_ms_-_2249755_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2172917, "end_ms": 2249755}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2231775 ms - 2311143 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2231775 ms - 2311143 ms\n\nContent: don't, the quality of the data is not good enough. So what you were talking about here is the standard in annotation and the quality of the result that is really hard to manage. Any other points that somebody can. Yeah, Aaron, Subjectivity in general, going off of what people have said, like anytime there's a gray area, like does this thing count? Or how do I interpret this? The interpretation, and it is not even clear there is subjective elements involved. Very, very good. This is really important. So let's look at some other aspects that you have not mentioned. One of the things is the scale. All right? You saw how detailed it is for a tiny little example of politically motivated protests. And that's that many events already in America. So now there are 150,000 online news articles being published each day. The sources are more online news articles. The analysis can. And this is continuous. It is not like I'm doing one experiment. I want to know what happens to monkeypox and then I", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2231775_ms_-_2311143_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2231775, "end_ms": 2311143}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2296087 ms - 2371291 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2296087 ms - 2371291 ms\n\nContent: published each day. The sources are more online news articles. The analysis can. And this is continuous. It is not like I'm doing one experiment. I want to know what happens to monkeypox and then I do it for two days or two or five months. Now we are talking about a continuous effort through forever. Absolutely forever. Global analysis means that they are in different languages. How many languages do we know and who are you going to get to recover all the languages? So this is truly global. So Eclipse has covered 2 million events, manual, event, manually. Okay, 2 million events, 80 plus languages, 200 plus researchers. They are part time usually, but they have to work Every week, because it's news story every week. And 243 countries or territories. The scale is huge. And they have been going on for I think close to 20 years actually, as I recall. So that's the scale. The second question is accuracy. I think we have touched on many of these issues, is that it's very. And in order to", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2296087_ms_-_2371291_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2296087, "end_ms": 2371291}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2357215 ms - 2428045 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2357215 ms - 2428045 ms\n\nContent: going on for I think close to 20 years actually, as I recall. So that's the scale. The second question is accuracy. I think we have touched on many of these issues, is that it's very. And in order to handle that, they have actually have a very elaborate manual coding and review process. Okay, and so this is what they wrote about the coding and sourcing process. It is an experienced. These are. These are all experienced researchers who have local context. It's not like me reading something in Africa. I was like, I don't understand. Who are they referring to? They have a lot of rules on who, what and when and so forth. And they. And then you talk to each other and so forth. So this is a really long professional process. On top of that, after they have come up with the review, we actually have three level. I mean, come up with the first annotations. It's three rounds of reviews. This is what it takes. You cannot skip any one of them. The researchers are reviewing it to ensure intracoder", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2357215_ms_-_2428045_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2357215, "end_ms": 2428045}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2414047 ms - 2497833 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2414047 ms - 2497833 ms\n\nContent: three level. I mean, come up with the first annotations. It's three rounds of reviews. This is what it takes. You cannot skip any one of them. The researchers are reviewing it to ensure intracoder reliability. And then there are the research managers. And then finally there is a final reviewer who makes sure that the data are correct. Okay, so this is a very elaborate process. And are there still mistakes? Yes, but the accuracy is nothing like what we have seen in CS coding, CS annotations. We have so many errors in the dataset that we have worked with. So what we are talking about is that on one hand there's a huge scale problem, you would like to automate it. On the other hand, they really needed to be accurate and automatic methods are not accurate so far. So how far can we make it so? That is the tension between these two things. We can use automatic coding, but are we good enough? That is the big problem. Let's talk a little bit about what people have done in the past. It turns", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2414047_ms_-_2497833_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2414047, "end_ms": 2497833}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2479365 ms - 2551947 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2479365 ms - 2551947 ms\n\nContent: That is the tension between these two things. We can use automatic coding, but are we good enough? That is the big problem. Let's talk a little bit about what people have done in the past. It turns out that just event detection, which is one class of document set analysis, is already a long studied subject. Starting in 1970s, there is a data set in ACE that is published in 2005. But because of the limits of technology, there are a lot of simplifying assumptions. The unit of analysis, sentence level, the extraction is done using keywords. In the past, keywords and rules, the emphasis are on words and spans. Spans are just a set of consecutive words in the text and it's not based on semantics and you can imagine what that means. So, for example, if you give me this article here, then we would pick out the fact that there are civilians, it is killed is the event that is being mentioned. The last weeks, the last days is like, what last week's event? What last days? Okay. Because that's in", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2479365_ms_-_2551947_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2479365, "end_ms": 2551947}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2539873 ms - 2620355 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2539873 ms - 2620355 ms\n\nContent: pick out the fact that there are civilians, it is killed is the event that is being mentioned. The last weeks, the last days is like, what last week's event? What last days? Okay. Because that's in the span. In order to figure out what the absolute thing is. It's not that trivial, and it was not covered in the previous work. So the sentence is not often not enough. And you have these uninformative events. And it is also. I like this example as, like, you're looking for numbers. When I say many civilians, you say zero people. They're not matching because they're just looking at words. So. And there are. So this is just not enough. The other thing that is very important is to understand spans that you extract, which we call extractive and abstractive. Abstractive means I give you the meaning. Okay? If you say they, that is a span. But if I mention the exact name, even though you know the single sentence says they. But you cannot just say they. So you have to turn it into the meaning.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2539873_ms_-_2620355_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2539873, "end_ms": 2620355}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2606723 ms - 2685791 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2606723 ms - 2685791 ms\n\nContent: Okay? If you say they, that is a span. But if I mention the exact name, even though you know the single sentence says they. But you cannot just say they. So you have to turn it into the meaning. And that concept of entity linking is called abstractive. And that makes a huge difference. So, for example, in a story, you may talk about the workers, but if it is referring to labor, it's important to know, because we are tracking laborers and so forth. So an example here is, for example, cwa. It's an organization that is mentioned on the entity list, but at the same time it is affiliated with AFL cio. You also want to know that it's the case. So that in the. When I am looking at other events that have to do with AFL cio, I know to include this one because, see, because CWA is affiliated with that organization. Okay, so it is. But you have to know. How do you know that CWA is actually affiliated with AFL cio? But if you don't note that this is affiliated with it, that when you do the", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2606723_ms_-_2685791_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2606723, "end_ms": 2685791}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2668973 ms - 2746985 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2668973 ms - 2746985 ms\n\nContent: with that organization. Okay, so it is. But you have to know. How do you know that CWA is actually affiliated with AFL cio? But if you don't note that this is affiliated with it, that when you do the summaries on AFL cio, you will get it wrong. Okay? So this is very difficult, but you have to do that. All right. So what is the advantage of abstractive linking? It is hard to do. Why do we need abstractive linking? Yes, David. To determine whether an event happens or not. To be specific about what you're referring as an event. Oh, I'm talking about extractive versus abstractive. Right. Focusing on the extractive versus abstractive. What is the relationship? Yes, we have to, you know, you have to detect what are the actors, for example, in an event. And, you know, it is happening. But the old way, we're just putting a span in. And the real new way, we say, you give me a list of entities. And I'm going to map it into one of these entities. Okay. I don't just say they or you know, the", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2668973_ms_-_2746985_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2668973, "end_ms": 2746985}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2735513 ms - 2813897 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2735513 ms - 2813897 ms\n\nContent: old way, we're just putting a span in. And the real new way, we say, you give me a list of entities. And I'm going to map it into one of these entities. Okay. I don't just say they or you know, the group in India. I want to know which group it is. Why do I need that in document set analysis? Yeah, because you can do like many to many mapping which allows both to. Identify when they're referring to maybe the. Same entities in different ways. So like many to one and also when there's links between entities, you know, one to many matchings. Yeah, so exactly. All right. If every record use a different word or using a subset of the relationship, you cannot just say, oh, here is organization, I want to know all the events it's related to. And you wouldn't even know that they are the same one and you cannot find them all. So in order to actually make some conclusion on what happens to a particular to the data, you really need to turn it to abstractive entity linking. And what it says to me", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2735513_ms_-_2813897_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2735513, "end_ms": 2813897}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2793529 ms - 2873785 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2793529 ms - 2873785 ms\n\nContent: you cannot find them all. So in order to actually make some conclusion on what happens to a particular to the data, you really need to turn it to abstractive entity linking. And what it says to me is that when we were doing this academic work, it is what we could do at the time. I can say that some group, they or whatever is participating in this event. It's kind of like syntactic language analysis and that's the old style nlp. But if you really want to translate to something that is actually useful in real time, in real life, you have to give me the name of the organization. But it is really hard because the document may truly just say the largest group, the largest militant group in India and you don't know which one it is because you don't have the context. But that's required. The summary here is that obviously we cannot take sentence, we have to go to article, we cannot just use keywords and rules, we have to do semantics. That's taken care of roughly by LLMs. Nobody's doing", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2793529_ms_-_2873785_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2793529, "end_ms": 2873785}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2860921 ms - 2939321 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2860921 ms - 2939321 ms\n\nContent: summary here is that obviously we cannot take sentence, we have to go to article, we cannot just use keywords and rules, we have to do semantics. That's taken care of roughly by LLMs. Nobody's doing keyword and rules matching anymore. But more importantly, we have to work on linking so that we can do analysis, we can do comparison. And that has to be done by, instead of span, you have to say here is the list of entity, tell me which one. Or I have enumerated value of different event types. Everybody has to map to the same event name. Okay? You don't just arbitrarily make up names for different events. So everything has to be so that you can do exact match when you finally do the queries, right? Because otherwise you've still got an LLP problem left. So now let's talk about how we build it using the latest and greatest LLM technology. So this work is under review and Sina, you know, whom you have seen many times seen him various times and he's the lead author of this. So what I want to", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2860921_ms_-_2939321_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2860921, "end_ms": 2939321}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2920193 ms - 3004277 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2920193 ms - 3004277 ms\n\nContent: using the latest and greatest LLM technology. So this work is under review and Sina, you know, whom you have seen many times seen him various times and he's the lead author of this. So what I want to say is that we started talking about, wow, coding problem is hard. And you want to specify exactly what you want. And so if you look at acled, they have a code book. We talked about a code book. Let's look at this code book. This is what it looks like. Can you read it fast enough? It absolutely have to address all the things that you are telling me about. Is it subjective? If it can be characterized in different ways, what should you be doing? Oh, it's still going, right? And this is what you need to know in order to agree on how to code. And what we talked about is the individual's code. And then there are three levels of reviews to make sure that the information is actually correct. That's a lot of material. Okay, so this is what's happening in real life. I didn't finish it. So let's", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2920193_ms_-_3004277_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2920193, "end_ms": 3004277}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 2988717 ms - 3057261 ms", "content": "Title: CS224V Lecture 14 > Transcript > 2988717 ms - 3057261 ms\n\nContent: then there are three levels of reviews to make sure that the information is actually correct. That's a lot of material. Okay, so this is what's happening in real life. I didn't finish it. So let's look at the high level picture of the code book. First of all, like anything else you have, basically it's a bunch of we're pulling out relations between nodes. In this case, the nodes or the things are the actors, which is the political party, organized groups, and there are 6,000 of them around the world that they have identified. And the relations or the things that they care about are the events. They have come up with 25 event types and for each kind of event there are details. If you tell me about peaceful protests, you want to know what the protesters are, the location, the crowd size and so forth. A different kind. If it is involves weapons, then what weapons? If there are deaths, how many deaths, and so forth. So every event has a set of arguments. And if I refer to an actor, a", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_2988717_ms_-_3057261_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 2988717, "end_ms": 3057261}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3040645 ms - 3111357 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3040645 ms - 3111357 ms\n\nContent: and so forth. A different kind. If it is involves weapons, then what weapons? If there are deaths, how many deaths, and so forth. So every event has a set of arguments. And if I refer to an actor, a group, then I have to refer to the entity in the database so that we can log everything that that particular entity does. So with this, that leads you to three problems. The first one is to figure out what the events are. The second one is for a particular event, I have to find the argument for those events and I need to do abstractive entity linking. So here are the three topics. So roughly, you know, this is the high level picture. We say this is what happens to qualitative coding. You create the structured data. Now we blow up the box for the, for qualitative coding, we're gonna start with event detection because that narrows down the kind of arguments that you have. Then the next step, you look at the argument and the database in this case has 6,000 elements. You cannot just give it to", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3040645_ms_-_3111357_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3040645, "end_ms": 3111357}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3097633 ms - 3175787 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3097633 ms - 3175787 ms\n\nContent: detection because that narrows down the kind of arguments that you have. Then the next step, you look at the argument and the database in this case has 6,000 elements. You cannot just give it to GPT and say, find me the element. And by the way if I just stare at those names, you know, they are not like, you know, expensive, moderate and cheap. Okay? They are just really names. It's like I refer to a word. Which name does it mean? Okay, that is also hard. So we actually break them down into two steps which I will describe. So let's start with the detecting event. So here is an example of an article about a Tesla Fremont MLK rally protesting racism and union bustling. So this is an article. Now I want to show you this because this is what we have to deal with. And I said now find me the event type. So if you look at this, you would say this is probably some kind of a protest. And now you have to map it to one of the 25 event types. So there's a long list. So you have to go search for", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3097633_ms_-_3175787_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3097633, "end_ms": 3175787}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3157383 ms - 3238637 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3157383 ms - 3238637 ms\n\nContent: type. So if you look at this, you would say this is probably some kind of a protest. And now you have to map it to one of the 25 event types. So there's a long list. So you have to go search for it. And in this particular case it is a peaceful protest. Okay. So this is primarily a classification task and it can be learned reasonably well if you have training data. So you don't have to read all the details of the code book because the annotation can help you with that. But annotation is nonetheless expensive. There are lots of classes and lots of edge cases. And because the events there are a lot of nuances and if you give you an event it can map to multiple categories, so which one do you pick? And so the question now is, can we get by without training data? It would be nice because now we can apply to new problems without having to get the training data. We know how hard it is to get the accurate training data. Okay. And if you don't do what they did, you've got very bad data. And", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3157383_ms_-_3238637_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3157383, "end_ms": 3238637}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3225413 ms - 3301645 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3225413 ms - 3301645 ms\n\nContent: we can apply to new problems without having to get the training data. We know how hard it is to get the accurate training data. Okay. And if you don't do what they did, you've got very bad data. And classification would just give you a big gigantic mess of, of a knowledge base. So the question is, can we use LLMs? So it turns out that with a simple prompt, with a zero shot LLM, we do pretty well. It's not perfect. Zero shot preview of what we're going to show is about 80%. The details of the experiment comes later. So it's not the, it's okay, it's not too difficult. Let's move on to the second problem. Given an event type, then you have to zoom in on the arguments. So for example, if you know it's a peaceful, peaceful protest, then these are the things that we want to know. So the argument has a description. You know, the thing is that as we move away from machine learning, machine training, we are reading all these descriptions are useful. You don't just look at input and output and", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3225413_ms_-_3301645_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3225413, "end_ms": 3301645}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3286859 ms - 3363155 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3286859 ms - 3363155 ms\n\nContent: has a description. You know, the thing is that as we move away from machine learning, machine training, we are reading all these descriptions are useful. You don't just look at input and output and guess what are you trying to get to the Arguments actually have the descriptions and this is what we rely on to get LLM to do it in zero shots. The first thing here is that you pick up the event, you know which one it is, you pick up the, the arguments, you give it the description and you say, go, pull this out from the text. Looks very simple, right? This is our first round. So what we found is that LLMs can struggle to follow complex instructions. And the standard thing that we know as programmers is that there are type constraints, right? The crowd size should be, should be. Oh, in this case the crowd size should be a string because I want to say over 100 or approximately or whatever it is. And also there are nested level arguments. If I look at a location, it is not just one place, but", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3286859_ms_-_3363155_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3286859, "end_ms": 3363155}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3349747 ms - 3422117 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3349747 ms - 3422117 ms\n\nContent: the crowd size should be a string because I want to say over 100 or approximately or whatever it is. And also there are nested level arguments. If I look at a location, it is not just one place, but it has city, state, country and so forth. And then there is relationship between different events. So it is not that easy to describe, like what the constraints are, what the argument types of this. So what are we going to do? We're going to take advantage of the fact that LLMs actually know Python data structures. So when we extract information, we're going to take free text and turn them into Python classes. Okay. It actually understands Python syntax well enough, so we are going to just use this, leveraging its Python capability, Python familiarity. So the event types are now classes. The event arguments are like the type fields, and you can have nested fields, nested classes, and so forth. So now we provide the coding guidelines or information about the arguments in actually kind of", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3349747_ms_-_3422117_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3349747, "end_ms": 3422117}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3405821 ms - 3486845 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3405821 ms - 3486845 ms\n\nContent: event arguments are like the type fields, and you can have nested fields, nested classes, and so forth. So now we provide the coding guidelines or information about the arguments in actually kind of like a code form with definition of the classes and the fields, and you don't. And just like humans, we have to put in comments, okay, you have to describe what these fields mean and so forth. And it is really amazing to think that LLM could just look at that and do the right thing like humans can. But we kind of have to break it down to this level. So we give it the class definitions and say basically stuff the date, the article into a instance of that class with the values to all the different fields. That's argument extraction. It is actually defining a Python class and its values. So now this makes. So we improve from what we saw earlier. And instead of just simple description, we actually have class definition, which is a little bit more formal. And also the output is a class", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3405821_ms_-_3486845_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3405821, "end_ms": 3486845}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3469475 ms - 3546041 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3469475 ms - 3546041 ms\n\nContent: So now this makes. So we improve from what we saw earlier. And instead of just simple description, we actually have class definition, which is a little bit more formal. And also the output is a class assignment. So that has been. That's very useful. So I wanted talk a little bit about the technology underlying the LLMs and that LLMs are actually good at creating structured out output. Okay, so keep that in mind. As we are doing coding, we're trying to get structured data out, you Let it give you the structured outputs. And the reason is that the LLM, this is how it does it to generate structured data, because you know, when I spit out the structured data I can have syntax errors, okay. And then, you know, it is no longer in structure that you can just run your queries over. So this is roughly what it does with the LLM system. Some of these LLM systems support for example OpenAI and also open source versions like SGLang, outlines and guidance. And what it actually does is that it", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3469475_ms_-_3546041_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3469475, "end_ms": 3546041}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3532201 ms - 3603349 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3532201 ms - 3603349 ms\n\nContent: roughly what it does with the LLM system. Some of these LLM systems support for example OpenAI and also open source versions like SGLang, outlines and guidance. And what it actually does is that it convert the Python cost definitions that we create into JSON schema. It convert the JSON schema into a context free grammar and then you pass the prompt and the JSON schema to the LLM to put it into the JSON. And as the LLM do the output, you know, it decodes the output and it actually choose the most likely token that conforms to the grammar. And this is the constraint part. In other words, it is not just spitting out words without, you know, just spitting out words on like based on next word prediction. It actually understands that there is a grammar it has to, has to stick with. Yes. Can this affect performance of the LLM? Because I've seen some papers I don't know if I can give citation off the top of my head that said that this sort of constraint decoding might affect MMLU accuracy.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3532201_ms_-_3603349_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3532201, "end_ms": 3603349}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3591853 ms - 3660055 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3591853 ms - 3660055 ms\n\nContent: affect performance of the LLM? Because I've seen some papers I don't know if I can give citation off the top of my head that said that this sort of constraint decoding might affect MMLU accuracy. And I found it personally that it's better for me to have it. So what LLM does the logic and then the other one, take that logic and formats it. It depends on your problem. It depends on your problem. What problem are you talking about? I'm just generally overall, like if you need an LLM to be able to display logical thinking while performing, you cannot generalize. Okay. In this case we are just extracting out the arguments and it has a syntax and if you don't get the syntax right, then I don't even know what you're saying. So the syntax is, you know, my point here is that the syntax is guiding it to just put it into the format that you want as it goes. So and so it is really a matter of what kind of a leap the LLM have to do to do the logical thinking. And that's why I asked you what the", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3591853_ms_-_3660055_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3591853, "end_ms": 3660055}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3643801 ms - 3717993 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3643801 ms - 3717993 ms\n\nContent: it to just put it into the format that you want as it goes. So and so it is really a matter of what kind of a leap the LLM have to do to do the logical thinking. And that's why I asked you what the problem is. Yes, Aaron. Process of steps 2, 3 and 4. This is done by the LLM, right? But like the system, okay, we are not doing it. So I'm trying to explain to you why is it doing the structured output so well. So don't feel, don't worry, you know, sometimes I would say oh, I'm going to get it to generate structured output, then it has to conform to the grammar and all that. Is it too hard? Should I just let it generate natural language? And the answer is, you don't have to worry about it. It is actually really good with putting the information in the form that you actually can then go and process using a program. So you're just asking? No, no, I'm just telling you how it is done inside. This is nothing. It's already doing it. It's not us doing it. This is what the. This is why the LLM is", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3643801_ms_-_3717993_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3643801, "end_ms": 3717993}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3703997 ms - 3776313 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3703997 ms - 3776313 ms\n\nContent: using a program. So you're just asking? No, no, I'm just telling you how it is done inside. This is nothing. It's already doing it. It's not us doing it. This is what the. This is why the LLM is good at. It is what I'm trying to tell you. It's already baked into some of these LLMs. Okay? But it's good to know that it's actually using grammar to guide its output. And this is the reason why it works well. So what it means now is that if you give me an article like this, if you go through this process where you have the, you know, it will kind of take us all this information and drop it into this Python format. Okay. And you know, you can all read it. Right? So the peaceful protest has protesters. These are all the protesters. And then the location. It has a country, it has an address, it fills it in the best cat. Crowd size more than 100. And there are no counter protesters. Okay, so if you have, once you create all these Python classes, then basically you have structured data and now", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3703997_ms_-_3776313_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3703997, "end_ms": 3776313}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3761731 ms - 3843449 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3761731 ms - 3843449 ms\n\nContent: it in the best cat. Crowd size more than 100. And there are no counter protesters. Okay, so if you have, once you create all these Python classes, then basically you have structured data and now you can run query over it. Okay, I just wanted to tie that into the ultimate goal is that I can now run program over your structured classes. Yes. Why do you use a Python instead of going directly into the JSON format? Because the JSON is also instructed data. Right, Very good question. I also ask about that too, and I think that it actually has something to do with the fact that we like the fact that classes Python have hierarchies. Okay, so now you can talk about events that are subclasses of another event. And otherwise if I look at JSON, I would have flattened them all out and I have to inherit all the other fields. Okay, so it is for us a little bit more semantically as a higher level, because, for example, you have class hierarchies, but when you're converting the Python into JSON, they", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3761731_ms_-_3843449_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3761731, "end_ms": 3843449}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3824817 ms - 3918931 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3824817 ms - 3918931 ms\n\nContent: all the other fields. Okay, so it is for us a little bit more semantically as a higher level, because, for example, you have class hierarchies, but when you're converting the Python into JSON, they have to flatten it. Okay, so but we're just using Python and you know, it works well enough, the Python, it's very good at Python and they are, as you, as I explained, you know, they actually are doing extra work to make sure that you go with the, with the JSON in the. You Know in the intermediate representation, because they're really good at. So now I have filled the class. But look, I get back to the question that I talked to you earlier. I have cwa, but I really want to say that CWA refers to. It's both the CWA and it is also affiliated with that AFL CIO thing. So I need to do one more step. Okay, so that's the abstract entity detection and linking problem. That's problem number three. So for anything that has entities, I now have to do one more step. So in the ACLID database, they have", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3824817_ms_-_3918931_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3824817, "end_ms": 3918931}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3898275 ms - 3980287 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3898275 ms - 3980287 ms\n\nContent: Okay, so that's the abstract entity detection and linking problem. That's problem number three. So for anything that has entities, I now have to do one more step. So in the ACLID database, they have all the entities and expert descriptions of every one of these entities. So you don't just look at the names and match, you actually can look at the descriptions, which is very helpful so that you can do a better job. And There are over 6,000 entities that we have to deal with. So now what do we do? Can I just stick 6,000 entities into GPT with all the descriptions and say, go, go find all the relevant once? No, but you see this classification problem, I'm talking about aclid, but it applies to everybody's work that has a huge classification problem. I want you guys to realize this is a very general need for you to increase the context for simple classifications here. We're talking about over 6,000 units and they all have descriptions. And so it adds up, right? So how do we handle it? The", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3898275_ms_-_3980287_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3898275, "end_ms": 3980287}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 3965661 ms - 4053657 ms", "content": "Title: CS224V Lecture 14 > Transcript > 3965661 ms - 4053657 ms\n\nContent: need for you to increase the context for simple classifications here. We're talking about over 6,000 units and they all have descriptions. And so it adds up, right? So how do we handle it? The techniques are kind of, I would say, simple and general. So you may find that to be a useful trick for your own projects. So here is an example. And so in this case, in the end, we want to map it to those protesters, all right? And then, for example, for Fremont, I want to map it to the country and the address and so forth, okay? So you can think of it as a retrieval task and that is that I give you this article and then I say, go retrieve the entity that is being talked about here. All right, so this is pretty standard, right? Here is the question. Give me the answer. Give me the paragraph that contains the answer. Except that in this case, you know, here is a gigantic entity list and the off the shelf retrievers just don't work. So what do we use or what do we do? We use this thing called a", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_3965661_ms_-_4053657_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 3965661, "end_ms": 4053657}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4038617 ms - 4117505 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4038617 ms - 4117505 ms\n\nContent: the answer. Except that in this case, you know, here is a gigantic entity list and the off the shelf retrievers just don't work. So what do we use or what do we do? We use this thing called a Rank GPT. Another useful tool for you guys, a useful concept. So the Rank GPT was originally designed for finding the best match among documents. So sometimes we do ir, we retrieve the documents and we actually run this on top of it to rank the documents. Okay? Because IR is pretty big, big and vague, you know, big and high level, you know, they are they are evaluated. You know, you just map it to an embedding and then you say, oh, they are similar. But if you really want to be better at finding the right documents, you do the next level, which is that you show me the query and all the documents and you rank the, you know, the documents, for example, returned by the ir. Okay? And so this is the prompt for it is actually relatively straightforward. And what you want to say is that you take all the", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4038617_ms_-_4117505_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4038617, "end_ms": 4117505}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4103189 ms - 4173657 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4103189 ms - 4173657 ms\n\nContent: rank the, you know, the documents, for example, returned by the ir. Okay? And so this is the prompt for it is actually relatively straightforward. And what you want to say is that you take all the documents and you put the documents in the list. And when we do ir, we would just start with the front to find the answer that we want. Okay, maybe, you know, so that's the basic idea. So we adapted this for this problem, and that is that instead of the query, we say, here is a document, the event type and the arguments and the description. Here is the list of entities and their descriptions. Then you find. So here we find the relevant entities, and then we have to assign the entities into their roles. So we kind of split into two parts. So that's the prompt for it. But there's still one more problem, and it is that the database is still too built large. There are 6.2 thousand entities and they won't fit in one prompt. So the solution here is to split them up. It is pretty kind of general", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4103189_ms_-_4173657_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4103189, "end_ms": 4173657}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4159419 ms - 4234847 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4159419 ms - 4234847 ms\n\nContent: problem, and it is that the database is still too built large. There are 6.2 thousand entities and they won't fit in one prompt. So the solution here is to split them up. It is pretty kind of general and obvious. You take the database and then you turn them into groups, 6,000. You do the batch 100 at a time. And we just ask GPT at this point to rank them with each of these subgroups so that we can. Oh, actually we modify it. We didn't actually ask them to rank it. We asked it to find all the relevant entities. And now once I have the relevant entities as candidates, then I do the second step, and I take 10 entities at a time. And for each entity, we give it the evidence from the article. And then we actually ask the LLM to extract. Oh, we give it the article and we ask the LLM to extract the evidence from the article. Okay. This is kind of what we did with Q and A. So it's much harder forcing it to show that, yes, I know that it is this particular group. And now we remove entities", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4159419_ms_-_4234847_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4159419, "end_ms": 4234847}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4220711 ms - 4316011 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4220711 ms - 4316011 ms\n\nContent: the evidence from the article. Okay. This is kind of what we did with Q and A. So it's much harder forcing it to show that, yes, I know that it is this particular group. And now we remove entities that don't have any evidence. We don't let it hallucinate. All right, so now we got the technique. Now we talk about the evaluation. But before that, I want to know who I have in the class today. Could you please take this QR code? And we know the problem, right? Sometimes you may not have the WI fi. Then just do it right after class. All right, I think everybody has taken the picture okay, so let's talk about the data set. This is actually the data set from aclid. We just have to clean it up and then we get the permission from aclid. They show us a number of their data, it's a public resource and they were very, very nice about it and they share with us a bunch of data and then we create a training data set. 17,000 instances and highly, well, high quality annotated data sets. And we have", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4220711_ms_-_4316011_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4220711, "end_ms": 4316011}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4296787 ms - 4383433 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4296787 ms - 4383433 ms\n\nContent: and they were very, very nice about it and they share with us a bunch of data and then we create a training data set. 17,000 instances and highly, well, high quality annotated data sets. And we have created the test and validation and so. Oh, 17,000 train, 12,000 deaf, 12,000 test and across 16 languages. Okay. This is a really amazing data set. We're very thankful to the ECLIPT group for making that happen. It is absolutely the best annotated data set excepted from a real life data set that was very carefully curated. And the problem is end to end abstract entity linking it does not spans which is very different from the old data set ACE that everybody was using. So what do we want to do with it? So ACLID is a really large world level effort as we described. So there are two, several very important questions. The first one is can the automatic qualitative coding at QC help ACLID expand their coverage in terms of size of events of the different languages, countries and so forth. Now", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4296787_ms_-_4383433_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4296787, "end_ms": 4383433}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4367279 ms - 4446391 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4367279 ms - 4446391 ms\n\nContent: important questions. The first one is can the automatic qualitative coding at QC help ACLID expand their coverage in terms of size of events of the different languages, countries and so forth. Now what we are saying here is that because we have a high quality data set, we can fine tune and get potentially very good results. That's question number one. The second question is we know that ACLID is one of a kind. Can we create new analysis with a lower level of effort? So this is why we want to look at in context learning, so that if I don't have already the training data, I can still go and annotate it. And it's like, what is the quality of that? What? Lemonade. Oh, do you like that acronym? What was it? Well, you can look it up. It's very long, eight letters. So anyways, the good news is that when we have this high quality lemonade high quality data set, you can now tell how good your in context learning is because you can compare it with the gold answer. Do you remember in the last", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4367279_ms_-_4446391_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4367279, "end_ms": 4446391}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4429191 ms - 4515835 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4429191 ms - 4515835 ms\n\nContent: that when we have this high quality lemonade high quality data set, you can now tell how good your in context learning is because you can compare it with the gold answer. Do you remember in the last few classes every time we say there is a discrepancy between in context learning and gold? In context learning is doing extremely well compared to gold. All right, but this is different. It's different in that our gold is very good, it's very well defined. Secondly, it's that so we have a very, you know, and secondly, this data set is hard. It's much harder than just about any of the academic data sets that we have. It is Just because of the large number of events, the large number of entities. So this is a really interesting experiment to understand the second question as well. What is the results in terms of the event detection? It's a classical classification task. We have F1 metric and this is fine tuned the llama 3.1 and GPT 4.0. That's in context learning. There is a discrepancy.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4429191_ms_-_4515835_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4429191, "end_ms": 4515835}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4498263 ms - 4583377 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4498263 ms - 4583377 ms\n\nContent: results in terms of the event detection? It's a classical classification task. We have F1 metric and this is fine tuned the llama 3.1 and GPT 4.0. That's in context learning. There is a discrepancy. Remember, Llama was trained with 17,000 pieces of data. Okay. And I would say that it is not 100% yet, but it is up there 87%, you know, 87 and 79 or 80%. But that is the first step. If you don't have the event detection right, you're not going to get the rest. So after that there are several more metrics that we're going to use. The one is the event argument extraction, which is abstractive. And here we give for this sub problem, we give you the right event and we ask can you figure out what the arguments are? Then we can combine the whole thing end to end. You say you have to do the event detection and you take the result from your event detection and then you do argument extraction. Meaning that I already lost some from the step one and it is compounding, all right, to get to the aee.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4498263_ms_-_4583377_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4498263, "end_ms": 4583377}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4566745 ms - 4647957 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4566745 ms - 4647957 ms\n\nContent: and you take the result from your event detection and then you do argument extraction. Meaning that I already lost some from the step one and it is compounding, all right, to get to the aee. So that's a harder metric. And we also want to know, we know that the abstract entity detection and linking problem is hard. I want to know how well we do for that subtask. Then that's the third metric is the aedl. So here are the results. So the first part is supervised. The last part is zero shot. And the translation in real time refers to the fact that, Remember, we have 16 languages. All right. Without the translation in test time, everybody is. Everything is done in their native languages. Translation in test time. We translated first everything into English and then we do the work. Okay. We want to know if there is a difference. And all in all, I would say that yeah, the translation in English helps a little bit, but not dramatically better. And maybe you can skip that step actually. But of", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4566745_ms_-_4647957_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4566745, "end_ms": 4647957}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4633421 ms - 4719857 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4633421 ms - 4719857 ms\n\nContent: to know if there is a difference. And all in all, I would say that yeah, the translation in English helps a little bit, but not dramatically better. And maybe you can skip that step actually. But of course we are still seeing the, you know, we are not at 100%, right? So let's take a look at the numbers. We already talked about the event detection problem. Let's look at the arguments. The argument, the first row here is that you are given the right event already and we are now at about 80%. Around 80%, a little bit lower without the translation. And for zest, which is zero shot, we are at 70%. We did an ablation and showed that the entity linking is really important. And then the end to end. This is the overall problem accuracy. We are at about close to 70%. Okay. These are not your, you know, we would like to be higher, but it is not bad for a very top, very hot event, Very hot NLP task. Remember, the abstract entity linking is really difficult. So we factored that out and now we see", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4633421_ms_-_4719857_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4633421, "end_ms": 4719857}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4701721 ms - 4790431 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4701721 ms - 4790431 ms\n\nContent: we would like to be higher, but it is not bad for a very top, very hot event, Very hot NLP task. Remember, the abstract entity linking is really difficult. So we factored that out and now we see what's going on and we split it into two categories so we have the overall results. We also split it according to whether the entities exist in the training data or not. And if it is in the training data then it does really well. The supervised version does reasonably well. But if you look at the AE numbers and the AEDL numbers you can see that the linking is a primary reason for the argument extraction to be where it is. Okay. It is really a matter of the, you know, you lose that much already just looking at the entity and then the drop from that with the AEE problem is not that different. So I would say that that is still the bottleneck is the entity linking. And if you have seen it before, it looks pretty good for, I mean this is better obviously for the fine tuned version, but if you have", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4701721_ms_-_4790431_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4701721, "end_ms": 4790431}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4776303 ms - 4859259 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4776303 ms - 4859259 ms\n\nContent: say that that is still the bottleneck is the entity linking. And if you have seen it before, it looks pretty good for, I mean this is better obviously for the fine tuned version, but if you have an entity it has never seen before, boom. It goes down to pretty low numbers. And zest, however, keeps that up because we actually don't, we haven't seen all those numbers because it is a zero shot thing. There is not much of a difference between whether it is in the training data or not since we are not using it. This is also interesting which is what happens to the different languages. 16 languages, right. So surprisingly Korean, Chinese come out even stronger than English, Korean, Chinese and German. Then we have the English, Italian and Indonesian, they are at the high end. At the other end of the spectrum it is Arabic and Myanmar, My Burmese, you know, that's a language book in Myanmar. And that brings it down to the 40 percentage. And one of the. So, so it's interesting you can tell the", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4776303_ms_-_4859259_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4776303, "end_ms": 4859259}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4837531 ms - 4918673 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4837531 ms - 4918673 ms\n\nContent: the spectrum it is Arabic and Myanmar, My Burmese, you know, that's a language book in Myanmar. And that brings it down to the 40 percentage. And one of the. So, so it's interesting you can tell the lower resource languages versus the higher resource languages and. But it's, it's what I, you know, so it can handle the languages in some ways. And as we get better, as the language model gets better with the lower resource languages, the accuracy will go up. So what do we learn? So if you look at the result analysis, look at the supervised version, I would say that if you look at the event detection errors, sometimes it is tricky because some event qualifies for multiple event type and you still have to figure out which one we should pick. And what we see here is that the experts know it is in the code book. Remember the large code book, we didn't pull out everything, we just pull out the structure and the descriptions. And there's information that the experts know that is recorded in", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4837531_ms_-_4918673_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4837531, "end_ms": 4918673}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4906121 ms - 4976239 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4906121 ms - 4976239 ms\n\nContent: in the code book. Remember the large code book, we didn't pull out everything, we just pull out the structure and the descriptions. And there's information that the experts know that is recorded in the codebook that is not included in the descriptions that we give to the language models. And we also see errors in the event argument, extraction errors. And to some extent, some of this cannot be solved without going to external data sources. It just refers to, you know, it uses a nickname or a descriptive name and you say, oh, which group is it? And the people, the real people, the researchers doing it, they are local, they actually know those groups and they are able to fill in the blanks. So this is going to be a difficult problem. So these are the things that we, this is the reason why I was talking about how we have to bring in expert knowledge, things that are outside of what is being written down. And now we are playing with this idea of how we incorporate this information as", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4906121_ms_-_4976239_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4906121, "end_ms": 4976239}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 4963111 ms - 5033555 ms", "content": "Title: CS224V Lecture 14 > Transcript > 4963111 ms - 5033555 ms\n\nContent: I was talking about how we have to bring in expert knowledge, things that are outside of what is being written down. And now we are playing with this idea of how we incorporate this information as rules or as external information, which I alluded to last time too when we were doing the FEC data. We have to put in extra rules that we learn from the experts and that will, you know, and as you are using it, as you find errors, you get the expert input, you put the feedback back in, then eventually it can improve in its performance. You know, our group is like, we're not interested in just getting to the 80%, we really want to bring it up. And this is the, and we believe that the next round has to do with working with experts and incorporating feedback into the system. And we would like to do it in a, in a problem, in agnostic way, just building into the framework. So for example, the ZEST project has, is we apply it to aclid, but you can apply it to other kinds of coding problems. So", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_4963111_ms_-_5033555_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 4963111, "end_ms": 5033555}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 5014581 ms - 5095957 ms", "content": "Title: CS224V Lecture 14 > Transcript > 5014581 ms - 5095957 ms\n\nContent: do it in a, in a problem, in agnostic way, just building into the framework. So for example, the ZEST project has, is we apply it to aclid, but you can apply it to other kinds of coding problems. So given that, we would not recommend that you use our technique, which is at 80, 70%, we would not recommend it because they really require much higher accuracy. But what we found is that it is actually useful for highlighting some potential errors in hand annotation. That's the level that we think can be actually used in practice. And when it comes to the zero shot, we think that ZEST actually works better than expected, considering it is zero shot and it is only a 10 point drop in end to end evaluation and it can handle unseen entities better, but it does struggle with the edge cases. We need more knowledge. So in conclusion here we think that this problem of AQC is relatively new paper in the new, you know, time, you know, it's now we have LLMs, we have to revisit this important topic and", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_5014581_ms_-_5095957_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 5014581, "end_ms": 5095957}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 5077285 ms - 5157507 ms", "content": "Title: CS224V Lecture 14 > Transcript > 5077285 ms - 5157507 ms\n\nContent: knowledge. So in conclusion here we think that this problem of AQC is relatively new paper in the new, you know, time, you know, it's now we have LLMs, we have to revisit this important topic and bring it up to the LLM standard. And we there are two things we talked about, which is lemonade, it is actually based on the real event data. I'm very excited that we finally are actually talking to a real group of world level organization and we are bringing, you know, tying the research in with what they are doing in real life. And we have found that for the fine tuning we are reaching close to 70F1. And we have built this zest, which is an AQC, which is designed for more of the real life problems solving, for example, the abstract entity detection and linking problem. Maybe some people here find that would find that useful for your project. And it is promising result 10 point lower and we can add to it and improve it using expert knowledge. All right, thank you. I'm a little bit long today", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_5077285_ms_-_5157507_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 5077285, "end_ms": 5157507}}
{"document_title": "CS224V Lecture 14", "section_title": "CS224V Lecture 14 > Transcript > 5144155 ms - 5161655 ms", "content": "that would find that useful for your project. And it is promising result 10 point lower and we can add to it and improve it using expert knowledge. All right, thank you. I'm a little bit long today and I will see you on, I will see you on Monday.", "block_metadata": {"id": "CS224V_Lecture_14_>_Transcript_>_5144155_ms_-_5161655_ms", "document_type": "transcript", "lecture_number": 14, "start_ms": 5144155, "end_ms": 5161655}}
