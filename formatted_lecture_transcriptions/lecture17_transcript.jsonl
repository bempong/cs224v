{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > EMNLP: The Late Interaction Model", "content": "Title: CS224V Lecture 17 > Chapter Summaries > EMNLP: The Late Interaction Model\n\nContent: This week we're going to talk about information retrieval and entity disambiguation. It's all about making it accurate enough, but at the same time fast enough. There is a question of time optimization and space optimization.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_EMNLP:_The_Late_Interaction_Model", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 309905, "end_ms": 580055}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Colbert V2 and the future of information retrieval", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Colbert V2 and the future of information retrieval\n\nContent: Colbert was developed at Stanford with Professor Zaharia Matei and his student Omar. It is contextualized late interaction. With Colbert V2, it takes it to a better performance and also at a lower latency. Can be used in many places from product search to question answering.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Colbert_V2_and_the_future_of_information_retrieval", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 580635, "end_ms": 922205}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Machine Reading: The Matching Paradigm", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Machine Reading: The Matching Paradigm\n\nContent: The basic idea is that you have to figure out for each document, out of the gazillion documents you have, what is the match? There are two extreme matching paradigms. One is a classic cross encoder. The other is a very expensive neural network.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Machine_Reading:_The_Matching_Paradigm", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 931915, "end_ms": 1540755}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > The first and second optimization", "content": "Title: CS224V Lecture 17 > Chapter Summaries > The first and second optimization\n\nContent: The first optimization is to find the top K matching documents from say 100 million documents. Ranking every document is too costly. The second optimization is on space, and that is that it is a lot of, lot of space for the embedding. And when your space goes up, your time goes up.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_The_first_and_second_optimization", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 1542095, "end_ms": 1795185}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Facebook AI similarity search: The", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Facebook AI similarity search: The\n\nContent: Face is the Facebook AI similarity search. It finds the closest k vectors. The second stage is to run through the true pipeline, which is the late interaction pipeline. And it actually does a better job with a lot of challenging downstream tasks.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Facebook_AI_similarity_search:_The", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 1796565, "end_ms": 2191575}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Quantum compression for large data sets", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Quantum compression for large data sets\n\nContent: Omar: The concept is to do residual compression. We represent each vector as a cluster ID and a 1 bit delta per dimension, 20 bytes per vector. This really has a significant impact on the information retrieval.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Quantum_compression_for_large_data_sets", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 2191735, "end_ms": 2561777}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Information Retrieval 101", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Information Retrieval 101\n\nContent: So what I want to talk about next is the. Oh, very. That doesn't work. I'm a little under the weather, if you don't haven't figured it out yet. All right, so any questions about information retrieval?", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Information_Retrieval_101", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 2561931, "end_ms": 2609195}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Neural Network: Named Entity Disambiguation", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Neural Network: Named Entity Disambiguation\n\nContent: The problem of named entity disambiguation can be separated into two parts. With Wikidata, every person has a unique id, regardless of how they are referred to in different countries. The key concept here is just add descriptions to the input. Once you add in the descriptions, then you can have a fixed entity.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Neural_Network:_Named_Entity_Disambiguation", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 2612575, "end_ms": 3162825}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Neurolinguistics 2.8: The Two Step Pipeline", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Neurolinguistics 2.8: The Two Step Pipeline\n\nContent: By 2022 there is this paper by Amazon. This is an efficient, zero shot capable approach to end to end entity linking. They also add type information to the entities. I think that what you are seeing in a lot of the work with AS NLP is improving.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Neurolinguistics_2.8:_The_Two_Step_Pipeline", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 3162945, "end_ms": 3558851}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Alexa's Unified Knowledge Base: The Refined Disambig", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Alexa's Unified Knowledge Base: The Refined Disambig\n\nContent: Refined is deployed by Amazon Alexa at web scale. It generalizes to 90 million entities at scale. Like everything else, things keep improving. Instead of face for information retrieval, we are using qudrant.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Alexa's_Unified_Knowledge_Base:_The_Refined_Disambig", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 3558963, "end_ms": 3862147}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Microsoft's Search engine, Quadrant", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Microsoft's Search engine, Quadrant\n\nContent: Is it also using a very similar method to this Covert and mgt? Which one for Perplexity or modern reg system? Any other questions? No.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Microsoft's_Search_engine,_Quadrant", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 3862341, "end_ms": 4036035}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Projects in the end of the quarter", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Projects in the end of the quarter\n\nContent: The course is kind of like 50% projects, 50% lectures. In the last class we will get together as a class and we want a one minute video from every group. We will do a poster after the one hour presentation. What you guys need to deliver by, you know, by two weeks plus two days.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Projects_in_the_end_of_the_quarter", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 4036775, "end_ms": 4220145}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > How many pages should I write in a paper?", "content": "Title: CS224V Lecture 17 > Chapter Summaries > How many pages should I write in a paper?\n\nContent: How many pages? So it's a very common question. I actually don't put a page limit on no minimum, no maximum. If you have very little to say but something very clear and crisp. Just take whatever pages that you need.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_How_many_pages_should_I_write_in_a_paper?", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 4220265, "end_ms": 4309021}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Related Work and the pursuit of knowledge", "content": "Title: CS224V Lecture 17 > Chapter Summaries > Related Work and the pursuit of knowledge\n\nContent: When you describe related work, it is a very easy way to show off what you have done. It is not just to say, oh, I know that you have read related work. This comparison is sometimes the only way you can bring out what your contribution is.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_Related_Work_and_the_pursuit_of_knowledge", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 4309163, "end_ms": 4413415}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > A week in the life of a project", "content": "Title: CS224V Lecture 17 > Chapter Summaries > A week in the life of a project\n\nContent: It is all about depth first. Whatever you cannot show in the demo, you can probably skip. If you want to say that it is good at personalization, then put the personalization in. I always recommend people to start with deciding on the demo and walking backwards.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_A_week_in_the_life_of_a_project", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 4413455, "end_ms": 4506375}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > In terms of the agent,", "content": "Title: CS224V Lecture 17 > Chapter Summaries > In terms of the agent,\n\nContent: As long as it has a system of LLM, you need nlp. So it doesn't have to be like, in a conversational format. And there are many, many projects possible, for sure.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_In_terms_of_the_agent,", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 4507915, "end_ms": 4575475}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > AI classes", "content": "Title: CS224V Lecture 17 > Chapter Summaries > AI classes\n\nContent: In AI classes, we don't have final exams. What is the incentive for coming to class or even watching the videos? We will put the information about the videos, the posters, and the final projects online. See you on Wednesday.", "block_metadata": {"id": "CS224V_Lecture_17_>_Chapter_Summaries_>_AI_classes", "document_type": "chapter summary", "lecture_number": 17, "start_ms": 4575815, "end_ms": 4707955}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 309905 ms - 390623 ms", "content": "Title: CS224V Lecture 17 > Transcript > 309905 ms - 390623 ms\n\nContent: Okay, I'm all set. Hi. All right. How is everybody? Yeah, well I was gone to the EMNLP conference where we, our group has presented three papers. Some of the material was covered in class already. And did you enjoy the two lectures last week? We have Harsha talking about the. Yeah, good. I'm happy to hear that. Harshim was talking about the GENIE framework and of course you've done the homework and you get a chance to look at what goes inside and how we created that. And then on Wednesday we have Jackie, that's very different, isn't it? The point really is that voice or text interface is just not enough. I mean you really have to combine that with GUI as well. That combination is non trivial and what Jackie did was to show how you integrate the two in a fundamental way so that it can actually be used in complex situations such as situations where you can even talk about using voice, working with say PowerPoint. Okay. Where you actually take it to the level where you're dealing with", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_309905_ms_-_390623_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 309905, "end_ms": 390623}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 372905 ms - 449783 ms", "content": "Title: CS224V Lecture 17 > Transcript > 372905 ms - 449783 ms\n\nContent: be used in complex situations such as situations where you can even talk about using voice, working with say PowerPoint. Okay. Where you actually take it to the level where you're dealing with real complicated hardware. So anyway, so I mean I thought it would be great for them to talk about the work since they have been working on this very, you know, working on this for a while and have very long have very good insights on this topic. So for this week we're going to do a little bit, something a little bit different and that is that we're going to go under the hood. We talk about some of the tools that we have been using. We just assume and let's get into the details of that. So the first. So there are two topics that we have alluded to. One is information retrieval. We have this black box. We say, here is the text. Go find me the one paragraph in the billions and billions of of text. Paragraphs of text and find me the right one. And if this does not do well, it doesn't return the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_372905_ms_-_449783_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 372905, "end_ms": 449783}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 433477 ms - 504401 ms", "content": "Title: CS224V Lecture 17 > Transcript > 433477 ms - 504401 ms\n\nContent: box. We say, here is the text. Go find me the one paragraph in the billions and billions of of text. Paragraphs of text and find me the right one. And if this does not do well, it doesn't return the right paragraph. There is nothing you can do, Game's over. So it is very important to find the right text. But of course that's a very hard problem because there's a lot of text on the Internet or even in the Wiki chats project we did. Wikipedia is very large, so how do you do that? So that's problem number one. The second one is entity disambiguation. You know, if I have to understand you, I have to know what you are saying. Suppose I take somebody, something like Lincoln. If I say how tall is Lincoln, what am I referring to? Obviously it's a person and that is Abraham Lincoln. But if I say where is Lincoln, Nebraska? The city is named After Lincoln, but now that's a city. So that's a totally different meaning. We're not asking where. So you have to know which Lincoln it is that I'm", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_433477_ms_-_504401_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 433477, "end_ms": 504401}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 492043 ms - 559523 ms", "content": "Title: CS224V Lecture 17 > Transcript > 492043 ms - 559523 ms\n\nContent: say where is Lincoln, Nebraska? The city is named After Lincoln, but now that's a city. So that's a totally different meaning. We're not asking where. So you have to know which Lincoln it is that I'm referring to. And that is somebody who's really famous. But even that is ambiguous in terms of what it could refer to. So if you don't understand what we are referring to, you won't be able to answer the question. So those are two very important questions. And it is, it requires a lot of work to solve this problem. So we're going to have two goals. We're going to talk about information retrieval. We're going to talk about this idea called late interaction model. And it is all about scale. Billions and billions of paragraphs. Find the right one or find the top 10. How do you handle scale? It's all about making it accurate enough, but at the same time fast enough. Nobody wants to wait forever, right? So there is a question of time optimization and space optimization. And that's what we're", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_492043_ms_-_559523_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 492043, "end_ms": 559523}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 547334 ms - 616851 ms", "content": "Title: CS224V Lecture 17 > Transcript > 547334 ms - 616851 ms\n\nContent: all about making it accurate enough, but at the same time fast enough. Nobody wants to wait forever, right? So there is a question of time optimization and space optimization. And that's what we're gonna talk about. With respect to named entity disambiguation. We're going to talk about how we figure out what it is. There are two pieces of information that are useful, which is entity description and typed information. So those are the topics we're gonna go over. And that basically is the outline for this course for this class. So for information retrieval, we're going to talk about two very, I would say, seminal papers that is in 2020. And this is actually developed at Stanford with Professor Zaharia Matei and his student Omar, who did this work. And the project is called Colbert. Well, you know Bert, right? Everybody knows Bert. That's a language model. But who is Colbert? It's a comedian. It comes very, it's very care, it's very clever. You know, it is contextualized late", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_547334_ms_-_616851_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 547334, "end_ms": 616851}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 603235 ms - 678447 ms", "content": "Title: CS224V Lecture 17 > Transcript > 603235 ms - 678447 ms\n\nContent: Well, you know Bert, right? Everybody knows Bert. That's a language model. But who is Colbert? It's a comedian. It comes very, it's very care, it's very clever. You know, it is contextualized late interaction. So that's how you get the C O, L. But there is a, there is a wonderful question whether we should call it Colbert or Colbert. Are you talking about the science or talking about the person this seems to be named after? Okay, anyways, so that's Colbert. That was 2020 and it has over 1200 citations. Because when you solve the text retrieval problem, tons of people are using it. And it is really one of the well cited papers in this space. And two years later in Naeckl, they presented Colbert V2. And that's when they improve it. One more round using space compression. What is it? So here is the basic idea of information retrieval. You have tons and tons of data out there and you need to find it. So you start with something like a question, what compounds protect the digestive system", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_603235_ms_-_678447_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 603235, "end_ms": 678447}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 665951 ms - 732343 ms", "content": "Title: CS224V Lecture 17 > Transcript > 665951 ms - 732343 ms\n\nContent: is the basic idea of information retrieval. You have tons and tons of data out there and you need to find it. So you start with something like a question, what compounds protect the digestive system against infection? Do you know? It's a good question. Somewhere in this text is the data is the answer that you want. So the question is, how do we find it? So you have to find these paragraphs that refer to him, but they don't just answer that question. It is kind of embedded in whatever text, whatever information that they want to write about. And in this case, it just talked about. In the stomach, gastric acid and proteases serve as powerful chemical defenses against ingested pathogens. So they don't use the exact word, but you have to understand the meaning and then you have to be able to pull it out, out of all these different paragraphs. So, for example, if you, you know, obviously this is a good old problem. In the good old days, even at the beginning of the, near the beginning of", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_665951_ms_-_732343_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 665951, "end_ms": 732343}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 720183 ms - 819737 ms", "content": "Title: CS224V Lecture 17 > Transcript > 720183 ms - 819737 ms\n\nContent: pull it out, out of all these different paragraphs. So, for example, if you, you know, obviously this is a good old problem. In the good old days, even at the beginning of the, near the beginning of the Internet, excuse me, even if you talk about Google search, you know, you get some keywords, you could go find it. So at the beginning it was using very basic, basic keyword search. And of course, we really want to do it at the semantic level with, oh, my voice is going away with nlp. So this is a very important concept and can be used in many places from product search, question answering, fact checking, informative dialogue, and so forth. So it is really fundamental and very basic. So how do you do that? It is really all about scale. So there is quality and efficiency. It's interesting to look at what, how, what we have done in all these, you know, various different years. I mean, the history. So mrr, I'm going to talk about that. It is a Data set and mrr, top 10 at 10 is whether you", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_720183_ms_-_819737_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 720183, "end_ms": 819737}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 799243 ms - 895963 ms", "content": "Title: CS224V Lecture 17 > Transcript > 799243 ms - 895963 ms\n\nContent: look at what, how, what we have done in all these, you know, various different years. I mean, the history. So mrr, I'm going to talk about that. It is a Data set and mrr, top 10 at 10 is whether you can find the right paragraph within the top, you know, the 10 paragraphs that you retrieve. And, you know, it is, we keep creeping along at this 20ish percentage. And the latency is of the order of 100 millisecond. And what we want is of course, higher quality and of course we want lower latency. So we want to be at that corner. So, so when the, when Bert first came out, now we can use the deep language model and you get a jump in the quality, but at the same time the query, the latency went up, went all the way up. This is a log scale. So now you're talking about 1000 level of milliseconds. What Colbert did is that it brings it down. It brings it down to roughly the same amount of time. But now we are at the 35% accuracy. That's Colbert. Then the other one is that with Colbert V2, it", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_799243_ms_-_895963_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 799243, "end_ms": 895963}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 879875 ms - 976517 ms", "content": "Title: CS224V Lecture 17 > Transcript > 879875 ms - 976517 ms\n\nContent: What Colbert did is that it brings it down. It brings it down to roughly the same amount of time. But now we are at the 35% accuracy. That's Colbert. Then the other one is that with Colbert V2, it takes it to a better performance and also at a lower latency. This is so it makes a huge difference. And so as you can imagine, Colbert is built on top of birds. So the question is, how do you do this? Something on my throat. All right, so the basic idea is that you have a question and now you have to figure out for each document, out of the gazillion documents you have, what is the match? Okay, you want to give it a number, the match between the query and the document. And unfortunately, if you give me a query, I have to check against all the documents that we have. That's lot of document. So for each one we're computing a value and then of course we want to pick the one 0.93 over 0.01 and so forth. So in a dumb, a dumb method would be that you just go through, you know, pick the pair of", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_879875_ms_-_976517_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 879875, "end_ms": 976517}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 960221 ms - 1039615 ms", "content": "Title: CS224V Lecture 17 > Transcript > 960221 ms - 1039615 ms\n\nContent: for each one we're computing a value and then of course we want to pick the one 0.93 over 0.01 and so forth. So in a dumb, a dumb method would be that you just go through, you know, pick the pair of request query and a document, compute the score. You know, hundreds of millions or billion paragraphs later, you have that many scores and you sort them and you find the best one. So how do you even design or compute the score? How do you build the retriever? So obviously we're going to do training and this is a very important data set. This is an IR data set. It's called Ms. Marco, it's produced by Microsoft and it is a human generated machine reading comprehension data set to start in 2016. And in 2018 it has turned into an IR data set. So the important thing is that we are really working with real user queries. I emphasize many times you give me fake queries. It's a totally different thing. You don't say, here's a paragraph, what is the query for it? This is actually a million real", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_960221_ms_-_1039615_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 960221, "end_ms": 1039615}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1027935 ms - 1092349 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1027935 ms - 1092349 ms\n\nContent: with real user queries. I emphasize many times you give me fake queries. It's a totally different thing. You don't say, here's a paragraph, what is the query for it? This is actually a million real world queries from Bing. And in order to answer these questions, it came from 8.8 million passages from the web pages. And the metric, there are many metrics and one of them is MRR 10, which is to find the right answer within the top 10 documents. So this is a starting point where you have question and paragraphs. Okay, because we're not trying to figure out what is the sentence that is closest to a paragraph. Because a question is not the same content, you're asking about what and when and so forth. It doesn't match the content exactly. So what you are trying to say is given a question like this, what is the right paragraph? So this requires a dataset, and this is a hand annotated dataset. So how do we go from there? You now have a dataset. So I'm going to talk about two extreme matching", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1027935_ms_-_1092349_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1027935, "end_ms": 1092349}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1078101 ms - 1159567 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1078101 ms - 1159567 ms\n\nContent: what is the right paragraph? So this requires a dataset, and this is a hand annotated dataset. So how do we go from there? You now have a dataset. So I'm going to talk about two extreme matching paradigms. The first one is a good old, I mean a classic cross encoder. Okay? You have a doc query, it's broken down to tokens, documents the same way. And you have a network that you train, knowing that in the end you want to compute a score. The ones that match have a high score, the ones that don't match, low score. So that's the encoder. And this is the best you can do in a sense, given the data set. Okay, so this is, you get fine grained interactions between the queries and the documents and they match all the way up. But on the other hand, that's very expensive. Okay, you have the hundreds of millions or billions, let's say billions documents to begin with, billion paragraphs to begin with. Every single question. Are you going to compute against all these documents? Okay, It's a very", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1078101_ms_-_1159567_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1078101, "end_ms": 1159567}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1142563 ms - 1225643 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1142563 ms - 1225643 ms\n\nContent: of millions or billions, let's say billions documents to begin with, billion paragraphs to begin with. Every single question. Are you going to compute against all these documents? Okay, It's a very expensive neural network to train and it's also a very expensive neural network to use by the time you actually you want to find for each query what the answer is. So there's training time and there is inference time. And this is just not doable. It is too expensive. And so if you take, suppose you finish training, if scoring each document takes only 10 milliseconds, that will consume 11 days with 100 million documents. Okay, so this is seriously undoable. So what do we do? What do you think we should do? What is interesting to note is that queries and documents are two totally different things. Okay? The documents are fixed. You have a set of documents that are fixed from whenever you want to do this lookup, but the questions come in and every one of them is different. And so what you", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1142563_ms_-_1225643_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1142563, "end_ms": 1225643}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1209707 ms - 1282845 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1209707 ms - 1282845 ms\n\nContent: Okay? The documents are fixed. You have a set of documents that are fixed from whenever you want to do this lookup, but the questions come in and every one of them is different. And so what you really want to do is to do something separately. So given the documents, you should do whatever you need to handle the documents ahead of time and then you can quickly take care of the new query that comes in. So that's very important that you separate the inference from the, you know, having to go through all the document one at a time. So the idea here is to have a bicoder, the bycoder is to separately embed, find the embedding for the query for the document. And then you take the, you know, you take that and you do a similarity comparison, right? You find a similarity score for the queries in the document. And what that means here is that the documents can be precomputed once and for all. I mean, you still have to go check against the document, but you don't have, you just, you just take the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1209707_ms_-_1282845_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1209707, "end_ms": 1282845}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1271253 ms - 1344471 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1271253 ms - 1344471 ms\n\nContent: document. And what that means here is that the documents can be precomputed once and for all. I mean, you still have to go check against the document, but you don't have, you just, you just take the final embedding that you can compute for that document and compare it with the queries, the query that comes in. So that solves a performance problem in a significant way. So we now have this independent dense encoding, but it is pretty coarse. Okay, so yeah, I'm training this whole model using the question and query data set. But you are now just talking about in the end a summary of a document. But if I give you one piece of one document or one, you know, if I give you one document, there are many, many questions that you can ask about that document, right? And if I just summarize it in one way, how is it going to match that particular query? So you cut the, you cut the connection, but at the same time it is a lot coarser. So that doesn't, you know, that hasn't worked too well at that", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1271253_ms_-_1344471_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1271253, "end_ms": 1344471}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1330409 ms - 1409265 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1330409 ms - 1409265 ms\n\nContent: way, how is it going to match that particular query? So you cut the, you cut the connection, but at the same time it is a lot coarser. So that doesn't, you know, that hasn't worked too well at that time. So what Colbert suggested is that we're going to do late interactions. That means that we separately, when we calculate the embedding, they are separated. But at the same time we then add a late interaction connection above that. This late interaction allows you to take each vector from the query and compare it. You see, you're doing a maxim which is like a maximum, is some kind of maximum similarity, such as cosine similarity. Okay? And for each vector here, for each element here, you can now do a maxim against all the, all the, you know, the entire vector, all the, all the vectors from the document. And one at a time, you're accumulating each of these elements together and then you can find the maxim. Okay, so this has got a lot more interactions between the queries and the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1330409_ms_-_1409265_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1330409, "end_ms": 1409265}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1394641 ms - 1468567 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1394641 ms - 1468567 ms\n\nContent: from the document. And one at a time, you're accumulating each of these elements together and then you can find the maxim. Okay, so this has got a lot more interactions between the queries and the documents. And it's not just one value for the query and one value for the document. So you get the choice that you get some advantage. Because if you look at the embedding for the document, it is an offline encoding, but it is not turned into one aggregate value that you do cosine similarity, you leave them as a, you know, N dimensional vectors, but you can pre compute them. And then at that point after the query has been encoded, you can now do this more complicated, more sophisticated interaction. And this is why it is called late interaction. So the important part is that the blues, all this work on the embedding of the document can be done offline once and for all. Okay, it's not once every time you do the query. And then when you use the code size similarity, you are finding the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1394641_ms_-_1468567_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1394641, "end_ms": 1468567}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1451823 ms - 1530295 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1451823 ms - 1530295 ms\n\nContent: this work on the embedding of the document can be done offline once and for all. Okay, it's not once every time you do the query. And then when you use the code size similarity, you are finding the nearest neighbor search, you are finding the query that matches closest, you're finding the document that matches closest to the query. Using this maxim calculations, we sum it up and Then after we get all the numbers, you sort them and you find the top N values that have the highest similarity between the vectors and the documents. So that's the basic idea. So using this method, it was shown that with if you take like 21 million passages from Wikipedia, it takes 70 milliseconds. Okay? Significant saving in time compared to the cross encoder and much better than the bicoder. So let us take a look at what it means is here's a real example of the matching. Suppose I have a question like when did the Transformers cartoon series come out? Have you guys seen them? The transformer paper, I must", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1451823_ms_-_1530295_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1451823, "end_ms": 1530295}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1516757 ms - 1592757 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1516757 ms - 1592757 ms\n\nContent: at what it means is here's a real example of the matching. Suppose I have a question like when did the Transformers cartoon series come out? Have you guys seen them? The transformer paper, I must say, is one of the hardest papers to read. I don't know if you have that same experience and people have to explain it using cartoons. So suppose you ask this question. There are many parts to this question. And suppose you have a document that actually said the animated video or whatever mentions a transformer. It was released on some date. Suppose this is, this is a document with all these different parts. What happened now is that because we don't combine them into a single score for each document, we leave them as a multidimensional vector. So when we, if you look at the query, the query has various parts, for example, for the when part, it is going to match with the on part of that document. And then when you get to the Transformers, it is matching the Transformers. And when you talk", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1516757_ms_-_1592757_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1516757, "end_ms": 1592757}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1579707 ms - 1662711 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1579707 ms - 1662711 ms\n\nContent: various parts, for example, for the when part, it is going to match with the on part of that document. And then when you get to the Transformers, it is matching the Transformers. And when you talk about the cartoon series, it matches the part that has the animated, the word animated and when it comes out, it is released. So in other words, we don't have a single value for the document, but for each vector in the each of the dimension of vectors in the query, it matches to the doc to the different vectors in the embedding for the document. And this is how you can get this fine grained interaction pulling out the right centers. Any questions about that? So that's the basic idea. So you delay the interaction and you pay for it only at the, at the last point. So the point now here is that, yeah, that seems to be faster, but we are still talking about 100 million documents. So we have skipped the bottom part. But I still have to do the rest. Obviously I have to encode the query and that is", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1579707_ms_-_1662711_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1579707, "end_ms": 1662711}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1647575 ms - 1726785 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1647575 ms - 1726785 ms\n\nContent: that seems to be faster, but we are still talking about 100 million documents. So we have skipped the bottom part. But I still have to do the rest. Obviously I have to encode the query and that is not a problem, but I have to now go and compare that. So this is what we do with one document. And now we have to do this for 100 million documents. It is still very, very expensive. So how do we solve this problem? So the paper describes. The first paper describes the first optimization, the second describes the second one. And that Is that if you give me a query, I have to find the top K matching documents from say 100 million documents. Ranking every document is too costly. So we want to narrow down the candidates in this process. When we do the narrowing down of the candidates, we're doing in a coarse way. We don't have to find the perfect answer. I just have to make sure that the answer that I'm looking for or maybe the top 10 is within the top 10,000. So I can afford to be not so good", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1647575_ms_-_1726785_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1647575, "end_ms": 1726785}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1710371 ms - 1792357 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1710371 ms - 1792357 ms\n\nContent: a coarse way. We don't have to find the perfect answer. I just have to make sure that the answer that I'm looking for or maybe the top 10 is within the top 10,000. So I can afford to be not so good because I'm going to use a cheaper method to narrow down the candidates. And we are not going to use the late interactions. I'm just going to look at the query and the documents without the fancy late interactions come up with the match, the closest match for the top 10,000. So we reduced 100 million to 10,000. That's a savings of like four orders. I mean, it is like 1,000 right there. After I reduce it down to the 10,000, then you can afford to actually run it through the real pipeline that you have there. So that's the first optimization. The second optimization is on space, and that is that it is, you know, this is a lot of, lot of space for the embedding. And when your space goes up, your time goes up. Okay, so the question is, how do we save the time on the, you know, can we compress", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1710371_ms_-_1792357_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1710371, "end_ms": 1792357}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1775645 ms - 1869469 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1775645 ms - 1869469 ms\n\nContent: it is, you know, this is a lot of, lot of space for the embedding. And when your space goes up, your time goes up. Okay, so the question is, how do we save the time on the, you know, can we compress the space? And that's the second optimization. So for the first one, instead of using this whole pipeline, I want to introduce this concept of face, which is the Facebook AI similarity search. And this is very, it's a very, very useful tool because what it does is that you give me high dimensional vectors, I can give you the k nearest neighbors. Okay, so the math definition is written down here, which is that suppose I have your billions vector x sub I and some dimension D. Face actually builds a data structure in the RAM for it. Okay, so the whole thing does not fit in memory, but face is actually optimizing it by building a structure in ram. And then, and then if you give me a new vector, it will find you the closest k vectors. And what do you mean by closest is the one with the minimum", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1775645_ms_-_1869469_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1775645, "end_ms": 1869469}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1851143 ms - 1937781 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1851143 ms - 1937781 ms\n\nContent: optimizing it by building a structure in ram. And then, and then if you give me a new vector, it will find you the closest k vectors. And what do you mean by closest is the one with the minimum Euclidean distance between the X and each of the x sub I because it's a multidimensional vector and it will return you the top k whatever it is that you want. So this is a very well optimized package that is available from Facebook. And so this is one of the, you know, basic subroutine, in a sense that is in Colbert. So let's see how it Works. So we say that there are two stages. The first one is that we're just going to use late, no late interaction. So I'm just going to look at the embeddings of the queries and the document and find the closest thing. So for example, here we show that for each document we have, it's n dimensional vectors, it has embeddings. And we are using the fast vector similarity search. So when you have a query, you have to go through the embedding. This is all", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1851143_ms_-_1937781_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1851143, "end_ms": 1937781}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1921709 ms - 2002301 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1921709 ms - 2002301 ms\n\nContent: each document we have, it's n dimensional vectors, it has embeddings. And we are using the fast vector similarity search. So when you have a query, you have to go through the embedding. This is all precomputed. The embeddings for the documents are all precomputed once and for all, offline. And then the query comes in, it goes through the embedding, you know, it finds it, it calculates the embedding for that particular query. Then it's at that point we actually look at each element in this embedding and for each one we find the closest similarity match with the documents. Okay, each document is embedding with n elements here, and for each one we are finding the closest one. And so now we are kind of looking at each part of the query and finding the documents that match each part because we notice that for the question you want, you really want to assemble it from different parts. So this is what you can do for each of these vectors. And then you figure out what are the documents given", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1921709_ms_-_2002301_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1921709, "end_ms": 2002301}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 1988845 ms - 2069476 ms", "content": "Title: CS224V Lecture 17 > Transcript > 1988845 ms - 2069476 ms\n\nContent: notice that for the question you want, you really want to assemble it from different parts. So this is what you can do for each of these vectors. And then you figure out what are the documents given the embeddings that match. You figure out which are the documents that those embeddings the closest match belong to, and those will be your candidates. That's the result of step one. So we are still talking about finding the top 10,000. Okay, this is not a small result at the end, but you now have the 10,000. And the second stage is to run through the true pipeline, which is the late interaction pipeline. So the concept here is that for each of the document, we go through this pipeline and we get the score and so forth, and we have 10,000 of them. And then you do a sort and you figure out what are the top 10. So this is how we can find the top 10 closest paragraphs. That should answer the question. Okay, so this late interaction makes a huge difference. In the experiment that was run,", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_1988845_ms_-_2069476_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 1988845, "end_ms": 2069476}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2049309 ms - 2134523 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2049309 ms - 2134523 ms\n\nContent: what are the top 10. So this is how we can find the top 10 closest paragraphs. That should answer the question. Okay, so this late interaction makes a huge difference. In the experiment that was run, there were all the various different techniques and it brings it up from, if you look at Ms. Marco, it brings it up to the 36, 37.5%. And then for the other, and it is applied to other domains and for every one of them it just does a better job using this late interaction. So you are kind of getting some optimization, some advantage of cross encoder and paying the cost closer to the by encoder. And so this is the result from the paper. But a lot of people, you know, many, many, many projects require information retrieval. So they take this, take, take this module and they apply it to the many different downstream tasks. And it actually does better with a lot of these challenging downstream tasks. And here are just some of the papers that have been using this method. And that was a huge", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2049309_ms_-_2134523_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2049309, "end_ms": 2134523}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2119929 ms - 2198103 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2119929 ms - 2198103 ms\n\nContent: different downstream tasks. And it actually does better with a lot of these challenging downstream tasks. And here are just some of the papers that have been using this method. And that was a huge success. I mean, it just, you know, it's like what we just did in the class. I said, look, we're going to call the information retriever, okay? You don't have to deal with it. And it is all done. And this is an amazing module and it is just very, very much used everywhere. But this is costly. If you look at the Colbert's index, it's an order of magnitude larger than the baseline 650 gig for Wikipedia. Okay, that's non trivial. And what it means is that when you look at the size of this, if you look at the size, it translates directly into cost. So the question now is, can we actually advance the quality advantage and reduce its footprint by an order of magnitude? So that's the question that was addressed by Colbert V2. Okay, so if you look at the document encoding the document for each", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2119929_ms_-_2198103_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2119929, "end_ms": 2198103}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2177735 ms - 2274065 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2177735 ms - 2274065 ms\n\nContent: the quality advantage and reduce its footprint by an order of magnitude? So that's the question that was addressed by Colbert V2. Okay, so if you look at the document encoding the document for each document, what does the element embeddings look like? Okay, so you know, you've got a whole bunch of numbers. And as a matter of fact, there are 128 of them. Okay, so it is, you know, 128 dimensions. And they are this floating point numbers, four bytes each. So each vector needs 512 bytes. And what we really want to do is bring it down. How far can we go? How about 20 bytes? That sounds like magic. Okay, so how do we do this? How do they deal with this? So the concept is to do residual compression. So first of all, you find a set of centroids, okay? You've got an embedding and they distribute them. You've got all these centroids. So if I have a particular vector that I want to represent, I figure out what is its closest centroid and I represent the vector by saying this centroid plus a", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2177735_ms_-_2274065_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2177735, "end_ms": 2274065}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2258923 ms - 2347531 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2258923 ms - 2347531 ms\n\nContent: them. You've got all these centroids. So if I have a particular vector that I want to represent, I figure out what is its closest centroid and I represent the vector by saying this centroid plus a delta. Okay? And now I don't have to represent the full numbers because I start with the base of the values of the centroid. Okay, so and you, so you choose the closest centroid C, sub T for that particular vector. And then we can represent this vector knowing the Cs of T, the centroid. And then you subtract it, you get the residual R. We do a quantized version because we actually don't need the full granularity of full 4 bytes for the floating point number because you have already got the centroid, it takes you to this particular spot and then you're just trying to quantize to describe the displacement, the residual or the difference. And you don't really need the full resolution. So this is just all a matter of storage. Once you store the data, when you use the information at search time,", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2258923_ms_-_2347531_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2258923, "end_ms": 2347531}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2331331 ms - 2418779 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2331331 ms - 2418779 ms\n\nContent: the residual or the difference. And you don't really need the full resolution. So this is just all a matter of storage. Once you store the data, when you use the information at search time, the information is actually expanded back to whatever it needs to be. So you just have to say, here is my centroid, I give you the index, I pick up the centroid value, you take the residual, you add them back together and you basically get the approximate version of the original vector. With this technique, it changes the representation significantly. We went from 512 bytes and we just go down to 20 bytes because, whoops, I went backwards. Because we first of all encodes the cluster ID, the centroid which is four bytes. And then because there are 128 dimensions for each one, we actually just use one bit. And that gets us to 16 bytes. And altogether it's just 20 bytes. So this is a huge compression from 500 and 12 bytes to 20 bytes. And once you represent this, you know, this information this way,", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2331331_ms_-_2418779_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2331331, "end_ms": 2418779}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2404635 ms - 2487119 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2404635 ms - 2487119 ms\n\nContent: And that gets us to 16 bytes. And altogether it's just 20 bytes. So this is a huge compression from 500 and 12 bytes to 20 bytes. And once you represent this, you know, this information this way, the question of course is do we lose anything? Do we lose in quality? I mean obviously the smaller is going to make the whole thing run a lot faster and the. So can we do that? So in summary here is that we represent each vector as a cluster ID and a 1 bit delta per dimension, 20 bytes per vector. And here is the result. Taking the result from V1 we showed earlier and which takes 154 gigabytes of storage, getting a performance of 36.2. And if you have two bit, if you just use two bit for the residual compression, which gives you a six times compression from 154 gigabytes to 25 gigabytes, the performance didn't change. And if you take it down to one bit, which now gives you a 10 times improvement, it just drops a little bit to 35.5. So it was a really amazing engineering solution here to", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2404635_ms_-_2487119_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2404635, "end_ms": 2487119}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2469303 ms - 2561083 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2469303 ms - 2561083 ms\n\nContent: performance didn't change. And if you take it down to one bit, which now gives you a 10 times improvement, it just drops a little bit to 35.5. So it was a really amazing engineering solution here to basically just reduce the cost, reduce the time by a significant amount without much of a degradation in performance. So this paper has been cited by over 1000 other papers and it's kind of interesting to see what are all the different applications that have been built with it. There are a lot of best paper awards that is building on top of Colbert itself, like a white box analysis of Colbert, sparse and bed and so forth. How you improve Colbert Extensions and then at the bottom are all the different applications. So this really has a significant impact on our, on the development of information retrieval. So this is, this is work done by mate and the student Omar. And Omar is now a professor at mit. And so it's all good. So this is a very, very important building block. So that's the IR", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2469303_ms_-_2561083_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2469303, "end_ms": 2561083}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2539627 ms - 2643145 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2539627 ms - 2643145 ms\n\nContent: retrieval. So this is, this is work done by mate and the student Omar. And Omar is now a professor at mit. And so it's all good. So this is a very, very important building block. So that's the IR part. So what I want to talk about next is the. Oh, very. That doesn't work. Just give me a second. I'm a little under the weather, if you don't haven't figured it out yet. I was, I went on this trip and I came back. I'm, I'm okay. I don't have Covid, I checked, but I'm just a little bit under the weather. All right, so any questions about information retrieval? All right, so let's talk about the second topic, which is named entity disambiguation. And as you see here, you know, we have this optimization where we take, we optimize time by separating the calculation into two parts, which is that you select the K candidates and then you do a more full blown RE ranking, you know, figuring out what the right rank is among the 10,000 candidates. We actually see the same, same technique again in", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2539627_ms_-_2643145_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2539627, "end_ms": 2643145}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2630269 ms - 2698855 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2630269 ms - 2698855 ms\n\nContent: you select the K candidates and then you do a more full blown RE ranking, you know, figuring out what the right rank is among the 10,000 candidates. We actually see the same, same technique again in named entity at disambiguation. So this is something that is useful for you as you think about your problems, whatever they happen to be. That this concept of a coarse grain filter selection and then followed with a more expensive RE ranker. And so let's take a look at the named entity disambiguation problem. So here is the idea. Suppose I give you something like Barack Obama, born in Hawaii, served as the 44th president of the United States. He graduated from Harvard Law School and won the Nobel Peace Prize in 2009. So there are a lot of terms in a single sentence and you want to know exactly who are we referring to, which Obama and so forth. I think that we talked about this entity disambiguation problem last time when we were talking about multilingual. Do you remember you say that my", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2630269_ms_-_2698855_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2630269, "end_ms": 2698855}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2684755 ms - 2754893 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2684755 ms - 2754893 ms\n\nContent: who are we referring to, which Obama and so forth. I think that we talked about this entity disambiguation problem last time when we were talking about multilingual. Do you remember you say that my advisor, he has an English entry and a Chinese entry, but they are the same person and you have to identify it to be the same person. But the good news is that with Wikidata, every person has a unique id, regardless of how they are referred to in different countries, and they bring it down to the same id. So we want to do the same thing here is that we tick these, all these entities and we bring it down to a unique identifier. So this is Barack Obama, this is Hawaii. And these are all QIDs, these are all the entities in the in wikidata. So the question is, how do you do that? So the basic problem of named entity disambiguation can be separated into two parts, which is that the first I give you the sentence and you have to figure out when I am referring to an entity, you have to figure out", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2684755_ms_-_2754893_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2684755, "end_ms": 2754893}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2740569 ms - 2822317 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2740569 ms - 2822317 ms\n\nContent: of named entity disambiguation can be separated into two parts, which is that the first I give you the sentence and you have to figure out when I am referring to an entity, you have to figure out the mention, which is a span that refers to the entity. So in this case England is an entity. And then the FIFA World cup, that's the three words together refers to one entity. So the problem now is that you first of all have to pull the mentions out, pull the entities out, and then you have to figure out which entity it is. And so you may be given a knowledge base and an entity list. We've seen this problem before. Do you remember when we were doing the eclipse data, the eclipt data, we're doing the event qualitative coding. It's like I give you a paragraph and I talk about the military group in India or a region, but you really want to figure out what is the name of the military group. You want to map it to one of these standard IDs, because you can have different newspapers referring to", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2740569_ms_-_2822317_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2740569, "end_ms": 2822317}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2804653 ms - 2895027 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2804653 ms - 2895027 ms\n\nContent: in India or a region, but you really want to figure out what is the name of the military group. You want to map it to one of these standard IDs, because you can have different newspapers referring to this group in the same way. And you really want to say that this is the activities of the same group. So everything is about tying it to a common global identity. So we've seen this problem before and now we're talking about how we're going to be able to do this disambiguation problem. So you have to find a mention and you have to find the entity. So there are many things that you can use for it. For example, question answering, relation extraction, which is the work that we just talked about. The ACLID work is about extracting relation. How are things related in an event and given different documents and they are all related to the same entity and so forth. Automated construction of knowledge base. So in acled we actually put everything together in the knowledge base. So one of the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2804653_ms_-_2895027_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2804653, "end_ms": 2895027}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2865329 ms - 2963263 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2865329 ms - 2963263 ms\n\nContent: documents and they are all related to the same entity and so forth. Automated construction of knowledge base. So in acled we actually put everything together in the knowledge base. So one of the stronger works in this area is a paper by the group in Facebook Fair and also, you know, University College London and Echo Polytechnique. And what they wanted to do is to do scalable zero shot entity linking. Zero shot means that I want to link something, I want to find the entities and I have not seen the entity before. Okay, I'm going to train a network on what I have, but then I have to be able to handle new entities that I do not train with because there are so many entities in this world. And so their key concept here is that we just add descriptions to the entities as input. You don't just give me a bunch of entities and a bunch of documents and say this one is this, this one is that what we are talk. What we are going to do is even, is to add in the descriptions. And once you add in", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2865329_ms_-_2963263_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2865329, "end_ms": 2963263}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 2949281 ms - 3035705 ms", "content": "Title: CS224V Lecture 17 > Transcript > 2949281 ms - 3035705 ms\n\nContent: give me a bunch of entities and a bunch of documents and say this one is this, this one is that what we are talk. What we are going to do is even, is to add in the descriptions. And once you add in the descriptions. Excuse me, and once you add in the descriptions, then you can have a fixed named entity disambiguator that takes this and be expanded to entities that has never seen before. So this is very, very easily understandable because that's, you know, you can do that and so can so can neural network. So for example, it is a simple sentence like my kids really enjoyed a ride in the Jaguar. So Jaguar could be a junior roller coaster and Jaguar can refer to the car. And because we are talking about kids, then the likelihood is that you're talking about the rollercoaster. So in other words, this description is entered as input into the system. And now you don't have to kind of memorize by just input output of sentences and the entities, but you can now take into account the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_2949281_ms_-_3035705_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 2949281, "end_ms": 3035705}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3016649 ms - 3114695 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3016649 ms - 3114695 ms\n\nContent: other words, this description is entered as input into the system. And now you don't have to kind of memorize by just input output of sentences and the entities, but you can now take into account the description to find, find the entity that is most likely related to the sentence that you put in. You see the similarity between this and the query, the information retrieval. You see the similarity between that. How do I map that to that problem? In a sense. So now this is given, there's a huge difference is that the related elements that you're going to pull out is a lot smaller. Okay, but you don't know which one it is. But now this entity, you know, how do you compare this with the problem that we just described? Yeah, the part of my kids really. Enjoyed it writing Jaguar becomes your query. And to set up all this, all possible occurrences of Jaguar within Wikipedia, those are your documents. And you. It's likely that my kids enjoy it. Right? At Jaguar would be semantically closer to", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3016649_ms_-_3114695_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3016649, "end_ms": 3114695}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3097773 ms - 3179995 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3097773 ms - 3179995 ms\n\nContent: And to set up all this, all possible occurrences of Jaguar within Wikipedia, those are your documents. And you. It's likely that my kids enjoy it. Right? At Jaguar would be semantically closer to Jaguar is a junior roller coaster than to a luxury vehicle. Precisely. So it is again a similarity search, a semantic similarity search. But it is applied now to the problem of entity disambiguation. Okay, so all the stuff that we were talking about are relevant here. And so in fact they also take a two step approach. Okay, and the two step, you know, they could have done this all in one step. Again, if you use a cross coder, okay. The fine grained interactions between query and retrieval in this case is the sentence and the possible, possible entities. But that is very costly. So again we're seeing the same architecture which is that it has the two step approach. The first one is a bike encoder which is that you take the sentences, you find the semantic, you map it into some dense space, the", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3097773_ms_-_3179995_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3097773, "end_ms": 3179995}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3164121 ms - 3252301 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3164121 ms - 3252301 ms\n\nContent: the same architecture which is that it has the two step approach. The first one is a bike encoder which is that you take the sentences, you find the semantic, you map it into some dense space, the same dense space, and then you can find the closest K entities in this space, the closest neighbors. And what method should we use to make this happen? How do we do this calculation? It's expensive. How do we do this calculation? What? Use what? The centroid we were describing the centroid? Yes. The centroid is a representation optimization. Okay, it is try to store the embeddings with smaller number of bits. That's the second optimization. What's the first optimization? Yes. Okay, so you are doing a vector search, you're finding the closest vectors and again you can be using the face module. Okay? So keep that in mind because you may find the need for finding similar vectors in your work. So after that then you do a cross encoder. The cross encoder is the same thing because it was a trained", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3164121_ms_-_3252301_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3164121, "end_ms": 3252301}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3233507 ms - 3321067 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3233507 ms - 3321067 ms\n\nContent: So keep that in mind because you may find the need for finding similar vectors in your work. So after that then you do a cross encoder. The cross encoder is the same thing because it was a trained module with sentences and entities. And the at the end of that you calculate, you have a score. It's exactly like the same as what we have. And then you pick the top N scores, or in this case maybe top one score, and you say that that is the entity that are referring to. Okay, so instead of. I think that what you are seeing in a lot of the work with AS NLP is improving. We don't just memorize input and outputs, we try to make it read, okay, the information is supplied and then we're doing a semantic match. And so that applies to the IR and it is also in the, in this case entity dissembled. So that was in 20. What was that? 2020. You know, things change every other year, right? So by 2022 there is this paper by Amazon. This is refined and it is an efficient, zero shot capable approach to end", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3233507_ms_-_3321067_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3233507, "end_ms": 3321067}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3300875 ms - 3369351 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3300875 ms - 3369351 ms\n\nContent: was in 20. What was that? 2020. You know, things change every other year, right? So by 2022 there is this paper by Amazon. This is refined and it is an efficient, zero shot capable approach to end to end entity linking. It's the same kind of problem, but they added another twist and that is that they also add type information to the entities. Okay, One of the things you have to remember is that we want to have a description, but sometimes you don't have a description and the description is not explicit. It's not explicit as a type. So for example, when I asked you how tall is Lincoln? And then I say, where is Lincoln, Nebraska? It is pretty clear, how tall is Lincoln as a person? I don't know anything about, I don't need to know anything about Lincoln, okay? He turns out to be very famous, but I may not have a description about him. But then when I say Lincoln, Nebraska, clearly I'm talking about a place. So the type is also very important. So what they did is that it is", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3300875_ms_-_3369351_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3300875, "end_ms": 3369351}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3356415 ms - 3427639 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3356415 ms - 3427639 ms\n\nContent: very famous, but I may not have a description about him. But then when I say Lincoln, Nebraska, clearly I'm talking about a place. So the type is also very important. So what they did is that it is representation, fine grained typing. This is the name for their project. So instead of just having candidate entity description, so for example, it talks about D. One is a country that is part of this, the men's cricket team, you know, all these information, but it also has a candidate entity type that just describes what kind it is. You know, the type is pretty good. It's not just a sports team, but it is a cricket team or football team and so forth. Okay, and this is their pipeline. So this is the complete pipeline. So what, there are two tasks, right? The one is to find the mention and the second one is map it to the entity. So in this case here you have a bidirectional transformer. It takes the sentence and it first of all marks. It marks the mention. B stands for. It is the beginning", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3356415_ms_-_3427639_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3356415, "end_ms": 3427639}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3412711 ms - 3487615 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3412711 ms - 3487615 ms\n\nContent: second one is map it to the entity. So in this case here you have a bidirectional transformer. It takes the sentence and it first of all marks. It marks the mention. B stands for. It is the beginning of a entity, zero mean or O means nothing. And then I means continuation. So what it now is to pull out that we have England and we have FIFA, FIFA World Cup. Those are dimensions. So that's the first stage of this transform. And at that point here is that. Yeah, so you pull out the England and the Pfeiffer World Cup. So this is the first stage is mentioned detection, just to say it's the begin inside and outside. Oh, that's what it means. I is inside, O is outside tagging. So now you pull out the mentions. And the second one is that you figure out the mention representation. It's using mean pooling. Then the next thing is that it uses the description score and what they have is a multiple simultaneous by encoder representations. Okay, so they are separately encoded and you're comparing", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3412711_ms_-_3487615_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3412711, "end_ms": 3487615}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3472991 ms - 3555491 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3472991 ms - 3555491 ms\n\nContent: pooling. Then the next thing is that it uses the description score and what they have is a multiple simultaneous by encoder representations. Okay, so they are separately encoded and you're comparing the similarity score. And on top of that it also looks at the entity typing, it's another score. And this is using L2 distance between the type vectors. And you sum them all up, you combine them and in this case it also adds one more thing and which is the entity prior. The entity prior is what it does is that sometimes you may not have information about something that you mentioned, but what it is using is a global entity prior. It uses a corpus and a popularity metric basically to pull out what everybody knows, what is popular. So if I mention Lincoln, if I just use the word Lincoln, the chances are I'm talking about Abraham Lincoln. If I don't want to refer to Abraham Lincoln, I would say Nebraska. Okay, so there is a prior. And if you don't use this prior, all bets are off because", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3472991_ms_-_3555491_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3472991, "end_ms": 3555491}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3540893 ms - 3620205 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3540893 ms - 3620205 ms\n\nContent: chances are I'm talking about Abraham Lincoln. If I don't want to refer to Abraham Lincoln, I would say Nebraska. Okay, so there is a prior. And if you don't use this prior, all bets are off because sometimes you just cannot tell from the context what am I referring to. So that's the Third element that they pull in. And so this goes into the combined score. So recap, here is the refine has various stages. The mention detection with the B begin inside outside, then mention representation via the mean pooling, entity typing, entity description, and finally a linear combination of these things. And that is refined. It is a very, very useful module which we have used very successfully in our projects. What is interesting is that this refined is deployed by Amazon Alexa at web scale. What does that mean? So this is the result here. And as you can see here in terms of time taken compared to previous work, one is so the wet all with bike encoder or cross encoder and so forth. And refine just", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3540893_ms_-_3620205_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3540893, "end_ms": 3620205}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3599443 ms - 3689861 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3599443 ms - 3689861 ms\n\nContent: that mean? So this is the result here. And as you can see here in terms of time taken compared to previous work, one is so the wet all with bike encoder or cross encoder and so forth. And refine just beats them all in this test in both time and performance. So at Amazon scale, we're now talking about populating a knowledge base with a billion web pages. And they do this multiple times a year. And it requires two days of processing 500 T4 GPUs. And it is a very uniform architecture. It generalizes to 90 million entities at scale. And that is the refined disambiguator. And we used it and actually for our project we also tried to improve it by pulling in information and just. We just, you know, to augment the data. You know, it is not perfect still to augment the data. We actually would ask for the. I think that was the Wikidata project where we really need entity disambiguation, a very good one. So what we actually did is that we called GPT and say, what kind of entity can you describe", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3599443_ms_-_3689861_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3599443, "end_ms": 3689861}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3676055 ms - 3759835 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3676055 ms - 3759835 ms\n\nContent: for the. I think that was the Wikidata project where we really need entity disambiguation, a very good one. So what we actually did is that we called GPT and say, what kind of entity can you describe what it is that the entity is about? And we also throw that in because LLMs have. It is very knowledgeable, a lot of. About a lot of entities. And so we also pull that in and that turns out to be quite useful as well. So in summary, here is that we talked about three tools. The first one is this efficient similarity search of dense matrix dense vectors. We talk about face, but like everything else, things keep improving. And if you look at what we are using today, we're using qudrant, okay? It is open source and that's what we are using Instead of face for information retrieval. We talked about Colbert. It really kind of turns the trajectory of information retrieval, but it is no longer the state of the art, okay? It just changes every so often. You know, a few lectures ago we are talking", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3676055_ms_-_3759835_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3676055, "end_ms": 3759835}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3741663 ms - 3827653 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3741663 ms - 3827653 ms\n\nContent: Colbert. It really kind of turns the trajectory of information retrieval, but it is no longer the state of the art, okay? It just changes every so often. You know, a few lectures ago we are talking about information retrieval and we Talked about using M3, which is a successor to Colbert. And by now we have already updated it to yet another piece of software. And this is MGTE by Alibaba. And it's Open source. And that paper was just presented last week in emnlp. Okay. It just goes to show that things just keep changing along the ways. But the most important thing is that these abstractions, the concept of information retrieval is very, very powerful. And we talked about the late interaction model. The concept of optimizing time where you use a cheaper method in the first round and then a more sophisticated method in the second round is very, very useful. We see the optimization in space and then we also talked about ned. They look like two totally different problems but in fact we kind", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3741663_ms_-_3827653_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3741663, "end_ms": 3827653}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3812509 ms - 3895327 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3812509 ms - 3895327 ms\n\nContent: sophisticated method in the second round is very, very useful. We see the optimization in space and then we also talked about ned. They look like two totally different problems but in fact we kind of. We see that we are more or less using the same ideas and that is that instead of just memorizing input outputs, we are now retrieving information so that it returned the NED problem into a document matching problem as well. And I think that that has turned out really well. So we are still using Refined. We haven't updated it as something else yet. But it is really amazing how fast we are progressing. Any questions about these topics? Yeah, I wanted to ask for more than reg systems like Perplexity and stuff like that. Is it also using a very similar method to this Covert and mgt? Which one for Perplexity or modern reg system? Perplexity. Perplexity. It appears that they just call search and then they summarizes the result from multiple search engines. It's what it looks like. If you look", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3812509_ms_-_3895327_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3812509, "end_ms": 3895327}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3874871 ms - 3961047 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3874871 ms - 3961047 ms\n\nContent: Perplexity or modern reg system? Perplexity. Perplexity. It appears that they just call search and then they summarizes the result from multiple search engines. It's what it looks like. If you look at Storm, we are using this at the bottom. Well, let's see, Storm is different, Storm is using. We're using Bing search. Okay, Bing, Bing, you know, I mean they have this problem. Bing is doing the information retrieval and the Ms. Marco is from Microsoft. Okay. I would imagine that this is a continuation of how they find search. So at the very bottom of the search engines they are using techniques like this. But Perplexity and Storm, they are sitting on top of it. So if you look at wiki Chat, which is the Wikipedia, we are now using MGTE because it is multilingual. It is. We're doing 25 different languages and we're just using. And that. That's what we have to do is that we have to do the. We use that for. You know, this is open source. So basically we. What we have to implement is", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3874871_ms_-_3961047_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3874871, "end_ms": 3961047}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 3943887 ms - 4047035 ms", "content": "Title: CS224V Lecture 17 > Transcript > 3943887 ms - 4047035 ms\n\nContent: different languages and we're just using. And that. That's what we have to do is that we have to do the. We use that for. You know, this is open source. So basically we. What we have to implement is Quadrant is Qudrant. Okay. So the. It has been trained, you know, the mgts, that's open source and trained. But when you give me a new query I am using Quadrant to find the answer, to find the best match. And by the way, MGT is actually a by encoder. Okay. It wasn't using the fancy Two stage kind of without and with late interactions. And it does really well, it performs well, it goes across the different languages and it is fast and it has a lot to do with better hand annotated data in different languages. So that concept of a basic of that building block is just fundamentally the same, but the implementations keep evolving. Me question. Any other questions? No. Okay, so I just wanted to take some time to talk a little bit about the projects in the end of the quarter. We are already at.", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_3943887_ms_-_4047035_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 3943887, "end_ms": 4047035}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4019287 ms - 4126177 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4019287 ms - 4126177 ms\n\nContent: the implementations keep evolving. Me question. Any other questions? No. Okay, so I just wanted to take some time to talk a little bit about the projects in the end of the quarter. We are already at. Before I do that, want to get you guys to do one thing for me. So how are things going with the projects? I mean this course is kind of like 50% projects, 50% lectures. How are things going with the lectures? Because this is the last week before the break and after the break you're coming back. And we cancel class on Monday because we actually take a three hour class time for Wednesday where we do the presentation for everybody in the class. And are there any. So we should talk a little bit about what are the. What you guys need to deliver by, you know, by into two weeks plus two days. That's what we have. Okay. So the class is large, so we don't have enough time for everybody to make a long presentation. So the format here is that in the last class we will get together as a class and we", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4019287_ms_-_4126177_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4019287, "end_ms": 4126177}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4106529 ms - 4191413 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4106529 ms - 4191413 ms\n\nContent: what we have. Okay. So the class is large, so we don't have enough time for everybody to make a long presentation. So the format here is that in the last class we will get together as a class and we want a one minute video from every group. It is just a highlight. It's like what you want to say basically. Tell me why I want to go to your poster. Because we will do a poster after the one hour presentation. The one hour presentation allows us all to see every project and so be brief and concise. Concise. But, but you know, make sure you talk about the most important points that you get. You know, what is it that you learn out of this project? So that is the key message because you get to have a poster where you can talk about it, talk about your work in more details. And for the poster, it's kind of like the classic, just the high level ideas. If you have a demo, please make sure you bring that. We can try it out if that works. That would be very nice. Of course, in the video, one of", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4106529_ms_-_4191413_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4106529, "end_ms": 4191413}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4176443 ms - 4266991 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4176443 ms - 4266991 ms\n\nContent: kind of like the classic, just the high level ideas. If you have a demo, please make sure you bring that. We can try it out if that works. That would be very nice. Of course, in the video, one of the things that you can do is to show a demo of how your system works. Because I think everybody's project is building a system. So it would be nice to see a demo of how this whole system works. And so that's the poster and then after that there is a final report which is due at the same time of the final exam for this course. Okay. I think that those states were all on the web. Any questions about that that you want to discuss? Yes, Mike. How many pages? So it's a very common question. And I actually don't put a page limit on no minimum, no maximum. You do what you need. Okay, but you don't need to pad it. If you have very little to say but something very clear and crisp. Just take whatever pages that you need to feel that. Okay. So one way is that. I mean, we. It is good to use, for", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4176443_ms_-_4266991_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4176443, "end_ms": 4266991}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4249611 ms - 4320553 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4249611 ms - 4320553 ms\n\nContent: need to pad it. If you have very little to say but something very clear and crisp. Just take whatever pages that you need to feel that. Okay. So one way is that. I mean, we. It is good to use, for example, a conference paper format. That would be, you know, something very. A lot of people are familiar with. And, you know, we seem to be very good at reading papers that look like that. I would say, okay, it is a little bit dense, but, you know, we can go through that. But I would recommend that again, you know, if you feel like you want to do something different for your format, it's okay. I mean, we write so many papers with so many guidelines. I just want to give you the little bit of freedom and say, this is how I want to write my paper. But there will be. The topics are the same, which is, what is the problem? What is the background? Okay, talk about related work. How is it that you are different from the related work? What people don't realize sometimes is that when you describe", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4249611_ms_-_4320553_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4249611, "end_ms": 4320553}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4306499 ms - 4383211 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4306499 ms - 4383211 ms\n\nContent: is, what is the problem? What is the background? Okay, talk about related work. How is it that you are different from the related work? What people don't realize sometimes is that when you describe related work, it is a very easy way to show off what you have done. It is not just to say, oh, I know that you have read related work, but this comparison is sometimes the only way you can bring out. It's an easy way to bring out what your contribution is. So that is an important section. You talk about the related work and you say, here, this is what is new. Clearly, we are talking about a class project. It is not like we're expecting a conference paper. Right? And what I am keen on is what you have learned. So suppose you pick a project that is very ambitious and you ended up solving a fifth of that or a tenth of that. But that is because you run into some hard problem. And that is all fine as well. Okay. I'm a big believer in this. It was interesting that for my PhD thesis, I never got", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4306499_ms_-_4383211_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4306499, "end_ms": 4383211}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4366595 ms - 4445301 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4366595 ms - 4445301 ms\n\nContent: of that or a tenth of that. But that is because you run into some hard problem. And that is all fine as well. Okay. I'm a big believer in this. It was interesting that for my PhD thesis, I never got to my thesis. I have a topic for my thesis, and I discovered so many problems that I actually waited till I come here and become a professor and actually solve the prosthesis. I finally did the thesis that I wanted to do with the help of four consecutive PhD students. So it was a hard thesis. I didn't know. So I'm all good for. I'm all good with that. It is the most important, important thing is, what did you find out? What did you learn? And I think that you have two weeks left. So I want to give you a little bit of hints about how you go about it. I try to say that early on, and that is that. Don't try to say, Here are the 10 steps that I have to do. And then you finish one and then, oh, it's just pretty. Now, as we're going to second, it is all about depth first. As soon, as quickly as", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4366595_ms_-_4445301_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4366595, "end_ms": 4445301}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4434321 ms - 4509267 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4434321 ms - 4509267 ms\n\nContent: Don't try to say, Here are the 10 steps that I have to do. And then you finish one and then, oh, it's just pretty. Now, as we're going to second, it is all about depth first. As soon, as quickly as you can possibly get to the point that you really want to experiment cutting all the corners that you can and get to that point. And one way of preparing for that is that you decide on what the demo is. Okay. Whatever you cannot show in the demo, you can probably skip. You have two weeks. If you want to say that it is good at personalization, then make sure that you put the personalization in. And maybe it is not as good in answering questions or whatever it is. That is not the main point of your work. So I always recommend people to start with deciding on the demo and walking backwards. As a matter of fact, if you go the other direction, you may find yourself having nothing to show. Okay. And I think that's a very, very common, common problem that I have seen in these projects. So, any", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4434321_ms_-_4509267_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4434321, "end_ms": 4509267}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4491949 ms - 4574087 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4491949 ms - 4574087 ms\n\nContent: a matter of fact, if you go the other direction, you may find yourself having nothing to show. Okay. And I think that's a very, very common, common problem that I have seen in these projects. So, any other questions? Yeah. So in terms of the form of the agent, I know that we have a lot of emphasis on conversational agents. Does our demo have to be conversational? No, as long as it has, like, it's a system of LLM, you need. LLM, you need nlp. Okay. And if you're not using LLM, it's interesting too. So it doesn't have to be like, in a conversational format. That's what I'm saying. No, no. I mean, if you look at STORM in the first round, I mean, the interactive Storm, the co. Storm is interactive. Yeah, but this is definitely not necessary. And there are many, many projects possible, for sure. Or if you are just trying to improve. I don't think anybody's doing it. But if somebody wants to just improve NED or Information Retrieval, you don't need to show what is on top of it. And you just", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4491949_ms_-_4574087_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4491949, "end_ms": 4574087}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4561603 ms - 4656111 ms", "content": "Title: CS224V Lecture 17 > Transcript > 4561603 ms - 4656111 ms\n\nContent: Or if you are just trying to improve. I don't think anybody's doing it. But if somebody wants to just improve NED or Information Retrieval, you don't need to show what is on top of it. And you just have to explain that. So. All right, so if you don't mind taking a picture of that. We do want to know, like, who's here today. I find it a little bit different. In AI classes, we don't have final exams. We say, here are the lectures and we prepare for that. And then you start working on the project. I mean, we have two assignments, and then you say you do work on projects, which is all good and well and we just find it a little bit. Find it necessary to get a sense of who is actually attending class. Since we don't have finals, should we have a final? No. Why not? So what is the incentive for coming to class or even watching the videos? Thank you. I'm asking the wrong people because you guys are all here. But I kind of. I mean, this is kind of the style for the NFL classes. I kind of", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4561603_ms_-_4656111_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4561603, "end_ms": 4656111}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Transcript > 4637375 ms - 4707955 ms", "content": "for coming to class or even watching the videos? Thank you. I'm asking the wrong people because you guys are all here. But I kind of. I mean, this is kind of the style for the NFL classes. I kind of enjoyed that. But it really. It kind of just goes back to relying on the students wanting to learn. I think a lot of the lectures were very close to the projects that people are doing, and I hope that that kind of makes it a little bit more obvious why it's a good thing to do to attend all the classes. Any other questions? So we will put the information about the videos, the posters, and the final projects online so we can go through it, and then we'll tell you what other sections that you have to write and so forth. Okay. All right. Thank you. See you on Wednesday. It.", "block_metadata": {"id": "CS224V_Lecture_17_>_Transcript_>_4637375_ms_-_4707955_ms", "document_type": "transcript", "lecture_number": 17, "start_ms": 4637375, "end_ms": 4707955}}
