{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > How to Analyze Data from Hybrid Sources", "content": "Title: CS224V Lecture 13 > Chapter Summaries > How to Analyze Data from Hybrid Sources\n\nContent: We're able to get data from hybrid sources. We have structured, we started with free text data and then we have structured data. And last, in the last class we talked about how to put them together and we got this hybrid.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_How_to_Analyze_Data_from_Hybrid_Sources", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 108375, "end_ms": 149344}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The agentic approach in C#", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The agentic approach in C#\n\nContent: In this class, we're going to study two hard real life data sets. The first one is Wikidata, the world's largest knowledge graph. The other one is just really tables, it's just SQL queries. And we're using these as the, as examples for this class. What we're introducing is this agentic approach.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_agentic_approach_in_C#", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 150844, "end_ms": 265455}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The agentic vs the normative approach to knowledge bases", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The agentic vs the normative approach to knowledge bases\n\nContent: We're looking at the agentic approach for two different data sets. This is all on Wikidata. And then we apply it to another data set, which is challenging in this different way.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_agentic_vs_the_normative_approach_to_knowledge_bases", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 265575, "end_ms": 322155}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Knowledge Graphs", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Knowledge Graphs\n\nContent: A knowledge graph is that we are representing information as a graph, nodes and edges. Many things are actually connected with each other. And you can do a lot with this graph. How do we get information out of it, given this representation?", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Knowledge_Graphs", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 324015, "end_ms": 461845}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The largest knowledge graph in the world, Wikidata", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The largest knowledge graph in the world, Wikidata\n\nContent: Wikidata is the largest life knowledge graph. It currently has 15 billion facts. Over 100 million entities, 10,000 properties, 25,000 contributors. It also connects to all kinds of databases outside of Wikipedia.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_largest_knowledge_graph_in_the_world,_Wikidata", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 463465, "end_ms": 837309}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Quantifying the number of students at the University of Washington", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Quantifying the number of students at the University of Washington\n\nContent: What are the musical instruments played by people who are affiliated with the University of Washington School of Music and have been educated at School of Washington? And how many people play each instrument? That is the kind of thing that you would do like in research.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Quantifying_the_number_of_students_at_the_University_of_Washington", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 837477, "end_ms": 1196285}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Spark and Wikidata: The power of tables", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Spark and Wikidata: The power of tables\n\nContent: The power of Sparkle and Wikidata is there. Once you express it in Sparkle, you don't have to figure out how to pull this information out. A lot of these tables are actually dynamically generated by executing a Sparkle query on a daily basis. Can we represent all this data as tables?", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Spark_and_Wikidata:_The_power_of_tables", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 1196585, "end_ms": 1497381}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > KBQA and Semantic Parsing", "content": "Title: CS224V Lecture 13 > Chapter Summaries > KBQA and Semantic Parsing\n\nContent: We need knowledge bases or knowledge graphs. Why is KBQA so challenging? And especially now that we have LLMs, does it solve the problem? The most recent work actually that we did is on semantic parsing. We then look at a more traditional method called subgraph traversal.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_KBQA_and_Semantic_Parsing", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 1497453, "end_ms": 1983255}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Wikidata: fine-tuned llama", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Wikidata: fine-tuned llama\n\nContent: If you use Wikidata using a fine tuned llama, you can get 76% answers correctly. But 4% of the answers are incorrect. None of the fine tuned solutions can handle properties that it has never seen before. So querying wikidata with sparkle is very difficult.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Wikidata:_fine-tuned_llama", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 1983755, "end_ms": 2383663}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Quantum data mining with Sparkle", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Quantum data mining with Sparkle\n\nContent: The second very popular method is subgraph retrieval. This method walks the graph one edge at a time. But it cannot handle the research questions of all the different properties that exist. This is why approaching it with sparkle is very difficult.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Quantum_data_mining_with_Sparkle", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 2383759, "end_ms": 2797345}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Knowing the musicians by a knowledge graph", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Knowing the musicians by a knowledge graph\n\nContent: You have to start with some entity like the University of Washington, tons of people connected to it. By reading the notes one at a time, you're going to take a long time because we know how big this database is. And we haven't even got to the instruments part. So you'll be compiling that table yourself.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Knowing_the_musicians_by_a_knowledge_graph", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 2803725, "end_ms": 2941685}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The agent-based approach to knowledge basis", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The agent-based approach to knowledge basis\n\nContent: The agentic approach for knowledge basis will be presented next week in Miami. It combines the best of everything from Sparkle subgraph retrieval and LLM. It is now deployed at Wikidata they're actually using it.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_agent-based_approach_to_knowledge_basis", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 2941725, "end_ms": 3355521}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Sparkle 2.8", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Sparkle 2.8\n\nContent: The whole concept is to build the sparkle query. You prompt the LLM to generate a thought and an action. You execute the action, you take the result and you repeat until you get the answer. It's not a single shot because it takes multiple rounds.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Sparkle_2.8", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 3355673, "end_ms": 3752665}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Learning from the Web with Google", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Learning from the Web with Google\n\nContent: The data sets were collected through search engines and also with crowdsourcing. One idea is to synthesize it. Another is to create a graph that links all the links. What do you worry about? Is this a good dataset?", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Learning_from_the_Web_with_Google", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 3753045, "end_ms": 4109397}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The toughest data sets", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The toughest data sets\n\nContent: And they are the toughest ones. If you compare the statistics is like the number of average clauses. We are much higher. The projections. Everything here is much bigger than what everybody else has done. It is a much tougher data set.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_toughest_data_sets", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 4109581, "end_ms": 4139415}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > Meeting the most people", "content": "Title: CS224V Lecture 13 > Chapter Summaries > Meeting the most people\n\nContent: So on the results on the Spinach data set, it looks like this. We did the fine tuning on popular questions. It is not much better. The next project is the one that is done in 2024, very recent. How you're doing it makes a huge difference.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_Meeting_the_most_people", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 4140235, "end_ms": 4514465}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The improvement of the Wikidata query system", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The improvement of the Wikidata query system\n\nContent: The bot is deployed on Wikidata forum and has 600 plus conversations in the wild. The success rate is at 78%, which is a lot better than the 40% that we were looking for. The next step is to fine tune an open source model so they can run it without using commercial systems.", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_improvement_of_the_Wikidata_query_system", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 4515235, "end_ms": 4773317}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Chapter Summaries > The agentic approach to data science", "content": "Title: CS224V Lecture 13 > Chapter Summaries > The agentic approach to data science\n\nContent: Journalists don't know how to do SQL. With this project you can use natural language to tap into the data. The data is in FEC and open elections. It turns out that the interpretations are very difficult. Expert knowledge is required. How do we incorporate that?", "block_metadata": {"id": "CS224V_Lecture_13_>_Chapter_Summaries_>_The_agentic_approach_to_data_science", "document_type": "chapter summary", "lecture_number": 13, "start_ms": 4773501, "end_ms": 5325695}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 108375 ms - 193199 ms", "content": "Title: CS224V Lecture 13 > Transcript > 108375 ms - 193199 ms\n\nContent: All right, so last week we were talking about how we can analyze data, right? We have structured, we started with free text data and then we have structured data. And last, in the last class we talked about how to put them together and we got this hybrid. We're able to get data from hybrid sources. Hello. Hello. Hey. This class has started, by the way. Excuse me. This class has started. All right, so, so when we're pulling data from the structured data, you know, when we pull the data from the structured data, we said it's really relatively easy so far. You just provide the schema, maybe a few examples, and it was able to do reasonably good semantic parsing. Is that enough? Well, what we're going to talk about today is that it is not enough in real life. Okay? So now we are kind of providing the background and now we're gonna get deeper into this problem and what we're gonna do is to study two hard real life data sets. The first one is Wikidata. It's the largest, the world's largest", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_108375_ms_-_193199_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 108375, "end_ms": 193199}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 181327 ms - 247631 ms", "content": "Title: CS224V Lecture 13 > Transcript > 181327 ms - 247631 ms\n\nContent: the background and now we're gonna get deeper into this problem and what we're gonna do is to study two hard real life data sets. The first one is Wikidata. It's the largest, the world's largest knowledge graph. If you can handle Wikidata, you can handle any knowledge graph. And then there is the other one which is just really tables, it's just SQL queries. And this is the Federal Election Commission data. And we're going to use these as the, as examples for this class. And what we're going to introduce is this agentic approach. Agentic approach means that it is a series of LLM calls where the LLM is actually performing actions and looking at the results and figuring out what to do next. So that's the, that's the basic idea behind the agentic approach. And what I want to do today is to make sure that you understand when and why and how to use the agentic approach. Because you know, you may not need to worry about the knowledge graphs because we will actually provide you with the code", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_181327_ms_-_247631_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 181327, "end_ms": 247631}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 237239 ms - 296181 ms", "content": "Title: CS224V Lecture 13 > Transcript > 237239 ms - 296181 ms\n\nContent: sure that you understand when and why and how to use the agentic approach. Because you know, you may not need to worry about the knowledge graphs because we will actually provide you with the code for it. But what is important in this class is that I want you guys to be able to do new projects on new problems, on challenging problems. And the agentic approach is a very general technique and I want you to pay attention to that and I hope that you will find it useful in your work. Okay, so there are really two data sets we're going to talk about. We're going to start with wikidata, which is a knowledge graph. And we're going to use the term knowledge base, kb, which is kind of like the standard term that has been used. So we want to start by saying why do we need kbs? The next question is why is kb? Question Answering kbqa this is a standard problem that people have worked on. Why is that problem challenging? And we're going to talk about previous work and then we're going to talk about", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_237239_ms_-_296181_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 237239, "end_ms": 296181}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 284815 ms - 352217 ms", "content": "Title: CS224V Lecture 13 > Transcript > 284815 ms - 352217 ms\n\nContent: is kb? Question Answering kbqa this is a standard problem that people have worked on. Why is that problem challenging? And we're going to talk about previous work and then we're going to talk about the agentic approach. And of course, we will have to be able to evaluate. And that means that we have to get a good data set and the evaluation. So this is all on Wikidata. And then we take this agentic approach and then we apply it to another data set, which is, which is challenging in this different way. And we talk about how that applies. Okay, so you'd be staring at two. We're looking at the agentic approach for two different data sets. So let's start with a knowledge graph. A knowledge graph is that we are representing information as a graph, nodes and edges. So here is an example of the work on astronomy, and all these concepts are well known. But in order to put this concept in a way that, in a structured way so they can retrieve information, we can put them in a graph. So, for", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_284815_ms_-_352217_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 284815, "end_ms": 352217}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 334359 ms - 409973 ms", "content": "Title: CS224V Lecture 13 > Transcript > 334359 ms - 409973 ms\n\nContent: the work on astronomy, and all these concepts are well known. But in order to put this concept in a way that, in a structured way so they can retrieve information, we can put them in a graph. So, for example, you have Kepler and he has the planetary motion. So we say that those sets of laws are published by this, by Kepler. And there are many things about him that we are interested in knowing, such as his ethnic group. So it points to Germany. Once you're looking at the German node here, there are all kinds of things about, all kinds of things about German people. And all the other German people are kind of connected by that particular node. So you can see that this is just a fragment of subgraph embedded in a graph with many, many, many nodes and many, many connections. So, for example, he's an astronomer. Of course, it connects to many more astronomers. If you go from that node to the other, to the other nodes attaching to it. And now you have, for example, space telescope that is", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_334359_ms_-_409973_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 334359, "end_ms": 409973}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 396631 ms - 469769 ms", "content": "Title: CS224V Lecture 13 > Transcript > 396631 ms - 469769 ms\n\nContent: he's an astronomer. Of course, it connects to many more astronomers. If you go from that node to the other, to the other nodes attaching to it. And now you have, for example, space telescope that is named after Kepler. The connection is just by naming and so forth. And this connects to NASA. And now you have a subgraph where NASA is connected to Kepler and so forth. Okay? So it just goes on everything. So many things are actually connected with each other. And you can do a lot with this graph. And the question is, how do we get information out of it, given this representation? Now, this is very, very different from tables, okay? So this is because the information is very sparse. Every node that you have has a bunch of properties that are very different from each other. And so really, the more efficient way of representing it is through these notes and edges rather than tables. So what we're going to talk about is wikidata. It is the single most. This is the largest life knowledge", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_396631_ms_-_469769_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 396631, "end_ms": 469769}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 453073 ms - 531411 ms", "content": "Title: CS224V Lecture 13 > Transcript > 453073 ms - 531411 ms\n\nContent: the more efficient way of representing it is through these notes and edges rather than tables. So what we're going to talk about is wikidata. It is the single most. This is the largest life knowledge graph. It Is it's life because it is still growing. It currently has 15 billion facts. Okay? That's a large number. And it is growing by a billion triples a billion set of facts per year. Over 100 million entities, 10,000 properties, 25,000 contributors. And it is in all, you know, all kinds of information in all kinds of languages are connected to this. So we all know about Wikipedia. There are many pages in Wikipedia and every single entry in the Wikipedia is in Wikidata, but not vice versa. Okay? We're talking about many more entries of information that are not. They don't even have a Wikipedia page, okay? So, you know, it is huge. So, for example, if you look at Palo Alto, a nearby city here, if you just take population, it will tell you in 2000, it's got 58,000. And by 2020, this", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_453073_ms_-_531411_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 453073, "end_ms": 531411}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 515771 ms - 594619 ms", "content": "Title: CS224V Lecture 13 > Transcript > 515771 ms - 594619 ms\n\nContent: page, okay? So, you know, it is huge. So, for example, if you look at Palo Alto, a nearby city here, if you just take population, it will tell you in 2000, it's got 58,000. And by 2020, this census, we are at 68,000, okay? So there's a lot of detailed information about every little place on Earth. Everything that you know of, you know, you're curious about, it's most likely it's going to be in there. And the information is not just like, what is the current population? It's got the historical information. It tells you where the information comes from and so forth. So this is just a tiny, tiny picture of what is going on in this state, in this big data set. So Wikidata is a represent. It's a knowledge graph. And in particular it is an RDF graph. RDF stands for resource. Oh, that's a good question. Resource. I forgot. What is it somebody looks at? Yeah, Resource Description Framework. Thank you. Okay, so we have many, many nodes. We have many entities, over 100 million. And what you", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_515771_ms_-_594619_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 515771, "end_ms": 594619}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 575977 ms - 647487 ms", "content": "Title: CS224V Lecture 13 > Transcript > 575977 ms - 647487 ms\n\nContent: question. Resource. I forgot. What is it somebody looks at? Yeah, Resource Description Framework. Thank you. Okay, so we have many, many nodes. We have many entities, over 100 million. And what you find is that there are many people with the same names, just about like Michael Jordan. Who is Michael Jordan? Very famous, right? Who is Michael Jordan? The basketball player? No, no, no, there's another Michael Jordan that's very famous. Who is it? No, there's a professor at Berkeley in AI. He's famous in our circle, but Michael Jordan, for example. So here's an example. If you just say Michael Jordan, I don't even know who you are talking to. Pick some, you know, if you just pick up some, pick somebody really famous here. So there are many entities with exactly the same name. So what they have done is that they assign a single ID for every single individual. It is called qid. QID is now, I don't know, over seven digits. I mean, seven, eight digits because there's so many of them. And", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_575977_ms_-_647487_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 575977, "end_ms": 647487}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 634263 ms - 703265 ms", "content": "Title: CS224V Lecture 13 > Transcript > 634263 ms - 703265 ms\n\nContent: have done is that they assign a single ID for every single individual. It is called qid. QID is now, I don't know, over seven digits. I mean, seven, eight digits because there's so many of them. And what is actually really curious is that the same person may show up in different Wikipedia, different languages. We talked about an example last time with my advisor. He's in the Chinese Wikipedia as well as an English Wikipedia, but they all point to the same id. So if you pick the person, you go to the ID and you can see how this person is referred to everywhere with potentially very different names. It also connects to all kinds of databases outside of Wikipedia. So I don't know, you probably didn't realize that if you take Madonna, she's represented in every single country by some local id and this wikidata will connect to all of them. So you can dig up everything that is being said about Madonna all around the world. Okay, so this is an amazing resource. And then, so those are the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_634263_ms_-_703265_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 634263, "end_ms": 703265}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 689479 ms - 759895 ms", "content": "Title: CS224V Lecture 13 > Transcript > 689479 ms - 759895 ms\n\nContent: id and this wikidata will connect to all of them. So you can dig up everything that is being said about Madonna all around the world. Okay, so this is an amazing resource. And then, so those are the notes and then you have the edges. The edges are called properties and they're also given PIDs. But the property English words are unique to each other. So it is not so complicated. But qids are really where the. Where a lot of work have gone into. And it is very tricky in order to figure out which identity that you are referring to in terms of the entities. So how, what does sparkle look like? If I say something like, just like who found its Stanford? This is the sparkle. It says select question mark X. X is going to show up as a parameter in these triple forms. The triple form always looks like some kind of entity, a property and then an entity. Okay, so this is a pattern. If you are missing the third one you are trying to figure out, sometimes it is a subject, sometimes it is the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_689479_ms_-_759895_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 689479, "end_ms": 759895}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 745521 ms - 819653 ms", "content": "Title: CS224V Lecture 13 > Transcript > 745521 ms - 819653 ms\n\nContent: like some kind of entity, a property and then an entity. Okay, so this is a pattern. If you are missing the third one you are trying to figure out, sometimes it is a subject, sometimes it is the object, and which one you are seeking is the question mark. You can even ask what is the property that connects A and B? Okay, Any one of them can be a parameter. So this is why there's a very uniform syntax actually when it comes to the sparkle query. And everything we said has a QID. So Stanford QID is Q41506. How many of you know that? It is very tricky, right? I mean, we don't know that the Property founded by is112. Okay? But nobody is going to write code like that. I mean, nobody can remember it, nobody can write code like that. So even if it is a simple thing like this, it's not even is not easy to do. And you can imagine that, you know, we just talked about billions and billions, you know, 15 billion triples in this database. We want to access it. And this is really hard. So there is a", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_745521_ms_-_819653_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 745521, "end_ms": 819653}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 805605 ms - 869449 ms", "content": "Title: CS224V Lecture 13 > Transcript > 805605 ms - 869449 ms\n\nContent: easy to do. And you can imagine that, you know, we just talked about billions and billions, you know, 15 billion triples in this database. We want to access it. And this is really hard. So there is a lot of interest, a lot of people are saying, let's go see if we can give this database a natural language interface. It will make a huge difference to make it, you know, to. It will make the, the data a lot easier for people to use. So that was a very simple question. Let us look at something a little bit, a little bit more interesting. Okay, so here's a question. What are the musical instruments played by people who are affiliated with the University of Washington School of Music and have been educated at School of Washington? It's not just affiliation. They actually were educated there. And how many people play each instrument? So what does the answer look like? The answer is going to look like instrument and numbers for each instrument and the number of people have to satisfy these two", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_805605_ms_-_869449_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 805605, "end_ms": 869449}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 856365 ms - 924181 ms", "content": "Title: CS224V Lecture 13 > Transcript > 856365 ms - 924181 ms\n\nContent: how many people play each instrument? So what does the answer look like? The answer is going to look like instrument and numbers for each instrument and the number of people have to satisfy these two conditions. That is the kind of thing that you would do like in research. So this is just a simple question. I mean, a question. And you can pull it out of, you know, wiki data. Isn't that cool, right? I mean, you can, of course, you may not be interested in this question, but you can come up with many more, many more interesting questions on your own. So how is this represented? This is a very tiny snippet of the information. Obviously, there are tons and tons of people educated in the University of Washington. Not all of them are on Wikidata, but there's quite a number of them. And so what is down in the middle here are all the individuals. They all are given some qid, lots of digits here. And we are interested in people who are affiliated of University of Washington School of Music,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_856365_ms_-_924181_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 856365, "end_ms": 924181}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 908499 ms - 977717 ms", "content": "Title: CS224V Lecture 13 > Transcript > 908499 ms - 977717 ms\n\nContent: is down in the middle here are all the individuals. They all are given some qid, lots of digits here. And we are interested in people who are affiliated of University of Washington School of Music, which is that note here. 980-35717. You want, they want to be educated in University of Washington. So that's the green, green edges. So this is P69, okay? And that is P1416 is the affiliation. And so, for example, this person here was educated at the University of Washington, but not affiliated with the School of Washington, with the School of Music. And therefore, this person is not going to be in the answer. Then you have to look at their instruments. Some people plays multiple instruments. Some people, you know, and these are all people. We pulled out all the ones that play instruments. There are plenty of people who don't play instruments. So this is a snippet again of the information that you are trying to get. And what are you trying. What does the answer look like? The answer looks", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_908499_ms_-_977717_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 908499, "end_ms": 977717}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 966299 ms - 1036259 ms", "content": "Title: CS224V Lecture 13 > Transcript > 966299 ms - 1036259 ms\n\nContent: are plenty of people who don't play instruments. So this is a snippet again of the information that you are trying to get. And what are you trying. What does the answer look like? The answer looks like I have to count all these nodes for each instrument and make a table out of it. Now that's a lot of work. Okay, so what does sparkle look like for this particular query? It's not the most complicated query, but you can already see the, the nature of the query. Right? You're really kind of compiling research Results here. So the first. So this says you execute this sparkle. So it's a select statement. You know, it's a query language. It's common select. It says instrument. You know what kind of instrument? It's gonna come as a QID instrumentlabel, which is the name that we are familiar with. And what I want to do is to count this number of students. And I put it up there, which is the count there. As you can see, this table is very, very long. It's not just even those values that we show", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_966299_ms_-_1036259_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 966299, "end_ms": 1036259}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1023963 ms - 1101959 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1023963 ms - 1101959 ms\n\nContent: And what I want to do is to count this number of students. And I put it up there, which is the count there. As you can see, this table is very, very long. It's not just even those values that we show here. And then we say, what is student? Student is a parameter we're seeking question student. The property of the student is given by the next few lines. So what does the first line say? Well, what is P1303? It's instrument. So what does the first line say? You're finding some student that plays an instrument with a parameter called question mark. Instrument. Okay, Because I care about all the instruments and all the students, and they show up as pairs. So you are bringing in a record that has a student and an instrument. You see the question mark here? And then we're using the question mark up on the top. Okay. The next line, there's a semicolon. It says student. The same student. There's only one parameter in this phrase here, in this line here in the same student, it shares the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1023963_ms_-_1101959_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1023963, "end_ms": 1101959}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1086951 ms - 1161527 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1086951 ms - 1161527 ms\n\nContent: mark up on the top. Okay. The next line, there's a semicolon. It says student. The same student. There's only one parameter in this phrase here, in this line here in the same student, it shares the property 1416 with which is the affiliation of and of what? It's the University of the school. The University of Washington School of Music. So that was the first filter, the first clause. The second one says that it is educated at University of Washington. That's a third clause. And the rest of it is just to say we're going to pull it out of English. And so now this is how we can answer this question, which is what are the musical instruments? That's the question mark, question mark, question mark, instrument. The people affiliated with the University of Washington. That's the first phrase. The second one is the education. And now we count and bring it back up. So this is the sparkle query. Any question? Do you guys. Can you write it now? Yes, you can look it up. So Wikidata consists of a", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1086951_ms_-_1161527_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1086951, "end_ms": 1161527}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1142341 ms - 1217579 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1142341 ms - 1217579 ms\n\nContent: second one is the education. And now we count and bring it back up. So this is the sparkle query. Any question? Do you guys. Can you write it now? Yes, you can look it up. So Wikidata consists of a lot of functions and search bars, and you can look these things up. But if you can look it up, you can imagine writing it, right? But you're not going to memorize it from scratch. There's no memorization possible with that many number of entries. So what we are seeing here is the power of wikidata. Sparco allows you to have relational algebra operations over knowledge graphs. Okay, so Knowledge graphs and data and tables have the same ability to store a lot, to store lots and lots of information. And you can ask about anything that you can ask using SQL in Tables. But now you have to write it in Spark. Okay, so the power is there. And what we've shown in this running example, I'm gonna use this example throughout this lecture. It shows that we are doing filters, we are picking the people", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1142341_ms_-_1217579_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1142341, "end_ms": 1217579}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1203329 ms - 1277421 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1203329 ms - 1277421 ms\n\nContent: in Spark. Okay, so the power is there. And what we've shown in this running example, I'm gonna use this example throughout this lecture. It shows that we are doing filters, we are picking the people who are affiliated with some particular value projections. We are pulling out the instruments. Typically, if my study, educated at an affiliation, belong to two different tables. I will be doing a joint joints and counts and max and mins and all the things that you are familiar with in SQL, you can do it here. So you get the power of query languages. And once you express it in Sparkle, you don't have to figure out how to pull this information out. Somebody implements it for you. You're not walking the graph, pooling them by hand. You just have to give it the query. Somebody's going to implement it and it actually can implement the query optimization because there can be many, many nodes that satisfy any of these requirements. So that's the power of Sparkle and Wikidata. And this is the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1203329_ms_-_1277421_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1203329, "end_ms": 1277421}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1263463 ms - 1331447 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1263463 ms - 1331447 ms\n\nContent: it and it actually can implement the query optimization because there can be many, many nodes that satisfy any of these requirements. So that's the power of Sparkle and Wikidata. And this is the reason why this dataset, the Wikidata, why is it so big? It's because people want to put information in there. It is actually used by researchers in many different fields to share their results in the same common representation. If everybody has their own databases, it doesn't work. So people are, the researchers are sharing the information. And if you go on the web, they would, it's pretty interesting. They would say, oh, I'm using this and this is what I do. Actually, if you go to Wikipedia pages, there are lots of times they have these tables, these tables, a lot of these tables are actually dynamically generated by executing a Sparkle query on a daily basis. Because the Sparkle query keeps changing, right? There's a new basketball game, okay? The data will already be changed. And so every", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1263463_ms_-_1331447_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1263463, "end_ms": 1331447}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1315427 ms - 1408387 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1315427 ms - 1408387 ms\n\nContent: generated by executing a Sparkle query on a daily basis. Because the Sparkle query keeps changing, right? There's a new basketball game, okay? The data will already be changed. And so every so often the Wikipedia pages are actually updated with these dynamic queries. So it's a very much a large data set. And you see it for mathematics, you know, all the different disciplines, okay, that's why we're at 15 billion triples. Excuse me, can we represent all this data as tables? Is it possible? First of all, is it possible? Yeah, I guess I have a nuanced answer. I think so. I think it's possible, possible to do a table of relations and a table of nodes and dump everything. But I don't think that's meaningful. And that's why My perfect. You can put it all on a table, but you lose all kinds of things and it's just a way of putting in the triples. Another way of thinking about putting into tables is that you pick every relation. Then you create a table with two columns, the left side and the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1315427_ms_-_1408387_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1315427, "end_ms": 1408387}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1394849 ms - 1460187 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1394849 ms - 1460187 ms\n\nContent: and it's just a way of putting in the triples. Another way of thinking about putting into tables is that you pick every relation. Then you create a table with two columns, the left side and the right side. Then at least the properties are factored out. Now, once I know that there is a pair, you can take this and then you find this in other tables. It is possible for you to put everything into a table by using this method, combining all the pairs that have the same properties. But once I have a particular node, if I want to look at all its properties. Oh my God. Okay, where are they? You have to check every single table. So theoretically, yes, you can put it in table and you can turn it into SQLs that will pull the information out of tables. Because you just say for all tables, find all the properties, find all the tables that this node is in and look up all the things it is connected to. But if that is a common query, you don't want to support that. That's the reason. Okay, so it's", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1394849_ms_-_1460187_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1394849, "end_ms": 1460187}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1448351 ms - 1512223 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1448351 ms - 1512223 ms\n\nContent: the properties, find all the tables that this node is in and look up all the things it is connected to. But if that is a common query, you don't want to support that. That's the reason. Okay, so it's not like you cannot put in tables. First of all, the table is if you. What I told you is just one way of putting it. There are other ways of organizing it. You can take the QID and you. And you have as many columns as there are possible properties. Okay. And that will be very, very sparse. Okay. So either way, it just doesn't seem to work well. And this is why people have graphs. Okay. This is important to know. It's not like all of a sudden we just say that we used to say, oh, everything can be in tables. And now we say, oh, you cannot put it in tables. It is really a matter of efficiency in a very big way. Okay. So I hope to have convinced you that we need knowledge bases or knowledge graphs. So why is KBQA so challenging? There are so many people working on this. And especially now", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1448351_ms_-_1512223_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1448351, "end_ms": 1512223}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1495949 ms - 1566911 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1495949 ms - 1566911 ms\n\nContent: in a very big way. Okay. So I hope to have convinced you that we need knowledge bases or knowledge graphs. So why is KBQA so challenging? There are so many people working on this. And especially now that we have LLMs, does it solve the problem? So I'm going to talk about. I may give you a very brief overview of the major techniques people have used because we have been talking about semantic parsing. I'm going to talk about semantic parsing first, but it is not the first technique that people tried. As a matter of fact, I would say this is the. The most recent work actually that we did is on semantic parsing. But I want to. Because you're familiar with semantic parsing, we stop there and then we look at a more traditional method called subgraph traversal. So semantic parsing Means I give you natural language and you translate into sparkle. So let me give you an example of a flow here. Suppose I ask you, where did Bronx take place? So the idea is that first of all, what does Bronx", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1495949_ms_-_1566911_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1495949, "end_ms": 1566911}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1552789 ms - 1627305 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1552789 ms - 1627305 ms\n\nContent: you natural language and you translate into sparkle. So let me give you an example of a flow here. Suppose I ask you, where did Bronx take place? So the idea is that first of all, what does Bronx mean? Okay, Bronx could be the place, could be whatever. But because of the sentence you're guessing, we can guess it is actually the movie A Bronze Tale. Okay? So this is called entity linking and it is a very tricky, very important piece. We rely on having a good entity linker and we use refined and, and we will talk about refined in this later lecture. So somebody is pulling out the entity. This entity is then fed in the semantic parser. So this semantic parsing is the one that connects a Bronx tail with that qid. Okay, so this is done with the entity linking module. Now I say given the English, given the qid, I'm going to run this on semantic parser. And the semantic parser that we used in this case was a llama, fine tuned with 3,000 examples, okay? And so it has seen enough input and", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1552789_ms_-_1627305_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1552789, "end_ms": 1627305}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1613017 ms - 1688189 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1613017 ms - 1688189 ms\n\nContent: given the qid, I'm going to run this on semantic parser. And the semantic parser that we used in this case was a llama, fine tuned with 3,000 examples, okay? And so it has seen enough input and output so that when you give me an input, I can try to give you an output. And the output in this case is the sparkle query. So this is the sparkle query. And from this question it says select distinct question mark X. We are seeking X such that X is the feeling location of this QID that movie. So that's what that triple means. And now that you have a triple, what do you do is that you now execute against the wiki data and you get a response. If you have a good query, if the information is there, then you can get the answer. And in this case the filming location is New Jersey and New York. So that's the result, says New Jersey and New York from Wikidata and the response generator and we'll put it into a sentence. Notice this is really important that the whole sentence tells you what the search", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1613017_ms_-_1688189_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1613017, "end_ms": 1688189}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1673165 ms - 1749501 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1673165 ms - 1749501 ms\n\nContent: the result, says New Jersey and New York from Wikidata and the response generator and we'll put it into a sentence. Notice this is really important that the whole sentence tells you what the search is. It doesn't just say Bronx took place at these locations. You are looking up the filming location. You say it is the filming location. Okay, you don't just say, just give me the locations. You say it in the full sentence. The filming location is this. And in this. Well, if in the case that we don't have a response, this is an experiment that we have done is that we ask GPT. Maybe GPT knows. So if the answer. So in this case it turns out when I say where did Bronx take place? It's ambiguous. It could be the story, it could be the filming location. But in this particular case, the user means to ask about the story where the story takes place and the user says, no, I don't want this. And at that point, an experiment that we did is that we just default to GPT. And then you ask, and then you", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1673165_ms_-_1749501_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1673165, "end_ms": 1749501}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1736245 ms - 1807457 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1736245 ms - 1807457 ms\n\nContent: ask about the story where the story takes place and the user says, no, I don't want this. And at that point, an experiment that we did is that we just default to GPT. And then you ask, and then you say, oh, GPT guesses that the movie took place in Bronx, New York. Okay, so there are in this, in this flow here we're combining the LLM guesses with a Wikidata retrieval with the idea that it is possible that I don't know the answer, we fail maybe through this pop route. But when I call GPT for the answer, I tell you it comes from GPT so that you know that if the answer comes straight from here, the answer is 100% correct with respect to Wikidata. I may not be answering your question, but it's never incorrect. And if that's still not what you want, so you may want to default it to GPT. So this is an example of a flow. So for this data set, for this paper, for this paper that we did, we have created a new data set which is adapted from Web Questions from Freebase. Freebase was a precursor", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1736245_ms_-_1807457_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1736245, "end_ms": 1807457}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1790959 ms - 1872461 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1790959 ms - 1872461 ms\n\nContent: is an example of a flow. So for this data set, for this paper, for this paper that we did, we have created a new data set which is adapted from Web Questions from Freebase. Freebase was a precursor to Wikidata. It is much smaller and it actually has a simple property that every node has the same number of properties, which is not true for Wikidata. And the questions were real, they come from Google Suggest. And these are real world popular questions. The data set is just a few thousand train, a little bit of depth and a bunch of tests. And we did use this for fine tuning Lama. And if you look at this and say, look, as we try to understand the role of retrieval and LLMs, we think that it is actually interesting to see how LLMs handle these sets of popular questions. So we know that GPT3 has been trained on Wikipedia and on the Internet. So it actually has seen the answers to a lot of these questions, probably to all of these questions. The question is whether they know how to answer", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1790959_ms_-_1872461_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1790959, "end_ms": 1872461}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1860925 ms - 1937409 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1860925 ms - 1937409 ms\n\nContent: has been trained on Wikipedia and on the Internet. So it actually has seen the answers to a lot of these questions, probably to all of these questions. The question is whether they know how to answer the question or not. So that was an experiment we did. And what is actually interesting is that GPT3 is only of course 4.0 is going to be better, but the shape of what you expect is the same. It's actually correct only 66% of the time. 27% is actually incomplete. 7% is totally incorrect in this experiment. So what does incomplete mean? Suppose I say where does Obama have a degree in chief Ning3 will say, oh, political science degree, which is true, but he also has a law degree, so it is only partially correct. Is this good or bad? It depends on your purpose. If you just want to know a little Bit of this, yes, but if I'm looking for drug interactions in the Wikidata database. No, because you want to know all the, all the drug interactions. But there is potential that it's completely wrong.", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1860925_ms_-_1937409_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1860925, "end_ms": 1937409}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1924309 ms - 1995523 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1924309 ms - 1995523 ms\n\nContent: of this, yes, but if I'm looking for drug interactions in the Wikidata database. No, because you want to know all the, all the drug interactions. But there is potential that it's completely wrong. So here's a question. Where does the name Melbourne come from? And GPT3 can come up with this? It comes from the Latin word melbournum. Actually I turn around and ask GPT, is this a Latin word? And they say no, okay, it's inconsistent even within itself. But anyways, that is the real answer. And so the answer is completely wrong. So here is where the problem is. I know that it is incomplete, it is incorrect 7% of the time. Do you know when you are seeing that 7%? If you know the answer? Yes. But if we don't know the answer, no, any answer that it gives you, if you don't know the answer already could be wrong. Okay, so that's where the problem is. It's not, you know, I just want to find out the answer to one question. I don't care how many questions you get right, I only care if you get my", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1924309_ms_-_1995523_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1924309, "end_ms": 1995523}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 1982227 ms - 2056217 ms", "content": "Title: CS224V Lecture 13 > Transcript > 1982227 ms - 2056217 ms\n\nContent: could be wrong. Okay, so that's where the problem is. It's not, you know, I just want to find out the answer to one question. I don't care how many questions you get right, I only care if you get my question right. But there is no way I can tell whether it belongs to that 7% or the 66%. So the conclusion here is that you have to fact check all the questions, all the answers. But if I get it from a database, I can guarantee you whatever is in the database is as correct as anybody knows it. For example, in the Wikidata database. So when we run this experiment we discover that if you use Wikidata using the fine tuned llama, we are able to get 76% answers correctly. And it is verifiable because it is verified by the Wikidata. So that is really nice to have. You don't have to fact check any of the information in this 3/4 of this data set. In the remaining 25%, if we go ask GPT for the answer and provide you with the answer, then the breakdown changes because Wikidata answers some of the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_1982227_ms_-_2056217_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 1982227, "end_ms": 2056217}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2040545 ms - 2117367 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2040545 ms - 2117367 ms\n\nContent: of the information in this 3/4 of this data set. In the remaining 25%, if we go ask GPT for the answer and provide you with the answer, then the breakdown changes because Wikidata answers some of the questions correctly. The combination here is that it's 4% of the answers are incorrect. So we kind of have the. Using this Wikidata has two effects. One is that you don't have to fact check 75% of the time. Secondly, you also get a better result. But I also want to show you that, I mean the parser does not necessarily always get you the answer. Okay, we're still dealing with this, this with AI, with, you know, this just using a fine tuned llama. So what should we should do now? So let's take a look at the weakness. Not only are we at 75%, the wiki web questions are the very popular questions. And most people ask very simple questions. They are not researchers. The people asking about the musical instruments, that's a researcher. Okay, it's a little bit more complicated looking at the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2040545_ms_-_2117367_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2040545, "end_ms": 2117367}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2107964 ms - 2179019 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2107964 ms - 2179019 ms\n\nContent: And most people ask very simple questions. They are not researchers. The people asking about the musical instruments, that's a researcher. Okay, it's a little bit more complicated looking at the whole table and so forth. But the easy wiki web questions are actually easy already. And the web, the popular questions use mostly the same properties as the training data. So it is not indicative of what it means to get information, to get queries, to ask queries from the wikidata. As it turned out that wikidata has about 3000 knowledge oriented properties. 7000 is about connecting to other databases and stuff like that, but 3,000 properties. And if you look across all the data sets that people have done, we're at hundreds of properties, nowhere touching the 3,000 properties. And if I am fine tuning, if I have never seen that property in my training data, I will not be able to change, generate you the correct sparkle query. There is no way. Okay, so that is something we know. So what we are", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2107964_ms_-_2179019_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2107964, "end_ms": 2179019}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2165789 ms - 2232869 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2165789 ms - 2232869 ms\n\nContent: if I have never seen that property in my training data, I will not be able to change, generate you the correct sparkle query. There is no way. Okay, so that is something we know. So what we are working on, what we have worked on is create a new dataset called Spinach. And in this experiment it has a lot more, but it is still 298 unique properties. But we will explain that this is in context learning. It actually has no advantage of whether you know of. I don't have to include all the, all the properties we're going to be able to handle properties we have never seen before. None of the fine tuned solutions can handle properties that it has never seen before. That's a huge difference. So querying wikidata with sparkle is very difficult. That's why people don't even attempt it. We're the first. We're kind of like the people who really tried to use SparQL because we know the advantage of using query language. Query language. First of all, it has many properties. We just described that.", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2165789_ms_-_2232869_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2165789, "end_ms": 2232869}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2219929 ms - 2289349 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2219929 ms - 2289349 ms\n\nContent: We're kind of like the people who really tried to use SparQL because we know the advantage of using query language. Query language. First of all, it has many properties. We just described that. How much do I have to fine tune to learn the properties? No fixed schema. Normally you take a person in a table, you take a row, you know what all the possible columns are. There is only a small number of them here. Every node, even of the same type may have different properties. Sometimes we know the time of birth for one person and sometimes we know what occupation of another person. But there are many, many possible properties and you don't know what you have. In this knowledge set there are many similar properties and even you just don't know which property or entity should be used. Suppose I Take a question about a location. Where did Isaac Newton live? Or where is Salesforce? This is just a location. You know, I'm looking for a location. And what properties would you use? There are many,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2219929_ms_-_2289349_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2219929, "end_ms": 2289349}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2277381 ms - 2339679 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2277381 ms - 2339679 ms\n\nContent: a question about a location. Where did Isaac Newton live? Or where is Salesforce? This is just a location. You know, I'm looking for a location. And what properties would you use? There are many, many properties that have to do with locations, and it really depends on what is put into the node. For example, a lot of times they say the administrative territorial entity, okay? Sometimes they tell you the residents, the state, the country. Sometimes you don't know where this person lives. So suppose you say, where did Isaac Newton live? Maybe, I don't know. Maybe only. The only thing that we knew we know is where he was born and where he died. We don't. Maybe we don't know where he lived. Okay, so how do I answer that question? You make the best guess. Okay. You know, you just say, oh, I'll tell you that he. He was born here and so forth. It's a good guess that he may have. He has lived part of their life in those places. So all of this is just a matter of, you know, you just don't know", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2277381_ms_-_2339679_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2277381, "end_ms": 2339679}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2326563 ms - 2387927 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2326563 ms - 2387927 ms\n\nContent: tell you that he. He was born here and so forth. It's a good guess that he may have. He has lived part of their life in those places. So all of this is just a matter of, you know, you just don't know where it is. If I say Salesforce, are we talking about the headquarters? Maybe you know where the headquarters are. Maybe the only time, you know, maybe for some companies, you know, all you know is where it was founded. So we don't even know how to answer this question. It's a little bit like retrieval from free text. You ask me a question, I retrieve it, and I answer whatever I know about that case. But I can't figure out what properties to use if I have to generate a semantic parse for it. So in other words, I have to kind of look at the content in order to come with the sparkle. And you have a chicken and egg problem. Okay, what are you asking? And what are you. And how do you know how to. You don't know how to ask. You have to look at the data. And so this is why approaching it with", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2326563_ms_-_2387927_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2326563, "end_ms": 2387927}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2374633 ms - 2444819 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2374633 ms - 2444819 ms\n\nContent: a chicken and egg problem. Okay, what are you asking? And what are you. And how do you know how to. You don't know how to ask. You have to look at the data. And so this is why approaching it with sparkle is very difficult. We made some progress on the popular questions, but definitely cannot handle the research questions of all the different properties that exist and all the different combinations of nodes of properties that each node has. So the second very popular method is subgraph retrieval. You remember the picture of Kepler? You know, it's like everything is connected. The Washington. Everybody is connected. So you're kind of walking through these graphs. So people say, why don't we map it to a graph retrieval problem? We retrieve the part of the graph based on the question. So suppose I ask the question, where did Canadian citizens with Turing Award graduate? Well, how many Canadian citizens have touring awards? Okay, so they have an easy question, actually. Okay, Think about", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2374633_ms_-_2444819_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2374633, "end_ms": 2444819}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2429565 ms - 2497085 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2429565 ms - 2497085 ms\n\nContent: I ask the question, where did Canadian citizens with Turing Award graduate? Well, how many Canadian citizens have touring awards? Okay, so they have an easy question, actually. Okay, Think about the music instrument case. It's a lot of people who satisfy it, but this is actually relatively easy. So you say, here's the Turing Award. There is a small handful, a small number of them. Then you've got. The entity is Canadian now that is much larger. Right. Where did Canadian citizens with Touring Award graduate? Okay, so you want to know which school they went to, but you have to first figure out who are Canadians who are Touring Award winners. And then you go look into it, and then you look at the Touring Award and then you pull all the people out and the Canadian. How are you going to get all the Canadians, all the famous Canadians? Because they are the only ones in the wikidata. Right. We don't need to find all the Canadians in the world. But anyways, and then you are now staring at", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2429565_ms_-_2497085_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2429565, "end_ms": 2497085}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2486893 ms - 2565853 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2486893 ms - 2565853 ms\n\nContent: all the Canadians, all the famous Canadians? Because they are the only ones in the wikidata. Right. We don't need to find all the Canadians in the world. But anyways, and then you are now staring at many, many possible nos. And then you say, we need somebody that handles that are in both camps. So Hinton, for example, is in both camps. Then you chase forward and then you say, oh, these are the schools they went to that this person, you know, the same person went to. And then they put together and they come up with the answer. It says Edinburgh and Cambridge. All right, so now I'm doing search through the graphs. Does this work? How well does it work? So suppose I want to find the tallest mountain. What do I need to do? Yeah, like get all the mountains in their heights and then rank them. Yeah. So you can see it. I can bring all the, all the notes in. And, and who's going to implement the code of fighting the tallest mountain? You have to. You just search for it, you compile it. So", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2486893_ms_-_2565853_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2486893, "end_ms": 2565853}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2552435 ms - 2616899 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2552435 ms - 2616899 ms\n\nContent: Yeah. So you can see it. I can bring all the, all the notes in. And, and who's going to implement the code of fighting the tallest mountain? You have to. You just search for it, you compile it. So you're not using the SQL power, because if you can express it, somebody will implement it for you. Right. Take the max of the height. But now it means that if this is what you want to do and you want to answer this question, you find the graph and then you figure out how to turn your query into code. Right. So that's an example. What if we. What about our running example? How can we handle the running example that we just talked about? Compiling the table and people, number of students for each instrument. It's a little bit more than rank now, but it's the same story. I have to dig up everything. The graph is much bigger. It's not just one mountain. You have people that is affiliated where they're affiliated and where they are studying. And you combine them and then you do a group by. And", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2552435_ms_-_2616899_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2552435, "end_ms": 2616899}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2604211 ms - 2671399 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2604211 ms - 2671399 ms\n\nContent: The graph is much bigger. It's not just one mountain. You have people that is affiliated where they're affiliated and where they are studying. And you combine them and then you do a group by. And then you have to count and all this stuff, you're implementing a lot of code here for a tiny little query from a Sparkle perspective. So working off all these nodes in the subgraph and then figure out how to calculate the answer is not easy, right? But if you pick a simple question like this, you can see this works. Okay, so that's the important question is what kind of questions can this method handle? So following this method, there is a more recent paper, this is 2024, and they said, let's use LLM and doing graph exploration. And it walks the graph one edge at a time, kind of like what we just saw. Suppose we say, what is the majority party now in the country where Canberra is located? So you mentioned Canberra. So we're going to stop there. You look for the triple. So this is LLM", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2604211_ms_-_2671399_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2604211, "end_ms": 2671399}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2656711 ms - 2724055 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2656711 ms - 2724055 ms\n\nContent: we just saw. Suppose we say, what is the majority party now in the country where Canberra is located? So you mentioned Canberra. So we're going to stop there. You look for the triple. So this is LLM thinking, right? We're now doing more agentic approach. You say, oh, I'm looking for things that have to do with Canberra. And then I look at the graph and the graph has all these properties and then say, oh, it's a capital of Australia. Then you see, so you think, and you say, oh, the most relevant one is Canberra information. But I cannot answer that question. But I know that it is capital of Australia. Now I'm going to find out something about Australia, right? Because I still don't know who the majority party is. So you are trying to take the leaps to say, oh, because of the country Australia. I'm going to look that up and there are many, many possible things that are associated with Australia. And so you look at the data and you say, oh, the most relevant one is the Prime Minister.", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2656711_ms_-_2724055_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2656711, "end_ms": 2724055}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2714395 ms - 2794669 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2714395 ms - 2794669 ms\n\nContent: I'm going to look that up and there are many, many possible things that are associated with Australia. And so you look at the data and you say, oh, the most relevant one is the Prime Minister. And then it is using LLM knowledge and I say, oh, this person has been the leader of the Labor Party and therefore I know that it's from labor party. And then you conclude the answer is Labor Party. Okay? So in a sense, if I don't know anything about Sparkle and if you give me the wikidata and you ask me to go find the answer, I'll do exactly that, right? You pick up whatever you can and you look up the nodes and then you say, oh, this is how I take a leap to this point and this point and this point. And I'm just looking at the node, looking at the graph one node at a time, linking them up based on the question. Okay, so how well does this work? Can it handle the running example that we have? Henry, what would you. What, what does that have? What happens if that. If you are given that running", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2714395_ms_-_2794669_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2714395, "end_ms": 2794669}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2769151 ms - 2874185 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2769151 ms - 2874185 ms\n\nContent: the question. Okay, so how well does this work? Can it handle the running example that we have? Henry, what would you. What, what does that have? What happens if that. If you are given that running example and you give this to this technique. Anybody here? Come on. Where would you start? Yes, did you. Do you. Yeah, you start with the University of Washington, which is an entity. And then I will look it up on what's related to Washington. Check your student and then go to which one from the music department stuff. But I might need some students. Well, you have to search. So if you look at educated at that place, there are tons and tons of people. You're going to look at each one of them and then for each one you say is it affiliated? And then the graph just, you know, you start with some number of nodes and then you reduce them. But there's tons of work that you are doing. LLM can't do that. Okay. Even LLMs get tired. Well, I don't. It will probably introduce mistakes along the way,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2769151_ms_-_2874185_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2769151, "end_ms": 2874185}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2861281 ms - 2928163 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2861281 ms - 2928163 ms\n\nContent: number of nodes and then you reduce them. But there's tons of work that you are doing. LLM can't do that. Okay. Even LLMs get tired. Well, I don't. It will probably introduce mistakes along the way, but it is very. The point here is that the question has a very high level explanation. If you can map it into the Sparkle query, you're basically mapping it to a program. If you don't do that, you just say I'm the one who is going to execute it. By reading the notes one at a time, you're going to take a long time because we know how big this database is. So you're just skipping all the advantage of concept of database queries. If you are staring at the nodes one at a time, it's 15 billion triples in that data in that knowledge graph. Okay. There are certain questions that this can answer, but this is nowhere near giving you the power of what you can get with the key data. Now that's where the problem lies. So you have to start with some entity like the University of Washington, tons of", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2861281_ms_-_2928163_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2861281, "end_ms": 2928163}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2915251 ms - 2990591 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2915251 ms - 2990591 ms\n\nContent: this is nowhere near giving you the power of what you can get with the key data. Now that's where the problem lies. So you have to start with some entity like the University of Washington, tons of people connected to it. Now what are you going to do? And we haven't even got to the instruments part. So you'll be compiling that table yourself. So that's why it is very challenging with previous work. So what are we going to talk about is the agentic approach for knowledge basis. And this is a paper, it is actually going to be presented next week. Yeah, next week in Miami. This is work that we did together with Harold Chitman who was at Wikimedia and He is now PhD student at Cornell Tech. So he saw some of our old work and he came to us and he says, oh, let's work together on this. And it's a very. Every time so far, every time we work with real data we learn something. And so the idea. So we have built it. So this is the agent. And so for example, you give me the query here, it would", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2915251_ms_-_2990591_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2915251, "end_ms": 2990591}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 2975431 ms - 3042807 ms", "content": "Title: CS224V Lecture 13 > Transcript > 2975431 ms - 3042807 ms\n\nContent: a very. Every time so far, every time we work with real data we learn something. And so the idea. So we have built it. So this is the agent. And so for example, you give me the query here, it would generate the answers like what we were talking about. We'll get into the details. And it is now deployed at Wikidata they're actually using it because they have a real problem of giving, helping people get access to wikidata. So what we're gonna do is to combine the best of everything from Sparkle subgraph retrieval and LLMs. So what do I get out of Sparkle Query is expressiveness. I give you the program. It's a high level program. I don't wanna execute it by staring at the graph implementation. Query optimizations. Once I can give you the query, you can do all that for me. The graph retrieval is important because we don't even know if you give me a node what kind of properties it has. So we have to examine the actual properties of the data. So those are the two things that we need. We're", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_2975431_ms_-_3042807_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 2975431, "end_ms": 3042807}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3030615 ms - 3095875 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3030615 ms - 3095875 ms\n\nContent: is important because we don't even know if you give me a node what kind of properties it has. So we have to examine the actual properties of the data. So those are the two things that we need. We're going to bring them all together with LLM. How are we going to use LLM? Okay, we have used LLM for fact checking and so forth. What technique did we do there? The most important, the thing that we have been doing all along is to say, let's just do it the way humans do it. If I want to answer a question, I do search and I do generation. I do claim checking. That's what I do by hand. That's what we're going to teach the system to do. Just imitate what the users can do with the idea that if the experts can do it, so can we. Okay, so we want to automate this approach. Humans. I can come up with this. I think that that was the question. What is your name? Angelina. Angelina. So you asked me the first question is how do I know where the wikidata numbers, where the QID and PIDs are? They have", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3030615_ms_-_3095875_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3030615, "end_ms": 3095875}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3083531 ms - 3150127 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3083531 ms - 3150127 ms\n\nContent: this. I think that that was the question. What is your name? Angelina. Angelina. So you asked me the first question is how do I know where the wikidata numbers, where the QID and PIDs are? They have tools. This is what humans use. And obviously we're going to do the same thing. We don't need to memorize the notes and properties. We can still come up with a query with a little bit of work. And that's what we're going to use as the premise. We just automate what the users do. And fine tuning is about memorizing things. We don't need to do this, we think. And we can just use ICL in context learning. So it takes us a while. I mean, it looks complicated. It was complicated. It took us a while to learn how to write Sparkle queries. So let me give you the basic idea. If you give me a big query like that question, the way I would approach it is I would start with something simple. Okay, can I find out people from University of Washington, can I find out who's educated and so forth or who", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3083531_ms_-_3150127_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3083531, "end_ms": 3150127}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3136279 ms - 3204247 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3136279 ms - 3204247 ms\n\nContent: like that question, the way I would approach it is I would start with something simple. Okay, can I find out people from University of Washington, can I find out who's educated and so forth or who plays the instruments? These are simple Queries and we're going to look up the information about the entities and the property pages through Wikidata. I don't know those IDs, and I want to understand what is available, what are the properties and so forth. And I do have to know, given a node, what are the properties that it has? And then we got to put them together. We build bigger and bigger sparkle queries to build towards the final sparkle. So this is what we would do by hand. And what this weaves together is besides query writing and doing the subgraph retrieval, the most interesting thing that I want to highlight here is the knowledge inquiry. I actually try to find out what I don't know, what I need to know. If I can retrieve information, then I can put it together, I don't know,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3136279_ms_-_3204247_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3136279, "end_ms": 3204247}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3189519 ms - 3262315 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3189519 ms - 3262315 ms\n\nContent: that I want to highlight here is the knowledge inquiry. I actually try to find out what I don't know, what I need to know. If I can retrieve information, then I can put it together, I don't know, straight off the top of our head. But the information must be written down somewhere for the humans to do it. And so we just tap into exactly that knowledge. So it turns out that in our work, what we found is that these are the tools that Wikidata have made available. Search Wikidata is that you give me some text, I give you the IDs. If you give me an entity QID, I tell you what is on that node, what properties it has, you give me a property id, I can give you some examples, and then and so forth. I can of course execute the sparkle for you. So for example, in this case, if you see that I'm looking for instrument, musical instrument, I call search Wikidata. So by the way, these things, they are API calls, but you can also do it at the human level using these web pages. Okay, they're all", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3189519_ms_-_3262315_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3189519, "end_ms": 3262315}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3248599 ms - 3323215 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3248599 ms - 3323215 ms\n\nContent: looking for instrument, musical instrument, I call search Wikidata. So by the way, these things, they are API calls, but you can also do it at the human level using these web pages. Okay, they're all available for manual lookups. So if you talk about music or instrument, this is the information it will tell you all, given that word, what are all the relevant QIDs. So in other words, it is doing ned, I talked about refined, you know, the named entity recognition. How do you know which ID it is? So when we do a search, those ID show up and then you can look at this and you know what you're looking for. And you can pick the right one. If we can pick the right one, LLM can pick the right one as well. So this is very, very useful. The text to the IDs, this is QIDs, these are PIDs. The affiliation, they have two IDs, it is the affiliation and then the string, affiliation, string, and so on. The next one is getting the data. Wikidata entry. If you give me a qid, I look it up. So here,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3248599_ms_-_3323215_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3248599, "end_ms": 3323215}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3307886 ms - 3376037 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3307886 ms - 3376037 ms\n\nContent: they have two IDs, it is the affiliation and then the string, affiliation, string, and so on. The next one is getting the data. Wikidata entry. If you give me a qid, I look it up. So here, University of Washington School of music. I look it up, it tells you all kinds of things on that one particular Note. It has 100 million notes, 100 million pages like this. Okay, Nobody's going to memorize it, but this is the information it takes. So you fetch it based on what you need. You fetch it and then it tells you, oh, it's field of work. The higher education, postgraduate education musicology, music located here have subsidiary here. All kinds of information can be used once you pull up the qid. But the details of this all is just mind boggling. And nobody is going to memorize this. So this is why you do have to look up the notes. And it's the specific note. It is not about any school. It is the school the University of Washington or the School of Music in University of Washington. Very,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3307886_ms_-_3376037_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3307886, "end_ms": 3376037}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3364249 ms - 3436945 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3364249 ms - 3436945 ms\n\nContent: why you do have to look up the notes. And it's the specific note. It is not about any school. It is the school the University of Washington or the School of Music in University of Washington. Very, very detailed. Another one is get property examples. So we know that we are looking up. Remember, we're looking up affiliation. We found that P1416 is the affiliation. That sounds like what we need because there's a description. And if you make this call, it will give you these pairs of information of who is basically the two question marks. The subject predicate, object. These are the subject predicate 1416 affiliation object. So these give you the pairs. By looking at the pairs you actually know a lot more about what that property is. And finally you can execute this. So these are the four resources that we used when we were doing our own human construction of Sparkle query. So what do we do? Give it to the LLM agent. Okay, we just say here, I know that these are useful. You use it, okay?", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3364249_ms_-_3436945_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3364249, "end_ms": 3436945}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3420131 ms - 3491725 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3420131 ms - 3491725 ms\n\nContent: that we used when we were doing our own human construction of Sparkle query. So what do we do? Give it to the LLM agent. Okay, we just say here, I know that these are useful. You use it, okay? And then when you execute it, you get that table. So this takes us to the concept of agentic approach for knowledge basis. So on the left side, the top we say reason only. This is the chain of thought concept. It's like you think, think, think, think, think. It is all within this language model. It knows what it knows, it doesn't know what it doesn't know. That's all they have. Then the act only is that I perform an action, I have a result, I look at the observation. So that's act only react. In this case, it's in this paper, Yao and iclear. It is the combination. I have an action, I take an action on the environment, I look at the output, the output goes back to lm. The LM look at the output and then you reason. So you're combining the reasoning with actions and results. Now you can do all", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3420131_ms_-_3491725_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3420131, "end_ms": 3491725}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3479699 ms - 3552617 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3479699 ms - 3552617 ms\n\nContent: on the environment, I look at the output, the output goes back to lm. The LM look at the output and then you reason. So you're combining the reasoning with actions and results. Now you can do all kinds of things. Okay, so in our context here, we are trying to get a user query the agent is going to get you the answer. So how we gonna do this is that we're gonna think and we're gonna issue actions. We take the result and then we keep going, okay, until we find the answer. It's not a single shot because it takes multiple rounds. So the system that we have is that it imitates what the user does. The whole concept is to build the sparkle query. I'm not going to look at the graph myself and give you the answer. It's impossible. It's many, many, many, many steps of retrieving nodes. So what we're going to do is that for N steps we have a history of the agent actions. I'm just implementing this picture here. Okay, so you have a history of what has happened. You prompt the LLM to generate a", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3479699_ms_-_3552617_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3479699, "end_ms": 3552617}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3535809 ms - 3615419 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3535809 ms - 3615419 ms\n\nContent: going to do is that for N steps we have a history of the agent actions. I'm just implementing this picture here. Okay, so you have a history of what has happened. You prompt the LLM to generate a thought and an action. You execute the action, you take the result and you put it back in the history and you repeat until you, you get the answer. Okay, so this is, I can show you the entire code. It's the new world of engineering. This is the zero shot ll policy prompt. It just says, oh, you have to write a wiki data sparkle query. You start by constructing very simple fragments. This is basically what I told you. And here are the possible actions. Qid the search wikidata, get property examples, execute sparkle. We added one more and then it stop. Otherwise it just keeps doing it. So it stops when you get the final answer and ends the process. And the rest of it is very much like how I would tell you to do it if I want you to do it yourself. And we wrap it in this, the syntax that generates", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3535809_ms_-_3615419_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3535809, "end_ms": 3615419}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3598701 ms - 3675969 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3598701 ms - 3675969 ms\n\nContent: you get the final answer and ends the process. And the rest of it is very much like how I would tell you to do it if I want you to do it yourself. And we wrap it in this, the syntax that generates the prompt for every single turn of the conversation of this, of this. It's not conversation this turn of every time you're trying a new sparkle query until you get to the answer. And so we kind of talked about this before. This is the Jinja syntax on how you write up the prompt that issues write up the code that issues the prompts for each of the of the tries of sparkle queries. So that's a basic idea. And because we are trying different queries, we want them to use different techniques. So we run the policy prompt with temperature one. And nonetheless, you still can see that sometimes it fall into a loop. It executes a sparkle query, cannot find the result, it repeats it, cannot find the result. And now you can have an infinite loop. Why should this happen? And how do we solve this", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3598701_ms_-_3675969_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3598701, "end_ms": 3675969}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3661893 ms - 3734407 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3661893 ms - 3734407 ms\n\nContent: fall into a loop. It executes a sparkle query, cannot find the result, it repeats it, cannot find the result. And now you can have an infinite loop. Why should this happen? And how do we solve this problem? Yes, Aaron, every new sparkle query, you could just check against the data list of previous queries. You check if it has happened before. It is very easy to observe that you are. You are repeating yourself so precisely. So if the repeated actions are found, you just reset exploration state before the repetition. You hope that it will go somewhere else. Okay? And it can still go into an infinite loop. So we kept the number of actions by either, you know, talking about the number of actions after taking rollbacks into account. That means that we throw away the rollbacks, we can do only 15 things. And if you include the rollback, we do 30 things. We just want to cap it because sometimes we know, sometimes we just cannot find the answer. So let's run through this example that we have", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3661893_ms_-_3734407_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3661893, "end_ms": 3734407}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3722567 ms - 3793007 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3722567 ms - 3793007 ms\n\nContent: 15 things. And if you include the rollback, we do 30 things. We just want to cap it because sometimes we know, sometimes we just cannot find the answer. So let's run through this example that we have been using. This running example. What other musical instruments? You've seen this question. So this is how the LLM do it. It says, first I need to identify the properties and entities related to people affiliated with the School of Washington, University of Washington School of Music. So it makes the call and it comes back and has a QID and it does a few more things and then it looks worse, musical instrument and so forth. You get the pid. So it is looking up all the entities and all the properties. And then after, after coming up with that, it starts to say, okay, I'm ready to execute. So it makes up this statement and it calls it an answer is wrong. And the reason is that it actually forgot what it has found and it's hallucinated this number, okay? And this WQ, the Q106, blah blah,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3722567_ms_-_3793007_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3722567, "end_ms": 3793007}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3773151 ms - 3857205 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3773151 ms - 3857205 ms\n\nContent: it makes up this statement and it calls it an answer is wrong. And the reason is that it actually forgot what it has found and it's hallucinated this number, okay? And this WQ, the Q106, blah blah, blah. And it turns out that has something to do with some this place. It's amazing, right? Random number, you are somewhere nowhere near where you are and it. And it observes that. It realizes that. It seems like there's an issue with the QID I use for the University of Washington. I should search for the correct qid. It actually does the search again. It has done it before. It just has forgotten because the query is a bit long. And at that point it puts into this query and it gets the answer and we solve it. Okay. It's amazingly easy to write this basic concept of energetic approach. You give it the actions, give you the story, and it just goes and does it. Now how do we evaluate it? There is a lot of work on coming with the data sets. So originally they were collected through search", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3773151_ms_-_3857205_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3773151, "end_ms": 3857205}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3836491 ms - 3928327 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3836491 ms - 3928327 ms\n\nContent: give it the actions, give you the story, and it just goes and does it. Now how do we evaluate it? There is a lot of work on coming with the data sets. So originally they were collected through search engines. Like the one we just said is like, who calls Google searching for what? And it also tries with crowdsourcing. What do you think? Is this a good dataset? What do you worry about? What do you think those queries look like? Have a guess. Yeah, I don't know. I'd Worry about the diversity of the queries, I guess. Well, I'm just asking a question. I can ask anything. Wikidata is huge. I don't have to worry about the scope. That's an interesting question. You don't have to worry about the scope. It's good at facts. And do you remember the web questions? Those are the popular questions. And what happens to those questions? They're usually very simple queries. You ask crowdsource workers, they find the path of least resistance. You want a query, here it is. They're too simple. You're not", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3836491_ms_-_3928327_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3836491, "end_ms": 3928327}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3915351 ms - 3987517 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3915351 ms - 3987517 ms\n\nContent: what happens to those questions? They're usually very simple queries. You ask crowdsource workers, they find the path of least resistance. You want a query, here it is. They're too simple. You're not going to touch the strength. You're not using the strength of Wikipedia. So people say, oh, that's not too good. So another idea is to synthesize it. You just say, look, here are some facts. Can you ask a question based on a bunch of facts that you can pull together? You say, oh, here's something about this school. And this school has these people and stuff make up a question based on that. And it's all synthesized. And is this okay? It turns out that it is very well known in our study is that the synthesized sentences are usually very stylized. They have the same pattern over and over again because what it does is algorithmic. And so here are some data sets and I'll tell you how we construct it. Our Spinach data set. 320 examples. 320 different query patterns. Query patterns is like what", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3915351_ms_-_3987517_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3915351, "end_ms": 3987517}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 3969665 ms - 4044401 ms", "content": "Title: CS224V Lecture 13 > Transcript > 3969665 ms - 4044401 ms\n\nContent: what it does is algorithmic. And so here are some data sets and I'll tell you how we construct it. Our Spinach data set. 320 examples. 320 different query patterns. Query patterns is like what kind of property and connection and count and stuff like that. That's a pattern. We abstract out all the actual QIDs and so forth and 100% of them are different because there's so many things to ask. But if you look at all the data set, everything is below 2% in terms of of varieties, even though they can have a very large size. But they are totally just repeating the same thing over and over again every single time. People tell me they have a data set that is synthesized. I take it with a grain of salt that is the number one thing to ask. You have a data set, is it synthesized? How did you synthesize it? It's very limited. What did we do? We discovered that through HAL that the wiki data has a forum because it's so hard to write Sparkle queries. They have a forum with that HTTP address with", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_3969665_ms_-_4044401_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 3969665, "end_ms": 4044401}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4027437 ms - 4094237 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4027437 ms - 4094237 ms\n\nContent: it? It's very limited. What did we do? We discovered that through HAL that the wiki data has a forum because it's so hard to write Sparkle queries. They have a forum with that HTTP address with that URL and it actually brings in experts that help you with writing Sparkle queries and they exchange notes on how to write it. The queries are all real, but they are all very difficult because otherwise they wouldn't be bringing it up in this forum. So the question that we showed you was. Here's another example of a question. It says, I would like to create a graph that links all the doctoral advisors and doctoral students. It looks like a family tree, but for doctoral advisors, student relations. How do I make this clear? Okay, clearly it's a very. There's a lot of information in here and you want to extract that graph out. Non trivial discussions in order to come up with the right query. So we use those. So we look at that data set and we clean it up. I don't want to get into details, but", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4027437_ms_-_4094237_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4027437, "end_ms": 4094237}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4081271 ms - 4153896 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4081271 ms - 4153896 ms\n\nContent: to extract that graph out. Non trivial discussions in order to come up with the right query. So we use those. So we look at that data set and we clean it up. I don't want to get into details, but we clean it up and then generate 155 validation examples. 165 test examples. Here is the natural language query that people are writing. Here is the actual answer that they are looking for. And this is real. And they are the toughest ones. So and so here is the result on the data set. If you compare the statistics is like the number of average clauses. We are much higher. The projections. We are always looking for multiple things and never just one thing. The average relations, the subjects. Everything here is much bigger than what everybody else has done and it is a much tougher data set. So let's get to evaluation. And before we get to evaluation, I would just like you guys to take a picture of this and go to the questionnaire. It is actually just to see who is attending the class. You can", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4081271_ms_-_4153896_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4081271, "end_ms": 4153896}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4140859 ms - 4225797 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4140859 ms - 4225797 ms\n\nContent: get to evaluation. And before we get to evaluation, I would just like you guys to take a picture of this and go to the questionnaire. It is actually just to see who is attending the class. You can answer this question trivially. So one of these days I'll pick a question hard and then we can answer them all in the class. Because I think that I really believe that you learn more by being here in person. And that's why, you know, I told everybody it is mandatory and I just want to figure out who's coming. Has everybody gotten the QR code? No, the WI FI key is not working. Okay, you have the picture. Okay. Okay, do it the minute the class is over. Just take the. Just capture the picture. If you cannot open the WI fi. All right, has everybody got it? Take the picture one. We're going. It's going away. All right. Okay. I think everybody has taken a picture. So the first thing you notice is that we have to figure out how to evaluate it. I think I'm going to let you look at the slides. It's", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4140859_ms_-_4225797_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4140859, "end_ms": 4225797}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4210307 ms - 4287921 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4210307 ms - 4287921 ms\n\nContent: away. All right. Okay. I think everybody has taken a picture. So the first thing you notice is that we have to figure out how to evaluate it. I think I'm going to let you look at the slides. It's also in the paper. The basic problem here is that if you give me results, this is the predicted result. It actually tells you the counties with the most people. I even give you the number of people. But the gold answer don't have the number in it. Clearly this answer is correct. But if you use the standard metric, you would say, oh, you've got some answers that are not in the QR and are not in the gold answer. And you'll be penalized for the wrong thing. Okay, so you do not want to penalize for extra, extra information. So but the whole concept is that how do we do matching of F1 given that it is a table? I need a table compared to another table. We don't want to penalize the extra columns. But then how do we account for it? So this is the math for calculating it. You can look at the slides.", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4210307_ms_-_4287921_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4210307, "end_ms": 4287921}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4273985 ms - 4357565 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4273985 ms - 4357565 ms\n\nContent: a table? I need a table compared to another table. We don't want to penalize the extra columns. But then how do we account for it? So this is the math for calculating it. You can look at the slides. I just want to get to the answers. So on the results on the Spinach data set, it looks like this. There are four techniques that we compare it with. You can Compare with direct GPT 4.0, it's close to nothing. The F1 is like 3.9. If you want to using 4.0 and generating sparkle, it's a little bit better. We are down at still 5%. This work is our work that I told you about. We did the fine tuning on popular questions. It is not much better. We are doing 7.1%, 7.1 F1 because we just don't see most of these properties. The next project is the one that is done in 2024, very recent. And this is using. This is the walking the graph which I told you about just now. And this is using GPT4. And this is a state of the arts in terms of older benchmarks and they got better. And we are now talking still", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4273985_ms_-_4357565_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4273985, "end_ms": 4357565}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4341211 ms - 4420943 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4341211 ms - 4420943 ms\n\nContent: This is the walking the graph which I told you about just now. And this is using GPT4. And this is a state of the arts in terms of older benchmarks and they got better. And we are now talking still at 7.2 F1. Okay. Certainly this is nowhere near what we need. So if you use the spinach technique, which is what we just talked about the agent, we are now at the test of 45.3 F1. Okay. It's got a significant jump. Even though the sun et al paper, it is still using GPT4. You're still using LLM and they are using an agentic approach. But the question is they did not use sparkle. Okay? They try to walk the graph directly. But what we're doing is all this time is to try to figure out how to write it in sparkle. Okay? So you can do agentic approach. How you're doing it makes a huge difference. So if you look at other data sets which doesn't have the complexity of what the spinach data set has, what we see here is that we're getting better results than anybody without fine tuning. This is all in", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4341211_ms_-_4420943_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4341211, "end_ms": 4420943}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4405347 ms - 4484637 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4405347 ms - 4484637 ms\n\nContent: you look at other data sets which doesn't have the complexity of what the spinach data set has, what we see here is that we're getting better results than anybody without fine tuning. This is all in context learning. Okay? I mean we compare it so. And then the answer here is that when it comes to the wiki. Let me see the wiki web questions. The only person. The only thing that we lose to is the. Is the work that we did on the fine tuning because we are training on the popular questions, attesting it on popular questions and it actually does better. And that was that 78. Oh, this is the def is the 77% that we showed you earlier. Okay. But it is very narrow. If you go beyond the popular questions it fails. I mean we just saw it, it was at 7% F1. Okay. So I think that we have made a substantial improvement. Okay. I mean our F1 is kind of close. Even though we are just off by a tiny little bit. Even if you look at the wiki web questions and comparing it with a fine tuned result. So we", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4405347_ms_-_4484637_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4405347, "end_ms": 4484637}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4470405 ms - 4548953 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4470405 ms - 4548953 ms\n\nContent: improvement. Okay. I mean our F1 is kind of close. Even though we are just off by a tiny little bit. Even if you look at the wiki web questions and comparing it with a fine tuned result. So we know that this is a very good technique compared to everything else. But we are still at the what 45%. So what do we do? And we did some analysis and we discovered that all the different actions are useful. And you can go to Spinach, G and E, you can run it, the code is there and then it is actually used on Wikidata as we mentioned. And we did the analysis and we see that sometimes it is 40% is in the property related problems. Sometimes it's just that the sparkle is really complex and we just fail. Not enough exploration. Okay, that means that if you give it more time it may find the answer. Sometimes it is inaccurate. Sparse semantic parsing, it just adds extra clauses. It just never, you know, it doesn't find the right answer. And some of it is formatting issues. So this is the error analysis", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4470405_ms_-_4548953_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4470405, "end_ms": 4548953}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4536123 ms - 4606293 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4536123 ms - 4606293 ms\n\nContent: it is inaccurate. Sparse semantic parsing, it just adds extra clauses. It just never, you know, it doesn't find the right answer. And some of it is formatting issues. So this is the error analysis on the. On the queries generated. So how do you solve this problem? One of the way to solve this problem of is to improve the exploration. Remember what we did is that if you don't like it, we just back. If you see the repetition, we just back up and let it just use the temperature. To give you a spread of techniques, you try a different query. So this paper in Yao at last year talked about a tree of thought and that is to keep track of what you are thinking so you don't repeat the same thing. And so for example the good old prompting is a. It's an input and output prompting. You can do a chain, it's got more for self consistency. You come up with multiple and you take a vote. But the tree of thought is to keep track of. I go down this route and the answer. If I can use the results to see if", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4536123_ms_-_4606293_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4536123, "end_ms": 4606293}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4594349 ms - 4670243 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4594349 ms - 4670243 ms\n\nContent: it's got more for self consistency. You come up with multiple and you take a vote. But the tree of thought is to keep track of. I go down this route and the answer. If I can use the results to see if it is right or wrong then you can say oh, let's back up. But if you keep track of all the different answers then you can Search the space a lot more systematically. We know that this is something that we should try, but we just haven't got to it because, you know, it seems to have gotten. It's just a next step. I would say it's a good thing to do, but we haven't got to it. And so that's the end of this experiment. What does it mean in real life? Okay, so it is deployed on Wikidata forum as we discussed. 600 plus conversations in the wild at this point. And they are all real and very hard queries. And so we can only handle so much manual evaluation. We picked almost 200 examples and we discovered the success rate is at 78%, which is a lot better than the 40% that we were looking for, the", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4594349_ms_-_4670243_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4594349, "end_ms": 4670243}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4652875 ms - 4724245 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4652875 ms - 4724245 ms\n\nContent: And so we can only handle so much manual evaluation. We picked almost 200 examples and we discovered the success rate is at 78%, which is a lot better than the 40% that we were looking for, the 40, 50% that we were looking at. And then we do the analysis. So the good news here is that the success rate is actually higher. Okay. And we also did the analysis. We found that half of the time they are similar to queries in the data set. We have no excuse we should be able to get them. Half of the time they are things that we have not even tested it with. Sometimes the queries are underspecified. Nobody can help you with that. Sometimes it is about correct. Here is a query, please correct it for me. We have not ever tried to make that work either. Sometimes it is just out of scope. It is manipulating strings and so forth. So all in all, I think that it is a really good outcome when you see that there is an improve. Usually the tests are easier than the real case, but in this case, the real", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4652875_ms_-_4724245_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4652875, "end_ms": 4724245}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4710283 ms - 4781617 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4710283 ms - 4781617 ms\n\nContent: strings and so forth. So all in all, I think that it is a really good outcome when you see that there is an improve. Usually the tests are easier than the real case, but in this case, the real case is actually easier than the tests. So this is all good. And one of the things that I want to note is that it's a chatbot. You give us some answer. If you don't like the answer, you can refine it, you can improve it, you can say, oh, I mean this and I don't mean this. It is not a one shot error deal that you have to deal with. And that's probably the reason why we actually can find the answer through the conversation. And the next step is actually Wikidata, the company is the Wikimedia, the parent company of Wikidata, wants to fine tune an open source model so they can run it without using the proprietary commercial system. So this is the next step. So that is what we're doing with Wikidata. I want to tell you about two minutes. Just quickly what happens to working on another data set? And", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4710283_ms_-_4781617_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4710283, "end_ms": 4781617}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4763397 ms - 4842719 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4763397 ms - 4842719 ms\n\nContent: proprietary commercial system. So this is the next step. So that is what we're doing with Wikidata. I want to tell you about two minutes. Just quickly what happens to working on another data set? And this data set is motivated by the fact that journalists in Order to write stories, they have to dig into the data of, you know, the numbers of numerical data, basically. And so our colleagues here worked with the New York Times on one of these articles about the late night parties of the Hawaiian politicians. And it takes them 100 plus hours where the journalists work with the data, where the journalists are working with people who are familiar with data in order to make the story happen. So the question is if you can cut that part out. A lot of journalists, they don't know how to do SQL. You cut out that part and let them directly get to the data using natural language. It will make it more effective and efficient to write these, to write many more stories. That's the motivation. So we", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4763397_ms_-_4842719_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4763397, "end_ms": 4842719}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4826533 ms - 4902223 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4826533 ms - 4902223 ms\n\nContent: cut out that part and let them directly get to the data using natural language. It will make it more effective and efficient to write these, to write many more stories. That's the motivation. So we actually, in about three weeks, we built, in just about three weeks, we built this agent that actually looks at all the campaign finance data from the Federal Election Commission. There's a lot of data and with this project you can use natural language to tap into the data. The data is in FEC and open elections. And what is hard about this data set is that it actually has 36 large relational databases. Remember we were talking about hybrid QA. This is a little database that you can bring into the LLM and ask it to answer questions. None of these, these are huge data sets, lots of columns, and there are 36 large relational databases. So we couldn't just call LLM to say, here are the schemas, generate this, generate the semantic parser. So what we've done is just do, do the agentic approach,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4826533_ms_-_4902223_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4826533, "end_ms": 4902223}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4888423 ms - 4956245 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4888423 ms - 4956245 ms\n\nContent: are 36 large relational databases. So we couldn't just call LLM to say, here are the schemas, generate this, generate the semantic parser. So what we've done is just do, do the agentic approach, the same agentic approach and see if it works. So this is the picture that we showed. The first one is we talked about spinach, and now in this case it is relational databases, tables. And again, we have to supply it with actions. And in this case, these are the actions that we provide, getting the tables because there are so many of them. Which table should we get? You read what the table information is, then you retrieve the table details, you execute and you stop. Okay, so it doesn't have the QID PIDs, but the print principle is the same. You find more knowledge about retrieve table details is to figure out what the tables are. Nobody is fine tuning the system to figure out what the tables are. You read it, okay, because there are 36 of them in this particular case. So whatever you can do,", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4888423_ms_-_4956245_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4888423, "end_ms": 4956245}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4944129 ms - 5010465 ms", "content": "Title: CS224V Lecture 13 > Transcript > 4944129 ms - 5010465 ms\n\nContent: to figure out what the tables are. Nobody is fine tuning the system to figure out what the tables are. You read it, okay, because there are 36 of them in this particular case. So whatever you can do, you can use the agentic approach for. So does it, Is this enough? The answer is actually not enough. All right, we'll bring it back. We'll talk about at the end. So this is how you get tables. This is how you get the details. This is how you execute. And we actually run it with the real journalists. And we discovered that agentic approach is necessary, but it is not enough. It turns out that the interpretations are very difficult. The tables have caveats. It's like contributions below 200 is not included in the tables. It won't add up when that happens. And so you require experts who are familiar with the data because it is not written anywhere but in this person's head. So we really have to capture the expertise of the expertise. And that's what we're going to talk about the next class.", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4944129_ms_-_5010465_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4944129, "end_ms": 5010465}}
{"document_title": "CS224V Lecture 13", "section_title": "CS224V Lecture 13 > Transcript > 4998253 ms - 5325695 ms", "content": "with the data because it is not written anywhere but in this person's head. So we really have to capture the expertise of the expertise. And that's what we're going to talk about the next class. So in conclusion, here is like we introduced the agentic approach. It's probably useful for a lot of your projects there. And the key thing we want to add is the ability to acquire knowledge and interpret the knowledge as you go. You don't have it all baked in and made available. And on the Wikidata we actually achieved 78% in a real life setting. And on the FEC data, the same approach handles the large number of tables. And more importantly, we discovered yet another problem and that is expert knowledge is required. And how do we incorporate that? We talk about it next class. All right. Okay, thank you very much and I will see you on Wednesday. It it.", "block_metadata": {"id": "CS224V_Lecture_13_>_Transcript_>_4998253_ms_-_5325695_ms", "document_type": "transcript", "lecture_number": 13, "start_ms": 4998253, "end_ms": 5325695}}
