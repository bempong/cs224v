{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Chemical Guys", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Chemical Guys\n\nContent: It. All right, let's get started. So how's everybody? Good.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Chemical_Guys", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 26035, "end_ms": 130912}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Projects Before Homework", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Projects Before Homework\n\nContent: The idea is that we're hearing about your custom projects, the projects that you propose. Today is a session before the homework. We want you guys to give feedback to everybody in the class. If you're interested, come by and chat.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Projects_Before_Homework", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 131088, "end_ms": 397545}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Knowledge Discovery: When curiosity drives the discovery", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Knowledge Discovery: When curiosity drives the discovery\n\nContent: Project is the knowledge discovery, which is basically curiosity driven browsing. Found something that's interesting or significant but is unknown in a corpus or in a piece of text that's undiscovered. Method is basically a prompt refining.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Knowledge_Discovery:_When_curiosity_drives_the_discovery", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 399245, "end_ms": 775445}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > How a Web of Discovery would work", "content": "Title: CS224V Lecture 7 > Chapter Summaries > How a Web of Discovery would work\n\nContent: This is a final example prompt for how this browsing would work. Your purpose is to find something that's interesting, fueled by personal motivation, surprising observation and unexpected connections makes along the way. We need to find a better way for refining these principles or refining these prompts.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_How_a_Web_of_Discovery_would_work", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 776025, "end_ms": 1091175}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Analysis of assessment and evaluation in education", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Analysis of assessment and evaluation in education\n\nContent: My question is like, what are the plans for evaluation? Like how do you tell if something's actually interesting? In this sense? We do a lot of human evaluation right now. In the lecture that I am going to give next is to talk about assessment and evaluation.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Analysis_of_assessment_and_evaluation_in_education", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 1094565, "end_ms": 1178195}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Applying the GPT to the African Studies Project", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Applying the GPT to the African Studies Project\n\nContent: Do you define interestingness to be. Something absolute or like relative? In this particular case we are reading the entire African times corpus. It is really a matter of improving the description of the evaluation task.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Applying_the_GPT_to_the_African_Studies_Project", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 1179335, "end_ms": 1318725}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Project Proposal", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Project Proposal\n\nContent: Our project is to build an investment portfolio advisory agent. The recommendations will be made based on users different goals, their risk tolerance levels. And if time allows, we will also build a rag based Q and A system for investment related questions.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Project_Proposal", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 1319105, "end_ms": 1854985}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > AI Financial Advisory Platform", "content": "Title: CS224V Lecture 7 > Chapter Summaries > AI Financial Advisory Platform\n\nContent:  AI Agent is designed for just our normal person. What's the motivation they switch to use this one? If they can save more cost or they can generate more additional revenue. Does that involve LLMs or just straight Python code?", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_AI_Financial_Advisory_Platform", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 1856485, "end_ms": 2019765}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Proposal: On Limiting Web Agents to Automate Grant Writing", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Proposal: On Limiting Web Agents to Automate Grant Writing\n\nContent: On leveraging web agents to automate resource collection for grant writing. In 2023, the US experienced climate catastrophes that caused a total of $93 billion in damages. We're hoping to leverage LLMs and AI to automate that process. Helpful feedback would be suggestions on like our implementation ideas.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Proposal:_On_Limiting_Web_Agents_to_Automate_Grant_Writing", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 2051275, "end_ms": 2243903}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Funding Grant Application: Can We Integrate With Perplexity", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Funding Grant Application: Can We Integrate With Perplexity\n\nContent: Thank you. I have two questions. Are you planning to integrate multimodal inputs? Like is there a need to upload images or schematics for grant applications?", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Funding_Grant_Application:_Can_We_Integrate_With_Perplexity", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 2243999, "end_ms": 2367169}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > How to write a technical grant in the Cloud", "content": "Title: CS224V Lecture 7 > Chapter Summaries > How to write a technical grant in the Cloud\n\nContent: You mentioned that you really wanted to help disadvantaged communities. Write a competitive grant would be quite technical. Do you think there's an application where you could also help these governments search through their own documentation stored internally, not just the publicly available stuff?", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_How_to_write_a_technical_grant_in_the_Cloud", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 2367337, "end_ms": 2561265}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Tailoring Interpersonal Learning", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Tailoring Interpersonal Learning\n\nContent: Final project Tailoring interactive learning Personalized quiz generation and feedback. With abundance of reasoning dataset like Math Vista, mmmu, we aim to develop a system that automates quiz generation. System would also provide personalized feedback to students proposed answers.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Tailoring_Interpersonal_Learning", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 2561345, "end_ms": 2904619}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Quizzation generation", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Quizzation generation\n\nContent: I wanted to ask if you thought a bit about evaluation. Currently we're thinking about first using some large model as a judge to judge whether like the maybe generated quiz is grounded. But I think it will be interesting if we can bring human in the loop and maybe do some like human study.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Quizzation_generation", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 2904747, "end_ms": 3146475}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > A Conversational Agent for Psychedelic Therapy", "content": "Title: CS224V Lecture 7 > Chapter Summaries > A Conversational Agent for Psychedelic Therapy\n\nContent: We're working on a conversational agent for psychedelic assisted therapy. In 2024, millions of people will take psychedelics, many of them for the first time. Having a virtual agent could also really help with harm reduction.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_A_Conversational_Agent_for_Psychedelic_Therapy", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 3167805, "end_ms": 3484537}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Simulated Psychedelics", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Simulated Psychedelics\n\nContent: Researchers are simulating a psychedelic experience to train more therapists to deal with this. There's more people interested in getting this kind of treatment than there are people to administer the treatment. If people have no other option, they should probably go to a human first.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Simulated_Psychedelics", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 3484641, "end_ms": 3867383}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Mocking the In-depth IRB", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Mocking the In-depth IRB\n\nContent: Do not even try to get it to be tested with any human subjects of any kind. This requires serious medical ir, which I don't think you can possibly get. For evaluation purposes, mocking is the way to go.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Mocking_the_In-depth_IRB", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 3867519, "end_ms": 3969417}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries >  LLM: DSPY Elevator Pitch", "content": "Title: CS224V Lecture 7 > Chapter Summaries >  LLM: DSPY Elevator Pitch\n\nContent: We have a special guest, a special request that I made to Michael and his partner. Michael and your partner is arof and they will be talking about dspy. DSPY is how you pronounce it and which may be very useful for your project.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>__LLM:_DSPY_Elevator_Pitch", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 3969561, "end_ms": 4040967}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Language Model Programming, Not Requiring Language Models", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Language Model Programming, Not Requiring Language Models\n\nContent: DSPY is the framework for programming not prompting language models. You can write a program that will automatically optimize your prompts for you to try to eke out the best performance. Instead of monolithic language models, you can build modular software that uses LMS as specialized components.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Language_Model_Programming,_Not_Requiring_Language_Models", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 4040991, "end_ms": 4611975}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > DSPY: The Right Prompt for your Program", "content": "Title: CS224V Lecture 7 > Chapter Summaries > DSPY: The Right Prompt for your Program\n\nContent: As your prompt length expands, so too does inference time. LLMs scales quadratically. You can also set limits. But we find few shot demonstrations are pretty important, usually.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_DSPY:_The_Right_Prompt_for_your_Program", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 4612015, "end_ms": 4750531}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Chapter Summaries > Projects due by Wednesday", "content": "Title: CS224V Lecture 7 > Chapter Summaries > Projects due by Wednesday\n\nContent: All right, so we're looking forward to all the presentations on Wednesday and next week and the proposals due. And if you want to talk to me, I have my office hour coming up. See you on Wednesday.", "block_metadata": {"id": "CS224V_Lecture_7_>_Chapter_Summaries_>_Projects_due_by_Wednesday", "document_type": "chapter summary", "lecture_number": 7, "start_ms": 4750643, "end_ms": 4863465}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 26035 ms - 278213 ms", "content": "Title: CS224V Lecture 7 > Transcript > 26035 ms - 278213 ms\n\nContent: It. All right, let's get started. All right. So how's everybody? Good. Okay, so you guys get to speak. So the idea is that we're hearing about your custom projects, the projects that you propose. And we have planned three sessions and today is a session before the homework. The project proposals is officially do. Okay, so this is a chance for you to talk about ideas and getting feed and get feedback even before you have to finish up the proposal. So let's start with our first person. It's Ahmed. It's Ahmed. Yeah. Yeah. So you want to show them? Yeah. Yes, that will be perfect. To the Internet. Just Internet, but no audio. Yes, audio. Yeah, we don't need audio. Okay. Do you have to do anything special about. Yeah, for rerouting like audio, we need. To change the output. Can you do. Where is the. Why don't you put it on the. On our website. Thank you. Yeah, I should be there all the time. Oh, it is on the website. It is under projects. Oh yeah, I did that. I looked at it. There. Should", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_26035_ms_-_278213_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 26035, "end_ms": 278213}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 231555 ms - 394100 ms", "content": "Title: CS224V Lecture 7 > Transcript > 231555 ms - 394100 ms\n\nContent: the. Why don't you put it on the. On our website. Thank you. Yeah, I should be there all the time. Oh, it is on the website. It is under projects. Oh yeah, I did that. I looked at it. There. Should be a sign here. Choose an account. It's public. Why? It's public. Even for Google Sheet. Maybe you can just share it from my laptop. I can give feedback. Should I log in? Why not? I didn't know that you can have lock in after public requires some. Just. Just. Let's just not worry about it. Let's just. Oh, after all. Are there any questions? So we're going to do the same thing. We want you guys to give feedback to everybody in the class and has everybody signed up to give a. To talk about your custom project if you have one in the next group. Two classes, huh? Yes. Right. Okay, good. And another and a reminder is that I have office hour after class and last week we have a pretty good showing. So if you're interested, come by and chat. Okay. It's going to be outside my office. Room 3. 3. 344.", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_231555_ms_-_394100_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 231555, "end_ms": 394100}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 378039 ms - 460671 ms", "content": "Title: CS224V Lecture 7 > Transcript > 378039 ms - 460671 ms\n\nContent: and a reminder is that I have office hour after class and last week we have a pretty good showing. So if you're interested, come by and chat. Okay. It's going to be outside my office. Room 3. 3. 344. Room 344. Sometimes we just meet outside. Okay, so first up we have Ahmed. He is a. He's studying abroad and so he made a video to share with us. I'm studying abroad, but I wanted to record that video just to give you a quick brief on what this project is. So this project. Project is the knowledge discovery, which is basically curiosity driven browsing. So what's the main goal that we have here? So the mingle that we have is finding something that's interesting or significant but is unknown in a corpus or in a piece of text that's undiscovered. So what do we mean by that we mean by unknown, something that we can't really know from asking a question because we don't know anything about it. So, for example, over this past summer, we were dealing with some history data set called the African", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_378039_ms_-_460671_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 378039, "end_ms": 460671}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 447697 ms - 506739 ms", "content": "Title: CS224V Lecture 7 > Transcript > 447697 ms - 506739 ms\n\nContent: that we can't really know from asking a question because we don't know anything about it. So, for example, over this past summer, we were dealing with some history data set called the African Times. And one of the things in this data set is that there was a dog that was held to trial. So if you don't know anything about this, how would you even come up with the question to ask about this dog? And of course it's something interesting, but you wouldn't even stumble upon it if you just kept asking questions because you don't know. You don't even know what the answer is. So how do we do this? So first, let's look at this graph. So this is the discovery process, and this is a graph from understanding science from 101 from Berkeley. And basically what this is saying is this. First, you have a personality. Think of it this way. How do you stumble on new information when you're scrolling through social media or something like this? First you have a personality, you have a curiosity, you have.", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_447697_ms_-_506739_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 447697, "end_ms": 506739}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 498335 ms - 553043 ms", "content": "Title: CS224V Lecture 7 > Transcript > 498335 ms - 553043 ms\n\nContent: Think of it this way. How do you stumble on new information when you're scrolling through social media or something like this? First you have a personality, you have a curiosity, you have. You have your own personal motivation. And then serendipity happens, which is you stumble upon a post or you stumble upon a video or something like this. And then that's when curiosity kicks in. You make some surprising observations. And these observations are basically based on your personality or the things that interest you. And then afterwards you'll be like, this is interesting. So you ask more questions about it and then you read more and then you repeat the whole process until you come up with something at the very end and that's actually good or interesting. So part one, first we need to have a discriminator. A discriminator that knows what is significant and what is not. So how do we do this? So first we can have a naive discriminator and just go and chatgpt and give it a piece of text and", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_498335_ms_-_553043_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 498335, "end_ms": 553043}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 540099 ms - 607953 ms", "content": "Title: CS224V Lecture 7 > Transcript > 540099 ms - 607953 ms\n\nContent: discriminator. A discriminator that knows what is significant and what is not. So how do we do this? So first we can have a naive discriminator and just go and chatgpt and give it a piece of text and be like, I was walking down the street and I noticed a cat and we started having a conversation or something like this. And then ask it what was interesting or odd from this piece of text. And obviously it's going to tell you that, you know, cats don't talk. So this is interesting. Or this is something that's odd. This is good, but this is kind of naive because first you're not sure that it's actually going to catch all the things that are kind of like niche interests for people. So say, for example, you are a historian and you are interested in events that have, you know, like, they spread across, you know, like a long period of time. If I'm If I were to read something like this, as, I don't know, an artist or something, maybe I would not find this interesting, but a historian would. So", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_540099_ms_-_607953_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 540099, "end_ms": 607953}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 596241 ms - 659055 ms", "content": "Title: CS224V Lecture 7 > Transcript > 596241 ms - 659055 ms\n\nContent: you know, like a long period of time. If I'm If I were to read something like this, as, I don't know, an artist or something, maybe I would not find this interesting, but a historian would. So just asking what is interesting is kind of naive. So we need a better approach. So how do we do this? So this is a method that we can use and it's basically a prompt refining. So basically the way it works, you have a data set, right, of like sentences, or you have a data set of paragraphs. And then you get an actual expert, maybe a historian, maybe if you want to mimic some other personality, you can go ahead and do that. But you get. Let's stick with the historian for now. So you get a historian and then the historian reads each sentence or each paragraph and then decides whether or not this is interesting or not. And then afterwards, you iteratively go through this whole data set and try to refine a prompt such that given this prompt and given any of the sentence, the LLM is going to be able", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_596241_ms_-_659055_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 596241, "end_ms": 659055}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 644775 ms - 712155 ms", "content": "Title: CS224V Lecture 7 > Transcript > 644775 ms - 712155 ms\n\nContent: or not. And then afterwards, you iteratively go through this whole data set and try to refine a prompt such that given this prompt and given any of the sentence, the LLM is going to be able to predict the annotation that the expert give to the sentence. In the very beginning, the prompt is not going to be good. But then afterwards you try to refine and refine and refine until at the very end you get a prompt that actually makes the LLM really, really good at mimicking or imitating this expert or historian by identifying what's which, like which paragraphs are interesting and which are not. And this prompt can be in, you know, different, different formats. So one format that we try to use is use some, like, some sets of, like, principles or sets of rules. You know, here's an example of what we were using. We were working with our awesome historian, Trevor, and these are like some of the, of the principles that we were talking about. So maybe, you know, like an event may be interesting", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_644775_ms_-_712155_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 644775, "end_ms": 712155}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 698441 ms - 765121 ms", "content": "Title: CS224V Lecture 7 > Transcript > 698441 ms - 765121 ms\n\nContent: we were using. We were working with our awesome historian, Trevor, and these are like some of the, of the principles that we were talking about. So maybe, you know, like an event may be interesting if a large number of people or has a large region or significance is measured by duration and so on and so forth. So these are the criteria of how you would identify if something is interesting or not. And then after you have this discriminator and you have this basically personality or you define what curiosity is for and certain LLM, you jump into the second step. And if we go back to the image again, this second step is basically asking questions. So after we have this serendipity and we made these surprising observations of reading something, then we start asking more questions. Oh, this is interesting. So tell me more about that. So in part two, we're taking the approach of browsing, which is basically we're just making the LLM ask a question and then we retrieve more information about", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_698441_ms_-_765121_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 698441, "end_ms": 765121}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 750013 ms - 810341 ms", "content": "Title: CS224V Lecture 7 > Transcript > 750013 ms - 810341 ms\n\nContent: interesting. So tell me more about that. So in part two, we're taking the approach of browsing, which is basically we're just making the LLM ask a question and then we retrieve more information about an answer to this question and make the LLM read again and use this criteria to identify what's interesting and based on what these interesting stuff, ask more questions, and so on and so forth. This is a final example prompt for how this browsing would work. I just tell it, you're a human discoverer, you're browsing without specific goals. You don't really have an idea in mind that you're trying to do. Think of it the same exact way you, you do with your smartphone. You just like scrolling on social, on social media or something. You don't really want to achieve anything, but you stumble on something that's interesting and so on without a specific goal. And then you're driven by curiosity and that chance of serendipity. And then as you read, your internal monologue guides you through the", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_750013_ms_-_810341_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 750013, "end_ms": 810341}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 801453 ms - 860827 ms", "content": "Title: CS224V Lecture 7 > Transcript > 801453 ms - 860827 ms\n\nContent: that's interesting and so on without a specific goal. And then you're driven by curiosity and that chance of serendipity. And then as you read, your internal monologue guides you through the process of discovery. So this internal monologue thing is really interesting because you need to have this monologue inside your brain of why this LLM identified this thing to be interesting and how it came up with the question it's going to ask. And then you make observations, you ask questions and then find inspirations exploring the literature. And then you get here the name of the literature and in what context or the context of this literature. And as you can see here, we're talking about observations, asking questions, finding inspiration, which is exactly what is in here. Then afterwards, your purpose is to find something that's interesting, fueled by personal motivation, surprising observation and unexpected connections makes along the way. And then I tell the LLM that you're going to ask", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_801453_ms_-_860827_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 801453, "end_ms": 860827}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 850115 ms - 905103 ms", "content": "Title: CS224V Lecture 7 > Transcript > 850115 ms - 905103 ms\n\nContent: purpose is to find something that's interesting, fueled by personal motivation, surprising observation and unexpected connections makes along the way. And then I tell the LLM that you're going to ask me a question and for every question I'll give you a paragraph related to it. And then you repeat the whole process again. And then I identify what this exploration is. So I tell it, your expression is a continuous loop. Each question opens new avenues of implementation, inquiry, driving you further into the unknown, your personality. And then I give it the personality. And since again, we were working with a personality of historians, so I just tell it, you're a historian, I can hear instead of just mentioning historian, I can give it, you know, like some background information, like you're a historian, you graduated from here, here and there. This is all, you know, like design choices. And you need a lot of testing in order to find the most optimal, the most optimal way to do this. But", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_850115_ms_-_905103_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 850115, "end_ms": 905103}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 890139 ms - 951643 ms", "content": "Title: CS224V Lecture 7 > Transcript > 890139 ms - 951643 ms\n\nContent: a historian, you graduated from here, here and there. This is all, you know, like design choices. And you need a lot of testing in order to find the most optimal, the most optimal way to do this. But then again, I tell it, here's what you're curious about and how you define what's interesting. And then I give it the refined Trevor point. So if you remember from up there, these were, you know, like the Trevor that, the points that Trevor wrote And then after I go through the whole process of intuitively refining these principles, I just give it to the model at the very end. So it's going to be asking questions based on these points. And then, yeah, before we move on to what's next, I can try to show you this is an example of what actually the model looks like. I just give it a random paragraph and then it's going to be like as I delve into the vivid account of the homey. And then it talks and it talks and it talks. And as you can see, because I told it, it's a historian, it's using the", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_890139_ms_-_951643_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 890139, "end_ms": 951643}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 940115 ms - 1001213 ms", "content": "Title: CS224V Lecture 7 > Transcript > 940115 ms - 1001213 ms\n\nContent: and then it's going to be like as I delve into the vivid account of the homey. And then it talks and it talks and it talks. And as you can see, because I told it, it's a historian, it's using the tone of a historian. And then the mention of this is interesting. And then it asks a question. And then when it asks us this question, I take this question and I retrieve, I query it over the data set and then retrieve some information related to that question and let the LLM read it again. And then, oh, and then it reads, reads, reads and so on and so forth. I run this for like five times and then at the very end I just ask it to generate a whole article of something that it found interesting based on everything that it has read. So, yeah, you end up with an article that you didn't even ask about, but it's an interesting article. It's something that if you're a historian, you would find interesting. So, yeah, what's next for this? Basically we need to find a better way for refining these", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_940115_ms_-_1001213_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 940115, "end_ms": 1001213}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 987765 ms - 1056761 ms", "content": "Title: CS224V Lecture 7 > Transcript > 987765 ms - 1056761 ms\n\nContent: but it's an interesting article. It's something that if you're a historian, you would find interesting. So, yeah, what's next for this? Basically we need to find a better way for refining these principles or refining these prompts or to find things that are basically interesting. And then we need also to test these, to test this method on more data sets, on more scenarios, and make sure that it works good with other fields as well, other than history, as well as trying different approaches, maybe another approach other than browsing or another approach rather than having writing the prompt this way is going to be better. But that's basically the whole idea of the project. And why I think it's really interesting is because you can literally just data set into this thing and let it run for a while and then at the very end it's going to tell you, oh, I found this, this and that and save yourself a whole lot of time. So, yeah, I hope you like it. And if you have any questions, just feel", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_987765_ms_-_1056761_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 987765, "end_ms": 1056761}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1045001 ms - 1132863 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1045001 ms - 1132863 ms\n\nContent: a while and then at the very end it's going to tell you, oh, I found this, this and that and save yourself a whole lot of time. So, yeah, I hope you like it. And if you have any questions, just feel free to email me at any time or reach out on Slack and yeah, thank you so much. There. Yeah, no, we have a. Actually we don't have anybody online. Oh no, yeah, I mean the tv, I be listening to the room. But anyways, we do have a. My question is like, what are the plans for evaluation? Like how do you tell if something's actually interesting? In this sense? We do a lot of human evaluation right now and I think you will, in the lecture that I am going to give next is to talk about assessment and evaluation. We go into details, but the basic idea here is that we found that there is a round of assessment. Basically you try to come up with the difficult cases, you try it and you get it to work and then you worry about the evaluation. And a lot of evaluation these days has to be done by hand and", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1045001_ms_-_1132863_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1045001, "end_ms": 1132863}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1121607 ms - 1190559 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1121607 ms - 1190559 ms\n\nContent: Basically you try to come up with the difficult cases, you try it and you get it to work and then you worry about the evaluation. And a lot of evaluation these days has to be done by hand and we try to automate it, but it is very tricky because the automation sometimes just doesn't work. You have to validate it. So we will talk about that in more details. It's a very good question, but I think that this is the, this project has to do with discovery of information. I don't know if you remember, we have this pyramid of knowledge in education and at the very top is knowledge discovery or the creation creativity. And that is because we know that LLMs are remembering a lot of things. And so really the creation is an area where LLMs are not known to. Be able to do. And this is what, why we are exploring it. All right, so let's move on to the. Yes, Another question. Yeah. For this, like this project, I'm wondering. Like, do you define interestingness to be. Something absolute or like", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1121607_ms_-_1190559_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1121607, "end_ms": 1190559}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1176311 ms - 1246527 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1176311 ms - 1246527 ms\n\nContent: we are exploring it. All right, so let's move on to the. Yes, Another question. Yeah. For this, like this project, I'm wondering. Like, do you define interestingness to be. Something absolute or like relative? Because I mentioned that like if we have a shorter text, we, the model might be fine with like outputting something. That is less interesting compared to like if the input is like a longer text. So in this particular case we are reading the entire African times corpus, which is very large. And what is interesting? We have to ask the historian. Trevor was telling us, why do you think it is interesting? So he wrote us a paragraph and then we apply it to the corpus. And actually I think before we have the evaluation, we just asked GPT to pull out anything that stands out and it just generates like thousands of things up there. And then we use the evaluation to filter them. And then when we see that there and then we have some annotated data that Trevor has done. So we compare them", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1176311_ms_-_1246527_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1176311, "end_ms": 1246527}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1233119 ms - 1304947 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1233119 ms - 1304947 ms\n\nContent: like thousands of things up there. And then we use the evaluation to filter them. And then when we see that there and then we have some annotated data that Trevor has done. So we compare them and when they. So we start with a paragraph from him and then there's still differences obviously. And then when there is a difference we show it back to Trevor. And Trevor would say, oh, we have to also consider this. And what we are doing really is to try to distill information out of the expert by showing them cases where the original description was not clear to me all this time is not because the error is not because the system gets it wrong. It's just that the instruction is not well defined. It is not good enough. So it is really a matter of improving the description of the evaluation task. Okay, so that's. I bet a lot of you will be running into that problem. It is because we really have to extract the information out of, you know, the tacit information out of the expert's mind. That's", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1233119_ms_-_1304947_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1233119, "end_ms": 1304947}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1292643 ms - 1496603 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1292643 ms - 1496603 ms\n\nContent: Okay, so that's. I bet a lot of you will be running into that problem. It is because we really have to extract the information out of, you know, the tacit information out of the expert's mind. That's what we discussed last time. I think that we were seeing a lot of that this year. Any more questions? All right, so should we go to the next. Yeah, why don't you run that? Who is the. I suppose you know who you are in this case. My name is Chen Yang. Today I'm going to present with my. Partner Haocheng about our project proposal. Our project is to build an investment portfolio advisory agent. Today, investment is becoming unnecessarily complicated and people without domain knowledge often finds it confusing and it's hard to make correct decisions. For example, if you go to JP. Morgan to start a self directed investment. Account, they offer more than 12 investment categories with various subcategories and each category contains a lot of jargons and terminologies that people often find", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1292643_ms_-_1496603_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1292643, "end_ms": 1496603}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1483227 ms - 1559093 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1483227 ms - 1559093 ms\n\nContent: a self directed investment. Account, they offer more than 12 investment categories with various subcategories and each category contains a lot of jargons and terminologies that people often find confusing or hard to understand understand. Also there's research and media shows that the higher and higher barrier of investment. It'S discouraging individual from doing the investment. Furthermore, the correct information sometimes can be very hard to get. Like in this case, if I'm searching a particular mutual fund, I get different information from different websites. Sometimes it's hard to match them like which one is which. So on all these combined making investment a very complicated problem. So our solution is to build a. LLM based investment portfolio advisory agent that. Can help people from doing investment and making recommendations. Due to the very tight timeline, we. Will mostly be focusing on two use cases. The first use case is we want our advisory agent to be able to provide", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1483227_ms_-_1559093_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1483227, "end_ms": 1559093}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1542725 ms - 1623615 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1542725 ms - 1623615 ms\n\nContent: from doing investment and making recommendations. Due to the very tight timeline, we. Will mostly be focusing on two use cases. The first use case is we want our advisory agent to be able to provide personalized investment recommendations. The recommendations will be made based on users different goals, their risk tolerance levels. Their timelines, their budget and so on. And if time allows, we will also build a second functionality which is a rag based Q and A system for investment related questions. There are three requirements for this project. The first is we want to make sure all the recommendations will be evidenced by historical performance as well as mathematical computations. Basically, we won't utilize the LLMs coding capability and reasoning capability to make recommendations. Every recommendations needs to back up by numbers. Let's say if the agent recommends one stock, it needs to give numbers from different perspectives. What's the expected, you know, income? What's the", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1542725_ms_-_1623615_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1542725, "end_ms": 1623615}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1607887 ms - 1734519 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1607887 ms - 1734519 ms\n\nContent: Every recommendations needs to back up by numbers. Let's say if the agent recommends one stock, it needs to give numbers from different perspectives. What's the expected, you know, income? What's the Risk level and why you are making this all based on numbers so it gives it no room for hallucination. Also, we want to provide kind of a logical reason why a recommendation is made. And finally, all our Q and A will be grounded by information provided by major brokers so that there's no Ms. Wrong information provided to users. Next, my partner Haochen will talk about. How we're going to build such agent. Please take it over Haocheng. For first step, we collect the user key information including the budget, investment timeline and the risk tolerance. Yes. Then we generate the multiple investment plans based on the user's preference. Each plan includes different percentage for the assets like stocks, bonds, funds, whatever like bitcoins is to ensure each plan is supported by the current and", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1607887_ms_-_1734519_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1607887, "end_ms": 1734519}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1713547 ms - 1822787 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1713547 ms - 1822787 ms\n\nContent: plans based on the user's preference. Each plan includes different percentage for the assets like stocks, bonds, funds, whatever like bitcoins is to ensure each plan is supported by the current and accurate data. We use information retrieval to pull the real time data from the sources like the APIs and the historical data. Yes. Then we will analyze our draft plans with the mathematical reasoning and the LM reasoning Slack and grounded the final plan which will show the assets, expected returns and the risks. Also it allows users to provide the feedback to refine the responses. Then we will continue the loop until the user is satisfied. Yes. And so far we have utilized a combination of the powerful APIs and the specialized data sets to gather the financial data. The APIs provide real time and historical data for various assets which allow us to make the data driven investment decisions. Also we include the sentiments and analyze and Q and A datasets specifically designed for the", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1713547_ms_-_1822787_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1713547, "end_ms": 1822787}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1802727 ms - 1906869 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1802727 ms - 1906869 ms\n\nContent: time and historical data for various assets which allow us to make the data driven investment decisions. Also we include the sentiments and analyze and Q and A datasets specifically designed for the financial tests and the documents to enhance our analysis. Let's all for our proposals. Any questions? Can you go to the last slide? So, first one, I have one question. So if you want to provide such an API, how can you guarantee that you can access to all kind of financial data which are also real time? Because most of the time this real time data is very expensive. Second one is when we create such financial advisory platform like AI Agent Normally we need to think about what's the proof of concept. So let's say right now people can pay or hire any expert, Right. Specific financial advisory and what's the motivation they switch to use this one? Right. If they can save more cost or they can generate more additional revenue. So what's the purpose? Yes, our purpose to design this project is", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1802727_ms_-_1906869_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1802727, "end_ms": 1906869}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1891941 ms - 2003857 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1891941 ms - 2003857 ms\n\nContent: what's the motivation they switch to use this one? Right. If they can save more cost or they can generate more additional revenue. So what's the purpose? Yes, our purpose to design this project is for just our normal person. Just for someone do not have the a lot of a lot of money. Or the experts also do not want to just handle the just such little money just about depends on the dollars or just it could provide guidance for us who do not know the financial knowledge to handle our Initial money. Yes. I was wondering when you talk about mathematical reasoning versus LLM reasoning, what do you foresee happening in that mathematical reasoning box? Does that involve LLMs or just straight Python code? I think we have not designed for these steps yet, but I think for this step we just want to see the variability for the LLM in just like the expected returns or risks and the tolerance for the LM maybe not so strong in these aspects. Yeah. Okay, so just like your standard, like financial", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1891941_ms_-_2003857_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1891941, "end_ms": 2003857}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 1980221 ms - 2099488 ms", "content": "Title: CS224V Lecture 7 > Transcript > 1980221 ms - 2099488 ms\n\nContent: to see the variability for the LLM in just like the expected returns or risks and the tolerance for the LM maybe not so strong in these aspects. Yeah. Okay, so just like your standard, like financial tools. Yeah. Okay. Anyone else? Thank you. Hi everyone, I'm Deeper Shreya and I'm joined by my project partner Shaina, and. We'Re super excited to present our proposal. On leveraging web agents to automate resource collection for grant writing. So as a brief roadmap, we'll first go into more of the motivation key question, followed by our project overview and some implementation ideas we had in mind. So, Deepa, Shreya and I share an interest in climate, and so that's the main motivation for our project. In 2023, the US experienced climate catastrophes that caused a total of $93 billion in damages. And so, in anticipation of increasingly frequent and extreme climate events, communities will have to apply for funding that's afforded by the Bipartisan Infrastructure Law and Inflation", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_1980221_ms_-_2099488_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 1980221, "end_ms": 2099488}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2087944 ms - 2150451 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2087944 ms - 2150451 ms\n\nContent: in damages. And so, in anticipation of increasingly frequent and extreme climate events, communities will have to apply for funding that's afforded by the Bipartisan Infrastructure Law and Inflation Reduction act to make their communities more resilient through infrastructure projects, etc. But they'll have to apply for grants. And the current grant writing process is very time consuming and resource intensive, and that often disadvantages those who need them most, which are smaller under resourced communities. So to help with the process of parsing through long, dense guidelines, and to gather all the documents, datasets, case studies and resources that are needed to write a competitive grant application, we're hoping to leverage LLMs and AI to automate that process. So our overall key research question that we defined so far is how can. We leverage LLMs to simplify the grant writing research process, especially for understaffed, overworked. Local governments applying for resilient", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2087944_ms_-_2150451_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2087944, "end_ms": 2150451}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2138955 ms - 2197449 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2138955 ms - 2197449 ms\n\nContent: research question that we defined so far is how can. We leverage LLMs to simplify the grant writing research process, especially for understaffed, overworked. Local governments applying for resilient infrastructure funding? Here's a project overview that kind of shows the roadmap that we initially have in mind. So we assume that our user has in mind which specific grant they want to apply for. So they'll have to upload a notice of funding opportunity that is available for every every single resilient funding grant. And this is a 50 to 100 page document where we can therefore extract the application requirements and then use a web agent that's grounded on a search engine API to gather relevant resources like different datasets, academic papers or news articles, and if needed, we'll also integrate user input to give back a set of documents that can be useful for the Climate Action grant writing process. So some of the ideas that we have right now for implementation include using web", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2138955_ms_-_2197449_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2138955, "end_ms": 2197449}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2184895 ms - 2241343 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2184895 ms - 2241343 ms\n\nContent: integrate user input to give back a set of documents that can be useful for the Climate Action grant writing process. So some of the ideas that we have right now for implementation include using web agents as Deepa Shree described, that is grounded on the Internet to be able to perform searches for relevant documents. At the moment, we're first intending to start with just searching for a certain type of resource as in just datasets or just academic reports and then moving on from there as soon as we have something that works. And some things that we're thinking about in particular are making sure that the documents that the agent returns are relevant and up to date and grant specific as opposed to more general resources and more importantly how to integrate user provided documents and data because that is most relevant to crafting a grant application. They have like project based data that is hyper specific. Yeah, thank you. Helpful feedback would be suggestions on like our", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2184895_ms_-_2241343_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2184895, "end_ms": 2241343}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2229999 ms - 2301775 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2229999 ms - 2301775 ms\n\nContent: and data because that is most relevant to crafting a grant application. They have like project based data that is hyper specific. Yeah, thank you. Helpful feedback would be suggestions on like our implementation ideas or evaluation or anything like that. Thank you. So I just have a very specific question because I also saw about this before. So have you ever compare this with like you know, ChatGPT01 or you know, Professor AI because you know this is the web agent, right? Probably this resource information is kind of open source to some extent. Have you thought about that and what's the difference and what's the idea you guys can bring on? Yep, thank you. Yeah, great question. So before trying this we tried using Perplexity and have some screenshots of when we tried using it because we're trying to get like hyper specific resources. We first asked like okay, what are resources like for this specific grant? What do I need? And it gave some pretty like general like high level overview", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2229999_ms_-_2301775_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2229999, "end_ms": 2301775}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2292191 ms - 2357741 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2292191 ms - 2357741 ms\n\nContent: trying to get like hyper specific resources. We first asked like okay, what are resources like for this specific grant? What do I need? And it gave some pretty like general like high level overview of stuff that you would need. And then when trying to ask a follow up question of like, okay, if these are the requirements for the application, then what type of data resources I would need for that? It can't follow up on the specific even though it's one of the resources it'll have that grants description in one of the sources it cites. It'll return very general Data sets of GitHub or Google data Hub or something like that. We're hoping that still doesn't take as much. It doesn't do as much work for a user as we would like. So we're trying to make it more specific. I have two questions. I guess the first one is I'm wondering is there, are you planning to integrate multimodal inputs? Like is there a need to upload images or schematics for grant applications? Yeah, we had in mind", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2292191_ms_-_2357741_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2292191, "end_ms": 2357741}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2346589 ms - 2402179 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2346589 ms - 2402179 ms\n\nContent: I guess the first one is I'm wondering is there, are you planning to integrate multimodal inputs? Like is there a need to upload images or schematics for grant applications? Yeah, we had in mind integrating some sort of multimodality, especially for the data. Sets or academic papers. So that was definitely something that we were considering for sure. What was your second question? Yeah, I guess my second question is you mentioned that you really wanted to help disadvantaged communities. And I think within these a lot. Of the information that you need to. Write a competitive grant would be quite technical. Did you have anything in mind about how you're going to, how you could leverage LLM to help, I guess explain or help these people who may not. Have the technical expertise to write an application with the technical material that's provided? Yeah, we understand that for the application process there's lots of parts that are very high expert knowledge such as like drafting construction", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2346589_ms_-_2402179_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2346589, "end_ms": 2402179}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2391507 ms - 2459701 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2391507 ms - 2459701 ms\n\nContent: with the technical material that's provided? Yeah, we understand that for the application process there's lots of parts that are very high expert knowledge such as like drafting construction plans or like cost benefit analyses that maybe, at least for the scope of the project that we're trying to do this quarter might not be something we can do with LLMs, but it's I think something that could do be very impactful for bridging like the technical gaps for people, for people in disadvantaged communities who usually don't have that technical literacy. So really good question. So I'm wondering if like these, some of these small local governments have like legacy databases because I imagine when you're writing these kind of grants you'll need to present like oh, this is the state of our roads or school system or whatever. I'm just curious, like do you think there's an application where you could also help these governments search through their own documentation stored internally, not just", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2391507_ms_-_2459701_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2391507, "end_ms": 2459701}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2448341 ms - 2526409 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2448341 ms - 2526409 ms\n\nContent: or school system or whatever. I'm just curious, like do you think there's an application where you could also help these governments search through their own documentation stored internally, not just the publicly available stuff? Yeah, definitely. I think that's like what we're really hoping for. The user integrated like the user input provide a data and documents because I think that's most important for this process to make sure that that's taken significantly in like collecting resources. So yes, for sure. Thank you. This is a great topic and I just want to bring up the fact that you may consider using for example Storm Coast Storm as the framework to build on top of one thing we have already done is, which is not publicly, you cannot see it publicly and that is that for Storm and co Storm you can supply your own knowledge corpus in terms of free text and of course we are adding the database search on it. So if you look at the proposal there is one of the proposals is to write", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2448341_ms_-_2526409_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2448341, "end_ms": 2526409}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2508097 ms - 2621595 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2508097 ms - 2621595 ms\n\nContent: co Storm you can supply your own knowledge corpus in terms of free text and of course we are adding the database search on it. So if you look at the proposal there is one of the proposals is to write targeted to be able to build, to use Chrome, I mean sorry Storm or Co Storm for goal directed documents. And I think this is an example of that. So whether you provide the guidelines as a generic input into the STORM system or specialize the STORM for This particular purpose, I think you will find that to be helpful. Thank you so much. All right. Okay, anybody else? All right, thank you. Hi everyone, my name is Zi Kui and he is Yicheng. Today we are going to talk about our final project Tailoring interactive learning Personalized quiz generation and feedback. The motivation is that with abundance of reasoning dataset like Math Vista, mmmu, we aim to develop a system that automates quiz generation and retrieval such that it would satisfy students needs. And we would like to develop the", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2508097_ms_-_2621595_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2508097, "end_ms": 2621595}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2607587 ms - 2675923 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2607587 ms - 2675923 ms\n\nContent: of reasoning dataset like Math Vista, mmmu, we aim to develop a system that automates quiz generation and retrieval such that it would satisfy students needs. And we would like to develop the system such that it would also provide personalized feedback to students proposed answers. And finally we would like to put students in a more interactive learning environment and we hope the students could have a deeper understanding of the concept. The design of our system is that we will first analyze students question and we want to identify which subject is a question about. We would also identify the student's education level, whether the user is from primary school or the user is a college student. And finally we would like to retrieve relevant Wikipedia concept and terms and generate a concrete answer to students. The reason we are doing this is that the Wikipedia is based on factual knowledge and it is much more reliable than purely relying on the language model. And finally we would", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2607587_ms_-_2675923_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2607587, "end_ms": 2675923}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2663027 ms - 2725361 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2663027 ms - 2725361 ms\n\nContent: answer to students. The reason we are doing this is that the Wikipedia is based on factual knowledge and it is much more reliable than purely relying on the language model. And finally we would provide the quiz question for students to answer. In the end we would also like to validate the student answer and provide some feedback. Yeah, and as we approach this question actually we talk with our CA and actually we have maybe two potential directions to go and maybe we can maybe do both of them, but I don't know. The first one is quiz retrieval so that is like according to our maybe motivation retrieval appropriate like the practice quizzes from the available like GSM 8K and also maybe the Mass Vista. These kind of data sets they can target exercises to reinforce the students learning. I assume this may be like more suitable for the mathematical reasoning and maybe also common sense reasoning quiz. The second one is like the quiz generation. Instead of retrieve the quiz from the data set", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2663027_ms_-_2725361_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2663027, "end_ms": 2725361}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2715961 ms - 2772367 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2715961 ms - 2772367 ms\n\nContent: this may be like more suitable for the mathematical reasoning and maybe also common sense reasoning quiz. The second one is like the quiz generation. Instead of retrieve the quiz from the data set we would like to generate appropriate quiz based on the retrieved articles from Wikipedia offering also the exercises to reinforce the student starting. But I assume this may only work for the common sense reason quiz because I doubt that maybe the generation quality of the large models for the math reasoning dataset may not be that great. Here is one example of how we maybe frame the quiz retrieval example. A student asks a question what is ldos? We can see that we retrieve this from Wikipedia, blah blah blah blah blah blah. Then we identify the core concept. The student may want to know what structure of the chemicals maybe this thing could lead to the Conclusion that it is Aldous. And after identify the concept, we may retrieve the quiz. This one is from the triple Mu. What kind of sugar", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2715961_ms_-_2772367_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2715961, "end_ms": 2772367}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2761995 ms - 2814791 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2761995 ms - 2814791 ms\n\nContent: of the chemicals maybe this thing could lead to the Conclusion that it is Aldous. And after identify the concept, we may retrieve the quiz. This one is from the triple Mu. What kind of sugar is this? L dose or ketos. And by presenting the visual image the students give the answer I think it is ldos. And we get the feedback correct. It looks like you have got enough knowledge to know what is ldos. As for the quiz generation, we are thinking of a different approach. That is we again identify the concept in this Wikipedia but this time we are searching models to generate a quiz like based on the article what is the key characteristic of Aldose? And the student may choose oh, I think it's B because this kind of group has at most carbon atom. And then we can also give feedback. Oh, it looks like you have also know how to determine this. So we are thinking about maybe we can use this as a more progressive like approach. First start with some primary school questions and maybe go more deeper", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2761995_ms_-_2814791_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2761995, "end_ms": 2814791}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2805511 ms - 2856889 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2805511 ms - 2856889 ms\n\nContent: you have also know how to determine this. So we are thinking about maybe we can use this as a more progressive like approach. First start with some primary school questions and maybe go more deeper into some details or go like more go to more difficult like direction. And we also have some like pilot studies to see like naive experiment to see that if ChatGPT can handle this generation and we figure out that if we navigately just give them the student question, they will give something like maybe not super relevant or super easy, but based on the article they can write generate some of the maybe questions that could be I think interesting for students to answer. Yeah. And as for evaluation, we are thinking about maybe there are two maybe dimensions that could be evaluated. The first one is grounding whether they retrieve the quiz or like the generation quiz can accurately reflect the concept the student is interested in. And the second interoperability whether the system can explain", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2805511_ms_-_2856889_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2805511, "end_ms": 2856889}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2847237 ms - 2915867 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2847237 ms - 2915867 ms\n\nContent: is grounding whether they retrieve the quiz or like the generation quiz can accurately reflect the concept the student is interested in. And the second interoperability whether the system can explain how the identified concept is related to student's question and whether the system can explain if the retrieved quiz is related to the student's question and concept. So with this kind of granting and interoperability evaluation, we aim to provide maybe a system that could correctly grant their generated quiz to the actual Wikipedia articles or the concept and also helps interpret how is this quiz relevant to your question? And how is our answer relevant to your concept? Yeah, thank you. Yeah, I think this is a really cool idea and could be really useful for like learning and trying to teach something. I wanted to ask if you thought a bit about evaluation because I think this is a challenging thing to evaluate this quiz generation since ideally I guess the human in the loop wouldn't know", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2847237_ms_-_2915867_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2847237, "end_ms": 2915867}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2904139 ms - 2970335 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2904139 ms - 2970335 ms\n\nContent: something. I wanted to ask if you thought a bit about evaluation because I think this is a challenging thing to evaluate this quiz generation since ideally I guess the human in the loop wouldn't know the answer they're trying to learn and stuff like that. So it's more complicated than just asking the human. Is this correct? Yeah, I definitely agree. I think the Human rather like their experience of maybe looking at this quiz and how they think of the provided answer. It's like should be the key evaluation. But I think that could be like very complex and maybe out of maybe like go beyond the scope of this like quarter long project. So currently we're thinking about first using some large model as a judge to judge whether like the maybe generated quiz is like grounded and maybe like they will provide some further knowledge exploration for students to go. But I think it will be interesting if we can bring human in the loop and maybe do some like human study. Yes. Thank you. So it seems", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2904139_ms_-_2970335_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2904139, "end_ms": 2970335}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 2947471 ms - 3020663 ms", "content": "Title: CS224V Lecture 7 > Transcript > 2947471 ms - 3020663 ms\n\nContent: provide some further knowledge exploration for students to go. But I think it will be interesting if we can bring human in the loop and maybe do some like human study. Yes. Thank you. So it seems to me that for questions like this the good GPT will be able to answer your question quite well so you can tell if the answer is correct for the most part. Yeah, but I mean that is. Oh, sorry to interrupt. That is the most likely easiest like common sense reasoning tasks, right? Well, I don't know why you call it common sense because none of those questions were, I mean they were just kind of standard book knowledge. As a matter of fact, common sense is something that humans do better as opposed to the computers do better. So I don't know why I have to quiz them on their common sense. But I think that the question is when we construct as a teacher, when we construct a question we have to be, we have to think harder in order to come up with a quiz that is not so obvious. Yes. And there are", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_2947471_ms_-_3020663_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 2947471, "end_ms": 3020663}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3007561 ms - 3064353 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3007561 ms - 3064353 ms\n\nContent: that the question is when we construct as a teacher, when we construct a question we have to be, we have to think harder in order to come up with a quiz that is not so obvious. Yes. And there are teaching goals to go with. You know, it's like oh, this is very similar and only this part, you know, you need to distinguish the less obvious parts and so forth. I don't know if that's what you have in mind. Yeah, I think that's definitely what we have in mind. So I think the identify concept part should be like the focus because if you just ask, if you just leave some blanks in the sentence and ask them to fail, it's very naive and not very interesting. So how we think about this is we may want to maybe first if we want to find some of the knowledge thing we want to maybe do some more distracted answers so that they will know the relation between the correct and distracted answers. And also there are some questions that could be related to not only the knowledge itself but also the problem", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3007561_ms_-_3064353_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3007561, "end_ms": 3064353}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3054345 ms - 3126231 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3054345 ms - 3126231 ms\n\nContent: answers so that they will know the relation between the correct and distracted answers. And also there are some questions that could be related to not only the knowledge itself but also the problem solving ability like the mathematic reasoning. So I'm still wondering whether like the retrieve or generation method can help they generate some kind of high quality maybe math reasoning task. My suggestion for you is to narrow is to go narrow. Don't try to do everything from grade school to college and don't Try to do everything from math to chemistry. You just focus on one area. And I think that would be funny is like if you can get an answer where a llama would get it wrong and the GPT will get it right, GPT1 0 will get it right, then you have a good test. Thank you. Thank you for your suggestion. Because there are not that many projects today, so we just take the time to do a little bit more discussion. And I think that it's good for people to hear about some of these ideas. But we will", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3054345_ms_-_3126231_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3054345, "end_ms": 3126231}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3114995 ms - 3200349 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3114995 ms - 3200349 ms\n\nContent: Because there are not that many projects today, so we just take the time to do a little bit more discussion. And I think that it's good for people to hear about some of these ideas. But we will be a little bit rushed in the second and the third day. So I just wanted to keep it to let everybody know, make sure that you give people permission to your files and so forth before we get started. Why don't they just use their own computer? Okay. Hello, everybody. My name is Jacob Frosto. I'm Ian. And we're presenting our work. Well, you haven't worked on it, but we're going to work on a conversational agent for psychedelic assisted therapy. So let's talk a little bit about that. First off, start with a quote. This is from the Zendo Project, which is a nonprofit organization that works in this space. They say that in 2024, which we're almost at the end of this year, millions of people will take psychedelics, many of them for the first time. So it's an interesting thing when we think about", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3114995_ms_-_3200349_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3114995, "end_ms": 3200349}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3188805 ms - 3251061 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3188805 ms - 3251061 ms\n\nContent: space. They say that in 2024, which we're almost at the end of this year, millions of people will take psychedelics, many of them for the first time. So it's an interesting thing when we think about that. What that means is we're talking about like in a clinical setting, definitely, but also in a recreational setting. Obviously the legality of this kind of varies from place to place. In the United States, these are controlled substances, so it's not quite accessible in many of these cases, like insurance isn't going to cover it because it's considered like private healthcare in that case. So. Yeah. But we do have kind of some institutions here and different organizations that also carry out some of this research. They actually go through and. And perform this with in person, like patients. And we've actually reached out to a couple of these organizations just to get some feedback and some ideas on this topic and why this might be something that's useful. And then one more note on this", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3188805_ms_-_3251061_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3188805, "end_ms": 3251061}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3237429 ms - 3302303 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3237429 ms - 3302303 ms\n\nContent: And we've actually reached out to a couple of these organizations just to get some feedback and some ideas on this topic and why this might be something that's useful. And then one more note on this is that while they do have a lot of clinical trials, a lot of. People will be doing without supervision. And this project we have here could be helpful. In the harm reduction space, there's hotlines for psychedelic use, but having a virtual agent could also really help to help with harm reduction. Yeah, in many cases, when people are going into these experiences, they don't have any background. They might be doing it alone. That can Be very scary and overwhelming. We just want a way for people to kind of have access to the right knowledge to know what to do and how to approach this. Some like, I guess the more clinical reasons for psychedelic assisted therapy include treating things like ptsd, alcohol abuse, other substance abuse. There's also studies that show some sort of inkling of", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3237429_ms_-_3302303_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3237429, "end_ms": 3302303}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3288143 ms - 3351213 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3288143 ms - 3351213 ms\n\nContent: I guess the more clinical reasons for psychedelic assisted therapy include treating things like ptsd, alcohol abuse, other substance abuse. There's also studies that show some sort of inkling of improvement in depression or anxiety disorders. But then also from a recreational standpoint, some people have reported, and this is from a global drug survey in 2017, that these three reasons, right. So deeper self awareness, a more spiritual understanding of the world, and then dealing with emotional trauma. These are also reasons for people going through this and, you know, getting access to these substances like lsd, like mdma. Yeah. And so the key question that we want to answer is, can an LLM powered agent guide a psychedelic therapy session or prepare people for a psychedelic, you know, therapy session, or maybe even integrate it into their daily lives afterwards while prioritizing harm reduction and avoiding hurdles to treatment such as stigma, the cost and access to it. And here's a", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3288143_ms_-_3351213_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3288143, "end_ms": 3351213}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3338937 ms - 3399187 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3338937 ms - 3399187 ms\n\nContent: session, or maybe even integrate it into their daily lives afterwards while prioritizing harm reduction and avoiding hurdles to treatment such as stigma, the cost and access to it. And here's a very rough, brief overview of what it is that we want to approach this quarter. So first off, collecting data, we're looking at three separate sources. So first off, everything clinical, right. There's over 24,000 academic papers from different journals. They're all in PubMed, some through the NIH. We want to make sure that we know the facts, that we have everything in line because it is a pretty serious thing that we're dealing with. We're also looking at both like mock sessions, but also real life sessions. So transcripts from real life sessions, that's something that we would like to take into consideration. And then some of the institutions that we mentioned before have very specific, very detailed guidelines that we'd like to use as part of this. Especially when considering safety, we want", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3338937_ms_-_3399187_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3338937, "end_ms": 3399187}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3388039 ms - 3451387 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3388039 ms - 3451387 ms\n\nContent: And then some of the institutions that we mentioned before have very specific, very detailed guidelines that we'd like to use as part of this. Especially when considering safety, we want to use initially just a direct API to an LLM. We're also considering just taking our own model and fine tuning it, or even breaking it apart and trying to move around that. Because we want to again, put in our own guardrails. It's pretty significant. It's something that hasn't really been done before. So we want to make sure that safety is a priority. And then we also want to make sure that there's a very minimalistic, simple and safe UI for users to go on and actually experience. We don't want to overwhelm them in any sense, even possibly making it multimodal in a sense of including music. That's a very big part of research in this space. And then finally, evaluation, which Looks like both setting some hard metrics but also looking at, you know, actual manual use. That would be interesting how we get", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3388039_ms_-_3451387_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3388039, "end_ms": 3451387}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3439011 ms - 3515557 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3439011 ms - 3515557 ms\n\nContent: part of research in this space. And then finally, evaluation, which Looks like both setting some hard metrics but also looking at, you know, actual manual use. That would be interesting how we get that. But then also refining this to make sure that any response that does come out of this doesn't put someone in a crisis. Right. Or that can mitigate a crisis in any way. So. Yeah. Last thoughts. I guess just for further context, there's already a lot of virtual agents being used in the mental health space. Some are just more geared towards dealing. With anxiety or stress or depression. But these kind of tools are already being used a lot. This is just a bit of a more specific application of it. Yeah. And with that, thank you. Any questions? Hi. This is super cool. I like how it's so like forward thinking and also, I don't know, shows the benefits of possible like psychedelic use. I'm curious because obviously it's. People on psychedelics are in a very vulnerable state and so it might be", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3439011_ms_-_3515557_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3439011, "end_ms": 3515557}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3501477 ms - 3564517 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3501477 ms - 3564517 ms\n\nContent: thinking and also, I don't know, shows the benefits of possible like psychedelic use. I'm curious because obviously it's. People on psychedelics are in a very vulnerable state and so it might be kind of scary to put their state in like the hands of like an LLM, you know. And so I was wondering if maybe you've considered this, doing this in the sense of like, kind of like you said, harm reduction and helping people if they are in that anxious state rather than guiding them on this full journey. Because that might be a very difficult task to tackle. Yeah, thank you for the question. That is a very important point. I think across the board. When we've talked to doctors on this topic, the feedback that we first receive is that, oh, this is a good idea, it's great for accessibility. But. But the last thing that people are going to want to do is look at a screen during this instance. So that's why we also want to be specific in saying that, okay, well if they have no other option, this is", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3501477_ms_-_3564517_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3501477, "end_ms": 3564517}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3553373 ms - 3616133 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3553373 ms - 3616133 ms\n\nContent: the last thing that people are going to want to do is look at a screen during this instance. So that's why we also want to be specific in saying that, okay, well if they have no other option, this is an option, they should probably go to a human first. Right. I think that's like across the board. That's kind of the message that we want to send. But also just there's an important aspect to if they go through such an experience and it is very overwhelming and maybe even life changing in some cases they may not have anyone to talk to about it. Right. That's another aspect. There is a pretty high stigma on this in many places, not just the US So it would be nice to be able to kind of walk through, okay, what is it that you experienced? Why is that significant to you? And then how can you integrate that into your everyday life, like take away something useful from that? So that's kind of the main goal, I would say, of this. Thank you. No, I was just going to say. The first thought was that", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3553373_ms_-_3616133_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3553373, "end_ms": 3616133}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3597471 ms - 3671241 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3597471 ms - 3671241 ms\n\nContent: that into your everyday life, like take away something useful from that? So that's kind of the main goal, I would say, of this. Thank you. No, I was just going to say. The first thought was that I wonder. From a practical sense whether the scale. Of this is too small to apply LLM, like maybe for the foreseeable future, it's enough to have a clinician go through the process versus an automated system for a scale of doing psychedelics in the US for example. Yeah, that's a pretty solid point. And that's the first question that we wanted to address. Again, the problem that a lot of these clinics are having is that many of them have been kind of like suppressed in a federal sense. There's not enough people. What's the best way to put it? There's more people interested in getting this kind of treatment than there are people to administer the treatment. So, that being said, I think the two solutions are A, train more people to act and take on that responsibility and that role, or B, have", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3597471_ms_-_3671241_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3597471, "end_ms": 3671241}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3657101 ms - 3723279 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3657101 ms - 3723279 ms\n\nContent: treatment than there are people to administer the treatment. So, that being said, I think the two solutions are A, train more people to act and take on that responsibility and that role, or B, have something like this so that people who don't even have access to it, maybe they don't even live near any of those institutions, can go to someone, go to something. So sounds like you were taking a. Lot of risks and liabilities in the process. Oh, yeah, this would. Yeah, it's pretty risky. Sounds good. Yeah, it's fair. Kind of building off of what you just said about the other solution being to train more people for this. I wonder if you could. I don't know how deep into the project you have gotten so far, but what if you pivoted to simulating a patient undergoing this so that you could train more therapists to be prepared to deal with this, and then that tackles it from the other angle where we're bringing more human therapists into this space rather than trying to be the therapist. Yeah, I", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3657101_ms_-_3723279_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3657101, "end_ms": 3723279}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3710605 ms - 3782411 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3710605 ms - 3782411 ms\n\nContent: therapists to be prepared to deal with this, and then that tackles it from the other angle where we're bringing more human therapists into this space rather than trying to be the therapist. Yeah, I mean, that's a pretty solid idea too. Yeah, I would agree. Especially because that's like that process of mocking these sessions is something that's already being done. So, yeah, I would consider it. There's a student here, or a postdoc, Ryan Louie, who does simulated therapy. Essentially, the AI is the patient, but I don't think he's expanded into psychotherapy. So you could maybe talk to him. That'd be cool. Can I. Can I message you after this? Yeah, of course. I'm wondering if another. I mean, I had another idea, like, rather than trying to simulate the therapist, because personally, I would hope that people who are doing this kind of therapy have a medically trained human in the loop while they're doing this sort of thing. But I'm curious about if you get rich data from either", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3710605_ms_-_3782411_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3710605, "end_ms": 3782411}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3770155 ms - 3828251 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3770155 ms - 3828251 ms\n\nContent: I would hope that people who are doing this kind of therapy have a medically trained human in the loop while they're doing this sort of thing. But I'm curious about if you get rich data from either clinicians or people undergoing this therapy about what their experience was like, how they fared over the coming weeks and months and whatever. I think it'd be really interesting to just mind that data in such a way that maybe you could have an agent that if someone says, oh, I have XYZ symptoms, then it could suggest what might be a good therapy to try. Because I know there's different. Like y'all said, lsd, mdma. There's just like a ton of psychedelics out there. So I don't know, I just wanted to throw that out there, see what your thoughts on that. Okay, so almost taking the expert's perspective without having to talk to an expert, just to get an idea of where to go, is that kind of the idea? Like, given that I'm. I'm having XYZ symptoms, which of these psychedelics would I benefit", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3770155_ms_-_3828251_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3770155, "end_ms": 3828251}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3817865 ms - 3882593 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3817865 ms - 3882593 ms\n\nContent: without having to talk to an expert, just to get an idea of where to go, is that kind of the idea? Like, given that I'm. I'm having XYZ symptoms, which of these psychedelics would I benefit from? Based on the data of, like, who benefited from this? What sorts of patients benefited from this thing, you know? Yeah, just like, that's. That's great. Yeah, that would be. That's a solid thing, too, again. So I think our real goal is, again, to just approach it in a way where we have all this data in one place. Anyone who's interested, anyone who's curious, they can go to this as a resource. That doesn't necessarily mean, like, go through the therapy process alone with this, because in many cases I don't think that's advisable. And that could definitely be part of it. Just suggesting finding resources and connecting people with clinicians who know what they're doing. Yeah, that's perfectly reasonable. Cool. I think it is a good idea to study this problem you mentioned a couple of times", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3817865_ms_-_3882593_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3817865, "end_ms": 3882593}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3861743 ms - 3936819 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3861743 ms - 3936819 ms\n\nContent: resources and connecting people with clinicians who know what they're doing. Yeah, that's perfectly reasonable. Cool. I think it is a good idea to study this problem you mentioned a couple of times about, oh, how are we going to get it tested and stuff like that. You are not doing any tests in this experiment in this class. Okay. Do not even think. Go that way. This requires serious medical ir, which I don't think you can possibly get with what you know for sure. For sure. You're not getting an IRB done anytime for this class. Okay. You don't. There is not enough here to. I mean, the risk is too high for the gain and it will not approve with what I have heard is what I believe in. So do not even try to get it to be tested with any human subjects of any kind. You know, you can just study it and talk about the issues and what, you know, dig up all the information, simulate users. You can do anything, but do not try it on humans. No. Yeah. I think for evaluation purposes, mocking is the", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3861743_ms_-_3936819_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3861743, "end_ms": 3936819}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3925667 ms - 3995507 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3925667 ms - 3995507 ms\n\nContent: and talk about the issues and what, you know, dig up all the information, simulate users. You can do anything, but do not try it on humans. No. Yeah. I think for evaluation purposes, mocking is the way to go. And then. Yeah. Yes. I mean, I just want to remind everybody this is serious irb. Time here. This is not like when we do a lot of user study. We just. It's an exempt situation. But this is not. No, it's definitely not. Definitely not. But it is very interesting that I think we'll probably learn a lot from you working on this. Yeah, I think focusing on the before. And after, just getting the knowledge and making it available in various different forms and ways that have been discussed. I think that would be great. Perfect. All right. Thank you. Thank you. Don't scare me here. You said all the presentations. Then we have a special guest, a special request that I made to Michael and his partner. Is he here? Yeah. A lot of you will be working on these LLM projects where you will have", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3925667_ms_-_3995507_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3925667, "end_ms": 3995507}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 3978315 ms - 4062249 ms", "content": "Title: CS224V Lecture 7 > Transcript > 3978315 ms - 4062249 ms\n\nContent: all the presentations. Then we have a special guest, a special request that I made to Michael and his partner. Is he here? Yeah. A lot of you will be working on these LLM projects where you will have a whole. A lot of considerations about how do you optimize the code. Michael and your partner is arof and they will be talking about dspy. DSPY is how you pronounce it and which may be very useful for your project. Thank you. I put it on the. You can just pull up. Or I can pull it up on my laptop too, but it's on. It's the DSPY elevator pitch at the bottom there. Awesome. Do you want to use this? Sure, yeah. I guess we can use both. Yeah. Thanks so much, Monica, for giving us a moment to talk about this. So, yeah, ARNAV and I are contributors to this library called dspy, which is the framework for programming not prompting language models is the idea. So you can write a program that will automatically optimize your prompts for you to try to eke out the best performance of your system. And", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_3978315_ms_-_4062249_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 3978315, "end_ms": 4062249}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4050951 ms - 4111559 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4050951 ms - 4111559 ms\n\nContent: for programming not prompting language models is the idea. So you can write a program that will automatically optimize your prompts for you to try to eke out the best performance of your system. And you don't have to work out the nitty gritty of exactly how to prompt the model. You can focus on the high level declarative things such as what your inputs and outputs to each language model call should be and in fact, homework. 1. The storm system was actually implemented in DSPY, so you've all actually used this library indirectly. So it's never been easier to prototype impressive AI demos. But turning them into reliable AI systems requires a level of control and modularity that monolithic language models, such as just making a single call to GPT4, don't really offer to tackle this. We instead argue that you should build modular software that uses LMS as specialized components. Here are some examples of what language model programs look like. But you're all very familiar with these", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4050951_ms_-_4111559_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4050951, "end_ms": 4111559}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4100651 ms - 4158887 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4100651 ms - 4158887 ms\n\nContent: instead argue that you should build modular software that uses LMS as specialized components. Here are some examples of what language model programs look like. But you're all very familiar with these because these are the sorts of things that we've been talking about in class all quarter Storm, where you have language models calling each other, analyzing different retrieved documents, and talking with each other. You could do SQL generation, where you have a specific call to a language model to generate SQL code and then execute it and then retrieve the documents and analyze them. So whenever you're putting these compound systems together, you can think. Think of it as these modular components where you're making single calls to a language model that each one can be optimized. Yeah. So is this working? Okay, cool. Yeah. So, I mean, I think, as we all know, we've tried out with different language models on different tasks. Language models are sensitive under the hood. You can try them", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4100651_ms_-_4158887_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4100651, "end_ms": 4158887}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4148959 ms - 4204353 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4148959 ms - 4204353 ms\n\nContent: this working? Okay, cool. Yeah. So, I mean, I think, as we all know, we've tried out with different language models on different tasks. Language models are sensitive under the hood. You can try them on one pipeline and you try it on a different data set for that same pipeline, and you'll see performance break out of the completely. So DSPY is kind of a framework that makes this easier and makes it extensible, where you can define your tasks and pipelines in a very declarative way. Awesome. Yeah. So as we all know, LLMs are highly sensitive to how they're prompted. So you could try to write a prompt like this where you say think step by step and here are all the details and I'll put these few shot examples together. And some libraries will have prompts that are 61 kilobytes in length. So you really work out this really long prompt to get your system to work. So these pipelines are modular in principle, but actually when people are building them, they're doing it in kind of messy and ad", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4148959_ms_-_4204353_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4148959, "end_ms": 4204353}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4193729 ms - 4253105 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4193729 ms - 4253105 ms\n\nContent: really work out this really long prompt to get your system to work. So these pipelines are modular in principle, but actually when people are building them, they're doing it in kind of messy and ad hoc ways sometimes. So can we enable systematic machine learning for these language model programs? That's where DSPY comes in. Programming, not prompting language models. And so instead of tweaking these brittle prompts, here's another example, and this one is for a multi hop retrieval task. So the idea is you'll ask a question that you need to pull documents from Wikipedia to answer, and so you'll search Wikipedia, you'll rewrite the search query based on what was retrieved, and you'll search again. And so this prompt defines like a search lookup, finish step, that you can, you can make several calls to language models orchestrated by this prompt, and it gets 33% with GPT 3.5 on a multi hop QA task. But instead, if you write a program in DSPY and focus on designing an effective program", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4193729_ms_-_4253105_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4193729, "end_ms": 4253105}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4238945 ms - 4298155 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4238945 ms - 4298155 ms\n\nContent: calls to language models orchestrated by this prompt, and it gets 33% with GPT 3.5 on a multi hop QA task. But instead, if you write a program in DSPY and focus on designing an effective program with some objectives, such as metrics that you can optimize, then what you'll end up with is a program that looks something like this, which does the exact same thing. So we generate a search query to search for documents on Wikipedia, and then given a question, we'll rewrite it into a search query. We'll search documents and Retrieve them, we'll put those into the context search again, and we'll finally generate an answer to your original question using all of the documents you've retrieved. So basically this program, instead of writing out this long prompt, we've declared that we want to go from context in question to a query and from context in a question to an answer. And that's essentially all the prompting that we need to do is say what our inputs and outputs are to the language model.", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4238945_ms_-_4298155_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4238945, "end_ms": 4298155}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4286965 ms - 4346889 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4286965 ms - 4346889 ms\n\nContent: context in question to a query and from context in a question to an answer. And that's essentially all the prompting that we need to do is say what our inputs and outputs are to the language model. Then we will compile this for you into optimized language model calls. Given this program that I just showed, we can dig into what an actual prompt would look like under the hood and you can see we have an instruction and we have these few shot demonstrations. I'm not going to go too deep into how all this works, but. But essentially what we do is we run your program several times to generate these traces. So we know we want to go from context and question to a search query. So we'll add this chain of thought reasoning and we'll just essentially run your program many times to generate all of these possible outputs. And whatever metric function you provide, such as exact match accuracy or LLM as a judge or any sort of function, F1 score for classification tasks will optimize these examples", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4286965_ms_-_4346889_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4286965, "end_ms": 4346889}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4334659 ms - 4393177 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4334659 ms - 4393177 ms\n\nContent: possible outputs. And whatever metric function you provide, such as exact match accuracy or LLM as a judge or any sort of function, F1 score for classification tasks will optimize these examples and put them in context for you and search for which ones will be the best. That's a good question. So you have a data set of inputs which would be some. So the input questions you would have, you wouldn't necessarily have an input context. And so what we would do is given an input question, we run your program that just takes in a question. It does this generate query retrieves documents. And so this, this version that has a context is like the second hop in this search query because we have this for loop. So then we already have context as the example. So we've like bootstrapped that context as well. Essentially, I guess, essentially just to put some more context behind this, this is like a declarative approach of what Michael's been explaining to you guys. This is a prompting pipeline, but", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4334659_ms_-_4393177_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4334659, "end_ms": 4393177}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4381921 ms - 4436053 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4381921 ms - 4436053 ms\n\nContent: well. Essentially, I guess, essentially just to put some more context behind this, this is like a declarative approach of what Michael's been explaining to you guys. This is a prompting pipeline, but put into like nice Pytorchy looking code, but it's really just Python outside of this. You can declare your language model you'd like to use, you can declare a retriever or a data set that you like to retrieve embeddings from. And then all of that is happening in the back end. But the core part of DSPY is what you have right here, where you just took what your prompt would be and you just constructed into inputs and expected outputs and you put Some kind of programming logic behind it. Everything else outside of that is declared outside of this class. So I think for the purposes of this demonstration, you could probably visit the repository to learn how that looks. We have a couple of notebooks, but that's like the base of what this pipeline looks like. But yeah, all we take in are", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4381921_ms_-_4436053_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4381921, "end_ms": 4436053}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4424813 ms - 4482511 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4424813 ms - 4482511 ms\n\nContent: demonstration, you could probably visit the repository to learn how that looks. We have a couple of notebooks, but that's like the base of what this pipeline looks like. But yeah, all we take in are whatever the inputs to your forward passes. So as long as you have a question, we can find the context for you. And I guess another thing to answer your question about how do we get that context and how do we know it's actually good, we do make an assumption, which is that if your final output of your program, since your metric function will take the answer in for its evaluation, we assume if your final program got the correct answer, then the intermediate steps were correct along the way. So we'll assume whatever context was retrieved was actually relevant. Which in some cases may not be a valid assumption. But on the whole it tends to optimize well. And so a program that's compiled to get a prompt like this with the same model GPT 3.5 on the same task multi hop QA can instead score 55%", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4424813_ms_-_4482511_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4424813, "end_ms": 4482511}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4466335 ms - 4532955 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4466335 ms - 4532955 ms\n\nContent: assumption. But on the whole it tends to optimize well. And so a program that's compiled to get a prompt like this with the same model GPT 3.5 on the same task multi hop QA can instead score 55% and we didn't have to do any manual prompt engineering at all. And then on a smaller model llama2.13 billion, we can get 50% accuracy on this benchmark. And then with a really small comparatively model T5, as long as we do fine tuning on the labels that we, the intermediate labels that we generate, we can actually get as high as 39%, which remember, is outperforming GPT 3.5, getting 33% before. So it is an additional fine tuning step. But you can this model's orders of magnitude smaller. So this is possible thanks to some new algorithms which optimize the prompts and LM weights in language model programs. So first we construct an initial prompt for each module via a template. We generate examples of every module via rejection sampling. So this is what I was talking about, about rerun your", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4466335_ms_-_4532955_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4466335, "end_ms": 4532955}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4521555 ms - 4581155 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4521555 ms - 4581155 ms\n\nContent: programs. So first we construct an initial prompt for each module via a template. We generate examples of every module via rejection sampling. So this is what I was talking about, about rerun your metric and then if it gets a low score, we'll discard that. If it gets a high score, we'll say this is something that we could put in the prompt. We'll use these examples to update the program's modules, which update the program's modules just means we'll try putting them in the prompt. And then we can do automatic few shot prompting with this optimizer, bootstrap few shot with random search, we can induce new instructions using the Mipro V2 optimizer. And if you want to do fine tuning, we have an optimizer for that, too. We have some papers on this, but essentially, yeah, lots of people are using dspy. In fact, ARNOF can probably speak more to this than I, but there's a lot of companies in industry using it. Databricks, VMware, Walmart is using it. And so there's academic projects such as", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4521555_ms_-_4581155_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4521555, "end_ms": 4581155}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4569419 ms - 4622875 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4569419 ms - 4622875 ms\n\nContent: dspy. In fact, ARNOF can probably speak more to this than I, but there's a lot of companies in industry using it. Databricks, VMware, Walmart is using it. And so there's academic projects such as Storm and Co Storm, which you're all familiar with, and a couple others that we've listed here, too. So we have some QR codes if you want to try it out. We think it can be really useful for your course projects, potentially. And then, of course, ARNAV and I are also students in the class and we'd be happy to help debug and chat with you all about your work. And yeah, we're working on some different benchmarking things for DSPY programs, among other interesting projects. So the more DSPY programs that are out there, it helps us too. So I think. Yeah. Anything else you wanted to mention? I guess the pitch of our presentation here is if you find yourself ever taking a lot of time trying to figure out what your right prompt is, you can just remember about DSPY and see if it solves your problems.", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4569419_ms_-_4622875_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4569419, "end_ms": 4622875}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4614039 ms - 4678053 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4614039 ms - 4678053 ms\n\nContent: pitch of our presentation here is if you find yourself ever taking a lot of time trying to figure out what your right prompt is, you can just remember about DSPY and see if it solves your problems. And we're open to questions as well, if there's time. We have like five minutes left in class, so I don't want to take the whole class. Is there a way to. You talked about, like, how you assume. The intermediate steps are correct. Right. Is there a way to maybe get. A hook and get the intermediate outputs out and then evaluate both that with. The answer with a custom. Yes, actually, you can do that. So our metric functions will take in three things. They'll take in the input that you gave to your program in the forward pass, they'll take in the output of your forward pass, and they will also take in a trace, which allows you to get all of the intermediate inputs and outputs. So if you wanted to find a more complicated metric function that tries to optimize all the individual parts, that's", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4614039_ms_-_4678053_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4614039, "end_ms": 4678053}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4668471 ms - 4746667 ms", "content": "Title: CS224V Lecture 7 > Transcript > 4668471 ms - 4746667 ms\n\nContent: in a trace, which allows you to get all of the intermediate inputs and outputs. So if you wanted to find a more complicated metric function that tries to optimize all the individual parts, that's also possible. Good question. Yeah. Okay, I'll let you do it. Yeah. So basically, for any given task, you're decreasing the context. You're decreasing the context that's being fed. To the LLM not necessarily. Our prompts could be pretty long. I would say we're decreasing the manual effort on the prompt engineering. Right. Because LLMs scales quadratically. Right. So technically, I'm just trying to think from an energy perspective. Okay, cool. Yeah. So you can also set limits. You could say, I only want five examples in context, or I only want four and we can get the maximum performance that we can achieve with that size as well. But you're right, as your prompt length expands, so too does inference time. Quadratically. But. And we also have optimizers that will just try to optimize your", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4668471_ms_-_4746667_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4668471, "end_ms": 4746667}}
{"document_title": "CS224V Lecture 7", "section_title": "CS224V Lecture 7 > Transcript > 4735851 ms - 4863465 ms", "content": "we can achieve with that size as well. But you're right, as your prompt length expands, so too does inference time. Quadratically. But. And we also have optimizers that will just try to optimize your instructions. But we find few shot demonstrations are pretty important, usually. Cool. Thank you so much. Yeah, thank you. So. All right, so we're looking forward to all the presentations on Wednesday and next week and the proposals due. And if you want to talk to me, I have my office hour coming up. And also our TAs are available for you to help you with defining your project. All right. See you on Wednesday. It.", "block_metadata": {"id": "CS224V_Lecture_7_>_Transcript_>_4735851_ms_-_4863465_ms", "document_type": "transcript", "lecture_number": 7, "start_ms": 4735851, "end_ms": 4863465}}
