{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Weekend Update", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Weekend Update\n\nContent: Have you guys put in your weekend updates? Do you remember everybody? It's due today because it's past the weekend. I just want to make sure everybody was putting them in.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Weekend_Update", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 26035, "end_ms": 101855}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Projects 1, Starting soon", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Projects 1, Starting soon\n\nContent: Your weekend update is due and we want to keep track of what's going on with your project every week. For this week, I have been arranging meetings with some groups to help you get launched. If you have a project that you feel like it would benefit from having a meeting with me, please let me know.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Projects_1,_Starting_soon", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 102805, "end_ms": 202245}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Machine Learning: Structured Data and Hybrid Data", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Machine Learning: Structured Data and Hybrid Data\n\nContent: How do we build these agents? The whole concept here is that we want to build a semantic parser. It's a neural semantic parser that takes English and turns it into an SQL query. What we're going to talk about this time is about knowledge basis.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Machine_Learning:_Structured_Data_and_Hybrid_Data", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 203705, "end_ms": 471905}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > In the Natural Language of Databases", "content": "Title: CS224V Lecture 11 > Chapter Summaries > In the Natural Language of Databases\n\nContent: Today we're going to focus on the structured data. A lot of times when people talk about informational seeking question, these very casual questions are actually formal queries. If we teach LLM more about formal queries, its thinking will get crisper the representation.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_In_the_Natural_Language_of_Databases", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 472725, "end_ms": 771843}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Microsoft SQL Server", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Microsoft SQL Server\n\nContent: All right, so how about this? Select address from restaurants where location equals to Palo Alto, Chinese equals to any cuisine, and ratings grade equals to 4.5. Now it gets a little bit more complicated. This is called subqueries. There are many different natural ways of saying things that require you to do sorting.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Microsoft_SQL_Server", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 771939, "end_ms": 1019295}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Semantic Parsing of SQL queries", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Semantic Parsing of SQL queries\n\nContent: Joins allow you to join information from two different tables. Can you imagine writing database queries for these questions? How far is the closest four star restaurant? Do you know how to write that query in a database?", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Semantic_Parsing_of_SQL_queries", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 1019755, "end_ms": 1471827}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Yelp: A Semantic Parser", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Yelp: A Semantic Parser\n\nContent: With Yelp you can basically do a few shots semantic parser. A lot of natural language sentences can be translated to formal database queries. The reason why it works surprisingly well is that restaurants are well known.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Yelp:_A_Semantic_Parser", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 1471971, "end_ms": 1869681}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > How to classify a cuisine in SQL 2.8", "content": "Title: CS224V Lecture 11 > Chapter Summaries > How to classify a cuisine in SQL 2.8\n\nContent: The full agent is relatively simple to build. Some turns are not queries. It's just to teach it more about what those fields mean rather than teach it SQL query syntax. Here is the agent, the entire agent. Give me a query.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_How_to_classify_a_cuisine_in_SQL_2.8", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 1869873, "end_ms": 2303281}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > GPT vs. Yelp: Incomplete", "content": "Title: CS224V Lecture 11 > Chapter Summaries > GPT vs. Yelp: Incomplete\n\nContent:  LLM is not about testing on real benchmarks anymore. What does fixing the database do us? It is already better than annotations. Can you try to do human evaluation with what data set? You have to get people to use it.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_GPT_vs._Yelp:_Incomplete", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 2303353, "end_ms": 2651109}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Problem of SQL query accuracy in LLM", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Problem of SQL query accuracy in LLM\n\nContent: There's nothing you can do to solve this problem. It's just not going to work until we get to the natural. I mean to the free text which allows us to do hybrid databases. The problem keeps evolving because LLM is good enough to do the next step.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Problem_of_SQL_query_accuracy_in_LLM", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 2651157, "end_ms": 2743173}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Hybrid Data", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Hybrid Data\n\nContent: How do you find a restaurant on Yelp? There is so much information in the reviews. The question now is how do we combine them? Combining knowledge basis and textual qa. How would you put it together?", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Hybrid_Data", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 2743229, "end_ms": 3153185}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > C#: Learning the Code books", "content": "Title: CS224V Lecture 11 > Chapter Summaries > C#: Learning the Code books\n\nContent: The concept of turning natural language into these columns is coding code books. It is a term that is used outside of CS and not just cs. Any other concepts that you want to bring up? I think that's a good session.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_C#:_Learning_the_Code_books", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 3153605, "end_ms": 3194377}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Machine Learning in the 21st century", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Machine Learning in the 21st century\n\nContent: There are four techniques that I want to bring up before we talk about the combination of putting LLM functions into queries. We're going to talk about it in the next class, but I want you to think through the problem by looking at how people have approached it.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Machine_Learning_in_the_21st_century", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 3194441, "end_ms": 3642375}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Quantum data retrieval, 1 and 2", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Quantum data retrieval, 1 and 2\n\nContent: A third proposal is very common actually is to convert everything to text. Back to just doing purely text based retrieval. We can definitely see an improvement, but it's still not that good.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Quantum_data_retrieval,_1_and_2", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 3643155, "end_ms": 4012695}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Does a Hybrid Data Set Add Value?", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Does a Hybrid Data Set Add Value?\n\nContent: The data is very different, very little. Only data set that actually use Wikidata is the web questions. Problems with linearization is that they just skip all the opportunities for using relational algebra. It is not really what I would say, a good hybrid data set.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Does_a_Hybrid_Data_Set_Add_Value?", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 4013635, "end_ms": 4086147}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Combining multi-hop questions", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Combining multi-hop questions\n\nContent: The hybrid QA data set of multi hop Question answering over tabular and textual data. Here are some very natural questions. In which year did the judoka bearer participate in the Olympic opening ceremony? Which event does the 31 Olympic flagbearer participate in?", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Combining_multi-hop_questions", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 4086291, "end_ms": 4481355}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Chapter Summaries > Machine Learning and the Problem of Hybrid Data", "content": "Title: CS224V Lecture 11 > Chapter Summaries > Machine Learning and the Problem of Hybrid Data\n\nContent: S3HQA can understand a few types like count compare using lexical analysis to identify the question type. But it cannot solve the cascading multi hop problems where the result with the data to retrieve depends on the first hop answer. This technique requires a better solution for hybrid data.", "block_metadata": {"id": "CS224V_Lecture_11_>_Chapter_Summaries_>_Machine_Learning_and_the_Problem_of_Hybrid_Data", "document_type": "chapter summary", "lecture_number": 11, "start_ms": 4482095, "end_ms": 5305305}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 26035 ms - 171501 ms", "content": "Title: CS224V Lecture 11 > Transcript > 26035 ms - 171501 ms\n\nContent: What are you doing here? Yes. Have you guys put in your weekend updates? Do you remember everybody? It's due today because it's past the weekend. Yeah, you remember, right? Well, we know that this is like, yeah, this is the first weekend update. I just want to make sure everybody was putting them in. All right, I think we're ready to get started. All right, just to remind everybody, today your weekend update is due and we want to keep track of what's going on with your project every week. We know that this is kind of like the first week after the proposal, and you have just recently got your assigned mentor. And we went through all the projects and we figured out what is the best one to work with you. And because this is the first week, we all know that it's kind of really important because it's when you decide on the experiment, on the data set, or on the approach or the problem that you are going to work on. And then for this week, I have been arranging meetings with some groups to", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_26035_ms_-_171501_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 26035, "end_ms": 171501}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 152859 ms - 224591 ms", "content": "Title: CS224V Lecture 11 > Transcript > 152859 ms - 224591 ms\n\nContent: it's when you decide on the experiment, on the data set, or on the approach or the problem that you are going to work on. And then for this week, I have been arranging meetings with some groups to help you get launched, especially with projects that are not so obvious. Okay. And if you have a project that you feel like it would benefit from having a meeting with me and please let me know and we can bring your mentor in and we can all meet together type of thing. I think this is kind of trickier in the first week, and I'm happy to do that. And I won't be able to do that every week. There are just too many groups. But if you feel like this is something we need to do, I'm happy to do that. So what we're going to do today is to continue with the lectures, talking about what we. How do we build these agents? You know, remember at the beginning we got you guys working with the systems. We didn't get into the details. Um, we talked about wiki, we talked about Storm, we talked about wiki", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_152859_ms_-_224591_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 152859, "end_ms": 224591}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 213943 ms - 275631 ms", "content": "Title: CS224V Lecture 11 > Transcript > 213943 ms - 275631 ms\n\nContent: build these agents? You know, remember at the beginning we got you guys working with the systems. We didn't get into the details. Um, we talked about wiki, we talked about Storm, we talked about wiki chat. And now we're going to begin to begin the series on talking about structured data and hybrid data and so forth. Okay, so the. Just to remind you, the overall picture here is that we say LLMs just cannot be left on its own to answer all kinds of questions. There are so much data in knowledge bases and in free text, we have to go retrieve them. We talked about rag, which is all unstructured code, and it is all based on information retrieval. And what we're going to talk about this time is about knowledge basis. We're going to start with the basics, which is the structured data, which is like SQL, where you have a table, where you have a schema and this is a picture that we introduced earlier and we didn't get into the details, so I want to remind everybody where we were. The whole", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_213943_ms_-_275631_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 213943, "end_ms": 275631}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 261427 ms - 334679 ms", "content": "Title: CS224V Lecture 11 > Transcript > 261427 ms - 334679 ms\n\nContent: like SQL, where you have a table, where you have a schema and this is a picture that we introduced earlier and we didn't get into the details, so I want to remind everybody where we were. The whole concept here is that we want to build a semantic parser. It's a neural semantic parser that takes English and turns it into an SQL query for your table. This semantic parser, we are just going to take it like a generic one and you provide the schema, provide the prompt and examples, and you generate the SQL without retraining the neural, the retraining the semantic parser, if we could. Okay, um, so this wasn't possible before, before the LLMs, but now with LLMs, we want to know where we are, what can we do? And so we're going to start with that. So, for example, if I show you this simple sentence, it looks very simple, right? Show me the best Japanese restaurant in Palo Alto, something that we would like to know. And that turns into an SQL query. Over here it says, select star from", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_261427_ms_-_334679_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 261427, "end_ms": 334679}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 322381 ms - 390731 ms", "content": "Title: CS224V Lecture 11 > Transcript > 322381 ms - 390731 ms\n\nContent: sentence, it looks very simple, right? Show me the best Japanese restaurant in Palo Alto, something that we would like to know. And that turns into an SQL query. Over here it says, select star from restaurants, which is a table containing all the information. And that simple sentence, the yellow part is a phrase, is a clause. I suppose there are two green parts. So there is the Japanese. It has to be in the cuisine. The location has to be Palo Alto. And it is. You want the best, so you order that by rating. You didn't say the best rated, you just say best. We have to be able to translate in natural language to something that we have in the table. In this case, it is the rating. You want the best, we just take the first one. Given that SQL statement, we go to the database and we select the result. And then once you have the result, you have to turn the returned records into a good answer. And at that point you have a response generator. And in this case it says, oh, I searched for this", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_322381_ms_-_390731_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 322381, "end_ms": 390731}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 378375 ms - 447077 ms", "content": "Title: CS224V Lecture 11 > Transcript > 378375 ms - 447077 ms\n\nContent: result. And then once you have the result, you have to turn the returned records into a good answer. And at that point you have a response generator. And in this case it says, oh, I searched for this restaurant. Notice the response here. It doesn't just say Daigo en Palo. It doesn't just say Daigo. Okay, there are two things in this answer that I want you to pay attention to. The number one is that it actually tells you what it did. It says, I search for the best Japanese restaurant in Palo Alto and find Daiko. It's a good restaurant, by the way. And the point there is that if I just say Daigo, and if I actually happen to misunderstood the sentence and thought you were talking about something else, you would not even detect the answer is wrong. So phrasing the question is like, required. You always have to phrase the question, but the way it is phrased is very natural. As opposed to, I think you said, blah, blah, blah, blah, blah. Okay, you just say, okay, and the whole sentence is", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_378375_ms_-_447077_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 378375, "end_ms": 447077}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 435157 ms - 501287 ms", "content": "Title: CS224V Lecture 11 > Transcript > 435157 ms - 501287 ms\n\nContent: You always have to phrase the question, but the way it is phrased is very natural. As opposed to, I think you said, blah, blah, blah, blah, blah. Okay, you just say, okay, and the whole sentence is like, oh, the best restaurant, blah, blah, blah, is Daigo. Okay? And then it continues, it confirms, it says specifically, it has 4.5 ratings, something that you didn't ask for. And it gives you a little bit more about sushi and Japanese cuisine. So this is the kind of question that we want to generate, okay? So that's the problem translating from natural language to formal query. So what we're going to do in this part of the lecture is just to focus on the structured data. Today we're going to cover two lectures, okay? This is the. We just focus on the structured data. And what I want to tell you is that a lot of times when people talk about informational seeking question, these very casual questions are actually formal queries. Before I did this work, I never thought of it this way. I", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_435157_ms_-_501287_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 435157, "end_ms": 501287}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 487229 ms - 560085 ms", "content": "Title: CS224V Lecture 11 > Transcript > 487229 ms - 560085 ms\n\nContent: you is that a lot of times when people talk about informational seeking question, these very casual questions are actually formal queries. Before I did this work, I never thought of it this way. I say, oh, you have a conversation about X and Y. But I didn't realize that just simple things that we are saying on a daily basis has a very precise formal representation. So that's what I want you to notice. And I have suspicion that if we teach LLM more about formal queries, its thinking will get crisper the representation. What I meant by that is that the neural net representation will be crisper in understanding how questions should be phrased. So this is something that we want to do, we want to verify, but that's the idea, it's a hypothesis. So we understand what it means to look at natural language questions. I want to get you to a point that when you hear people talk, you say, oh, that's a database query of these things. This is a filter, this is a projection, this is a sort, and so", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_487229_ms_-_560085_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 487229, "end_ms": 560085}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 549955 ms - 615195 ms", "content": "Title: CS224V Lecture 11 > Transcript > 549955 ms - 615195 ms\n\nContent: language questions. I want to get you to a point that when you hear people talk, you say, oh, that's a database query of these things. This is a filter, this is a projection, this is a sort, and so forth, okay? And then we're going to talk about how you build one. We're just going to stick with something very basic, which is something like, such as consulting a table for restaurant. And then we talk about the evaluation issues. Last class, we spent a whole class discussing evaluation. And you're going to keep seeing that theme throughout this course. So I cannot emphasize more how important query languages are. The language is domain agnostic, okay? It means that if I understand how queries are constructed, I can ask questions about anything. And it has all the. If you give me a table of information, everything that you can possibly ask for are just a composition of a few operators. These relational algebra operators, selection, projection, Cartesian product union and set difference,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_549955_ms_-_615195_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 549955, "end_ms": 615195}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 600507 ms - 670755 ms", "content": "Title: CS224V Lecture 11 > Transcript > 600507 ms - 670755 ms\n\nContent: information, everything that you can possibly ask for are just a composition of a few operators. These relational algebra operators, selection, projection, Cartesian product union and set difference, very simple operators. There are some extended ones like sort and aggregate operators and so forth. And this is, you know, this kind of topic. You can imagine the early days when people did not formalize Databases, everybody tries to represent information and they figure ways of pulling information out and you know, but then some. And then actually the people who did this work were able to, you know, they got a Turing Award for working on databases. It's so important, we have to leverage it. And here what we are adding is the natural language interface to databases. It's very expressive, succinct, and well defined. So I want you to get the intuition of what natural language looks like in databases. So it is kind of hard for me to give you a natural language and ask you for a database", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_600507_ms_-_670755_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 600507, "end_ms": 670755}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 658375 ms - 736751 ms", "content": "Title: CS224V Lecture 11 > Transcript > 658375 ms - 736751 ms\n\nContent: succinct, and well defined. So I want you to get the intuition of what natural language looks like in databases. So it is kind of hard for me to give you a natural language and ask you for a database query. So I'm going to go backwards. I'm going to show you a database query and you tell me what the natural language is. This is the basic where clause. It's a filter operator. It says select star from restaurants where location equals to Palo Alto and Chinese equals to any cuisines. That means Chinese shows up in any part of the cuisine's field. What is this asking what's in natural language? Anybody? Yes, please. Chinese restaurants in Palazzo. Asking for a Chinese restaurant in Palo Alto. A list is all the Chinese restaurants. The reason why we have any here is because a restaurant can have multiple cuisines. It could be Chinese, vegetarian, for example. So here is just to say I'm looking for a Chinese restaurant where the cuisine Chinese shows up in that restaurant. So this is all", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_658375_ms_-_736751_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 658375, "end_ms": 736751}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 722297 ms - 812463 ms", "content": "Title: CS224V Lecture 11 > Transcript > 722297 ms - 812463 ms\n\nContent: have multiple cuisines. It could be Chinese, vegetarian, for example. So here is just to say I'm looking for a Chinese restaurant where the cuisine Chinese shows up in that restaurant. So this is all the restaurants? Naturally, it is all. The star means select all the restaurants that are all the Chinese restaurants in Palo Alto. Yeah, so that's the English sentence. You don't necessarily say all, but that's what it means. What if I say and rating Greater equals to 4.5. How would you put it in English? Yeah, where the rating should be at least 4.5. At least. So very good. All right, so how about this? Select address from restaurants where location equals to Palo Alto, Chinese equals to any cuisine, and ratings grade equals to 4.5. That's called a projection, by the way. When I say select address is that I pull the record out and I project on the column address. Okay, so that's how I implement it in the compiler. In retrieving this query. What does it say in English? Yeah. Give me the", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_722297_ms_-_812463_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 722297, "end_ms": 812463}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 787323 ms - 877833 ms", "content": "Title: CS224V Lecture 11 > Transcript > 787323 ms - 877833 ms\n\nContent: address is that I pull the record out and I project on the column address. Okay, so that's how I implement it in the compiler. In retrieving this query. What does it say in English? Yeah. Give me the addresses of Chinese restaurants in Palau at least 4.5. Okay, so you can see yourself doing this. I want the address of the Chinese restaurants, blah, blah, blah, blah. Okay, so now it gets a little bit more complicated. This is called subqueries. So earlier when we say where we have a clause, these are the atom filter, it just says parameter equals value or greater than value. But the filter itself could be a whole entire query of its own. And that's what we mean by a subquery here. So here is a longer statement now and it's the same as before. And I say an ID equals to the. Select the restaurant ID from reviews where author equals to Bob. This is a squiggly equals. It just means that the author, it doesn't have to match exactly the word Bob. It could be Bob Smith or and so forth. It's", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_787323_ms_-_877833_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 787323, "end_ms": 877833}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 862545 ms - 957471 ms", "content": "Title: CS224V Lecture 11 > Transcript > 862545 ms - 957471 ms\n\nContent: ID from reviews where author equals to Bob. This is a squiggly equals. It just means that the author, it doesn't have to match exactly the word Bob. It could be Bob Smith or and so forth. It's like it has the word Bob in it. That's why you have got this squiggly. So that itself, the bottom line is itself a query. And the result then is used in the filter in the higher level query. Okay, what does this say? Yeah, exactly. It is reviewed by Bob. Right, so this is the reviews. So it's very long here, but it's very, it's still pretty natural in English. Okay, how about this one? I keep adding to it and the sentences didn't get that much longer. Right, so what does this one say? It's the same. And then I added order by field, descending or ascending. And this is order by rating. Descending. What am I asking for? Yeah, priority from highest rating to lowest. Is that how you gonna say it? Provide Chinese restaurants where the author is bought with rating. The rating is like priority rating.", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_862545_ms_-_957471_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 862545, "end_ms": 957471}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 935751 ms - 1014355 ms", "content": "Title: CS224V Lecture 11 > Transcript > 935751 ms - 1014355 ms\n\nContent: am I asking for? Yeah, priority from highest rating to lowest. Is that how you gonna say it? Provide Chinese restaurants where the author is bought with rating. The rating is like priority rating. From higher to lower, from higher to lower. So what we are looking for is the rating. But a lot of times I just want. So the full sentence should be limit one, Then I would be asking for the best or limit two or limit three is top three or top four. And oftentimes I think in the context of something like restaurants, you don't need to hold list. But to be more precise, it would be with the word limit in it. And then you'll be asking for the top one, top three. So people are not thinking about sorting necessarily once I just ask for the top three. Okay, you have to sort it, but I'm not thinking about how you build it. But in English it would be very common to say I just want the top rated or the best or the, you know, the recommended or whatever. So there are many different natural ways of", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_935751_ms_-_1014355_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 935751, "end_ms": 1014355}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1002191 ms - 1083491 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1002191 ms - 1083491 ms\n\nContent: about how you build it. But in English it would be very common to say I just want the top rated or the best or the, you know, the recommended or whatever. So there are many different natural ways of saying things that rely require you to do sorting. Okay, so what else? So tables, one of the most important things are joins. Joins allow you to join information from two different tables. Okay, so here is the example. Here we just say that, look, restaurants is a table, reviews are a table. We used it earlier, before and now I have a statement. I say select star from restaurants joined with reviews on the fact that it is restaurants ID goes to the reviews of restaurant id, where location is this and the rest of it. What does this say? Why do I need to join. Yes. Restaurants and the reviews for Chinese restaurants. Exactly. So you are pulling out the restaurants and joining it with the review tables and so presenting it at the same time. Okay. With their reviews. All right, so you see,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1002191_ms_-_1083491_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1002191, "end_ms": 1083491}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1068571 ms - 1134191 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1068571 ms - 1134191 ms\n\nContent: for Chinese restaurants. Exactly. So you are pulling out the restaurants and joining it with the review tables and so presenting it at the same time. Okay. With their reviews. All right, so you see, this is like if I say this, this is what you need to generate. That's a semantic parser problem going backwards. Very natural sentences. So it just goes to show that a lot of things that people say needs to do database queries. And so this is an experiment done a little bit earlier, but I actually just tried it on Siri. It's not doing any better. Okay, so people have been building question answering agents for a long time. And here are some examples of questions that we have tried. If you look at these questions, can you imagine writing the database queries for it? That's what I want you to see. Show me restaurants rated at least four stars with at least 100 reviews. You know how it is with restaurants if they're only, you know, there are not enough reviews, it doesn't matter what the", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1068571_ms_-_1134191_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1068571, "end_ms": 1134191}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1123855 ms - 1204151 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1123855 ms - 1204151 ms\n\nContent: see. Show me restaurants rated at least four stars with at least 100 reviews. You know how it is with restaurants if they're only, you know, there are not enough reviews, it doesn't matter what the rating says, Right? Because it is just the owner putting in the first set of reviews. So this kind of question is very, very common, you know, because today I say I look at the rating, I look at the reviews, and so why don't I just say it? Show me restaurants rated at least 4 stars with at least 100 reviews. Nobody can answer that question or show me restaurants rated higher than 4.5. I mean, I'm not picky. I don't need 5, 4.5. I picked my number, 3.5, whatever number it is. Google in that case answered it. So give me another example. How far is the closest four star restaurant? Do you know how to write that query in a database? With a database, suppose you know your current location, how would you write that query? Yeah, just like based off your locations. Or a list of all the restaurants.", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1123855_ms_-_1204151_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1123855, "end_ms": 1204151}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1176527 ms - 1263425 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1176527 ms - 1263425 ms\n\nContent: to write that query in a database? With a database, suppose you know your current location, how would you write that query? Yeah, just like based off your locations. Or a list of all the restaurants. Based off distance and just traverse it and select the first one which would be the closest to you, has a radius. Traverse means sort. Okay, so you first of all have to calculate the distance, right? You said you take the location, you have to find the location of the restaurant, find the distance, and you sort on the distance. Okay, so you see, you're coding, right? But this is expressible in this SQL query just as it is, just with the standard syntax, very natural sentences. Right? How far is the closest four star restaurant? That's easy. Find a W3C employee that went to Oxford, you Probably don't know who it is. Does anybody know who that is, by the way? And nobody can answer that question. It turns out to be Tim Berners Lee, who is the inventor of the Internet. Like, you know, this is", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1176527_ms_-_1263425_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1176527, "end_ms": 1263425}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1247045 ms - 1333239 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1247045 ms - 1333239 ms\n\nContent: don't know who it is. Does anybody know who that is, by the way? And nobody can answer that question. It turns out to be Tim Berners Lee, who is the inventor of the Internet. Like, you know, this is like Turing Award winner and all that. But they don't know that. I mean, none of the assistants know it. And what is the database query for that? Suppose you have a table for employees, like LinkedIn, information about the employer, information about where they went to school. And you do a join between these two things so you can find the same single person that shows up on the table on the two sides. So that's a join who worked for Google, who graduated from Stanford and won a Nobel Prize Prize, a join graduates, alums, and the Nobel Prize, and show me hotels with checkout time later than 12. You'll have to tick the hotel checkout time and do the comparison. All right, so everything here is a very simple database lookup. But unfortunately, you will see that most of these existing", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1247045_ms_-_1333239_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1247045, "end_ms": 1333239}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1318967 ms - 1389151 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1318967 ms - 1389151 ms\n\nContent: than 12. You'll have to tick the hotel checkout time and do the comparison. All right, so everything here is a very simple database lookup. But unfortunately, you will see that most of these existing assistants, they don't do that. Why? Because they actually don't do semantic parsing. They figure out what are the most common questions, like what is the best restaurant around here? I just ask Siri anything about rating, and then as they show restaurants rated higher than 4.5, and then it will pop up a bunch of restaurants and say, what rating are you interested in finding out about? Okay. They are not answering the questions. They just pick up some of the most common questions that they have coded and give you that as the answer. They don't really go dig into the databases. So that's the reason why I think that this is an opportunity here. It's like the system that we're building. If you translate into the database, I don't care whether they are common questions or rare questions, as", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1318967_ms_-_1389151_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1318967, "end_ms": 1389151}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1377455 ms - 1436445 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1377455 ms - 1436445 ms\n\nContent: reason why I think that this is an opportunity here. It's like the system that we're building. If you translate into the database, I don't care whether they are common questions or rare questions, as long as they can be answered by the database. I can give you the answers. Because people ask questions for whatever reasons they want. Okay. It's not a very. You know, some of them are reasonably common, but it doesn't matter. There are many, many possible questions. And this is what I can do. So, for example, let's go back to the restaurant case. If you look at Yelp, Yelp has a whole bunch of APIs, and it has a bunch of. This is the schema with a bunch of fields which is very natural. Name, address, location, phone number, opening hours, cuisines, price rating, number of reviews, reviews, popular dishes, Very, very common. So once you have this database, and if you're able to ask, generate the formal SQL queries against it, all those questions that I show you that have to do with", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1377455_ms_-_1436445_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1377455, "end_ms": 1436445}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1423317 ms - 1490537 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1423317 ms - 1490537 ms\n\nContent: popular dishes, Very, very common. So once you have this database, and if you're able to ask, generate the formal SQL queries against it, all those questions that I show you that have to do with restaurants can be etched. So here is just an example of a conversation. You start by saying, oh, my dad is visiting in Mountain View. What do you think about dinner choices? It is not, what are the restaurants in Palo Alto? Okay. However, no problem. If you take GPT and you know, ChatGPT, it will come up with something like select staff from restaurants where location is close to Mountain View. It extracts out all the information that is irrelevant to the database lookup and it just does the reasonable job. As you expected. It comes back with a decent answer that we constructed. And then you say, oh, I found Pascal. And it talks about this Spanish restaurant. And then you say, oh, how about Italian? I just want to switch. I really like the food. When I visited Florence last year, nothing to", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1423317_ms_-_1490537_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1423317, "end_ms": 1490537}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1478337 ms - 1544833 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1478337 ms - 1544833 ms\n\nContent: oh, I found Pascal. And it talks about this Spanish restaurant. And then you say, oh, how about Italian? I just want to switch. I really like the food. When I visited Florence last year, nothing to do with the query. And it extracts it to the exact information that you really need for the database. So that's the concept here. A lot of sentences translates to simple database queries. What is the ham called in Italian? Oops, that is not a database lookup. In the conversation you can expect that. Then they say, we'll be visiting Stanford. How about in Palo Alto? I switch from the cuisine searching around cuisines to a location here. No problem. It has an update and it remembers because of the conversation that has gone on that you are still talking about an Italian restaurant. Okay, so this is a conversational semantic parser. It is not just looking at the last sentence. It has a lot of ellipses. Right. I did not say Italian. But you have to remember it from the context and that's a", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1478337_ms_-_1544833_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1478337, "end_ms": 1544833}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1531129 ms - 1609313 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1531129 ms - 1609313 ms\n\nContent: is a conversational semantic parser. It is not just looking at the last sentence. It has a lot of ellipses. Right. I did not say Italian. But you have to remember it from the context and that's a semantic parser problem. So the thing that I want to make you be aware of is that it's very natural. A lot of natural language sentences can be translated into formal database queries. Natural language is compositional. Okay? In a single phrase, seven words long, I may be asking for three clauses filled in the database queries. So I want to ask you a question. If I give you Yelp, how many possible queries are there? You saw the schema. They're only about little less than 10. How many queries can I make out of it? Can I memorize all the queries and give you the answers? From taking natural language to the formal query, how many queries are there? Yeah, we'll say it's this because you can make infinite amount of words with. Like a very small subsets. Precisely. Okay. It is a graph grammar based", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1531129_ms_-_1609313_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1531129, "end_ms": 1609313}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1592179 ms - 1666635 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1592179 ms - 1666635 ms\n\nContent: to the formal query, how many queries are there? Yeah, we'll say it's this because you can make infinite amount of words with. Like a very small subsets. Precisely. Okay. It is a graph grammar based on the grammar is compositional. You can keep composing them arbitrarily. And if I give you an expression greater than 3, greater than 5, greater than whatever that's already infinite. Any of these clause can potentially be unbounded. I would say unbounded concept. It is innumerable. Okay, because I can look at the constructs and I say which constructs are you using? I go from all the single constructs, two constructs, two colossus and so forth. But it is really infinite. This is the standard about language. Natural language is infinite. The grammar, the formal language is infinite. And LLMs is all about language. It understands language. It is very natural for it to actually learn the translation to the formal queries. It is not about memorizing the content in the database, but it is", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1592179_ms_-_1666635_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1592179, "end_ms": 1666635}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1654139 ms - 1729085 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1654139 ms - 1729085 ms\n\nContent: is all about language. It understands language. It is very natural for it to actually learn the translation to the formal queries. It is not about memorizing the content in the database, but it is about writing it down as compound database queries. So let's try to build one. How hard or easy is it? It turns out that with Yelp you can basically do a few shots semantic parser. So you use LLM of some kind. You say you are a semantic parser. Generate a query for a restaurant database with the following signature. You give it the signature plus a few examples. This is why we call it few shot. And it seems to perform pretty well. That's the starting point. If you look into it, there are a couple of places where we need to do a little bit more work. I should mention that the reason why it works surprisingly well is that restaurants are well known. If you give me a table with all the chemistry names and interactions and stuff like that, and maybe something even fancier, or a table of this", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1654139_ms_-_1729085_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1654139, "end_ms": 1729085}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1716729 ms - 1788001 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1716729 ms - 1788001 ms\n\nContent: surprisingly well is that restaurants are well known. If you give me a table with all the chemistry names and interactions and stuff like that, and maybe something even fancier, or a table of this people in a storybook or whatever it has never seen before, that's a totally different story. But when it comes to things like restaurants, it understands when the opening hours, closing hours, it knows all the terminology already, so it does really well. So what we talk about here may not translate to every single database, but you. But what is important is that LLMs already know the basic SQL syntax, okay? And the observed issue is with the enumerated values. So for example, in a typical database is that they have codes. It's like we don't when it comes to price. We just match it to one of say four codes. It doesn't have the exact number ranges and so forth. It's like cheap, moderate, expensive, luxury. Okay, these are the categories. For example, with Yelp. So it turns out that if you", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1716729_ms_-_1788001_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1716729, "end_ms": 1788001}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1770499 ms - 1849391 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1770499 ms - 1849391 ms\n\nContent: say four codes. It doesn't have the exact number ranges and so forth. It's like cheap, moderate, expensive, luxury. Okay, these are the categories. For example, with Yelp. So it turns out that if you give me something like this as enumerated type with about 10 values, very well separated values, then LLMs just know how to use these values. You just declare them and then it will pick them. So for example, you say, well, what are some good inexpensive Chinese restaurants? It will know that you're looking for price is equals to cheap. You have to use the word that is being used in the enumerated type, because in the database it says cheap, it doesn't say inexpensive. So you have to match it to the specific word that is in the database. Now, the problem here is that if you look at the cuisines, which is a list of words, it has actually over 200 values. If you stick 200 values of each enumerated type has 200 values, you have multiple of them. It just gets big as a list. So if you put all", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1770499_ms_-_1849391_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1770499, "end_ms": 1849391}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1830343 ms - 1915099 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1830343 ms - 1915099 ms\n\nContent: which is a list of words, it has actually over 200 values. If you stick 200 values of each enumerated type has 200 values, you have multiple of them. It just gets big as a list. So if you put all the choices of all the fields into the schema, it gets long. So it doesn't do as well. Doesn't do very well. Okay. And as we mentioned, the database search expects an exact match, so we have to do something smart about it. So give you an example. If I say, show me a cafe, you might want to say where coffee is in the cuisine list. And as it turns out that Yelp doesn't have coffee, it actually has coffee and tea or cafe. Okay, but you don't know which one it is using. You cannot just stick in the word easily. I mean, you can, because there are too many of them, 200 of them. The LLM is not able to decide and choose one of them. You know, if you just put it all in a gigantic list as part of a schema. So what do we do? So for the large enumerated types, we play a little trick. You just say that", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1830343_ms_-_1915099_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1830343, "end_ms": 1915099}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1899873 ms - 1968349 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1899873 ms - 1968349 ms\n\nContent: decide and choose one of them. You know, if you just put it all in a gigantic list as part of a schema. So what do we do? So for the large enumerated types, we play a little trick. You just say that show me a cafe. We want to keep the semantic parser job simple. And so we say, let us just say that you can put whatever word you want, say coffee, in this, any cuisine. But I know that I'm actually doing a classification. Okay, I need to pick up cafe and find the thing that is closest to it. That's a classification. But we want to let you know. You want to leave that out from the semantic parser. The semantic parser, just think that it has whatever cuisine it does. So it just parses that to coffee equals to any cuisine. And at that point we say that, oh, but cuisine is an enumerated type, equals means classification. And now I have to do a classification. And when I do the classification, I decide, for example, that coffee and tea and cafe are both very close to coffee. And you have to", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1899873_ms_-_1968349_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1899873, "end_ms": 1968349}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 1955521 ms - 2029035 ms", "content": "Title: CS224V Lecture 11 > Transcript > 1955521 ms - 2029035 ms\n\nContent: means classification. And now I have to do a classification. And when I do the classification, I decide, for example, that coffee and tea and cafe are both very close to coffee. And you have to put both of them in. Why? Because some restaurants are coded as one or the other. So you want to pick up all of them in your choice in the query. So now we change the equal operator to from a text field into a value classification operation. And now we have to implement this classify function, which is that given X, which is derived from the semantic parser, in this case coffee. And then you say I have a list which is all the possible values I need to do a classification. And then once I know what are the possible classes, then we rewrite it like we did here and we say that coffee and tea is equal to any cuisine or cafe is equal to any cuisine. We want to pick up anything that is close to what we are looking for. So that's how we want to implement it. So the question is, how do you implement", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_1955521_ms_-_2029035_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 1955521, "end_ms": 2029035}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2014771 ms - 2092047 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2014771 ms - 2092047 ms\n\nContent: to any cuisine or cafe is equal to any cuisine. We want to pick up anything that is close to what we are looking for. So that's how we want to implement it. So the question is, how do you implement classify? How would you implement classify? I have a word. I have a whole list. Yeah, the word in the list. And say which of these best applies or none of them. That would work up to maybe 200 names. And that's what I would do. And then we will talk about cases when that's not good enough and we will defer that and we have to go do extra work. And this problem can get harder. But the first round is that you just use LLM okay, for something like restaurants, no problem. And we just pick up whichever one that you want. In this case, I don't have to find the best one, I just find multiple that actually work. So the full agent is relatively simple to build. So the first thing we notice is that some turns are not queries. We showed that already. We just let our alarms answer those and then we do", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2014771_ms_-_2092047_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2014771, "end_ms": 2092047}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2079583 ms - 2152039 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2079583 ms - 2152039 ms\n\nContent: work. So the full agent is relatively simple to build. So the first thing we notice is that some turns are not queries. We showed that already. We just let our alarms answer those and then we do a few shots. The few shots can. It's just to teach it more about what those fields mean rather than teach it SQL query syntax. You really don't need to do that. So this is the agent, the entire agent. Give me a query. The first thing I want to do is to check if I need to do a database lookup. If you ask me about ham or this and that, I can just answer your question using LLM. If I. So you can actually click on the slides and you will see the whole prompt. It's not that complicated. You just say, here is your restaurant virtual assistant and here are some examples. And this goes. And this shows you some of the examples and the style of answers that you. And so let's take a look at the first one. It says that you Can I help you? And they mention something. You decide, oh yeah, check the database", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2079583_ms_-_2152039_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2079583, "end_ms": 2152039}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2135493 ms - 2222801 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2135493 ms - 2222801 ms\n\nContent: some of the examples and the style of answers that you. And so let's take a look at the first one. It says that you Can I help you? And they mention something. You decide, oh yeah, check the database is equals to yes. And here is an example where you check the database. It is no, this is just to do work on the classification. After you get the classification, you do the semantic parser. And the semantic parser shows you some examples of the. Here is the schema. And then there are just A few examples of how you translate the user query into target and so forth. So even if the user doesn't say three, he say, what are some Japanese restaurants in Kansas City? In the context of restaurants, we're not going to give you the whole list. So we'll just give you a sum number of them. And sometimes we can choose to throw in the fact that you want to limit the numbers based on ratings and so forth. Okay, so you can kind of, kind of adjust it a little bit for your problem domain. Then you go to", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2135493_ms_-_2222801_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2135493, "end_ms": 2222801}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2208777 ms - 2275179 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2208777 ms - 2275179 ms\n\nContent: we can choose to throw in the fact that you want to limit the numbers based on ratings and so forth. Okay, so you can kind of, kind of adjust it a little bit for your problem domain. Then you go to the. Now you have a query, it goes through the SQL query and it looks up the database and get the results. And as we are doing the SQL query, that's where the classification enumeration comes from. Right? Because it just passes. And this is, oh, you're looking for a coffee in the cuisine, but coffee is not one of the allowed lists. And how do we figure out which is allowed? We go through this classify Enum, which is another LLM prompt. We get the answer, we come back and response and if I don't have a result, I have to generate another result, another response and says that, oh, you know, the data is not there. So you can take a look at the slides and then you can look at the prompt. It's very simple. It's so easy to build to get this first round. So the answer here is that they're", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2208777_ms_-_2275179_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2208777, "end_ms": 2275179}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2261835 ms - 2348887 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2261835 ms - 2348887 ms\n\nContent: the data is not there. So you can take a look at the slides and then you can look at the prompt. It's very simple. It's so easy to build to get this first round. So the answer here is that they're relatively easy to build. The hard question is the evaluation, actually. So what do we do? We have been working on this prior to LLMs. We have a whole bunch of data set. We have a data set and then we measure the query accuracy against our, you know, measure the accuracy of our LLM based parser on the annotated data. We actually get very poor results. What do we do? What do we do when we get a poor result? It's poor. What do we do? What do you think? Yeah, manually. Look at some of the errors. Look at the errors, look at the errors. Exactly. Okay, we look at the errors, we spot check the results by hand and you have to do that. You just don't know what you're getting unless you do that. Don't assume that. Don't assume anything. And so what we discovered is that suppose I say something like", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2261835_ms_-_2348887_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2261835, "end_ms": 2348887}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2335383 ms - 2407163 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2335383 ms - 2407163 ms\n\nContent: by hand and you have to do that. You just don't know what you're getting unless you do that. Don't assume that. Don't assume anything. And so what we discovered is that suppose I say something like what's some great Mexican food around Bernal Heights? And the goat target looks like this. And the predicted target by LLM puts in a rating in there. Okay, you are looking for great restaurant. And it says, oh, maybe rating greater than equals to 4. What if you say rating greater than equals to 4.5 it's equally good. But as a matter of fact, the annotated answer actually doesn't say anything about rating. We dropped that. So there is actually, I would say the hand annotation was worse than what GPT is already naturally doing. So here's another example. This is from another data set we didn't repair ourselves. It says name all cards with. This is a pretty, pretty specific question here. And if you look at the gold answer the first one, select the id. The predicted target says select the name", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2335383_ms_-_2407163_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2335383, "end_ms": 2407163}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2392811 ms - 2467621 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2392811 ms - 2467621 ms\n\nContent: ourselves. It says name all cards with. This is a pretty, pretty specific question here. And if you look at the gold answer the first one, select the id. The predicted target says select the name with the rest of them being identical, which is better? I would say the second one. Okay, you're going after the name of this card. So what we discovered throughout is that it is so good that the gold target is actually the problem. So in our discovery process, it's like this. We first of all try to get good result. And we realize it's like, oh, that's because we coded this way. But GPT likes to code it another way. So let us try to get, you know, we change the prompts to show GPT what you're supposed to do. Okay, so that's the first thing we tried. And then we kind of realize it's like we have so many conventions that are not justifiable. Okay, so why don't we change the. Maybe we should fix the annotations in the benchmark so that we can fix. We can match. And then afterwards we just", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2392811_ms_-_2467621_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2392811, "end_ms": 2467621}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2454821 ms - 2526787 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2454821 ms - 2526787 ms\n\nContent: have so many conventions that are not justifiable. Okay, so why don't we change the. Maybe we should fix the annotations in the benchmark so that we can fix. We can match. And then afterwards we just realized, why on earth are we fixing the annotations by learning from GPT. GPT is doing it right already. So why are we trying to. What does fixing the database do us? It is already better than annotations. And this is done by our students. This is not done by Amazon Turks, people who are not familiar with anything that we are doing. This is the best we can possibly do. What on earth? What does it mean to fix the annotations in the benchmark? And this is the first time we realize that we have landed on this new planet that I was trying to tell you about. It's nothing like what you have learned in the past where you just keep training, improving on the gold. The gold is suspicious. What should we do next? What should we do next? You guys are going to be facing these problems, right? What", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2454821_ms_-_2526787_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2454821, "end_ms": 2526787}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2507213 ms - 2593049 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2507213 ms - 2593049 ms\n\nContent: in the past where you just keep training, improving on the gold. The gold is suspicious. What should we do next? What should we do next? You guys are going to be facing these problems, right? What do you think you would do next after seeing this? Yes, there's a data set. Try human evaluation. Can you try to do human evaluation with what data set? You try to get people to use it. Very good. You have to get people to use it and hopefully you get somebody who is not yourself. Try it on a real user that's the thing about LLM is it's not about testing on real benchmarks anymore. Okay. The first thing, remember the first thing I'm trying to do, I'm not trying to. I'm not trying to evaluate. I want to know if it does well with whatever means. And then you worry about evaluation. Remember that there is this big piece that has nothing to do with training data. And so what did I do? I tried it on a real user. I just grabbed a colleague and I say, look, I built this. Try it. And guess what this", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2507213_ms_-_2593049_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2507213, "end_ms": 2593049}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2579681 ms - 2654901 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2579681 ms - 2654901 ms\n\nContent: is this big piece that has nothing to do with training data. And so what did I do? I tried it on a real user. I just grabbed a colleague and I say, look, I built this. Try it. And guess what this is. What? The sentence he used. Find me a spicy Chinese restaurant in Palo Alto. And we failed because I don't have spicy in my schema. It doesn't belong in the schema. But that's what people want. People don't want just a Chinese restaurant. I want a spicy Chinese restaurant. So how do you figure that one out? Should I just tell him, go away. We don't know how to answer this question. Or do we fix it? And how do we fix it? Where's the answer? Yeah, sources of information that might not be in the existing schema. It is not in the existing schema, but it's actually in Yelp. Where is it in Yelp reviews or you have to look through the reviews. So this is the first time we realize that, you know, the information is not there. There's nothing you can do to solve this problem. And it is such a", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2579681_ms_-_2654901_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2579681, "end_ms": 2654901}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2643165 ms - 2718515 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2643165 ms - 2718515 ms\n\nContent: reviews or you have to look through the reviews. So this is the first time we realize that, you know, the information is not there. There's nothing you can do to solve this problem. And it is such a nasty problem that we say, okay, I'm just. It's just not going to work until we get to the natural. I mean to the free text which allows requires us to do hybrid databases. And it's like there's no point in doing evaluation on Yelp because it is just going to fail in real life. You can keep making these limited questions to check on the database accuracy, but why it seems to be good enough. So we basically paused this and we went on to study the hybrid database problems. Okay, so just keep this in mind. The problem keeps evolving because LLM is good enough to do the next step if you provide it with the information. So here, this is a little exercise. We did not get to evaluation on this. We're going to do this after we add the hybrid data. But this used to be a huge problem just parsing", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2643165_ms_-_2718515_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2643165, "end_ms": 2718515}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2703541 ms - 2795269 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2703541 ms - 2795269 ms\n\nContent: it with the information. So here, this is a little exercise. We did not get to evaluation on this. We're going to do this after we add the hybrid data. But this used to be a huge problem just parsing SQL query for databases. If you give me something well known, small table. It just works with a few shots. But we cannot evaluate using old benchmarks. Don't waste time on old benchmarks. We see that we cannot address the full problem with our free text retrieval. Go solve that problem next okay, so let's. That is the introduction to the next to the next lecture, which is on the hybrid data, which is a little bit more complicated. So how do you find a restaurant on Yelp? Today is like after you kind of find a high level, we go, and I don't know about you, I go read some reviews. Okay, but how many reviews can you read? You just read the first few and you say it's good enough or whatever. But there are so much information in the reviews. So the question now is how do we combine them? So if", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2703541_ms_-_2795269_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2703541, "end_ms": 2795269}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2781689 ms - 2856305 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2781689 ms - 2856305 ms\n\nContent: many reviews can you read? You just read the first few and you say it's good enough or whatever. But there are so much information in the reviews. So the question now is how do we combine them? So if you look at the Yelp, you've got the. This is what Yelp actually contains. There are the blues are all the schema, you know, this schema that we talked about. And then there's free text like popular dishes and previews and reviews and so forth. There is just a lot of useful information around that and there's a lot of text. Okay. And you know, thousands and thousands of rows. Obviously a tool can help you because I'm not going to read all those reviews. I want to be able to retrieve. But we have talked about retrieval, right? I mean we talked about it with the wiki chat pipeline and we see that how we are able to retrieve. So the question is now we have hybrid data. I need to combination. What do we do? So just a tiny example here, I need a family friendly restaurant that's in reviews and", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2781689_ms_-_2856305_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2781689, "end_ms": 2856305}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2839303 ms - 2913777 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2839303 ms - 2913777 ms\n\nContent: how we are able to retrieve. So the question is now we have hybrid data. I need to combination. What do we do? So just a tiny example here, I need a family friendly restaurant that's in reviews and but if I say I need a family friendly restaurant in Palo Alto, this part in his reviews, that part is in database. So you need both. So a lot of the things that we are talking about have hybrid data. Restaurants, reviews, popular dishes. If I give you products, you want to read the reviews and descriptions, courses, there are reviews, ratings and what people say and so forth. Even if I give you business processes, I get the receipts and statements, but there are kinds of regulations or invoice or the contracts. That's all in free text for medical, same thing. There's just, just about all the data that we care about have this combination of information. We have so far talked about ir, we have talked about structured. How do you put it together? Well, how would you put it together? So this is", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2839303_ms_-_2913777_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2839303, "end_ms": 2913777}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2900313 ms - 2974167 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2900313 ms - 2974167 ms\n\nContent: that we care about have this combination of information. We have so far talked about ir, we have talked about structured. How do you put it together? Well, how would you put it together? So this is the hybrid data QA problem, right? Combining knowledge basis and textual qa. So you have learned both. Now what would you do? How would you put it together? Here is my database. Yelp. You are now a 224V project is to make Yelp work. What would you do? I just want you guys to think and talk a little bit about it. And because once you think about it, what I'm going to show you makes a lot more sense because you can see if they are what you are thinking of or they are better or worse than what you have in mind. Right. What do you think? How would you go about it? Yeah. I will first approach the fuchsia thing we did at the beginning before. And then if the answer is not. Oh yeah, I will look first in the database. In the database. Right. If I'm not using SQL and if I'm unable to find the", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2900313_ms_-_2974167_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2900313, "end_ms": 2974167}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 2958503 ms - 3041493 ms", "content": "Title: CS224V Lecture 11 > Transcript > 2958503 ms - 3041493 ms\n\nContent: the fuchsia thing we did at the beginning before. And then if the answer is not. Oh yeah, I will look first in the database. In the database. Right. If I'm not using SQL and if I'm unable to find the answer, well, I will go to the unstructured part, which perhaps might contain the answer. Okay. Structured first, unstructured later. Okay, that's one strategy. Anybody else? Yeah, you can start from the question. Can you separate the keys of the question into a knowledge base or a pretext base and then compile them together and then come. So you split the problem, but now I have to figure out how to split the problem, but I split the problem and I do both and I combine them. Okay, that's good. Yeah, you an SQL syntax and then inside you have LLM functions that call the LLM to kind of handle a few tests. Yes, that's what we've been discussing. Yes, like that. I like that answer. Yes, it might be a use case. Where like as you try to search for fields that don't like, exist in your table,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_2958503_ms_-_3041493_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 2958503, "end_ms": 3041493}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3027435 ms - 3099319 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3027435 ms - 3099319 ms\n\nContent: a few tests. Yes, that's what we've been discussing. Yes, like that. I like that answer. Yes, it might be a use case. Where like as you try to search for fields that don't like, exist in your table, you can add those fields. I can add those fields. How do I add those fields and where do I add those fields? From the text of the reviews, like if I read the reviews, I could have another boolean field that says spicy or not spicy. Very good. Okay, so you say that. Why do you need that? Why can't you just search from the text? What is the advantage of turning that into a field? To make it faster. Someone wants to search for it again. Ah, to search it for it again. What is another example of advantage? Any other advantages? Probably, for example, anybody can help help him here. Are you following on this? Yeah, it'd be a lot easier to like analyze the data. Like you could see, like, oh, hey look, the spicy dishes have average of 3 stars, non spicy of average of 5 star or 4 stars. You know,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3027435_ms_-_3099319_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3027435, "end_ms": 3099319}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3088791 ms - 3153185 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3088791 ms - 3153185 ms\n\nContent: on this? Yeah, it'd be a lot easier to like analyze the data. Like you could see, like, oh, hey look, the spicy dishes have average of 3 stars, non spicy of average of 5 star or 4 stars. You know, so you can, you can, you can like as a restaurant owner. See, now you can start doing correlations. Yeah, because now you have tables, you can do correlations, you can do sorts, you can do averages, you can do sums. If you're. If it is not just a boolean, of course. And because that's relational algebra. So that's a difference. The relational algebra allows you to do aggregated calculations like the Total, the sum, the average, the whatever. And you just don't get this. If I'm using ir, I have to pull them out one at a time and I have to read the whole, you know, find, find all the values from every single record and then put it into a table in a sense in order so that you can sum it up or do whatever it is. So there is this concept that maybe you can go from natural language to a schema.", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3088791_ms_-_3153185_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3088791, "end_ms": 3153185}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3140089 ms - 3207437 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3140089 ms - 3207437 ms\n\nContent: every single record and then put it into a table in a sense in order so that you can sum it up or do whatever it is. So there is this concept that maybe you can go from natural language to a schema. Okay. And that is the term coding. We are coding it, having a code book. I think I mentioned that a little bit. Our class is kind of like recursive because I want to get you going at the highest level with all the ideas. But I want to also make sure that you understand why we have to go to those techniques. And that concept of turning natural language into these columns is coding code books, which is a term that is used outside of CS and not just cs. Okay, very good. Any other concepts that you want to bring up? I think that's a good session. So let's just talk about what people have been doing. There are four techniques that I want to bring up before we talk about the combination of putting LLM functions into queries. And that is the language S uQL. We're going to talk about it in the", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3140089_ms_-_3207437_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3140089, "end_ms": 3207437}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3197233 ms - 3270539 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3197233 ms - 3270539 ms\n\nContent: There are four techniques that I want to bring up before we talk about the combination of putting LLM functions into queries. And that is the language S uQL. We're going to talk about it in the next class, but I want you to think through the problem by looking at how people have approached it. And so the first one is that it's a little bit like what David is talking about and a little bit like, what is your name? Monica. Monica. All right. I won't even forget that. So it's a little bit like, can we, can we tell what query it is and do whichever. So this is the first paper. This is 2023. Could it just be one year ago? This is still 2024. This is 2023. And what they did is that they don't just talk about any free text. It is the free text is the subjective knowledge which is the, for example, reviews. So it's kind of like a, a subset of all the things that can go into free text. But it really doesn't matter. The whole problem that they are solving is a task oriented data set like", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3197233_ms_-_3270539_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3197233, "end_ms": 3270539}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3255533 ms - 3330281 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3255533 ms - 3330281 ms\n\nContent: example, reviews. So it's kind of like a, a subset of all the things that can go into free text. But it really doesn't matter. The whole problem that they are solving is a task oriented data set like finding a restaurant, finding a hotel. But they also want to take into account the subjective information that is only written down in text. And according to this paper, they said this is the first data set which contains subjective knowledge seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. And the data set is multi. Was it very well studied for dialogues? And I do want to Mention that. It turns out that this is one of the so poorly annotated. It just goes to show that Google went through it, Amazon went through it. It is still poorly annotated. Just like our PhD students annotating Yelp. It is still poorly annotated. Just keep that in mind. So this is their dataset and this is the largest task oriented dataset with subjective knowledge,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3255533_ms_-_3330281_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3255533, "end_ms": 3330281}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3315593 ms - 3406149 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3315593 ms - 3406149 ms\n\nContent: Just like our PhD students annotating Yelp. It is still poorly annotated. Just keep that in mind. So this is their dataset and this is the largest task oriented dataset with subjective knowledge, SK t o d at the bottom hours and that's the size 20,000. It is manual dialogue, TOD task oriented data. It has query aspect, sentiment, knowledge and so forth. Okay, so this is kind of like state of the Art 2023. And what they have is basically a binary classifier. You have a turn detection, the yellow is going through the domain specific API in db. They actually did not use SQL. It was just matching to slots. It is like a lame SQL. The blue box is really ir. I don't want to get into the details of the IR answers, how we do ir, because we've discussed this a lot already. But you see that this is a binary classification. So that's their method. The second paper that I want to bring up is the one from Stanford. It's 2021 and they built chatbot. 2021 is anything before LLMs is a different story.", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3315593_ms_-_3406149_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3315593, "end_ms": 3406149}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3386745 ms - 3468445 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3386745 ms - 3468445 ms\n\nContent: binary classification. So that's their method. The second paper that I want to bring up is the one from Stanford. It's 2021 and they built chatbot. 2021 is anything before LLMs is a different story. Okay, Everything is harder. It was chirpy cardinal and they were, I think they were, they won the second prize in this competition. And it is an open chat. And the whole story is, the whole goal here is can you engage a user actively for as long as you can. They're just comparing how long they can talk without the user getting bored, upset or just tired. So this is their pipeline. There is a lot of different pieces because at that time LLMs don't exist. And then they just have to create many, many modules out of it. But the most important thing I want to mention is this basic concept that when you give me a question, there are different modules to answer different kinds of questions. Sometimes it is just the language model chatting, sometimes it is looking up a database, sometimes you're", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3386745_ms_-_3468445_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3386745, "end_ms": 3468445}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3454869 ms - 3538803 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3454869 ms - 3538803 ms\n\nContent: you give me a question, there are different modules to answer different kinds of questions. Sometimes it is just the language model chatting, sometimes it is looking up a database, sometimes you're looking up free text. So they actually build a, they take the input and they run it across the different modules and then you choose the one at the end. Okay, so there it determined the RGs to be run. Sometimes it is multiple and then you just pick the right one after the results are formulated by the different modules. The good news is that you can just take existing modules, free text and databases and just run them in parallel and use it somehow. Come, you know, choose one or the other. And what is the weakness here? If you just choose one or the other. Why? Because for example news, the stylite phrase, the answer in news pretty much differs. For example, in a conversational matter, right? The user might be looking for a conversational. But then I give you a news which is not a news. I", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3454869_ms_-_3538803_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3454869, "end_ms": 3538803}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3528623 ms - 3594883 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3528623 ms - 3594883 ms\n\nContent: phrase, the answer in news pretty much differs. For example, in a conversational matter, right? The user might be looking for a conversational. But then I give you a news which is not a news. I give a news piece which doesn't match. Oh, you have to ask it properly, right? I mean the fact that I just used what is if I choose just KB or just knowledge base or text. Why do I have a problem with diversity? That's the question I'm asking. What is it that you miss here when you do one or the other? Yeah, answers are a combination of knowledge. From both the answers. I mean the examples that I even I show family friendly and in Palo Alto, one is from the knowledge base, one is from free text. If you use this method, what happens? You can only do like one or the other. You can't answer both. So one, you return all kinds of restaurants that are in Palawan and then over here you return all kinds of things that are in, that have space, that have Chinese food. And now what is the answer? Now you", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3528623_ms_-_3594883_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3528623, "end_ms": 3594883}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3581853 ms - 3657179 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3581853 ms - 3657179 ms\n\nContent: you return all kinds of restaurants that are in Palawan and then over here you return all kinds of things that are in, that have space, that have Chinese food. And now what is the answer? Now you have to do a join. I mean you have to do it intersection of those two in this particular case. And you just don't get it, right. Oh, so here is an example, right? We did try it using that method and it just gives you some, let's see, what is it? It just gives you. It just doesn't give you the right answer. I think it gives you an Italian restaurant, but that is just not romantic. I think I cut the question out, answer out somehow. But what is interesting is that, you know, is it common? It turns out that in our experiment we just ask people to ask questions about restaurants. 55 out of the 100 questions need a combination. Okay, so we just cannot answer those questions. So Those are the two methods, 1 and 2. A third quest, a third answer. A third proposal is very common actually is to convert", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3581853_ms_-_3657179_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3581853, "end_ms": 3657179}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3641347 ms - 3728211 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3641347 ms - 3728211 ms\n\nContent: need a combination. Okay, so we just cannot answer those questions. So Those are the two methods, 1 and 2. A third quest, a third answer. A third proposal is very common actually is to convert everything to text. So this is done by unique QA group, this is 2021 and they convert everything to text. There's text semi structured data like tables. They just read the tables and turn them into text. And then the knowledge base itself, it's all text. And now when everything is in text, you can just treat it as a IR problem, right? So, so if the tables are linearized, wiki data, it's very tricky. It is like entity, property, entity. So here you say that this is the Star wars episode, the cast member is Portman and that turns into a sentence. The first nodes and edge. You Know the node, edge node is turned into a sentence and then that is yet another part of the sentence. So you're writing out the graph in just a set of texts. Okay, and the good news, the advantage of this method is that you", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3641347_ms_-_3728211_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3641347, "end_ms": 3728211}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3710427 ms - 3792487 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3710427 ms - 3792487 ms\n\nContent: is turned into a sentence and then that is yet another part of the sentence. So you're writing out the graph in just a set of texts. Okay, and the good news, the advantage of this method is that you just use text retrieval and it works because everything is in there. Is that a good answer? Yeah. Back to just doing purely text based retrieval. Right. You kind of lose the whole. But you have all the information in text. Do you lose the benefit of the structure that you had before? Okay, what is the benefit of the structure? You know, where things relate to what I mean, as we were just talking about, like we can aggregate over certain. You are losing relational algebra. Okay. You may be able to pull out the text piece by piece, but what does join? How are you going to do join? How are you going to do filters, intersections, sorting? Okay, you're losing relational algebra operators. Yes. So, but that's what they proposed. Okay. This is the state of, you know, what people are doing at that", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3710427_ms_-_3792487_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3710427, "end_ms": 3792487}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3778863 ms - 3850235 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3778863 ms - 3850235 ms\n\nContent: to do filters, intersections, sorting? Okay, you're losing relational algebra operators. Yes. So, but that's what they proposed. Okay. This is the state of, you know, what people are doing at that time. And then what they showed is this result. Okay, I want you to look at this result and tell me what you, what have. What do you learn from this result? What should you be able to learn from a table of results like this? So what they did is that they take. They took three sets, two, three data sets. They run it on just the text using the same text retrieval method. Just the text, just the tables, Just the wikidata part in text. Text plus tables and then text plus tables plus wikidata. And this is the result that they reported. And does it show? What does it show? What do you learn from this exercise? It's a lot of work to get those numbers. So what does, when you look at this, what are you observing? It's more important that I teach you how to look at numbers and critique it as opposed", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3778863_ms_-_3850235_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3778863, "end_ms": 3850235}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3837143 ms - 3925779 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3837143 ms - 3925779 ms\n\nContent: exercise? It's a lot of work to get those numbers. So what does, when you look at this, what are you observing? It's more important that I teach you how to look at numbers and critique it as opposed to what exactly are these numbers? Okay, but this is what you show. What do we. What, what do you say? What does it tell you? A bunch of numbers. Is it good or bad? Okay, is it good? Are there numbers? So the whole point here is that they have all turned it all into text. Are the numbers good? So, for example, if you just do ir, you get the number one is Wikipedia text. Right? Without this paper just using standard IR on Wikipedia text, you get the first row and the rest of it is a result of them turning tables and wikidata into more text. Yes. A question before I attempt to answer this. These are percentages, right? Yes. Okay. I mean, we can definitely see an improvement, but I would say that it's still not that good. Because, like, probabilistically, it's almost like flipping a coin,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3837143_ms_-_3925779_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3837143, "end_ms": 3925779}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3915221 ms - 3995417 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3915221 ms - 3995417 ms\n\nContent: are percentages, right? Yes. Okay. I mean, we can definitely see an improvement, but I would say that it's still not that good. Because, like, probabilistically, it's almost like flipping a coin, right? 50%. It is not very good. I mean, that's the first thing to notice. It's like if you are saying it is better, you're going from 49 to 54. Okay, 49 is given. It is just standard, just text retrieval. 49 to 54, 64 to 64.1, 50.6 to 57.8. And I would say that, yeah, there is some amount of improvement, but where is the. How much is it and what does it actually show about this method? Okay, now, absolutely. It is not very good. And that is the first point I would make. Most people just look at AI papers and they say, oh, it's better, but what is the absolute value? It's not very good. We're not living with 54 and 64 and so forth. Okay. And then you look at the delta and then you say, what are. What is the delta? And as a matter of fact, I would say that the data set, it's. What does it tell", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3915221_ms_-_3995417_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3915221, "end_ms": 3995417}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 3980039 ms - 4059523 ms", "content": "Title: CS224V Lecture 11 > Transcript > 3980039 ms - 4059523 ms\n\nContent: living with 54 and 64 and so forth. Okay. And then you look at the delta and then you say, what are. What is the delta? And as a matter of fact, I would say that the data set, it's. What does it tell you about the data set? Yeah, a lot of redundant information where the tables and wiki data. The text already captured. Yeah, the text captures. So it doesn't tell me a whole lot about this approach. It could be that it doesn't add much. It could be that the data set actually doesn't push the need for handling hybrid data. I would say the results are like, what are you showing? How hard is this? The data is very different, very little. The only data set that actually use Wikidata is the web questions. And you can take a look at that and you say that it's either in Wikidata or in text. Roughly. That's what you are actually seeing from these numbers. Okay. It is not really what I would say, a good hybrid data set. So if we test it on Yelp, what happened is that we linearized according to", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_3980039_ms_-_4059523_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 3980039, "end_ms": 4059523}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4046387 ms - 4126933 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4046387 ms - 4126933 ms\n\nContent: That's what you are actually seeing from these numbers. Okay. It is not really what I would say, a good hybrid data set. So if we test it on Yelp, what happened is that we linearized according to what they did. And so, um, so we discovered, for example, they cannot handle the algebraic operators that we talked about and it cannot do column operators. Um, for example. And so I would say that the problems with linearization as we discuss is that they just skip all the opportunities for using relational algebra. Okay, so now let's get to the last one, which is doing a little bit better, which is that I'm going to do both and I'm going to combine the results. And for this, let's start with a good data set. This is actually a decent data set. They call it the hybrid QA data set of multi hop Question answering over tabular and textual data. So this is the data set. It is very realistic. So this is about flag bearers of Myanmar at the Olympics. Very narrow, okay? And they have a table that", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4046387_ms_-_4126933_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4046387, "end_ms": 4126933}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4107061 ms - 4206269 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4107061 ms - 4206269 ms\n\nContent: answering over tabular and textual data. So this is the data set. It is very realistic. So this is about flag bearers of Myanmar at the Olympics. Very narrow, okay? And they have a table that has the name of the Olympics, the year, the season, and the flag bearer. And then there is information attached to the, to each Olympics. And then there is information attached to each flag bearer. So there's free text on the Olympics, free text on the flag bearer. And here are some very natural questions. In which year did the judoka bearer. I don't even know what that it's not judo. Judoka bearer participate in the Olympic opening ceremony? Okay, in which year? So what you need to do, how did you find that question answer, can you do it by hand? So you first of all have to look for judoka. And that is only in the Wikipedia pages, all right? And you find that the person, the top person is Burmese Judoka. And then it from there you know the name. And then from that name you can go to. It is", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4107061_ms_-_4206269_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4107061, "end_ms": 4206269}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4186691 ms - 4291341 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4186691 ms - 4291341 ms\n\nContent: that is only in the Wikipedia pages, all right? And you find that the person, the top person is Burmese Judoka. And then it from there you know the name. And then from that name you can go to. It is mentioned 2016. It's in the Wikipedia page. So that you can find in Wikipedia page. So the second question, which event does the 31 Olympic flagbearer participate in? So you can start with 31, the Olympic bearer. And then for the event you will have to look at the text. Okay, So I do not. I cannot search the text directly without looking at the. Looking at the table. So what they did is to show you six types, the six types of queries. The first one you have to look at the table. T is table. P is for passage. That goes from the table to the passage. The second one from passage to table. Passage to table to passage is the third. Then you have to do table and S. What is S? So what does it say? Huh? Is it seasoned? No. Let's see which male bearer participated in the men's event. So you first", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4186691_ms_-_4291341_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4186691, "end_ms": 4291341}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4258995 ms - 4358953 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4258995 ms - 4358953 ms\n\nContent: to table to passage is the third. Then you have to do table and S. What is S? So what does it say? Huh? Is it seasoned? No. Let's see which male bearer participated in the men's event. So you first of all have to look at the. So that table, it says that the men's 100K, you know the name of that person. And then you go to the. You need to figure out it is gender. Oh, that's this gender table, maybe the sex table. And then you have to combine them in order to figure out make sure that that's a guy. Okay? So I think that that's what the S is. And it is multiple tables. And then you have table for comparison, P for comparison and superlative, which is like the youngest, the oldest, which is the sorting that we are talking about. So what we are seeing here is that they require to do A and then B. You cannot Just do A and B. It's not either A or B. You cannot do A and B and then compare. You have to do A and then B. How many types are there? Are there only six types? How many types can", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4258995_ms_-_4358953_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4258995, "end_ms": 4358953}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4340073 ms - 4416645 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4340073 ms - 4416645 ms\n\nContent: then B. You cannot Just do A and B. It's not either A or B. You cannot do A and B and then compare. You have to do A and then B. How many types are there? Are there only six types? How many types can there be? Yeah, because I feel like you're going to keep composing. You're going to keep composing. It's the same question. It's infinite. But these are the common ones. And it is very natural. That's the most important thing is that I'm not doing anything fancy. You know, to go from when was the flag bearer of Rio Olympic born? There's P to T to P. It actually does take you from passage to table, back to passage. Okay. It's very, very natural question. Nothing fancy. So this is a good data set. And then you really kind of show that you see some compositionality. So here is the state of the art model that was done in 2023 and it says this is a three stage approach for this multi hop problem. And they are using LLMs. It goes like this. The picture is a little bit messy because there were", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4340073_ms_-_4416645_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4340073, "end_ms": 4416645}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4400581 ms - 4474991 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4400581 ms - 4474991 ms\n\nContent: art model that was done in 2023 and it says this is a three stage approach for this multi hop problem. And they are using LLMs. It goes like this. The picture is a little bit messy because there were errors in the annotation. They were trying to get around the error. So just ignore step one. Step two, that part is just think of it as retrieval. If the data set was well annotated, you don't need this middle piece. Okay. Just imagine that it is just retrieval and then you go into that table. So what it does is that you first of all retrieve from the row and then you retrieve from the passage independently. Then for each one of them, you pull out the information for the answers from rows and answers from passages. And then you put all the rows and passages and you rank them. So there's a lot of raw material that was retrieved by going from one hop from the sentence to the source of the information. You take this information and you give it to the LLL and it can integrate them. Okay. And", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4400581_ms_-_4474991_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4400581, "end_ms": 4474991}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4460001 ms - 4535925 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4460001 ms - 4535925 ms\n\nContent: of raw material that was retrieved by going from one hop from the sentence to the source of the information. You take this information and you give it to the LLL and it can integrate them. Okay. And combine them using the smart of LLM. All right. And then it generates the answer. And so that's the reasoning step. And so to summarize here is that it retrieves the information and then you do the. You sort it for relevance and then you try to combine the information. It can, it actually understands a few types. It hot codes a few types like count compare using lexical analysis to identify the question type. So it's template based and we know that there are infinite number of possible combinations. So that concept is a little bit like it can handle the easier patterns that they hard coded. And it uses chain of thought prompting. So that is the method. And here are the results. And there are many, many papers before this. And then this is the S3HQA and here are the results. What does it", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4460001_ms_-_4535925_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4460001, "end_ms": 4535925}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4518745 ms - 4608045 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4518745 ms - 4608045 ms\n\nContent: And it uses chain of thought prompting. So that is the method. And here are the results. And there are many, many papers before this. And then this is the S3HQA and here are the results. What does it say to you? They have the best result, except for that column, the F1 compared to DEHG. But what does it say to you? And then they also have a human results because human is not 100%. But the fact that the human gets to this point, that means that what it says to me is that the data set is reasonably well annotated. This measures the errors that can possibly be there because if there are errors, humans will not be able to replicate that. You see now you have to look at data. It is all a result of an experiment. It is not absolutely good or bad. You first of all have to wonder if the data set is good. And by looking at the human, I would say it's reasonably good, not 100%. And what else do we see from here? You were going to say something. I mean, it's better, but not significantly better", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4518745_ms_-_4608045_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4518745, "end_ms": 4608045}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4592161 ms - 4665807 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4592161 ms - 4665807 ms\n\nContent: is good. And by looking at the human, I would say it's reasonably good, not 100%. And what else do we see from here? You were going to say something. I mean, it's better, but not significantly better than previous results. It's really not significantly better. It's just, it's only, I would say it's in the noise almost right. There's a little bit better. It's kind of consistent, it's true. But it is so small, relatively, you are not making a breakthrough. I would say it's not breaking through the next level of importance of next level of accuracy. Yes. In machine learning, do y'all not do statistical significance or confidence intervals? That's a good question. But this is small. You can eyeball it when it is so small you can eyeball it and ask for significant. You know, I don't think we need to do more tests to figure out the significance. Okay, okay. But if we were to see like a larger gap, like let's say like a 5% increase or a 10% increase, then would we go through like the", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4592161_ms_-_4665807_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4592161, "end_ms": 4665807}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4650861 ms - 4723743 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4650861 ms - 4723743 ms\n\nContent: we need to do more tests to figure out the significance. Okay, okay. But if we were to see like a larger gap, like let's say like a 5% increase or a 10% increase, then would we go through like the statistical significance to see if it's actually better? That's a good point. I mean, if you look at all HCI papers, they all require you to do P tests and so forth to talk about significance. But it's kind of not so meaningful to ask with numbers like this. Okay. And the point here is that they have a fundamental problem is that the data are retrieved in just one hop. You can combine them if it turns out to be something combinable, but you just cannot solve the cascading multi hop problems where the result with the data to retrieve depends on the first hop answer. And of course there is first hop, second hop, third hop and so forth. They also, they are not the analysis the lexical analysis is kind of weak. It does not really understand the full semantics of arbitrary compositions. And they,", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4650861_ms_-_4723743_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4650861, "end_ms": 4723743}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4708129 ms - 4785631 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4708129 ms - 4785631 ms\n\nContent: second hop, third hop and so forth. They also, they are not the analysis the lexical analysis is kind of weak. It does not really understand the full semantics of arbitrary compositions. And they, you know, they can do one count, one compare, but sometimes you need both. So it is not complete in any way, but it actually kind of shows you, I think that data set is good. The data set is showing you and you know, something that requires this type of, you know, a better, better solution for hybrid data. And so I would say the conclusion here is that it is actually very natural to have questions that require composition of information retrieved from database and text. We talked about four methods. Trying to do just by just text or just structure is inadequate. Then there are different approaches to combine them. One is to flatten everything, turn everything into text. It sounds terrible, but a lot of people believe in that. And I just want to comment on the fact that there is value in this", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4708129_ms_-_4785631_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4708129, "end_ms": 4785631}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4767615 ms - 4854417 ms", "content": "Title: CS224V Lecture 11 > Transcript > 4767615 ms - 4854417 ms\n\nContent: to combine them. One is to flatten everything, turn everything into text. It sounds terrible, but a lot of people believe in that. And I just want to comment on the fact that there is value in this method for simple questions. Okay, if you have simple questions, this is, this is a pretty good technique. The worst combination here is that you try to do something really hard and it turns out that linearization works well already and you feel like you have not done anything. Look at your data set. The first thing is that you have to make sure the data set matches the thing that you want to test. And the data set that this linearization paper was tested on, actually they do not have a lot of hops. And so any numbers that you see, you have to understand what dataset it is coming from. And then the hybrid dataset is very useful. And we are going to be using that in the next lecture. And we use that to evaluate our methods. And this ad hoc thing where you combine them and not really fully", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4767615_ms_-_4854417_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4767615, "end_ms": 4854417}}
{"document_title": "CS224V Lecture 11", "section_title": "CS224V Lecture 11 > Transcript > 4839455 ms - 5305305 ms", "content": "the hybrid dataset is very useful. And we are going to be using that in the next lecture. And we use that to evaluate our methods. And this ad hoc thing where you combine them and not really fully taking compositionality and completeness seriously will just will not give you the robust answer that you want. And that's the motivation of why we do the SQL where we combine the LLM and information retrieval with relational algebra operators together. And that's the lecture for Wednesday. Okay. All right, I'll see you on Wednesday. It. It.", "block_metadata": {"id": "CS224V_Lecture_11_>_Transcript_>_4839455_ms_-_5305305_ms", "document_type": "transcript", "lecture_number": 11, "start_ms": 4839455, "end_ms": 5305305}}
